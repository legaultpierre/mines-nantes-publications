exact observability and controllability for linear neutral type systems. the problem of exact observability is analyzed for a wide class of neutral type systems by an infinite dimensional approach. the duality with the exact controllabil-ity problem is the main tool. it is based on an explicit expression of a neutral type system which corresponding to the abstract adjoint system. a nontrivial relation is obtained between the initial neutral system and the system obtained via the adjoint abstract state operator. the characterization of the duality between controllability and observability is deduced, and then observability conditions are obtained.
mean cross section prediction in pwr-mox using neural network. fuel depletion calculation codes require one-group mean cross sections of manynuclides to solve bateman's equations. in such codes, the mean cross sections are assed by themean of iterative calls of boltzmann equation solver thanks to neutron transport codes. this is atime consuming task, especially with monte carlo codes such as mcnp. this paper presents amethodology based on neural network for building a cross section predictor for a pwr reactorloaded with any mox fuel. this approach allows performing fuel depletion calculation in lessthan one minute with an excellent accuracy. a maximum deviation of 3% on actinides is obtainedat the end of cycle between inventories calculated from neural networks and from the referencecoupled neutron transport / fuel depletion calculation.
mgo effect on an ads neutronic parameters. accelerator driven systems (ads) are specifically studied for their capacity intransmuting minor actinides (ma). electronuclear scenarios involving ma transmutation in adsare widely researched. the dynamic fuel cycle simulation code class (core library foradvanced scenarios simulations) is used for predicting the inventory evolution induced by acomplex nuclear fleet. for managing reactors, the code class is based on physic models. a fuelloading model (flm) provides the fuel composition at beginning of cycle (boc) according tothe storages composition and the reactor requirements. a cross section predictor (csp) estimatesmean cross sections needed for solving evolution equations. physic models are built from reactorscalculation set ahead of the scenario calculation. an ads standard composition at boc is amixture of plutonium and ma oxide. the high number of fissile isotopes present in the sub-criticalcore leads to an issue for building an ads flm. a high number of isotopic vectors at boc isneeded to get an exhaustive simulation set. also, ads initial reactivity is adjusted with an inertmatrix that induces an additional degree of freedom. the building of an ads flm for classrequires two steps. for any heavy nuclide composition at beginning of cycle, the core reactivitymust be imposed at a sub-critical level. also, the reactivity coefficient evolution should bemaintained during the irradiation. for building a flm, a simulation set has been built. reactorsimulations are done with the transport code mcnp6 (monte carlo n particle transport code).the ads geometry is based on the efit (european facility for industrial-scale transmutation)concept. the simulation set is composed of more than 8000 randomized. a complete neutronicstudy is presented that highlight the effect on mgo on neutronic parameters.
the discovery initiative - overcoming major limitations of traditional server-centric clouds by operating massively distributed iaas facilities. instead of the current trend consisting of building larger and  larger data centers (dcs) in few strategic locations, the discovery initiative proposes to leverage any network point of presences (pop, i.e., a  small or medium-sized network center) available through the internet. the key idea is to demonstrate a widely distributed cloud platform that can better match the geographical dispersal of users. this involves radical changes in the way resources are managed, but leveraging computing resources around the end-users will enable to  deliver a new generation of highly efficient and sustainable utility computing (uc) platforms, thus providing a strong alternative to the actual cloud model based on mega dcs (i.e. dcs composed of tens of thousands resources).critical to the emergence of such distributed cloud platforms is the availability of appropriate operating mechanisms. although, some of protagonists of cloud federations would argue that it might be possible to federate a significant number of micro-clouds hosted on each pop, we emphasize that federated approaches aim at delivering a brokering service in charge of interacting with several cloud management systems, each of them being already deployed and operated independently by at least one administrator. in other words, current federated approaches do not target to operate, remotely, a significant amount of uc resources geographically distributed but  only to use them. the main objective of discovery is to design,  implement, demonstrate and promote a unified system in charge of turning a complex, extremely large-scale and widely distributed infrastructure into a collection of abstracted computing resources which is efficient, reliable, secure and friendly to operate and  use.  after presenting the discovery vision, we explain the different  choices we made, in particular the choice of revising the openstack solution leveraging p2p mechanisms. we believe that such a strategy is promising considering the architecture complexity of such systems and the velocity of open-source initiatives.
centrality dependence of the charged-particle multiplicity density at mid-rapidity in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev. the pseudorapidity density of charged particles ($\mathrm{d}n_\mathrm{ch}/\mathrm{d}\eta$) at mid-rapidity in pb-pb collisions has been measured at a center-of-mass energy per nucleon pair of $\sqrt{s_{\rm nn}}$ = 5.02 tev. it increases with centrality and reaches a value of $1943 \pm 54$ in $|\eta|&lt;0.5$ for the 5% most central collisions. a rise in $\mathrm{d}n_\mathrm{ch}/\mathrm{d}\eta$ as a function of $\sqrt{s_{\rm nn}}$ for the most central collisions is observed, steeper than that observed in proton-proton collisions and following the trend established by measurements at lower energy. the centrality dependence of $\mathrm{d}n_\mathrm{ch}/\mathrm{d}\eta$ as a function of the average number of participant nucleons, ${\langle n_\mathrm{part} \rangle}$, calculated in a glauber model, is compared with the previous measurement at lower energy. a constant factor of about 1.2 describes the increase in $\frac{2}{\langle n_\mathrm{part} \rangle}\langle \mathrm{d}n_\mathrm{ch}/\mathrm{d}\eta \rangle$ from $\sqrt{s_{\rm nn}}$ = 2.76 tev to $\sqrt{s_{\rm nn}}$ = 5.02 tev for all centrality intervals, within the measured range of 0-80% centrality. the results are also compared to models based on different mechanisms for particle production in nuclear collisions.
charge-dependent flow and the search for the chiral magnetic wave in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. we report on measurements of a charge-dependent flow using a novel three-particle correlator with alice in pb-pb collisions at the lhc, and discuss the implications for observation of local parity violation and the chiral magnetic wave (cmw) in heavy-ion collisions. charge-dependent flow is reported for different collision centralities as a function of the event charge asymmetry. while our results are in qualitative agreement with expectations based on the cmw, the nonzero signal observed in higher harmonics correlations indicates a possible significant background contribution. we also present results on a differential correlator, where the flow of positive and negative charges is reported as a function of the mean charge of the particles and their pseudorapidity separation. we argue that this differential correlator is better suited to distinguish the differences in positive and negative charges expected due to the cmw and the background effects, such as local charge conservation coupled with strong radial and anisotropic flow.
impact of reaction cross section on the unified description of fusion excitation function. a systematics of over 300 complete and incomplete fusion cross section data points covering energies beyond the barrier for fusion is presented. owing to a usual reduction of the fusion cross sections by the total reaction cross sections and an original scaling of energy, a fusion excitation function common to all the data points is established. a universal description of the fusion exci-tation function relying on basic nuclear concepts is proposed and its dependence on the reaction cross section used for the cross section normalization is discussed. the pioneering empirical model proposed by bass in 1974 to describe the complete fusion cross sections is rather successful for the incomplete fusion too and provides cross section predictions in satisfactory agreement with the observed universality of the fusion excitation function. the sophisticated microscopic transport dywan model not only reproduces the data but also predicts that fusion reaction mechanism disappears due to weakened nuclear stopping power around the fermi energy.
putting into practice another idea of cultural democratization : the “panier culture” case. new social structurations emerge so as to respond to the stakes of democratization which cross the cultural sector. they intend to revive democratic values through the use of the internet. the case of an art short-chain-value created in 2011 allows us to question the role of the internet in the working out of activities created both by artists and a public. the internet is a media which operates all along a continuum of places: public, open, closed, interpersonal. the description of the way this “culture-basket” functions induces that difficulties having democratization at stake are located at the interfaces of these different places.
energy of molecular structures in 12c, 16o, 20ne, 24mg, and 32s. the energy of the 12 c, 16 o, 20 ne, 24 mg and 32 s 4n-nuclei has been determined within a generalized liquid drop model and assuming different planar and three-dimensional shapes of α-molecules : linear chain, triangle, square, tetrahedron, pentagon, trigonal bipyramid, square pyramid, hexagon, octahedron, octogon and cube. the potential barriers governing the entrance and decay channels via α absorption or emission as well as more symmetric binary and ternary reactions have been compared. the rms radii of the linear chains differ from the experimental rms radii of the ground states. the binding energies of the three-dimensional shapes at the contact point are higher than the ones of the planar configurations. the alpha particle plus a-4 daughter configuration leads always to the lowest potential barrier. the binding energy can be reproduced within the sum of the binding energy of n α particles plus the number of bonds multiplied by 2.4 mev or by the sum of the binding energies of one alpha particle and the daughter nucleus plus the coulomb energy and the proximity energy.
nanosecond-level time synchronization of autonomous radio detector stations for extensive air showers. to exploit the full potential of radio measurements of cosmic-ray air showers at mhz frequencies, a detector timing synchronization within 1 ns is needed. large distributed radio detector arrays such as the auger engineering radio array (aera) rely on timing via the global positioning system (gps) for the synchronization of individual detector station clocks. unfortunately, gps timing is expected to have an accuracy no better than about 5 ns. in practice, in particular in aera, the gps clocks exhibit drifts on the order of tens of ns. we developed a technique to correct for the gps drifts, and an independent method used for cross-checks that indeed we reach nanosecond-scale timing accuracy by this correction. first, we operate a "beacon transmitter" which emits defined sine waves detected by aera antennas recorded within the physics data. the relative phasing of these sine waves can be used to correct for gps clock drifts. in addition to this, we observe radio pulses emitted by commercial airplanes, the position of which we determine in real time from automatic dependent surveillance broadcasts intercepted with a software-defined radio. from the known source location and the measured arrival times of the pulses we determine relative timing offsets between radio detector stations. we demonstrate with a combined analysis that the two methods give a consistent timing calibration with an accuracy of 2 ns or better. consequently, the beacon method alone can be used in the future to continuously determine and correct for gps clock drifts in each individual event measured by aera.
assessing both solution diversity and solution quality in constraint programming. techniques for solving optimization problems naturally prioritize the value of an objective function, typically producing a single solution. while this is essential from the optimality point of view, there are scenarios where it may be advantageous to consider multiple solutions, as distinct as possible from one another. unfortunately, constraint-based techniques that can be used to generate diverse solutions are designed for satisfaction problems. they do not assess objective quality. in this work, we tackle this issue with a generic paradigm for assessing both solution diversity and solution quality, that can be implemented in most existing solvers. as there is no requirement for the initial solution to be optimal, there is therefore no theoretical restriction on solving large problems, using for instance large neighborhood search. we show that our technique can be specialized to produce diverse solutions of high quality in the context of over-constrained problems. furthermore, our paradigm allows us to consider diversity from a different point of view, based on generic concepts expressed by global constraints. our experiments yield encouraging computational results.
opportunistic scheduling in clouds partially powered by green energy. the fast growth of demand for computing and storage resources in data centers has considerably increased their energy consumption.  improving the utilization of data center resources and integrating renewable energy, such as solar and wind, has been proposed to reduce both the brown energy consumption and carbon footprint of the data centers. in this paper, we propose a novel framework opportunistic scheduling broker infrastructure (pika) to save energy in small mono-site data centers. in order to reduce the brown energy consumption, pika integrates resource overcommit techniques that help to minimize the number of powered-on physical machines (pms). on the other hand,   pika dynamically schedules the jobs and adjusts the number of powered-on pms to match the variable renewable energy supply. our simulations with a real-world job workload and solar power traces demonstrate that pika saves brown energy consumption by up to 44.9% compared to a typical scheduling algorithm.
effect of the dynamic topology on the performance of pso-2s algorithm for continuous optimization. pso-2s is a multi-swarm pso algorithm using charged particles in a partitioned search space for continuous optimization problems. in order to improve the performance of pso-2s, this paper proposes a novel variant of this algorithm, called dpso-2s, which uses the dcluster neighborhood topologies to organize the communication networks between the particles. experiments were conducted on a set of classical benchmark functions. the obtained results prove the effectiveness of the proposed algorithm.
integrated strategic and tactical optimization of animal-waste sourced biopower supply chains. —many models have been recently developed for the optimization of biomass related supply chains. however, models for biopower supply chains powered by animal waste have not received much attention yet. in this paper, we propose a mixed integer linear programming model for supplier selection and procurement planning for a biopower plant. the model integrates time window constraints for the collection of animal waste as well as inventory constraints. we show that the model is intractable with a state-of-the art commercial solver and propose a heuristic approach based on the adaptive large neighbourhood search (alns) framework. we show the efficiency of this approach on a case study in central france.
a facility location problem for the design of a collaborative distribution network. null
rationalization of the solvation effects on the ato+ ground-state change. 211at radionuclide is of considerable interest as a radiotherapeutic agent for targeted alpha therapy in nuclear medicine, but major obstacles remain because the basic chemistry of astatine (at) is not well understood. the ato+ cationic form might be currently used for 211at-labeling protocols in aqueous solution and has proved to readily react with inorganic/organic ligands. but ato+ reactivity must be hindered at first glance by spin restriction quantum rules: the ground state of the free cation has a dominant triplet character. investigating ato+ clustered with an increasing number of water molecules and using various flavors of relativistic quantum methods, we found that ato+ adopts in solution a kramers restricted closed-shell configuration resembling a scalar-relativistic singlet. the ground-state change was traced back to strong interactions, namely, attractive electrostatic interactions and charge transfer, with water molecules of the first solvation shell that lift up the degeneracy of the frontier π* molecular orbitals (mos). this peculiarity brings an alternative explanation to the highly variable reproducibility reported for some astatine reactions: depending on the production protocols (with distillation in gas-phase or “wet chemistry” extraction), 211at may or may not readily react.
risks governance and risky operations helping each other to face unexpected events: a few contributions of pragmatist philosophy. this paper seeks to highlight a few contributions of pragmatist philosophy to the understanding of risks governance. the capacity to handle unexpected events is a necessity for high risks organizations, as known by the academic community for about thirty years (rochlin, la porte, &amp; roberts, 1987). in this communication, we study a french regulated process called “safety demonstration” which force nuclear operators to demonstrate the reliability of their technical solutions before implementing them. this process is settled since 2006 and is embedded in the french regulation of nuclear risks governance. it is currently facing its difficulties in grabbing the management of unexpected events, which until now were managed through rules. dismantling operations brings new situations, where unexpected events are more common and more significant.
from ventriloquism to high reliability: object of activity and figures’ significance. this paper presents the results of a communicational study of high reliability organizations (hro). starting from a gap in the hro literature, we seek to deepen the understanding of communicational nature of interactions within hros. in particular, we contribute to the question related to the track: what forms of talk do we find in organizational practice, and how do they differ in shaping and constituting organizational phenomena in our study of reliability? we draw on the concept of ventriloquism (cooren, 2010) and examine its impact and importance on the production of high reliability. we base our work on two empirical case studies. the first concerns heavy handling activity in a naval defense industry, and the second concerns the care provided to demented patients in a short-term geriatric ward. we use actor network theory, or ant (latour, 2005), to build an original analysis framework of ventriloquism that qualifies the figures in terms of “actor”, “actant” or “object.” through a comparative approach, we show that ventriloquism can serve or disserve the high reliability of an organization. specifically, we demonstrate that the nature of the object of activity (engeström, 1987) is a crucial element for the relevance of ventriloquism. we describe the impact of ventriloquism on hros and build a preliminary typology of talk practices that foster it. we conclude by discussing the theoretical, methodological and practical contributions of our research.
measurement of $\theta_{13}$ in double chooz using neutron captures on hydrogen with novel background rejection techniques. the double chooz collaboration presents a measurement of the neutrino mixing angle $\theta_{13}$ using reactor $\overline{\nu}_{e}$ observed via the inverse beta decay reaction in which the neutron is captured on hydrogen. this measurement is based on 462.72 live days data, approximately twice as much data as in the previous such analysis, collected with a detector positioned at an average distance of 1050m from two reactor cores. several novel techniques have been developed to achieve significant reductions of the backgrounds and systematic uncertainties. accidental coincidences, the dominant background in this analysis, are suppressed by more than an order of magnitude with respect to our previous publication by a multi-variate analysis. these improvements demonstrate the capability of precise measurement of reactor $\overline{\nu}_{e}$ without gadolinium loading. spectral distortions from the $\overline{\nu}_{e}$ reactor flux predictions previously reported with the neutron capture on gadolinium events are confirmed in the independent data sample presented here. a value of $\sin^{2}2\theta_{13} = 0.095^{+0.038}_{-0.039}$(stat+syst) is obtained from a fit to the observed event rate as a function of the reactor power, a method insensitive to the energy spectrum shape. a simultaneous fit of the hydrogen capture events and of the gadolinium capture events yields a measurement of $\sin^{2}2\theta_{13} = 0.088\pm0.033$(stat+syst).
radiolytic corrosion of uranium dioxide induced by he2+ localized irradiation of water: role of the produced h2o2 distance. null
dynamic packing with side constraints for datacenter resource management. resource management in datacenters involves assigning virtual machines with changing resource demands to physical machines with changing capacities. recurrently, the changes invalidate the assignment and the resource manager recomputes it at runtime. the assignment is also subject to changing restrictions expressing a variety of user requirements. the present chapter surveys this application of vector packing—called the vm reassignment problem—with an insight into its dynamic and heterogeneous nature. we advocate flexibility to answer these issues and present btrplace, a flexible and scalable heuristic solution based on constraint programming.
anisotropic emission of thermal dielectrons from au+au collisions at $\sqrt{s_{nn}}=200$~gev with epos3. dileptons, as an electromagnetic probe, are crucial to study the properties of quark-gluon plasma (qgp) created in heavy ion collisions. we calculated the invariant mass spectra and the anisotropic emission of thermal dielectrons from au+au collisions at the relativistic heavy ion collider (rhic) energy $\sqrt{s_{nn}}=200$~gev based on epos3. this approach provides a realistic (3+1)-dimensional event-by-event viscous hydrodynamic description of the expanding hot and dense matter with a very particular initial condition, and a large set of hadron data and direct photons (besides $v_{2}$ and $v_{3}$ !) can be successfully reproduced. thermal dilepton emission from both qgp phase and the hadronic gas are considered, with the emission rates based on lattice qcd and a vector meson model, respectively. we find that the computed invariant mass spectra (thermal contribution + star cocktail) can reproduce the measured ones from star at different centralities. different from other model predictions, the obtained elliptic flow of thermal dileptons is larger than the star measurement referring to all dileptons. a clear centrality dependence of thermal dilepton occurs not only to elliptic flow $v_{2}$ but also to high orders for centrality in 0-60\%. at a given centrality, $v_{n}$ of thermal dileptons decreases neatly with $n$ for $2 \leq n \leq 5$.
online monitoring of the osiris reactor with the nucifer neutrino detector. the detection of electron antineutrinos emitted in the decay chains of the fission products in nuclear reactors, combined with reactor core simulations, provides an efficient tool to assess both the thermal power and the fissile content of the whole nuclear core. this new information could be used by the international agency for atomic energy (iaea) to enhance the safeguards of civil nuclear reactors. we report the first results of the nucifer experiment demonstrating the concept of "neutrinometry" at the pre-industrialized stage. a novel detector has been designed to meet requirements discussed with the iaea for the last ten years as well as international nuclear safety standards. nucifer has been deployed at only 7.2m away from the osiris research reactor core (70 mw) operating at the saclay research center of the french alternative energies and atomic energy commission (cea). we describe the performances of the 1 m3 detector remotely operating at a shallow depth equivalent to 12m of water and under intense background radiation conditions due to the very short baseline. we present the first physics results, based on 145 (106) days of data with reactor on (off), leading to the detection of 40760 electron antineutrino candidates. the mean number of detected antineutrinos is 281 (7) neutrino/day, to be compared with the prediction 272 (23) neutrino/day. as a first societal application we quantify, on the basis of our data, how antineutrinos could be used for the plutonium management and disposition agreement.
ultraviolet behavior of 6d supersymmetric yang-mills theories and harmonic superspace. we revisit the issue of higher-dimensional counterterms for the n=(1,1) supersymmetric yang-mills (sym) theory in six dimensions using the off-shell n=(1,0) and on-shell n=(1,1) harmonic superspace approaches. the second approach is developed in full generality and used to solve, for the first time, the n=(1,1) sym constraints in terms of n=(1,0) superfields. this provides a convenient tool to write explicit expressions for the candidate counterterms and other n=(1,1) invariants and may be conducive to proving non-renormalization theorems needed to explain the absence of certain logarithmic divergences in higher-loop contributions to scattering amplitudes in n=(1,1) sym.
transport theory from the nambu-jona-lasinio lagrangian. starting from the (polyakov-) nambu-jona-lasinio lagrangian, (p)njl, we formulate a transport theory which allows for describing the expansion of a quark-antiquark plasma and the subsequent transition to the hadronic world --without adding any new parameter to the standard (p)njl approach, whose parameters are fixed to vacuum physics. this transport theory can be used to describe ultrarelativistic heavy-ion reaction data as well as to study the (first-order) phase transition during the expansion of the plasma. (p)njl predicts such a phase transition for finite chemical potentials. in this contribution we give an outline of the necessary steps to obtain such a transport theory and present first results.
radiation chemical behavior of aqueous butanal oxime solutions irradiated with helium ion beams. null
oxidation and/or reduction of manganese species by γ-ray and he2+ particle irradiation in highly concentrated carbonate media. null
prediction of mgo volume fraction in an ads fresh fuel for the scenario code class. subcritical reactors, also called accelerator driven systems (ads), are specifically studied for their capacity in transmuting minor actinides (ma). nuclear fuel cycle scenarios involving ma transmutation in ads are widely researched. the nuclear fuel cycle simulation tool code class (core library for advanced scenarios simulations) is dedicated to the inventory evolution calculation induced by a complex nuclear fleet. for managing reactors, the code class includes physic models. loading models aim to provide the fuel composition at beginning of cycle according to the stocks isotopic composition and the reactors requirements. a cross section predictor aims to provide mean cross sections needed for solving bateman equations. physic models are built from reactors calculation set ahead of the scenario calculation. an ads standard composition at boc is a mixture of plutonium and ma oxide. the high number of fissile isotopes present in the subcritical core leads to an issue for building an ads fuel loading model. a high number of isotopic vector at boc is needed to get an exhaustive simulation set. also, ads initial reactivity is adjusted with an inert matrix which induces an additional degree of freedom. the building of an ads fuel loading model for class requires two steps. for any heavy nuclide composition at beginning of cycle, the core reactivity must be imposed at a subcritical level. also, the reactivity coefficient evolution should be maintained during the irradiation. in this work, the mgo volume fraction is adjusted to reach the first requirement. the methodology based on a set of reactor simulations and neural network utilization to predict the mgo volume fraction needed to reach a wanted keff for any initial composition is presented. also, a complete neutronic study is done that highlight the effect on mgo on neutronic parameters. reactor simulations are done with the transport code mcnp6 (monte carlo n particle transport code). the ads geometry is based on the efit (european facility for industrial-scale transmutation) concept. the simulation set is composed of more than 8000 randomized runs from which a neural network has been built. the resulting mgo prediction method allows reaching a keff at 0.96 and the distribution standard deviation is around 200 pcm.
study of alternative routes for the production of innovative radionuclides for medical applications. nuclear medicine is a specialty that uses radioactive nuclei for therapy or diagnosis of diseases such as different types of cancer. these radionuclides are coupled to carrier molecules to target sick cells. currently, only few isotopes are used in clinical practice. however, many others may be of medical interest due to their emitted radiation and/or their half-life that can be adapted to the carrier molecule transit time and to the pathology. the aim of this phd thesis is to study the production of innovative radionuclides for therapy and diagnosis applications in collaboration with the giparronax, which possesses a multi-particle high energy cyclotron. a fundamental physical parameter to access the production rate of a radionuclide is the production cross section. experimental data were measured for a selection of radionuclides: photon emitter (tc-99m) and positron emitter (sc-44g) for diagnosis, as well as electron emitters (re-186, tb-155 and sn-117m) and α particles emitters (th-226, ra-223 and bi-213) for therapeutic applications. these acquired data are obtained using alternative production routes compared to the commonly used. data related to the contaminants produced during the irradiations were also extracted. the experimental cross section values arecompared with theoretical model predictions. the large set of data obtained contributes to the theoretical physicist studies allowing to constrain their models to improve and/or validate them.
the role of spin-orbit coupling on the chemical bonding in at2 and ato+: analysis via effective bond orders. null
effective bond orders from spin-orbit configuration interaction approaches. null
approximation algorithms for scheduling with a limited number of communications. we consider the case of a uet tree and an unlimited number of processors. we give a 2-approximation algorithm for minimizing the total number of communications, when there are no communication delays. this algorithm allows us to design a 6-approximation algorithm for the makespan minimization problem when there are unit length communication delays and the processors are interconnected by a single bus. finally, we compare our 6-approximation algorithm with other algorithms by simulations. we show that its mean performance is regular (around 1.25) and we explain, for certain cases, the performance of these algorithms by considering some parameters of the problem.
single-machine scheduling with time window-dependent processing times. in the one-machine scheduling problems analysed in this paper, the processing time of a job depends on the time at which the job is started. more precisely, the horizon is divided into time windows and with each one a coefficient is associated that is used to determine the actual processing time of a job starting in it. two models are introduced, and one of them has direct connections with models considered in previous papers on scheduling problems with time-dependent processing times. various computational complexity results are presented for the makespan criterion, which show that the problem is np-hard, even with two time windows. solving procedures are also proposed for some special cases.
a heuristic approach to the water networks pumping scheduling issue. in order to improve the efficiency of drinking water networks, we develop a model predictive control method, whichoptimizes pumping scheduling by taking into account electricity tariffs and network constraints on a daily basis. weestimate a 10% discount in the energy bill, an amount which depends strongly on the characteristics of the networkunder study and the quality of the current strategy.
precision measurement of the mass difference between light nuclei and anti-nuclei. the measurement of the mass differences for systems bound by the strong force has reached a very high precision with protons and anti-protons. the extension of such measurement from (anti-)baryons to (anti-)nuclei allows one to probe any difference in the interactions between nucleons and anti-nucleons encoded in the (anti-)nuclei masses. this force is a remnant of the underlying strong interaction among quarks and gluons and can be described by effective theories, but cannot yet be directly derived from quantum chromodynamics. here we report a measurement of the difference between the ratios of the mass and charge of deuterons and anti-deuterons, and $^{3}{\rm he}$ and $^3\overline{\rm he}$ nuclei carried out with the alice (a large ion collider experiment) detector in pb-pb collisions at a centre-of-mass energy per nucleon pair of 2.76 tev. our direct measurement of the mass-over-charge differences confirm cpt invariance to an unprecedented precision in the sector of light nuclei. this fundamental symmetry of nature, which exchanges particles with anti-particles, implies that all physics laws are the same under the simultaneous reversal of charge(s) (charge conjugation c), reflection of spatial coordinates (parity transformation p) and time inversion (t).
from regulatory obligations to enforceable accountability policies in the cloud. the widespread adoption of the cloud model for service delivery triggered several data protection issues.as a matter of fact, the proper delivery of these services typically involves sharing of personal/businessdata between the different parties involved in the service provisioning. in order to increase cloudconsumer’s trust, there must be guarantees on the fair use of their data. accountability provides thenecessary assurance about the data governance practices to the different stakeholders involved in a cloudservice chain. in this context, we propose a framework for the representation of accountability policies.such policies offer to end-users a clear view of the privacy and accountability clauses asserted by theentities they interact with, as well as means to represent their preferences. our framework offers twoaccountability policy languages: (i) an abstract language called aal devoted for the representation ofpreferences/clauses in an human readable fashion, and (ii) a concrete one for the implementation ofenforceable policies.
abstract accountability language: translation, compliance and application. with the rise of the services-based economy andthe democratization of on-line services, more and more users(individual and/or business) use on-line applications in their dailylives. usually personal data transits between different actorsinvolved in a service’s delivery chain (e.g. application/storageservice providers) and thus might raise some privacy issues.accountability, which is the property of an entity of beingresponsible for its acts, can help mitigate data privacy anddata disclosures issues in such applications. in this paper, wepropose a translational semantics for our accountability languageand we present some expected properties. we introduce anatural criterion to achieve the accountability compliance oftwo clauses and few heuristics to speed up the resolution time.we demonstrate the feasibility of our verification process with arealistic health care use case and the tspass theorem prover.
checking accountability with a prover. today on-line services are the cornerstone of onlineapplications such as e-commerce, e-government and e-healthapplications. however, they raise several challenges about dataprivacy. accountability, which is the property of an entity of beingresponsible for its acts, meets some of these challenges and henceincreases user’s trustworthiness in on-line applications. in thiswork, we propose an approach to assist the design of accountableapplications. in particular, we consider an application’s abstractcomponent design and we introduce a logical approach allowingvarious static verifications. this approach offers effective meansto early check the design and the behavior of an applicationand its offered/required services. we motivate our work with arealistic use case coming from the a4cloud project and validateour proposal with experiments using a theorem prover.
un algorithme de recherche à voisinage large pour le problème de tournées de véhicules à deux échelons, routes multiples et contraintes de synchronisation. null
synchronisation entre tournées dans un problème de tournées de véhicules multi-échelons. null
comparative measurement of prompt fission γ-ray emission from fast-neutron-induced fission of u235 and u238. prompt fission γ-ray (pfg) spectra have been measured in a recent experiment with the novel directional fast-neutron source licorne at the alto facility of the ipn orsay. these first results from the facility involve the comparative measurement of prompt γ emission in fast-neutron-induced fission of u235 and u238. characteristics such as γ multiplicity and total and average radiation energy are determined in terms of ratios between the two systems. additionally, the average photon energies were determined and compared with recent data on thermal-neutron-induced fission of u235. pfg spectra are shown to be similar within the precision of the present measurement, suggesting that the extra incident energy does not significantly impact the energy released by prompt γ rays. the origins of some small differences, depending on either the incident energy or the target mass, are discussed. this study demonstrates the potential of the present approach, combining an innovative neutron source and new-generation detectors, for fundamental and applied research on fission in the near future.
adding storage simulation capacities to the simgrid toolkit: concepts, models, and api. for each kind of distributed computing infrastructures, i.e., clusters, grids, clouds, data centers, or supercomputers, storage is a essential component to cope with the tremendous increase in scientific data production and the ever-growing need for data analysis and preservation. understanding the performance of a storage subsystem or dimensioning it properly is an important concern for which simulation can help by allowing for fast, fully repeatable, and configurable experiments for arbitrary hypothetical scenarios. however, most simulation frameworks tailored for the study of distributed systems offer no or little abstractions or models of storage resources.in this paper, we detail the extension of simgrid, a versatile toolkit for the simulation of large-scale distributed computing systems, with storage simulation capacities. we first define the required abstractions and propose a new api to handle storage components and their contents in simgrid-based simulators. then we characterize the performance of the fundamental storage component that are disks and derive models of these resources. finally we list several concrete use cases of storage simulations in clusters, grids, clouds, and data centers for which the proposed extension would be beneficial.
measurement of an excess in the yield of j/$\psi$ at very low $p_{\rm t}$ in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. we report on the first measurement of an excess in the yield of j/$\psi$ at very low transverse momentum ($p_{\rm t}&lt; 0.3$ gev/$c$) in peripheral hadronic pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev, performed by alice at the cern lhc. remarkably, the measured nuclear modification factor ($r_{\rm aa}$) of j/$\psi$ in the rapidity range $2.5&lt;y&lt;4$ reaches about 7 (2) in the $p_{\rm t}$ range 0-0.3 gev/$c$ in the 70-90% (50-70%) centrality class. the j/$\psi$ production cross section associated with the observed excess is obtained under the hypothesis that coherent photoproduction of j/$\psi$ is the underlying physics mechanism. if confirmed, the observation of j/$\psi$ coherent photoproduction in pb-pb collisions at impact parameters smaller than twice the nuclear radius opens new theoretical and experimental challenges and opportunities. in particular, coherent photoproduction accompanying hadronic collisions may provide insight into the dynamics of photoproduction and nuclear reactions, as well as become a novel probe of the quark-gluon plasma.
pseudorapidity and transverse-momentum distributions of charged particles in proton-proton collisions at $\mathbf{\sqrt{\textit s}}$ = 13 tev. the pseudorapidity ($\eta$) and transverse-momentum ($p_{\rm t}$) distributions of charged particles produced in proton-proton collisions are measured at the centre-of-mass energy $\sqrt{s}$ = 13 tev. the pseudorapidity distribution in $|\eta|&lt;$ 1.8 is reported for inelastic events and for events with at least one charged particle in $|\eta|&lt;$ 1. the pseudorapidity density of charged particles produced in the pseudorapidity region $|\eta|&lt;$ 0.5 is 5.31 $\pm$ 0.18 and 6.46 $\pm$ 0.19 for the two event classes, respectively. the transverse-momentum distribution of charged particles is measured in the range 0.15 $&lt;$ $p_{\rm t}$ $&lt;$ 20 gev/c and $|\eta|&lt;$ 0.8 for events with at least one charged particle in $|\eta|&lt;$ 1. the correlation between transverse momentum and particle multiplicity is also investigated by studying the evolution of the spectra with event multiplicity. the results are compared with calculations from pythia and epos monte carlo generators.
accurate and real-time doppler frequency estimation with multiplicative noise for velocity measurements using optical feedback interferometry. this paper proposes to analyze a real-time signal processing algorithm for on-line velocity measurement of a moving target, with a laser sensor using optical feedback interferometry. this velocity is based on an accurate estimation of the doppler frequency from a spectral analysis of the laser diode output signal. this study takes into account both main causes of disrupting noises: additive noise and multiplicative noise caused by the speckle effect. a very simple expression is given for the doppler frequency estimation bias based on an autoregressive spectral analysis method intensive monte carlo simulations, which are compared with experimental results, show that relative bias and standard deviations of about 0.1% and 0.5% respectively can be obtained for a wide range of velocities.
asymmetric hydrogen bonding and orientational ordering of water at hydrophobic and hydrophilic surfaces. 
computational molecular modeling of the adsorption and transport in clay nanopores in the context of nuclear waste disposal applications. 
the corrosion behavior of carbon steel in sulfide aqueous media at 30 degrees c. in this paper, we studied the effect of sulfide ions on the corrosion behavior of carbon steel to simulate the geological disposal of high-level radioactive waste. in geological storage conditions, sulfidogenic environment was sustained by sulfate-reducing bacteria. corrosion tests were conducted in systems in a controlled atmosphere of 5% h-2/n-2. batch experiments were conducted at 30a degrees c for 1 month with steel coupons immersed in na2s solutions. the structural characterization of the corrosion products was investigated by scanning electron microscope/energy dispersive x-ray spectroscopy, confocal micro-raman spectrometry, and x-ray diffraction. in the absence of sulfide ion, a magnetite (fe3o4) corrosion product layer was formed on steel surface while in the presence of sulfide ions we observed the formation of a poorly crystallized irons sulfide at low-sulfide concentration (1 mg/l) and a solid adherent pyrrhotite layer at higher sulfide concentration (5-15 mg/l). the strong drop in steel corrosion rate with sulfide concentration was revealed and related to the formation of well-crystallized pyrrhotite.
investigating the extensive air shower properties: tackling the challenges of the next generation cosmic ray observatory with the codalema experiment. our knowledge on ultra-high energy cosmic rays and their underlying sources and acceleration mechanisms is steadily improving thanks to the large observatories nowadays in operation. however the need for a next generation instrument is emerging from their experimental limitations and the scientific questions currently out of reach within a reasonable time line. within this scope, the main features of the radio detection of extensive air showers are investigated and confronted to these challenging requirements. codalema is the last experiment currently running in europe dedicated to the cosmic ray detection using the observation of its induced radio electric field. the latest experimental upgrade and the synthesis of its operation features and the upcoming technical developments are presented. the main results of codalema will be presented with special emphasis put on some of the new aspects of the data analysis offered by the codalema3 autonomous station array. finally, the opportunities provided by the nancay observatory for efficient r&amp;d activities and especially the upcoming technical developments are listed. (c) 2013 elsevier b.v. all rights reserved.
radio detection of extensive air showers at the pierre auger observatory. deployed at the end of 2010 at the pierre auger observatory, the first stage of the auger engineering radio array, aera24, consists of 24 radio stations covering an area of 0.5 km(2). aera measures the radio emission from cosmic-ray induced air showers. the amplitude of this radio emission is used to constrain the characteristics of the primary particle: arrival direction, energy and nature. these studies are possible thanks to an instrumentation development allowing self-triggered and externally triggered measurements in the mhz domain and an improved understanding of radio emission processes. in may 2013, 100 new stations were installed to cover an area of similar or equal to 6 km(2), for a total of 124 stations. this stage 2 will provide higher statistics and will enhance both the estimate of the nature of the primary cosmic ray and the energy resolution above 10(17) ev as an addition to detectors such as the auger fluorescence telescopes and particle detectors. we will present the main results obtained with the stage 1 of aera and the current status of the experiment. we will end with a brief overview of the ghz-experiments installed at the pierre auger observatory. (c) 2013 elsevier b.v. all rights reserved.
use of experimental designs to establish a kinetic law for a gas phase photocatalytic process. the photocatalytic degradation of three common indoor vocs - acetone, toluene and heptane - is investigated in a dynamic photocatalytic oxidation loop using box-behnken designs of experiments. thanks to the experimental results and the establishment of a kinetic rate law based on a simplified mechanism, a predictive model for the voc degradation involving independent factors is developed. the parameters under investigation are initial concentration, light intensity and air velocity through the photocatalytic medium. the obtained model fits properly the experimental curves in the range of concentration, light intensity and air flow studied.
strontium-82 and future germanium-68 production at the arronax facility. the arronax cyclotron is fully operational since the end of 2010. it delivers projectiles (p, d, alpha) at high energy (up to 70 mev for protons) and high intensity(2*375 mu a for protons). the main fields of application of arronax are radionuclide production for nuclear medicine and irradiation of inert or living materials for radiolysis and radio-biology studies. a large part of the beam time will be used to produce radionuclides for targeted radionuclide therapy (copper-67, scandium-47 and astatine-211) as well as for pet imaging (scandium-44, copper-64, strontium-82 for rubidium-82 generators, and germanium-68 for gallium-68 generators). since june 2012, large scale production of sr-82 has started with rubidium chloride (rbcl) targets. several improvements are being explored which consist of changing the target material from rbcl to rb metal and introducing an additional target behind the rubidium assembly. thus, a target alloy of nickel/gallium for germanium-68 production has been developed. it is obtained by electroplating and exhibits a better thermal behavior than the natural gallium target used in most production facilities.
experimental investigation on the combustion, performance and pollutant emissions of biodiesel from animal fat residues on a direct injection diesel engine. fat trap grease is a cheap source for biodiesel production, but until now its transformation into biofuel is not well studied. the effects of the use of biodiesel from degraded raw material on combustion in engines are not discussed by researchers. in this paper, engine tests were performed on biodiesel produced from afr (animal fat residues) collected from fat traps. this fuel presents differences in chemical composition and on physical properties as compared to standard biodiesel produced from vegetable oils. tests were performed on a single cylinder, air cooled, direct injection diesel engine at 1500 rpm, performance and pollutant emissions were measured and the combustion parameters were analyzed. the use of afrbd in the engine resulted in similar performance with slight brake thermal efficiency decrease at low loads and a slight increase at high loads. a decrease of 9% of power output at 1500 rpm was also detected. as per pollutant emissions, a drastic reduction of unburned hydrocarbons between 32% and 45% was obtained with afrbd (animal fat residue biodiesel). particulate matter emissions were also reduced at low and medium load ranges, but no differences were detected at high loads. nitric oxides emissions were also measured and slight increase was detected at low loads and a slight reduction was noticed at full engine load. (c) 2014 elsevier ltd. all rights reserved.
aqueous alteration of vhtr fuels particles under simulated geological conditions. very high temperature reactor (vhtr) fuels consist of the bistructural-isotropic (biso) or tristructural-isotropic (triso)-coated particles embedded in a graphite matrix. management of the spent fuel generated during vhtr operation would most likely be through deep geological disposal. in this framework we investigated the alteration of biso (with pyrolytic carbon) and triso (with sic) particles under geological conditions simulated by temperatures of 50 and 90 c-circle and in the presence of synthetic groundwater. solid state (scanning electron microscopy (sem), micro-raman spectroscopy, electron probe microanalyses (epma) and x-ray photoelectron spectroscopy (xps)) and solution analyses (icp-ms, ionique chromatography (ic)) showed oxidation of both pyrolytic carbon and sic at 90 c-circle. under air this led to the formation of sio2 and a clay-like mg silicate, while under reducing conditions (h-2/n-2 atmosphere) sic and pyrolytic carbon were highly stable after a few months of alteration. at 50 c-circle, in the presence and absence of air, the alteration of the coatings was minor. in conclusion, due to their high stability in reducing conditions, htr fuel disposal in reducing deep geological environments may constitute a viable solution for their long-term management. (c) 2014 elsevier b.v. all rights reserved.
assignment of raman-active vibrational modes of tetragonal mackinawite: raman investigations and ab initio calculations. the mackinawite mineral was prepared as a carbon steel corrosion product in sulfidogenic waters at 90 degrees c after 2 months. the tetragonal crystal structure of the material was confirmed by rietveld refinement of x-ray diffraction (xrd) data, and vibrational modes were analysed by micro-raman spectroscopy. despite a large number of studies on the formation and the stability of tetragonal mackinawite, the interpretation of the raman spectra remains uncertain. in the present study, we report on the first calculation of the raman-active vibrational modes of mackinawite using density functional perturbation theory and direct methods with blyp + dispersion correction. based on the comparison between calculated and experimental results, the four fundamental vibrational modes were assigned as 228 cm(-1) (b-1g), 246 cm(-1) (e-g), 373 cm(-1) (a(1g)) and 402 cm(-1) (e-g).
biodiesel production unit from lab scale to industrial pilot plant: material and energy balance. the modern society is, nowadays, facing two major problems: the energy sources depletion and the degradation of the ecologic system because of wastes rejection. the energetic valorization of wastes contributes on the resolution of both problems. in the present work, a feasibility study of an industrial pilot scale installation for the production of biodiesel from waste grease traps is lead. the installation is meant to transform 1000 tons of fat trap grease per year to biodiesel by transesterification. the daily production of the unit reaches 3200 1 of biodiesel. all necessary equipments were sized following process engineering design and based on lab scale optimization experiments. installation energy balance was also realized and it showed that the energy required for the installation functioning does not exceed 3.5% of the heating value of produced biodiesel.
turbulence-combustion interaction in direct injection diesel engine. the experimental measures of chemical species and turbulence intensity during the closed part of the engine combustion cycle are today unattainable exactly. this paper deals with numerical investigations of an experimental direct injection diesel engine and a commercial turbocharged heavy duty direct injection one. simulations are carried out with the kiva3v2 code using the re-normalized group (k-epsilon) model. a reduced mechanism for n-heptane was adopted for predicting auto-ignition and combustion processes. from the calibrated code based on experimental in-cylinder pressures, the study focuses on the turbulence parameters and combustion species evolution in the attempt to improve understanding of turbulence-chemistry interaction during the engine cycle. the turbulent kinetic energy and its dissipation rate are taken as representative parameters of turbulence. the results indicate that chemistry reactions of fuel oxidation during the auto-ignition delay improve the turbulence levels. the peak position of turbulent kinetic energy coincides systematically with the auto-ignition timing. this position seems to be governed by the viscous effects generated by the high pressure level reached at the auto-ignition timing. the hot regime flame decreases rapidly the turbulence intensity successively by the viscous effects during the fast premixed combustion and heat transfer during other periods. it is showed that instable species such as co are due to deficiency of local mixture preparation during the strong decrease of turbulence energy. also, an attempt to build an innovative relationship between self-ignition and maximum turbulence level is proposed. this work justifies the suggestion to determine otherwise the self-ignition timing.
algebraic technique for the stiffness model reduction in elastostatic calibration of robotic manipulators. the paper deals with elastostatic calibration of a serial industrial robot. in contrast to other works, all compliance sources associated with both links and joints elasticity are taken into account. particular attention is paid to the model parameters identification using end-point measurements only. for such experimental setup, the model is transformed into the form suitable for calibration with the sufficient rank of the corresponding observation matrix. the main contributions are in developing algebraic, physical and heuristic techniques that allow user to obtain complete model with minimal number of parameters. the advantages of the developed approach are confirmed by an experimental study that deals with identification of the elastostatic model parameters for a 6 dof serial industrial robot.
pascs 2014: privacy and accountability for software and cloud services. 
upgrade of corrosiveness nature of fish waste bio-oil using a hybrid catalyst (mgo/na(2)co(3)) optimization process. in this work, catalytic cracking of waste fish oil (wfo) to bio-fuel for diesel engine was studied over hybrid catalysts (sodium carbonate na2co3/magnesium oxide). the experiments were conducted using a fix-bed reactor. the effect of catalyst-to-wfo ratio and the amount of each catalyst were studied over the yields of bio-oil and acid value av, of the bio-oil following central composite design (ccd). the statistical analysis showed that catalyst significantly affected the bio-oil yield and acid value. a higher bio-oil yield over 70 wt% with a lower acid value (2.7 mg(koh)/g(oil)) were identified at catalyst-to-wfo of 1:7 by using the same amount of sodium carbonate and magnesium oxide. the optimum bio-oil was analyzed and properties have been investigated and compared to diesel fuel physical properties.
experimental investigation on the effects of raw materials degradation on performance, combustion and emissions of a single cylinder engine running on biodiesel from waste lipids. in this paper, a single cylinder air cooled for strokes direct injection diesel engine was used to compare biodiesel from fat trap grease (afrbd) with biodiesel from waste cooking oil (wcobd) and with diesel fuel. the main difference between both biodiesel samples resides in the presence of short chain and branched methyl esters on afrbd, and in its lower non saturated fatty acids content. comparison was based on engine performance, combustion parameters and emissions. afrbd resulted on a slight drop of brake power at 1500 rpm but it increased engine efficiency at full load. biodiesel reduced polluting emissions of the engine as compared to diesel fuel. wcobd recorded higher reduction of unburned hydrocarbon, carbon monoxide and particulate matter emissions but it increased the nitric oxides emissions. afrbd has the advantage of reducing all pollutant emissions, including nitric oxides.
experimental study on geometric and elastostatic calibration of industrial robot for milling application. the paper is devoted to geometric and elastostatic calibration of industrial robot for milling application. particular attention is paid to the analysis of the experimental results and enhancement identification routines. in contrast to other works, the identification results have been validated using separate set of measurements that were not used in calibration. the obtained geometric and elastostatic models essentially improve robot positioning accuracy in milling applications.
are icts needed for innovative firms to succeed? a survey of french smes. firms' performance can be explained by many factors, including their innovativeness. investment in icts is also seen as a source of competitiveness. our research tests these two sources of performance and examines possible synergies with ict supporting innovation. based on the few existing academic studies raising the issue of synergies between icts and innovation, three hypotheses are formulated: the positive influences of (1) innovativeness and of (2) ict resources on firm performance and (3) the influence of icts on innovation performance. the model is complemented with two control variables: market dynamism and firms' sectors. an empirical survey is conducted among 1.992 small-and-medium enterprises (smes) in france, complemented with an investigation of their financial performance. with a final sample of 1.088 firms, we test the direct effects of innovativeness and ict resources (software diffusion and level of ict skills) and the combined effect of innovativeness and dedicated icts. dedicated icts variable captures how innovation depends on specific investments in icts or more intensive use of existing icts in the firm. this variable constitutes the major conceptual originality of our research. our econometric results show that innovativeness has a positive effect on performance only if it is accompanied with dedicated icts. on the other side, econometric regression emphasizes an unexpected direct negative effect of innovativeness and of the level of ict skills on smes' financial performance. we discuss these results taking place in the specific organizational context of smes: innovative smes might not always be the most financially efficient firms; the return on investment of icts has also to be questioned. however our results confirm that synergies between innovativeness and icts are a factor of smes' performance.
distributed building temperature control with power constraints. generally, heating, cooling and air conditioning (hvac) systems are designed to handle worst case loads, and this over-design of hvac equipment is one of the main reasons for building energy inefficiency. when the hvac system is not over-designed, there exists a trade-off between the comfort of the building's occupants and the available heating/cooling power at critical load hours. we propose a distributed approach that maximizes the comfort of the building's occupants under several power constraints. we prove by means of graph theoretical tools and passivity analysis, that the proposed controller asymptotically reaches an optimal equilibrium without the need of full information. finally, some simulations and comparisons are presented to illustrate the performance of our method.
computational morphology for a soft micro air vehicle in hovering flight. bio-inspired by moth or humming-birds, several micro air vehicles (mav) have recently been developed. in these systems, the motion of the lifting surface is complex and modeled by flapping motion-controlled soft wings. the synchronization of the actuated periodic flapping motion of the wings and that of passive twisting degrees of freedom (dof) allows producing hovering flight. depending on the feature of the mav such as stiffness, and geometric characteristic, hovering flight can be naturally stable or not. in the first case hovering is obtained via an open loop control (or a local closed loop control) while in the second case a global control strategy is required to achieve the same objectives. thus, it is crucial to take into account the influence of the design of the mav on the control in order to choose an appropriate morphology that can reduce the burden of control. in this paper mathematical tools and methodologies are proposed to achieve this objective and reduce the computational cost of control.
ground-state reversal induced by solvation: electronic structure of ato&lt;sup&gt;+&lt;/sup&gt; in water. null
inclusive quarkonium production at forward rapidity in pp collisions at $\sqrt{s}=8$ tev. we report on the inclusive production cross sections of j/$\psi$, $\psi$(2s), $\upsilon$(1s), $\upsilon$(2s) and $\upsilon$(3s), measured at forward rapidity with the alice detector in pp collisions at a center-of-mass energy $\sqrt{s}=8$ tev. the analysis is based on data collected at the lhc and corresponds to an integrated luminosity of 1.28 pb$^{-1}$. quarkonia are reconstructed in the dimuon-decay channel. the differential production cross sections are measured as a function of the transverse momentum $p_{\rm t}$ and rapidity $y$, over the $p_{\rm t}$ ranges $0&lt;p_{\rm t}&lt;20$ gev/$c$ for j/$\psi$, $0&lt;p_{\rm t}&lt;12$ gev/$c$ for all other resonances, and for $2.5&lt;y&lt;4$. the cross sections, integrated over $p_{\rm t}$ and $y$, and assuming unpolarized quarkonia, are $\sigma_{{\rm j}/\psi} = 8.63\pm0.04\pm0.79$ $\mu$b, $\sigma_{\psi{\rm (2s)}} = 1.18\pm0.08\pm0.21$ $\mu$b, $\sigma_{\upsilon{\rm(1s)}} = 68\pm6\pm7$ nb, $\sigma_{\upsilon{\rm(2s)}} = 25\pm5\pm4$ nb and $\sigma_{\upsilon{\rm(3s)}} = 9\pm4\pm1$ nb, where the first uncertainty is statistical and the second one is systematic. these values agree, within at most $1.2\sigma$, with measurements performed by the lhcb collaboration in the same rapidity range.
charged-particle multiplicities in proton-proton collisions at $\sqrt{s}$ = 0.9 to 8 tev. a detailed study of pseudorapidity densities and multiplicity distributions of primary charged particles produced in proton-proton collisions, at $\sqrt{s} =$ 0.9, 2.36, 2.76, 7 and 8 tev, in the pseudorapidity range $|\eta|&lt;2$, was carried out using the alice detector. measurements were obtained for three event classes: inelastic, non-single diffractive and events with at least one charged particle in the pseudorapidity interval $|\eta|&lt;1$. the use of an improved track-counting algorithm combined with alice's measurements of diffractive processes allows a higher precision compared to our previous publications. a kno scaling study was performed in the pseudorapidity intervals $|\eta|&lt;$ 0.5, 1.0 and 1.5. the data are compared to other experimental results and to models as implemented in monte carlo event generators phojet and recent tunes of pythia6, pythia8 and epos.
measurement of electrons from heavy-flavour hadron decays in p-pb collisions at $\sqrt{s_{\rm nn}} = 5.02$ tev. the production of electrons from heavy-flavour hadron decays was measured as a function of transverse momentum ($p_{\rm t}$) in minimum-bias p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev with alice at the lhc. the measurement covers the $p_{\rm t}$ interval $0.5&lt;p_{\rm t}&lt;12$ gev/$c$ and the rapidity range $-1.06 &lt; y_{\rm cms} &lt; 0.14$ in the centre-of-mass reference frame. the contribution of electrons from background sources was subtracted using an invariant mass approach. the nuclear modification factor $r_{\rm ppb}$ was calculated by comparing the $p_{\rm t}$-differential invariant cross section in p-pb collisions to a pp reference at the same centre-of-mass energy, which was obtained by interpolating measurements at $\sqrt{s}= 2.76$ tev and $\sqrt{s} =7$ tev. the $r_{\rm ppb}$ is consistent with unity within uncertainties of about 25%, which become larger for $p_{\rm t}$ below 1 gev/$c$. the data are described by recent model calculations that include cold nuclear matter effects.
azimuthal anisotropy of charged jet production in $\sqrt{s_{\rm nn}}$ = 2.76 tev pb-pb collisions. we present measurements of the azimuthal dependence of charged jet production in central and semi-central $\sqrt{s_{\mathrm{nn}}}$ = 2.76 tev pb-pb collisions with respect to the second harmonic event plane, quantified as $v_{2}^{\mathrm{ch~jet}}$. jet finding is performed employing the anti-$k_{\mathrm{t}}$ algorithm with a resolution parameter $r$ = 0.2 using charged tracks from the alice tracking system. the contribution of the azimuthal anisotropy of the underlying event is taken into account event-by-event. the remaining (statistical) region-to-region fluctuations are removed on an ensemble basis by unfolding the jet spectra for different event plane orientations independently. significant non-zero $v_{2}^{\mathrm{ch~jet}}$ is observed in semi-central collisions (30-50\% centrality) for 20 $&lt;$ $p_{\mathrm{t}}^{\rm ch~jet}$ $&lt;$ 90 ${\mathrm{gev}\kern-0.05em/\kern-0.02em c}$. the azimuthal dependence of the charged jet production is similar to the dependence observed for jets comprising both charged and neutral fragments, and compatible with measurements of the $v_2$ of single charged particles at high $p_{\mathrm{t}}$. good agreement between the data and predictions from jewel, an event generator simulating parton shower evolution in the presence of a dense qcd medium, is found in semi-central collisions.
direct photon production in pb-pb collisions at $\sqrt{s_\rm{nn}}$ = 2.76 tev. direct photon production at mid-rapidity in pb-pb collisions at $\sqrt{s_{_{\mathrm{nn}}}} = 2.76$ tev was studied in the transverse momentum range $0.9 &lt; p_\mathrm{t} &lt; 14$ gev$/c$. photons were detected with the highly segmented electromagnetic calorimeter phos and via conversions in the alice detector material with the $e^+e^-$ pair reconstructed in the central tracking system. the results of the two methods were combined and direct photon spectra were measured for the 0-20%, 20-40%, and 40-80% centrality classes. for all three classes, agreement was found with perturbative qcd calculations for $p_\mathrm{t} \gtrsim 5$ gev$/c$. direct photon spectra down to $p_\mathrm{t} \approx 1$ gev$/c$ could be extracted for the 20-40% and 0-20% centrality classes. the significance of the direct photon signal for $0.9 &lt; p_\mathrm{t} &lt; 2.1$ gev$/c$ is $2.6\sigma$ for the 0-20% class. the spectrum in this $p_\mathrm{t}$ range and centrality class can be described by an exponential with an inverse slope parameter of $(297 \pm 12^\mathrm{stat}\pm 41^\mathrm{syst})$ mev. state-of-the-art models for photon production in heavy-ion collisions agree with the data within uncertainties.
centrality evolution of the charged-particle pseudorapidity density over a broad pseudorapidity range in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the centrality dependence of the charged-particle pseudorapidity density measured with alice in pb-pb collisions at $\sqrt{s_{\rm nn}}$ over a broad pseudorapidity range is presented. this letter extends the previous results reported by alice to more peripheral collisions. no strong change of the charged-particle pseudorapidity density distributions with centrality is observed, and when normalised to the number of participating nucleons in the collisions, the evolution over pseudorapidity with centrality is likewise small. the broad pseudorapidity range allows precise estimates of the total number of produced charged particles which we find to range from $162\pm22$ (syst.) to $17170\pm770$ (syst.) in 80-90% and 0-5 central collisions, respectively. the total charged-particle multiplicity is seen to approximately scale with the number of participating nucleons in the collision. this suggests that hard contributions to the charged-particle multiplicity are limited. the results are compared to models which describe $\mbox{d}n_{\mbox{ch}}/\mbox{d}\eta$ at mid-rapidity in the most central pb-pb collisions and it is found that these models do not capture all features of the distributions.
measurement of d$_s^+$ production and nuclear modification factor in pb-pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev. the production of prompt d$_s^+$ mesons was measured for the first time in collisions of heavy nuclei with the alice detector at the lhc. the analysis was performed on a data sample of pb-pb collisions at a centre-of-mass energy per nucleon pair, $\sqrt{s_{\rm nn}}$, of 2.76 tev in two different centrality classes, namely 0-10% and 20-50%. d$_s^+$ mesons and their antiparticles were reconstructed at mid-rapidity from their hadronic decay channel d$_s^+\rightarrow\phi\pi^+$, with $\phi\rightarrow$k$^-$k$^+$, in the transverse momentum intervals $4&lt; p_{\rm t}&lt;12$ gev/$c$ and $6&lt; p_{\rm t}&lt;12$ gev/$c$ for the 0-10% and 20-50% centrality classes, respectively. the nuclear modification factor $r_{\rm aa}$ was computed by comparing the $p_{\rm t}$-differential production yields in pb-pb collisions to those in proton-proton (pp) collisions at the same energy. this pp reference was obtained using the cross section measured at $\sqrt{s}= 7$ tev and scaled to $\sqrt{s}= 2.76$ tev. the $r_{\rm aa}$ of d$_s^+$ mesons was compared to that of non-strange d mesons in the 10% most central pb-pb collisions. at high $p_{\rm t}$ ($8&lt; p_{\rm t}&lt;12$ gev/$c$) a suppression of the d$_s^+$-meson yield by a factor of about three, compatible within uncertainties with that of non-strange d mesons, is observed. at lower $p_{\rm t}$ ($4&lt; p_{\rm t}&lt;8$ gev/$c$) the values of the d$_s^+$-meson $r_{\rm aa}$ are larger than those of non-strange d mesons, although compatible within uncertainties. the production ratios d$_s^+$/d$^0$ and d$_s^+$\d$^+$ were also measured in pb-pb collisions and compared to their values in proton-proton collisions.
multiplicity and transverse momentum evolution of charge-dependent correlations in pp, p-pb, and pb-pb collisions at the lhc. we report on two-particle charge-dependent correlations in pp, p-pb, and pb-pb collisions as a function of the pseudorapidity and azimuthal angle difference, $\mathrm{\delta}\eta$ and $\mathrm{\delta}\varphi$ respectively. these correlations are studied using the balance function that probes the charge creation time and the development of collectivity in the produced system. the dependence of the balance function on the event multiplicity as well as on the trigger and associated particle transverse momentum ($p_{\mathrm{t}}$) in pp, p-pb, and pb-pb collisions at $\sqrt{s_{\mathrm{nn}}} = 7$, 5.02, and 2.76 tev, respectively, are presented. in the low transverse momentum region, for $0.2 &lt; p_{\mathrm{t}} &lt; 2.0$ gev/$c$, the balance function becomes narrower in both $\mathrm{\delta}\eta$ and $\mathrm{\delta}\varphi$ directions in all three systems for events with higher multiplicity. the experimental findings favor models that either incorporate some collective behavior (e.g. ampt) or different mechanisms that lead to effects that resemble collective behavior (e.g. pythia8 with color reconnection). for higher values of transverse momenta the balance function becomes even narrower but exhibits no multiplicity dependence, indicating that the observed narrowing with increasing multiplicity at low $p_{\mathrm{t}}$ is a feature of bulk particle production. .
experimental investigation on ng dual fuel engine improvement by hydrogen enrichment. the crude oil graduate depletion, as well as aspects related to environmental pollution and global warming instigated many researches concerning alternative fuels. natural gas (ng) is one of the most attractive available fuels. a promising technique for its use in internal combustion engines is the dual fuel concept. one of the main problems with this technique is that, at low loads, the engine efficiency decreases compared to conventional diesel. the unburned hydrocarbons and carbon monoxide emissions are also higher in dual fuel mode. an effective method to compensate the demerits of limited lean-burn ability and slow burning velocity of ng is to mix it with a fuel that possesses wide flammability limit and fast burning velocity. hydrogen (h-2) is thought to be the best gaseous candidate for natural gas. in the present work, ng enrichment with various h-2 blends is investigated as a technique for improving dual fuel mode, especially at low loads. impact on engine performance and emissions is experimentally examined. total bsfc is considerably reduced. an important benefit in terms of bte, reaching to increase a 12% with the 10%h-2 blend compared to the pure ng case, is also achieved. thc and co emissions are in general reduced as a result of the improvement of gaseous fuel utilization. co2 emissions are also in general reduced. even though a slight increase is in overall observed for no emissions, its almost insignificant. copyright (c) 2014, hydrogen energy publications, llc. published by elsevier ltd. all rights reserved.
quarkonium suppression in heavy-ion collisions from coherent energy loss in cold nuclear matter. the effect of parton energy loss in cold nuclear matter on the suppression ofquarkonia (j/psi, upsilon) in heavy-ion collisions is investigated, byextrapolating a model based on coherent radiative energy loss recently shown todescribe successfully j/psi and upsilon suppression in proton-nucleuscollisions. model predictions in heavy-ion collisions at rhic (au-au, cu-cu,and cu-au) and lhc (pb-pb) show a sizable suppression arising from the soleeffect of energy loss in cold matter. this effect should thus be considered inorder to get a reliable baseline for cold nuclear matter effects in quarkoniumsuppression in heavy-ion collisions, in view of disentangling hot from coldnuclear effects.
chiral transport equation from the quantum dirac hamiltonian and the on-shell effective field theory. we derive the relativistic chiral transport equation for massless fermions and antifermions by performing a semiclassical foldy-wouthuysen diagonalization of the quantum dirac hamiltonian. the berry connection naturally emerges in the diagonalization process to modify the classical equations of motion of a fermion in an electromagnetic field. we also see that the fermion and antifermion dispersion relations are corrected at first order in the planck constant by the berry curvature, as previously derived by son and yamamoto for the particular case of vanishing temperature. our approach does not require knowledge of the state of the system, and thus it can also be applied at high temperature. we provide support for our result by an alternative computation using an effective field theory for fermions and antifermions: the on-shell effective field theory. in this formalism, the off-shell fermionic modes are integrated out to generate an effective lagrangian for the quasi-on-shell fermions/antifermions. the dispersion relation at leading order exactly matches the result from the semiclassical diagonalization. from the transport equation, we explicitly show how the axial and gauge anomalies are not modified at finite temperature and density despite the incorporation of the new dispersion relation into the distribution function.
experimental cross section evaluation for innovative mo-99 production via the (alpha,n) reaction on zr-96 target. the high-specific activity mo-99 accelerator-based production, via the (alpha,n) reaction on zr-96-enriched target, has been investigated in the present work. the excitation function measurement has been performed in the energy range 8-34 mev at the arronax facility, using the well-known stacked foils technique on natural zirconium as target. a general good agreement in the cross section trend has been observed, once compared to former measurements. a different (i.e. higher) peak value and a shift of about 2 mev towards larger energies have however been found. assuming a fully enriched zr-96 target irradiated by an alpha-beam at suitable energy (e = 25 mev), the mo-99 production yield has thus been estimated. at last the alternative production routes, based on the zr-96(alpha,n)mo-99 and mo-100(p,x)mo-99/tc-99m reactions, are compared.
carbon steel corrosion in clay-rich environment. we investigated the carbon steel corrosion in carbon dioxide clay-rich environment to understand its behavior under geological conditions. the results show the formation of magnetite as the main corrosion product in the first step of the corrosion process, followed by the formation of different corrosion products with complex mixtures of iron-oxide, hydroxycarbonate, hydroxychloride and sulfide phases. these results strongly contrast with similar experiments conducted under h-2 atmosphere where the major corrosion products consisted of iron sulfides. it appears then important to consider all the geochemical parameters including gas composition to better study corrosion of steel buried in geological formations. (c) 2014 elsevier ltd. all rights reserved.
adsorption study using optimised 3d organised mesoporous silica coated with fe and al oxides for specific as(iii) and as(v) removal from contaminated synthetic groundwater. this work presents the possibility of optimising 3d organised mesoporous silica (oms) coated with both iron and aluminium oxides for the optimal removal of as(iii) and as(v) from synthetic contaminated water. the materials developed were fully characterised and were tested for removing arsenic in batch experiments. the effect of total al to fe oxides coating on the selective removal of as(iii) and as(v) was studied. it was shown that 8% metal coating was the optimal configuration for the coated oms materials in.removing arsenic. the effect of arsenic initial concentration and ph, kinetics and diffusion mechanisms was studied, modelled and discussed. it was shown that the advantage of an organised material over an un-structured sorbent was very limited in terms of kinetic and diffusion under the experimental conditions. it was shown that physisorption was the main adsorption process involved in as removal by the coated oms. maximum adsorption capacity of 55 mg as(v) g(-1) was noticed at ph 5 for material coated with 8% al oxides while 35 mg as(v) g(-1) was removed at ph 4 for equivalent material coated with fe oxides. (c) 2014 elsevier inc. all rights reserved.
use of coffee mucilage as a new substrate for hydrogen production in anaerobic co-digestion with swine manure. coffee mucilage (cm), a novel substrate produced as waste from agricultural activity in colombia, the largest fourth coffee producer in the world, was used for hydrogen production. the study evaluated three ratios (c1-3) for co-digestion of cm and swine manure (sm), and an increase in organic load to improve hydrogen production (c4). the hydrogen production was improved by a c/n ratio of 53.4 used in c2 and c4. the average hydrogen production rate in c4 was 7.6 nl h-2/l(cm)d, which indicates a high hydrogen potential compare to substrates such as pome and wheat starch. in this condition, the biogas composition was 0.1%, 50.6% and 39.0% of methane, carbon dioxide and hydrogen, respectively. the butyric and acetic fermentation pathways were the main routes identified during hydrogen production which kept a bu/ac ratio at around 1.0. a direct relationship between coffee mucilage, biogas and cumulative hydrogen volume was established. (c) 2014 elsevier ltd. all rights reserved.
geological disposal of nuclear waste: ii. from laboratory data to the safety analysis - addressing societal concerns. after more than 30 years of international research and development, there is a broad technical consensus that geologic disposal of highly-radioactive waste will provide for the safety of humankind and the environment, now, and far into the future. safety analyses have demonstrated that the risk, as measured by exposure to radiation, will be of little consequence. still, there is not yet an operating geologic repository for highly-radioactive waste, and there remains substantial public concern about the long-term safety of geologic disposal. in these two linked papers, we argue for a stronger connection between the scientific data (paper i, grambow et al., 2014) and the safety analysis, particularly in the context of societal expectations (paper ii). in this paper (ii), we assess the meaning of the technical results and derived models (paper i) for the determination of the long-term safety of a repository. we consider issues of model validity and their credibility in the context of a much broader historical, epistemological and societal context. safety analysis is treated in its social and temporal dimensions. this perspective provides new insights into the societal dimension of scenarios and risk analysis. surprisingly, there is certainly no direct link between increased scientific understanding and a public position for or against different strategies of nuclear waste disposal. this is not due to the public being poorly informed, but rather due to cultural cognition of expertise and historical and cultural perception of hazards to regions selected to host a geologic repository. the societal and cultural dimension does not diminish the role of science, as scientific results become even more important in distinguishing between the conflicting views of the risk of geologic disposal of nuclear waste. (c) 2014 elsevier ltd. all rights reserved.
state of fukushima nuclear fuel debris tracked by cs137 in cooling water. it is still difficult to assess the risk originating from the radioactivity inventory remaining in the damaged fukushima nuclear reactors. here we show that cooling water analyses provide a means to assess source terms for potential future releases. until now already about 34% of the inventories of cs-137 of three reactors has been released into water. we found that the release rate of cs-137 has been constant for 2 years at about 1.8% of the inventory per year indicating ongoing dissolution of the fuel debris. compared to laboratory studies on spent nuclear fuel behavior in water, cs-137 release rates are on the higher end, caused by the strong radiation field and oxidant production by water radiolysis and by impacts of accessible grain boundaries. it is concluded that radionuclide analyses in cooling water allow tracking of the conditions of the damaged fuel and the associated risks.
antistrange meson-baryon interaction in hot and dense nuclear matter. we present a study of in-medium cross sections and (off-shell) transition rates for the most relevant binary reactions for strange pseudoscalar meson production close to threshold in heavy-ion collisions at energies available at the facility for antiproton and ion research. our results rely on a chiral unitary approach in coupled channels which incorporates the s and p waves of the kaon-nucleon interaction. the formalism, which is modified in the hot and dense medium to account for pauli blocking effects, mean-field binding on baryons, and pion and kaon self-energies, has been improved to implement unitarization and self-consistency for both the s- and the p-wave interactions at finite temperature and density. this gives access to in-medium amplitudes in several elastic and inelastic coupled channels with strangeness content s = -1. the obtained total cross sections mostly reflect the fate of the lambda (1405) resonance, which melts in the nuclear environment, whereas the off-shell transition probabilities are also sensitive to the in-medium properties of the hyperons excited in the p-wave amplitudes [lambda, sigma, and sigma* (1385)]. the single-particle potentials of these hyperons at finite momentum, density, and temperature are also discussed in connection with the pertinent scattering amplitudes. our results are the basis for future implementations in microscopic transport approaches accounting for off-shell dynamics of strangeness production in nucleus-nucleus collisions.
d-meson propagation in hadronic matter and consequences for heavy-flavor observables in ultrarelativistic heavy-ion collisions. we employ recently published cross sections for d mesons with hadrons and calculate the drag and diffusion coefficients of d mesons in hadronic matter as a function of the momentum of d mesons as well as of the temperature of the medium. calculating in our approach the spatial diffusion coefficient, d-x, at zero chemical potential we see a very smooth transition between our calculations for the hadron gas and the lattice qcd calculations. applying the results for the transport coefficients of d mesons in a fokker-planck equation, which describes the evolution of d mesons during the expansion of a hadron gas created in ultrarelativistic heavy-ion collisions, we find that the value of r-aa is little influenced by hadronic rescattering, whereas in the elliptic flow the effects are stronger. we extend our calculations to the finite chemical potentials and calculate the spatial diffusion coefficients of d mesons propagating through the hadronic medium following isentropic trajectories, appropriate at future fair (facility for antiproton and ion research, darmstadt) and nica (nuclotron-based ion collider facility, dubna) heavy-ion experiments. for the isentropic trajectory with s/rho(net)(b) = 20 we find a perfect matching of results for d mesons in hadronic matter and for charm quarks in partonic matter treated within the dynamical quasiparticle model approach.
inverse determination of convective heat transfer between an impinging jet and a continuously moving flat surface. in this study an inverse method is developed to determine the heat flux distribution on a moving plane wall. the method uses a thin layer of material (the measurement medium) glued on the conveyor belt. the heat flux distribution on the moving wall is then determined by an inverse method based on the temperature measurement by infrared thermography on the upper surface of the measurement medium. a finite element based inverse algorithm of a steady state heat conduction advection in the eulerian frame is performed. the algorithm entails the use of the tikhonov regularization method, along with the l-curve method to select an optimal regularization parameter. both the direct solution of moving boundary problem and the inverse design formulation are presented. the accuracy of the inverse method is examined by simulating the exact and noisy data with four different values of the surface-to-jet velocity ratio, and two different materials (pvc and aluminum) for the measurement medium. the results show a greater sensitivity to the convective heat flux allowing a better estimation of heat flux distribution for the pvc layer. an alternative underdetermined inverse scheme is also studied. this configuration allows a different extend between the retrieval heat flux surface and the measurement temperature surface. (c) 2014 elsevier inc. all rights reserved.
phosphorus adsorption onto an industrial acidified laterite by-product: equilibrium and thermodynamic investigation. the present research investigates the uptake of phosphate ions from aqueous solutions using acidified laterite (als), a by-product from the production of ferric aluminium sulfate using laterite. phosphate adsorption experiments were performed in batch systems to determine the amount of phosphate adsorbed as a function of solution ph, adsorbent dosage and thermodynamic parameters per fixed p concentration. kinetic studies were also carried out to study the effect of adsorbent particle sizes. the maximum removal capacity of als observed at ph5 was 3.68mg pg(-1). it was found that as the adsorbent dosage increases, the equilibrium ph decreases, so an adsorbent dosage of 1.0gl(-1) of als was selected. adsorption capacity (q(m)) calculated from the langmuir isotherm was found to be 2.73mgg(-1). kinetic experimental data were mathematically well described using the pseudo first-order model over the full range of the adsorbent particle size. the adsorption reactions were endothermic, and the process of adsorption was favoured at high temperature; the g and h values implied that the main adsorption mechanism of p onto als is physisorption. the desorption studies indicated the need to consider a naoh 0.1m solution as an optimal solution for practical regeneration applications. (c) 2014 curtin university of technology and john wiley &amp; sons, ltd.
approaches to surface complexation modeling of ni(ii) on callovo-oxfordian clayrock. callovo-oxfordian formation (cox) is as potential host formation for emplacement of long-term nuclear waste repositories in france. the objective of this work is to assess whether a simplified ``bottom-up'' approach may explain the retention of ni(ii) by the cox considering two levels of `upscaling': (i) from clay surfaces to rock clay fraction and (ii) from clay fraction to whole rock samples. to this end, ni(ii) adsorption was investigated by batch equilibrium, xps, and exafs techniques on a representative sample extracted at the location where the storage is supposed to be built (clay content of about 50%) and on the corresponding carbonate-free &lt;2 mu m fractions. the results showed that a simplified ``bottom-up'' approach based on published models available for illite and montmorillonite cannot explain ni(ii) adsorption on the &lt;2 pm fraction when the retention is controlled only by surface complexation on the reactive clay edge sites. an operational model based on the generalized composite modeling approach was used instead. the developed model considers an interaction between two ni(ii) species with one type of clay edge sites. the model developed for the clay fraction gives a satisfactory estimation of ni(ii) adsorption data for the representative clayrock sample. complementary experiments were performed by x-ray photoelectron (xps) and x-ray absorption (exafs) spectroscopies for both clay fraction and raw cox sample at high ni(ii) loadings. spectroscopic data were characterized by similar fitting parameters considering formation of ni phyllosilicate. this indicates that the clay fraction governs the retention of ni(ii), as it was concluded from the batch experiments. complementary adsorption experiments preformed with cox samples having clay contents representative of the variability occurring at the formation scale (i.e. 1.5-47% in weight) show that one cannot neglect the retention properties of the non-clay phases, mainly dominated by calcite, when the clay content becomes the minority. retention values in the range of 60-300 l/kg can finally be given for describing adsorption properties of trace concentrations of ni(ii) for the clay contents representative of the majority of the callovo-oxfordian formation. (c) 2014 elsevier b.v. all rights reserved.
j/psi production in p-pb collisions with alice at the lhc. the alice collaboration has measured the inclusive j/psi production in p-pb collisions at the nucleon-nucleon center-of-mass energy root s-nn = 5.02 tev. the comparison of the results on the nuclear modification factor as a function of rapidity and transverse momentum with theoretical predictions shows a fair agreement with a shadowing based model and energy loss models with or without a shadowing contribution. the j/psi production has also been measured by computing the j/psi nuclear modification factor as a function of the event activity, as well as the yield and mean transverse momentum dependence with the relative charged-particle multiplicity measured at mid-rapidity. (c) 2014 elsevier b.v. all rights reserved.
gluon radiation by heavy quarks at intermediate energies and consequences for the mass hierarchy of energy loss. we extend the gunion bertsch calculation of gluon radiation in single scattering to the case of finite mass quarks. this case applies to the radiative energy-loss of heavy quarks of intermediate energies propagating in a quark gluon plasma. we discuss more specifically the dead cone effect as well as the mass hierarchy of the collisional and radiative energy loss and provide some predictions for observables sensitive to the mass hierarchy of energy loss in ultrarelativistic heavy ion collisions.
heavy-flavor observables at rhic and lhc. we investigate the charm-quark propagation in the qgp media produced in ultrarelativistic heavy-ion collisions at rhic and the lhc. purely collisional and radiative processes lead to a significant suppression of final $d$-meson spectra at high transverse momentum and a finite flow of heavy quarks inside the fluid dynamical evolution of the light partons. the $d$-meson nuclear modification factor and the elliptic flow are studied at two collision energies. we further propose to measure the triangular flow of $d$ mesons, which we find to be nonzero in non-central collisions.
steel slag filters to upgrade phosphorus removal in small wastewater treatment plants: removal mechanisms and performance. electric arc furnace steel slag (eaf-slag) and basic oxygen furnace steel slag (bof-slag) were used as filter substrate in horizontal subsurface flow laboratory-scale filters designed to remove phosphorus (p) from a synthetic solution (similar to 10 mg p/l). the main objective of this study was to evaluate the influence of various parameters, including slag type, slag size, and slag composition, on p removal performance. also, a series of chemical and mineralogical analyses was performed to determine the mechanisms of p removal achieved by steel slag in the filters. over a period of 52 weeks of filter operation, small-size eaf-slag (5-16 mm) and small-size bof-slag (6-12 mm) removed 98% and &gt;99% of the inlet total phosphorus (tp), whereas big-size eaf-slag (20-40 mm) and big-size bof-slag (20-50 mm) removed 88% and 95% of the influent tp, respectively. the main mechanism of p removal was related to cao dissolution from slag followed by ca phosphate precipitation and accumulation of the precipitates into the filters. p removal performance improved with increasing the cao-slag content and with decreasing slag size, most probably because the specific surface available for cao dissolution was increased. also, the experimental results suggested that small-size slag was more efficient than big-size slag for the self-filtration of p precipitates. chemical and mineralogical analyses indicated that, after precipitation, ca phosphates may crystallise into the most stable form of hydroxyapatite. (c) 2014 elsevier b.v. all rights reserved.
effect of thermal regeneration of spent activated carbon on volatile organic compound adsorption performances. thermal swing adsorption (tsa) is widely used as a process in industry for gas purification and air treatment. this study enlightens the effects of thermal regeneration of an activated carbon spent with volatile organic compounds (vocs) on its adsorption capacities. ketone group vocs (acetone and methyl ethyl ketone (mek)) were selected for this study as they are sensitive to oxidation reactions at low temperature, and for this reason, are responsible for many reported fire accidents on industrial units. cyclic adsorption-desorption experiments were performed using a thermo-gravimetric analyzer (tga). first cycle adsorption capacity of acetone and mek at 20 degrees c was 5.06 mol/kg and 6.41 mol/kg, respectively. the regeneration was performed with air at temperature ranging from 80 degrees c to 160 degrees c. multiple cycles were successively repeated to assess the variations in the material adsorption performances. for acetone, it was found that after first regeneration cycle conducted at 80 degrees c, the adsorption capacity was restored at nearly 95%, and remained unchanged after 8 successive cycles. for mek, continuous degradation of the adsorption capacities was observed, which was more drastic when the temperature was low and reached 3.4 mol/kg after 8 cycles. this could be related to the partial decomposition of the chemisorbed mek molecules. effect of air humidity was furthermore examined and a protocol was developed to guarantee good regeneration efficiency of mek spent activated carbon at a moderate temperature. (c) 2014 taiwan institute of chemical engineers. published by elsevier b.v. all rights reserved.
structural arrangements of isomorphic substitutions in smectites: molecular simulation of the swelling properties, inter layer structure, and dynamics of hydrated cs-montmorillonite revisited with new clay models. three new structural models of montmorillonite with differently distributed al/si and mg/al substitutions in the tetrahedral and octahedral clay layers are systematically developed and studied by means of md simulations to quantify the possible effects of such substitutional disorder on the swelling behavior, the interlayer structure, and mobility of aqueous species. a very wide range of water content, from 0 to 700 mg(water)/g(clay) is explored to derive the swelling properties of cs-montmorillonite. the determined layer spacing does not differ much depending on the clay model. however, at low water contents up to 1-layer hydrate (similar to 100 mg(water)/g(clay)) the variation of specific locations of the tetrahedral and octahedral substitutions in the two tot clay layers slightly but noticeably affects the total hydration energy of the system. using atom-atom radial distribution functions and the respective atomic coordination numbers we have identified for the three clay models not only the previously observed binding sites for cs+ on the clay surface but also new ones that are correlated with the position of tetrahedral substitution in the structure. the mobility of cs+ ions and h2o diffusion coefficients, as expected, gradually increase both with increasing water content and with increasing distance from the clay surface, but they still remain 2 to 4 times lower than the corresponding bulk values. only small differences were observed between the three cs-montmorillonite models, but these differences are predicted to increase in the case of higher charge density of the clay layers and/or interlayer cations.
cation exchange on interstratified illite/smectite minerals: experimental study and molecular dynamics simulations. 
indoor air purification by photocatalytic oxidation: fate of contaminants released in the gas phase and key parameter optimization for the improvement of the process efficiency and safety. 
a polynomial approach for optimal control of switched nonlinear systems. optimal control problems for switched nonlinear systems are investigated. we propose an alternative approach for solving the optimal control problem for a nonlinear switched system based on the theory of moments. the essence of this method is the transformation of a nonlinear, nonconvex optimal control problem, that is, the switched system, into an equivalent optimal control problem with linear and convex structure, which allows us to obtain an equivalent convex formulation more appropriate to be solved by high-performance numerical computing. consequently, we propose to convexify the control variables by means of the method of moments obtaining semidefinite programs. copyright (c) 2013 john wiley &amp; sons, ltd.
parton-hadron dynamics in heavy-ion collisions. the dynamics of partons and hadrons in relativistic nucleus-nucleus collisions is analyzed within the novel parton-hadron-string dynamics (phsd) transport approach, which is based on a dynamical quasiparticle model for the partonic phase (dqpm) including a dynamical hadronization scheme. the phsd approach is applied to nucleus-nucleus collisions from low sps to lhc energies. the traces of partonic interactions are found in particular in the elliptic flow of hadrons and in their transverse mass spectra. we investigate also the equilibrium properties of strongly-interacting infinite parton-hadron matter characterized by transport coefficients such as shear and bulk viscosities and the electric conductivity in comparison to lattice qcd results.
core-corona model analysis of the low energy beam scan at rhic. the centrality dependence of spectra of identified particles in collisions between ultrarelativistic heavy ions with a center of mass energy (root s) of 39 and 11.5agev is analyzed in the core-corona model. we show that at these energies the spectra can be well understood assuming that they are composed of two components whose relative fraction depends on the centrality of the interaction: the core component which describes an equilibrated quark gluon plasma and the corona component which is caused by nucleons close to the surface of the interaction zone which scatter only once and which is identical to that observed in proton-proton collisions. the success of this approach at 39 and 11.5 agev shows that the physics does not change between this energy and root s = 200 agev for which this model has been developed (aichelin 2008). this presents circumstantial evidence that a quark gluon plasma is also created at center of mass energies as low as 11.5 agev. (c) 2014 wiley-vch verlag gmbh &amp; co. kgaa, weinheim.
improved measurements of the neutrino mixing angle theta(13) with the double chooz detector (vol 10, 086, 2014). the double chooz experiment presents improved measurements of the neutrino mixing angle θ13 using the data collected in 467.90 live days from a detector positioned at an average distance of 1050 m from two reactor cores at the chooz nuclear power plant. several novel techniques have been developed to achieve significant reductions of the backgrounds and systematic uncertainties with respect to previous publications, whereas the efficiency of the ν¯¯¯e signal has increased. the value of θ13 is measured to be sin2 2θ13 = 0.090+ 0.032− 0.029 from a fit to the observed energy spectrum. deviations from the reactor ν¯¯¯e prediction observed above a prompt signal energy of 4 mev and possible explanations are also reported. a consistent value of θ13 is obtained from a fit to the observed rate as a function of the reactor power independently of the spectrum shape and background estimation, demonstrating the robustness of the θ13 measurement despite the observed distortion.
medium-induced soft gluon radiation in forward dijet production in relativistic proton-nucleus collisions. considering forward dijet production in the q -&gt; qg partonic process, we derive the spectrum of accompanying soft gluon radiation induced by rescatterings in a nuclear target. the spectrum is obtained to logarithmic accuracy for an arbitrary energy sharing between the final quark and gluon, and for final transverse momenta as well as momentum imbalance being large as compared to transverse momentum nuclear broadening. in the case of equal energy sharing and for approximately back-to-back quark and gluon transverse momenta, we reproduce a previous result of liou and mueller. interpreting our result, we conjecture a simple formula for the medium-induced radiation spectrum associated to hard forward 1 -&gt; n processes, which we explicitly check in the case of the g -&gt; gg process.
exergetic study of catalytic steam reforming of bio-ethanol over pd-rh/ceo2 with hydrogen purification in a membrane reactor. an exergy analysis of the ethanol steam reforming process using a pd-rh catalyst supported on ceria in a pd-ag membrane reactor has been performed based on experimental data. both chemical and physical exergy for chemical species together with thermodynamic loss calculations have been considered to evaluate the exergy efficiency of the process. the system has been studied under different operational conditions in terms of temperature (873-923 k), pressure (4-12 bar), and feed flow rate using a water ethanol mixture of 50-50% volume (s/c = 1.6). a significant exergy efficiency dependency on the fuel flow rate and pressure has been encountered when considering the heat loss from the reactor. the best results regarding the exergetic and thermal efficiency have been obtained at high flow rates and 8-12 bar, which has been attributed to the higher hydrogen permeation through the membrane. the chemical reactions involved in the reforming process and the heat losses are the main sources of exergy destruction. copyright (c) 2014, hydrogen energy publications, llc. published by elsevier ltd. all rights reserved.
subcritical hydrothermal liquefaction of microalgae residues as a green route to alternative road binders. the valorization of scenedesmus sp. microalgae byproducts was investigated, as a potential route for the production of road binders from renewable sources. under hydrothermal liquefaction conditions, a water-insoluble viscous material was obtained in a ca. 55% yield, which consists of an oily fatty acid-based fraction mixed with organic and inorganic solid residues (up to 22%). although the chemical composition of the obtained materials completely differs from that of petroleum-based bitumen, similar viscoelastic properties were observed in some cases, depending on the hydrothermal liquefaction experimental conditions. a rheological simple material could thus be obtained, which compared well with a bitumen reference.
multibody system dynamics for bio-inspired locomotion: from geometric structures to computational aspects. this article presents a set of generic tools for multibody system dynamics devoted to the study of bio-inspired locomotion in robotics. first, archetypal examples from the field of bio-inspired robot locomotion are presented to prepare the ground for further discussion. the general problem of locomotion is then stated. in considering this problem, we progressively draw a unified geometric picture of locomotion dynamics. for that purpose, we start from the model of discrete mobile multibody systems (mmss) that we progressively extend to the case of continuous and finally soft systems. beyond these theoretical aspects, we address the practical problem of the efficient computation of these models by proposing a newton-euler-based approach to efficient locomotion dynamics with a few illustrations of creeping, swimming, and flying.
an experimental approach to measure particle deposition in large circular ventilation ducts. the topic of this study is related to airborne particle dynamics in indoor environments. lab-scale experiments have been performed to investigate particle deposition velocity to six different surfaces orientations (with respect to gravity) for fully developed turbulent flow in horizontal large circular ventilation ducts. monodispersed aerosol particles (1-6 mu m) were used in the deposition experiments. a very low particle mass (40 ng) was measured reliably above background level on duct surfaces by a means of a nondestructive stencil technique associated with fluorescence analysis. for 26 mu m particles (diffusion and impaction regime), deposition rates to floors were much greater than rates to the ceiling and greater than rates to the wall. for 1-mu m particles, the effect of surface orientation to particle deposition was not significant. results were compared to the very few similar and published studies. this work was conducted in the frame of the cleanairnet project which aimed at producing new knowledge, models, and techniques to help controlling the safety food stuffs, through a better control of aerosol particle (bioaerosols) transport and deposition in the ventilation networks of the food industry.
optimal pose selection for calibration of planar anthropomorphic manipulators. the paper is devoted to the calibration experiment design for serial anthropomorphic manipulators with arbitrary number of links. it proposes simple rules for the selection of manipulator configurations that allow the user to essentially improve calibration accuracy and reduce identification errors. although the main results have been obtained for the planar manipulators, they can be also useful for calibration of more complicated mechanisms. the efficiency of the proposed approach is illustrated with several examples that deal with typical planar manipulators and an anthropomorphic industrial robot. (c) 2014 elsevier inc. all rights reserved.
vapor hydration of a simulated borosilicate nuclear waste glass in unsaturated conditions at 50 degrees c and 90 degrees c. vapor hydration of a simulated typical french nuclear intermediate-level waste (ilw) glass in unsaturated conditions has been studied in order to simulate its behaviour under repository conditions before complete saturation of the disposal site. the experiments were conducted for one year at 50 degrees c and 90 degrees c and the relative humidity (rh) was maintained at 92% and 95%. the glass hydration was followed by fourier transform infra-red spectroscopy (ftir). the surface of the reacted glass was characterised by scanning electron microscopy (sem) and transmission electron microscopy (tem). the chemical and mineralogical composition of the alteration products were studied by energy dispersive x-ray spectroscopy (edx) and mraman spectroscopy, respectively. the glass hydration increased with temperature and rh and led to the formation of a depolymerized gel layer depleted in alkalis. the glass hydration rate decreased with time and remained almost unchanged for the last three months of exposure. overall, the ilw glass hydration rate was similar to that obtained with the son68 high-level waste glass.
the effect of temperature on carbon steel corrosion under geological conditions. we investigated the role of temperature on the carbon steel corrosion under simulated geological conditions. to simulate the effect of temperature increase due to radioactive decay, we conducted batch experiments using callovo-oxfordian (cox) claystone and synthetic water formation with steel coupons at 30 degrees c and 90 degrees c for 6 months. the corrosion products have been studied by scanning electron microscope/energy dispersive x-ray spectroscopy, x-ray diffraction and micro-raman spectroscopy. at 30 degrees c, experiments showed the formation of magnetite and iron sulphide, indicating the activation of sulphate-reducing bacteria. at 90 degrees c a continuous iron sulphide layer was identified on steel surface due to the reduction by hydrogen of pyrite originating from claystone into pyrrhotite and hydrogen sulphide. thus, sulphide production may occur even in the absence of microbial activity at high temperature and must be taken into consideration regarding the near-field geochemical evolution. (c) 2014 elsevier ltd. all rights reserved.
h2s biofiltration using expanded schist as packing material: influence of packed bed configurations at constant ebrt. backgroundh(2)s biofiltration was carried out using expanded schist as packing material completed with a synthetic material (up20). a comparison of different hydrodynamic configurations was made based on biofilter performances and pressure drop measurements. three biofilters (namely bf52, bf102 and bf160) differing in bed height (52 cm, 102 cm and 160 cm, respectively) and diameter (14 cm, 10 cm and 8 cm, respectively) were designed in order to contain the same volume of expanded schist (8 l). resultsbiofilters were operated for more than 6 months at a constant flow rate (1.5 nm(3) h(-1) corresponding to an empty bed residence time of 19 s). elimination capacities and removal efficiencies were calculated according to loading rates varying from 0 to 57 g m(-3) h(-1) (inlet concentration up to 300 mg m(-3)). biofilter performances were modeled and biokinetic constants were calculated using the ottengraf model and a modified michaelis-menten model. in terms of elimination capacity, biofilter configurations can be ordered from the most to the least efficient: bf160&gt;bf102&gt;bf52 (maximum removal rates of 36.4, 30.3 and 25.1 g m(-3) h(-1), respectively). conclusionfrom the ottengraf model, it was calculated that the specific surface area covered with biofilm, relative to bf52, was 21% and 45% higher for bf102 and bf160, respectively. (c) 2014 society of chemical industry.
constraining the nuclear matter equation of state around twice saturation density. using data on elliptic flow measured by the fopi collaboration we extract constraints for the equation of state (eos) of symmetric nuclear matter with the help of the microscopic transport code iqmd. best agreement between data and calculations is obtained with a `soft' equation of state including a momentum dependent interaction. from the model it can be deduced that the characteristic density related to the observed flow signal is around twice saturation density and that both compression within the fireball and the presence of the surrounding spectator matter is necessary for the development of the signal and its sensitivity to the nuclear equation of state.
measurement of w-boson production in p-pb collisions at root(nn)-n-s=5.02 tev with alice at the lhc. in hadronic collisions, electroweak bosons are produced in initial hard scattering processes and they are not affected by the strong interaction. in proton proton collisions, they have been suggested as standard candles for luminosity monitoring and their measurement can improve the evaluation of detector performances. in nucleus-nucleus and proton nucleus collisions, w-bosons allow one to check at first order the validity of binary collision scaling, while small deviations allow to study the nuclear modifications of parton distribution functions. the w-boson production in p pb collisions at root(nn)-n-s = 5.02 tev is measured via the contribution of w-boson decays to the inclusive pt-differential muon yield reconstructed with the alice muon spectrometer at forward (2.03 &lt; ems &lt; 3.53) and backward (-4.46 &lt; y(cms)(mu) &lt; 2.96) rapidity. this paper reports the production cross section of muons from w-boson decays for p(t)(mu) &gt; 10 gev/c and the yields normalised to the average number of binary nucleon-nucleon collisions as a function of the event activity.
inclusive j/psi production in pp, p-pb and pb-pb collisions at forward rapidity with alice at the lhc. the alice collaboration has measured the inclusive j/psi, production at forward rapidity in pp, p-pb and pb-pb collisions at the lhc. the pp measurements are crucial for a deeper understanding of the physics involving hadroproduction processes and provide the baseline for p-pb and pb-pb measurements. the comparison of the results on the nuclear modification factor in p-pb collisions as a function of rapidity or transverse momentum with theoretical predictions shows a fair agreement with a shadowing based model and energy loss models with or without a shadowing contribution. the nuclear modification factor in pb-pb as a function of collision centrality or transverse momentum shows a weaker suppression than at lower energies and can be described by statistical hadronization or transport models, suggesting a contribution to the j/psi, production due to the (re)combination of charm quarks, especially at low transverse momentum. the extrapolation to pb-pb collisions of the cold nuclear matter effects evaluated in p-pb collisions supports the (re)combination mechanism interpretation.
transverse momentum dependence of d-meson production in pb-pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev. the production of prompt charmed mesons d$^0$, d$^+$ and d$^{*+}$, and their antiparticles, was measured with the alice detector in pb-pb collisions at the centre-of-mass energy per nucleon pair, $\sqrt{s_{\rm nn}}$, of 2.76 tev. the production yields for rapidity $|y|&lt;0.5$ are presented as a function of transverse momentum, $p_{\rm t}$, in the interval 1-36 gev/$c$ for the centrality class 0-10% and in the interval 1-16 gev/$c$ for the centrality class 30-50%. the nuclear modification factor $r_{\rm aa}$ was computed using a proton-proton reference at $\sqrt{s} = 2.76$ tev, based on measurements at $\sqrt{s} = 7$ tev and on theoretical calculations. a maximum suppression by a factor of 5-6 with respect to binary-scaled pp yields is observed for the most central collisions at $p_{\rm t}$ of about 10 gev/$c$. a suppression by a factor of about 2-3 persists at the highest $p_{\rm t}$ covered by the measurements. at low $p_{\rm t}$ (1-3 gev/$c$), the $r_{\rm aa}$ has large uncertainties that span the range 0.35 (factor of about 3 suppression) to 1 (no suppression). in all $p_{\rm t}$ intervals, the $r_{\rm aa}$ is larger in the 30-50% centrality class compared to central collisions. the d-meson $r_{\rm aa}$ is also compared with that of charged pions and, at large $p_{\rm t}$, charged hadrons, and with model calculations.
transverse energy production and charged-particle multiplicity at midrapidity in various systems from $\sqrt{s_{nn}}=7.7$ to 200 gev. measurements of midrapidity charged particle multiplicity distributions, $dn_{\rm ch}/d\eta$, and midrapidity transverse-energy distributions, $de_t/d\eta$, are presented for a variety of collision systems and energies. included are distributions for au$+$au collisions at $\sqrt{s_{_{nn}}}=200$, 130, 62.4, 39, 27, 19.6, 14.5, and 7.7 gev, cu$+$cu collisions at $\sqrt{s_{_{nn}}}=200$ and 62.4 gev, cu$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev, u$+$u collisions at $\sqrt{s_{_{nn}}}=193$ gev, $d$$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev, $^{3}$he$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev, and $p$$+$$p$ collisions at $\sqrt{s_{_{nn}}}=200$ gev. centrality-dependent distributions at midrapidity are presented in terms of the number of nucleon participants, $n_{\rm part}$, and the number of constituent quark participants, $n_{q{\rm p}}$. for all $a$$+$$a$ collisions down to $\sqrt{s_{_{nn}}}=7.7$ gev, it is observed that the midrapidity data are better described by scaling with $n_{q{\rm p}}$ than scaling with $n_{\rm part}$. also presented are estimates of the bjorken energy density, $\varepsilon_{\rm bj}$, and the ratio of $de_t/d\eta$ to $dn_{\rm ch}/d\eta$, the latter of which is seen to be constant as a function of centrality for all systems.
mesurement of am at ultra-trace concentrations in environnemental samples. null
separation, detection and quantification of the actinides in an environmental matrix. null
competitive sorption and selective sequence of cu(ii) and ni(ii) on montmorillonite: batch, modeling, epr and xas studies. heavy metal ions that leach from various industrial and agricultural processes are simultaneously present in the contaminated soil and water systems. the competitive sorption of these toxic metal ions on the natural soil components and sediments significantly influences their migration, bioavailability and ecotoxicity in the geochemical environment. in this study, the competitive sorption and selectivity order of cu(ii) and ni(ii) on montmorillonite are investigated by combining the batch experiments, x-ray diffraction (xrd), electron paramagnetic resonance (epr), surface complexation modeling and x-ray absorption spectroscopy (xas). the batch experimental data show that the coexisting ni(ii) exhibits a negligible influence on the sorption behavior of cu(ii), whereas the coexisting cu(ii) reduces the ni(ii) sorption percentage and changes the shape of the ni(ii) sorption isotherm. the sorption species of cu(ii) and ni(ii) on montmorillonite over the acidic and near-neutral ph range are well simulated by the surface complexation modeling. however, this model cannot identify the occurrence of surface nucleation and the co-precipitation processes at a highly alkaline ph. based on the results of the epr and xas analyses, the microstructures of cu(ii) on montmorillonite are identified as the hydrated free cu(ii) ions at ph 5.0, inner-sphere surface complexes at ph 6.0 and the surface dimers/cu(oh)(2)(s) precipitate at ph 8.0 in the single-solute and the binary-solute systems. for the ni(ii) sorption in the single-solute system, the formed microstructure varies from the hydrated free ni(ii) ions at the ph values of 5.0 and 6.0 to the inner-sphere surface complexes at ph 8.0. for the ni(ii) sorption in the binary-solute system, the coexisting cu(ii) induces the formation of the inner-sphere complexes at ph 6.0. in contrast, ni(ii) is adsorbed on montmorillonite via the formation of ni phyllosilicate co-precipitate/alpha-ni(oh)(2)(s) precipitate at ph 8.0. the selective sequence of cu(ii) &gt; ni(ii) for binding on montmorillonite can be ascribed to the differences in the metal properties and the compatibility between the configurations of the montmorillonite binding sites and those of the cu(ii)o-6/ni(ii)o-6 polyhedra. the derived findings in this study could provide significant information for the evaluation of the competitive sorption behaviors at solid/water interfaces and the fate of the coexisting heavy metal ions in multicomponent environmental systems. (c) 2015 elsevier ltd. all rights reserved.
observables in ultrarelativistic heavy-ion collisions from two different transport approaches for the same initial conditions. for nucleus-nucleus collisions at energies currently available at the bnl relativistic heavy ion collider (rhic), we calculate observables in two different transport approaches, i.e., the n-body molecular dynamical model ``relativistic quantum molecular dynamics for strongly interacting matter with phase transition or crossover'' (rsp) and the two-body parton hadron string dynamics (phsd), starting out from the same distribution in the initial energy density at the quark gluon plasma (qgp) formation time. the rsp dynamics is based on the nambu-jona-lasinio (njl) lagrangian, whereas in phsd the partons are described by the dynamical quasiparticle model (dqpm). despite the very different description of the parton properties and their interactions and of the hadronization in both approaches, the final transverse momentum distributions of pions turn out to be quite similar, which is less visible for the strange mesons owing to the large njl cross sections involved. our findings can be attributed, in part, to a partial thermalization of the quark degrees of freedom in central au + au collisions for both approaches. the rapidity distribution of mesons shows a stronger sensitivity to the nature of the degrees of freedom involved and to their interaction strength in the qgp.
nmr and computational molecular modeling studies of mineral surfaces and interlayer galleries: a review. this paper reviews experimental nuclear magnetic resonance (nmr) and computational molecular dynamics (md) investigations of the structural and dynamical behavior of cations, anions, h2o, and co2 on the surfaces and in the interlayer galleries of layer-structure minerals and their composites with polymers and natural organic matter (nom). the interaction among mineral surfaces, charge-balancing cations or anions, h2o, co2, and nom are dominated by coulombic, h-bond, and van der waals interactions leading to statically and dynamically disordered systems and molecular-scale processes with characteristic room-temperature frequencies varying from at least as small as 10(2) to &gt;10(12) hz. nmr spectroscopy provides local structural information about such systems through the chemical shift and quadrupolar interactions and dynamical information at frequencies from the sub-kilohertz to gigahertz ranges through the t-1 and t-2 relaxation rates and line shape analysis. it is often difficult to associate a specific structure or dynamical process to a given nmr observation, however, and computational molecular modeling is often effective in providing a much more detailed picture in this regard. the examples discussed here illustrate these capabilities of combining experimental nmr and computational modeling in mineralogically and geochemically important systems, including clay minerals and layered double hydroxides.
wetland pollutant dynamics and control. 
accelerator-based production of mo-99: a comparison between the mo-100(p,x) and zr-96(alpha,n) reactions. innovative accelerator-based production routes for mo-99 (and tc-99m) have been studied, comparing the mo-100(p,x)mo-99,tc-99m and zr-96(alpha,n)mo-99 reactions, for which a new set of measurement has been made. theoretical and experimental cross sections have been analysed and used to calculate mo-99 production yields and specific activity (sa), considering fully enriched and commercially available target materials. results show that the low sa resulting from the p-based route forces the use of alternative generator systems, while the alpha-based reaction provides very high sa mo-99 but much lower yield. benefits and drawbacks of direct tc-99m production via the mo-100(p,2p) reaction are also discussed.
identification of the manipulator stiffness model parameters in industrial environment. the paper addresses a problem of robotic manipulator calibration in real industrial environment. the main contributions are in the area of the elastostatic parameter identification. in contrast to other works the considered approach takes into account the elastic properties of both links and joints. particular attention is paid to the practical identifiability of the model parameters, which completely differs from the theoretical one that relies on the rank of the observation matrix only, without taking into account essential differences in the model parameter magnitudes and the measurement noise impact. this problem is relatively new in robotics and essentially differs from that arising in geometrical calibration. to solve the problem, physical algebraic and statistical model reduction methods are proposed. they are based on the stiffness matrix sparseness taking into account the physical properties of the manipulator elements, structure of the observation matrix and also on the heuristic selection of the practically non-identifiable parameters that employ numerical analyses of the parameter estimates. the advantages of the developed approach are illustrated by an application example that deals with the elastostatic calibration of an industrial robot in a real industrial environment. (c) 2015 elsevier ltd. all rights reserved.
underwater navigation based on passive electric sense: new perspectives for underwater docking. in underwater robotics, several homing and docking techniques are currently being investigated. they aim to facilitate the recovery of underwater vehicles, as well as their connection to underwater stations for battery charging and data exchange. developing reliable underwater docking strategies is a critical issue especially in murky water and/or in confined and cluttered environments. commonly used underwater sensors such as sonar and camera can fail under these conditions. we show how a bio-inspired sensor could be used to help guide an underwater robot during a docking phase. the sensor is inspired by the passive electro-location ability of electric fish. exploiting the electric interactions and the morphology of the vehicle, a sensor-based reactive control law is proposed. it allows the guidance of the robot toward the docking station by following an exogenous electric field generated by a set of electrodes fixed to the environment. this is achieved while avoiding insulating perturbative objects. this control strategy is theoretically analysed and validated with experiments carried out on a setup dedicated to the study of electric sense. though promising, these results are but a first step towards the implementation of an approach to docking in more realistic conditions, such as in turbid salt water or in the presence of conductive perturbative objects.
geometric calibration of industrial robots using enhanced partial pose measurements and design of experiments. the paper deals with geometric calibration of industrial robots and focuses on reduction of the measurement noise impact by means of proper selection of the manipulator configurations in calibration experiments. particular attention is paid to the enhancement of measurement and optimization techniques employed in geometric parameter identification. the developed method implements a complete and irreducible geometric model for serial manipulator, which takes into account different sources of errors (link lengths, joint offsets, etc). in contrast to other works, a new industry-oriented performance measure is proposed for optimal measurement configuration selection that improves the existing techniques via using the direct measurement data only. this new approach is aimed at finding the calibration configurations that ensure the best robot positioning accuracy after geometric error compensation. experimental study of heavy industrial robot kuka kr-270 illustrates the benefits of the developed pose strategy technique and the corresponding accuracy improvement. (c) 2015 elsevier ltd. all rights reserved.
new energy value chain through pyrolysis of hospital plastic waste. in this paper, the evolution in thermochemical behaviours of hospital plastic wastes and changes in chemical composition and characteristics of pyrolysis liquid products have been investigated by using different fixed bed reactor scales. the main objective is to identify the critical technical parameters enabling thermochemical process adaptation in function of raw materials chemical structure, with the aim of maximising the yield of condensable fraction and optimising its energetic properties related to internal combustion engines. it is a step-by-step procedure using three reactor capacity levels, which allows various aspects approach of thermochemical process development from the evaluation of global reaction kinetic parameters to the measurement of physicochemical properties of the final pyrolysis products. in order to reduce the gas and solid fractions with corresponding increasing of condensable products, the transposition of thermal and kinetic information provided by thermogravimetric analysis (tga) to larger reactors is used to control of process parameters. in this experimental work the mass of samples increases from 0.05 g in the thermogravimetric analyser to 600 g in the bench scale reactor. gas-chromatography techniques have been used to identify the chemical composition of gases (gc/tcd) and liquids (gc/fid-ms). it was established that changing the reactor scale does not result in significant differences in pyrolysis product distribution, neither in gas composition. on the other hand, the aspect and the quality of condensable fraction display a high variability. also, the energy contained in the final valuable pyrolysis product was compared with the energy demand during the thermochemical transformation in order to evaluate the energy efficiency of the process. (c) 2015 elsevier ltd. all rights reserved.
effect of climate, wastewater composition, loading rates, system age and design on performances of french vertical flow constructed wetlands: a survey based on 169 full scale systems. the main objective of our work was to study the efficiency of the 2 stages french vertical flow constructed wetland system and its compact version by compiling data of 169 full scale systems in operation for up to 12 years. design parameters and treatment performances, mostly based on 24 h composite samples performed by independent local authorities, have been compared to see how climate, wastewater composition, loading rates, system age and design were influencing the treatment performances. a bit more than 97% of the samples analysed at outlet of the plants were fulfilling the most common french discard limits (in mg/l 125 cod, 25 bod). removal efficiencies were not affected by factors including hydraulic and organic loads (until 60 cm/d), defect of maintenance and temperature within the ranges observed. age of treatment plants had an effect only during start-up, but after 0.5-2 years of operation, performance was constant. similarly, the feedback from 5 years of data obtained with ``compact vfcws'' showed that this system met french standards and outperformed the first stage of ``classical vfcws''. this survey of 169 full scale systems represents a confirmation of the good performance and the robustness of the french vfcws. (c) 2014 elsevier b.v. all rights reserved.
on the trail of a new state of matter. following the results collected in the past 30 years within the heavy-ion scientific program, the progress achieved so far at the cern lhc during the first data taking period is reviewed. (c) 2015 academie des sciences. published by elsevier masson sas. all rights reserved.
static stability of manipulator configuration: influence of the external loading. the paper deals with the manipulator static stability analysis under the influence of the external loading. it proposes a new technique that allows evaluating both static stability of the end-effector location and static stability of the kinematic chain configuration. this approach extends the classical notion of the manipulator static stability that is completely defined by the properties of the cartesian stiffness matrix. the advantages of the new approach are illustrated by examples that deal with parallel manipulators and their serial chains. the analysis showed that the manipulator workspace may include elastostatic singularities where the chain configurations become unstable under the influence of external loading. (c) 2014 elsevier masson sas. all rights reserved.
thick wood particle pyrolysis in an oxidative atmosphere. oxidative pyrolysis of pine wood particles was analysed thermo-gravimetrically. the effects of the concentration of oxygen in the surrounding gas and of particle size were investigated. three different oxygen concentrations (0%, 10% and 20% v/v) and three different sized cylindrical pine wood samples (4 mm, 8 mm and 12 mm in diameter and 15 mm long) were tested. two types of macro-tg apparatuses were used; the first was non isothermal and was used at a heating rate of 20 degrees c/min, and the second was isothermal used at two temperatures, 400 degrees c and 600 degrees c. in the low heating rate non isothermal apparatus, results showed that oxygen had a strong influence on pyrolysis behaviour, but particle size did not. in the high heating rate isothermal apparatus, particle size had a significant influence on conversion: transfer phenomena limit oxidative pyrolysis. (c) 2015 elsevier ltd. all rights reserved.
odum-tennenbaum-brown calculus vs emergy and co-emergy analysis. in a recent paper tennenbaum introduced a new method of calculating emergy that requires only ordinary (i.e. linear) algebra. we prove on a simple example with one feedback and one split that ordinary algebra as developed by tennenbaum in his paper is not sufficient to tackle the problem of emergy analysis. in particular, we point out the problem of enumerating pathways which are relevant for emergy analysis, i.e. which avoid the double counting problem of feedbacks. hence, the emergy co-emergy analysis cannot work at least for energy system diagram with splits and feedbacks. le corre and truffet have already proved that the emergy path-finding problem deals with idempotent (thus non-linear) algebra. (c) 2015 elsevier b.v. all rights reserved.
improvement of thermal stability of maghemite nanoparticles coated with oleic acid and oleylamine molecules: investigations under laser irradiation. we investigated the influence of the coating of maghemite nanoparticles (nps) with oleic acid and oleylamine molecules on the thermal stability of maghemite and on the gamma -&gt; alpha-fe2o3 phase transformation. the uncoated maghemite nps were synthesized by coprecipitation and the coated nps by thermal decomposition of organometallic precursors. the morphology and size of the coated nps were characterized by transmission electron microscopy and magnetic and structural properties by fe-57 mossbauer and raman spectroscopies. the phase stability of coated maghemite nps was examined under in situ laser irradiation by raman spectroscopy. the results indicate that coated gamma-fe2o3 nps are thermally more stable than the uncoated nps: the phase transformation of maghemite into hematite was observed at 15 mw for uncoated nps of 4 nm, whereas it occurs at 120 mw for the coated nps of similar size. the analysis of the raman baseline profile reveals clearly that the surface coating of maghemite nps results both in reducing the number of surface defects of nanoparticles and in delaying this phase transition.
cyclotron production of high purity sc-44m,sc-44 with deuterons from (caco3)-ca-44 targets. introduction: due to its longer half-life, sc-44 (t-1/2 = 3.97 h) as a positron emitter can be an interesting alternative to ga-68 (t-1/2 = 67.71 min). it has been already proposed as a pet radionuclide for scouting bone disease and is already available as a ti-44/sc-44 generator. sc-44 has an isomeric state, (sc)-s-44m (t-1/2 = 58.6 h), which can be co-produced with sc-44 and that has been proved to be considered as an in-vivo pet generator sc-44m/sc-44. this work presents the production route of sc-44m/sc-44 generator from ca-44(d,2n), its extraction/purification process and the evaluation of its performances. methods: irradiation was performed in a low activity target station using a deuteron beam of 16 mev, which favors the number of (sc)-s-44m atoms produced simultaneously to sc-44. typical irradiation conditions were 60 min at 02 mu a producing 44 mbq of sc-44 with a (sc)-s-44/(sc)-s-44m activity ratio of 50 at end of irradiation. separations of the radionuclides were performed by means of cation exchange chromatography using a dga (r) resin (triskem). then, the developed process was applied with bigger targets, and could be used for preclinical studies. results: the extraction/purification process leads to a radionucleidic purity higher than 99.99% (sc-43, sc-46, sc-48 &lt; dl). sc-44m/sc-44 labeling towards dota moiety was performed in order to get an evaluation of the specific activities that could be reached with regard to all metallic impurities from the resulting source. reaction parameters of radiolabeling were optimized, reaching yields over 95%, and leading to a specific activity of about 10-20 mbq/nmol for dota. a recycling process for the enriched ca-44 target was developed and optimized. conclusion: the quality of the final batch with regard to radionucleidic purity, specific activity and metal impurities allowed a right away use for further radiopharmaceutical evaluation. this radionucleidic pair of (sc)-s-44m/(sc)-s-44 offers a quite interesting pet radionuclide for being further evaluated as an in-vivo generator. (c) 2015 elsevier inc. all rights reserved.
electronic structures of the xf&lt;sub&gt;3&lt;/sub&gt; (x = cl, br, i, at) fluorides and topology of their potential energy surfaces. null
electronic structures and geometries of the xf&lt;sub&gt;3&lt;/sub&gt; (x = cl, br, i, at) fluorides. the potential energy surfaces of the group 17 xf&lt;sub&gt;3&lt;/sub&gt; (x = cl, br, i, at) fluorides have been investigated for the first time with multiconfigurational wave function theory approaches. in agreement with experiment, bent t-shaped &lt;i&gt;c&lt;/i&gt;&lt;sub&gt;2v&lt;/sub&gt; structures are computed for clf&lt;sub&gt;3&lt;/sub&gt;, brf&lt;sub&gt;3&lt;/sub&gt; and if&lt;sub&gt;3&lt;/sub&gt;, while we predict that an average &lt;i&gt;d&lt;/i&gt;&lt;sub&gt;3h&lt;/sub&gt; structure would be experimentally observed for atf&lt;sub&gt;3&lt;/sub&gt;. electron correlation and scalar relativistic effects strongly reduce the energy difference between the &lt;i&gt;d&lt;/i&gt;&lt;sub&gt;3h&lt;/sub&gt; geometry and the &lt;i&gt;c&lt;/i&gt;&lt;sub&gt;2v&lt;/sub&gt; one, along the xf&lt;sub&gt;3&lt;/sub&gt; series, and in the x = at case, spin-orbit coupling also slightly reduces this energy difference. atf&lt;sub&gt;3&lt;/sub&gt; is a borderline system where the &lt;i&gt;d&lt;/i&gt;&lt;sub&gt;3h&lt;/sub&gt; structure becomes a minimum, i.e., the &lt;i&gt;pseudo&lt;/i&gt; jahn-teller effect is inhibited since electron correlation and scalar-relativistic effects create small energy barriers leading to the global &lt;i&gt;c&lt;/i&gt;&lt;sub&gt;2v&lt;/sub&gt; minima, although both types of effects interfere.
simgrid vm: virtual machine support for a simulation framework of distributed systems. as real systems become larger and more complex, the use of simulator frameworks grows in our research community. by leveraging them, users can focus on the major aspects of their algorithm, run in-siclo experiments (i.e., simulations), and thoroughly analyze results, even for a large-scale environment without facing the complexity of conducting in-vivo studies (i.e., on real testbeds). since nowadays the virtual machine (vm) technology has become a fundamental building block of distributed computing environments, in particular in cloud infrastructures, our community needs a full-fledged simulation framework that enables us to investigate large-scale virtualized environments through accurate simulations. to be adopted, such a framework should provide easy-to-use apis as well as accurate simulation results. in this paper, we present a highly-scalable and versatile simulation framework supporting vm environments. by leveraging simgrid, a widely-used open-source simulation toolkit, our simulation framework allows users to launch hundreds of thousands of vms on their simulation programs and control vms in the same manner as in the real world (e.g., suspend/resume and migrate). users can execute computation and communication tasks on physical machines (pms) and vms through the same simgrid api, which will provide a seamless migration path to iaas simulations for hundreds of simgrid users. moreover, simgrid vm includes a live migration model implementing the precopy migration algorithm. this model correctly calculates the migration time as well as the migration traffic, taking account of resource contention caused by other computations and data exchanges within the whole system. this allows user to obtain accurate results of dynamic virtualized systems. we confirmed accuracy of both the vm and the live migration models by conducting several micro-benchmarks under various conditions. finally, we conclude the article by presenting a first use-case of one consolidation algorithm dealing with a significant number of vms/pms. in addition to confirming the accuracy and scalability of our framework, this first scenario illustrates the main interest of simgrid vm: investigating through in-siclo experiments pros/cons of new algorithms in order to limit expensive in-vivo experiments only to the most promising ones.
global optimization based on contractor programming. in this paper, we will present a general pattern based on contractor programmingfor designing a global optimization solver. this approach allows to solve problems with awide variety of constraints. the complexity and the performance of the algorithm rely on theconstruction of contractors which characterize the feasible region.
antenna design and distribution for a lofar super station in nançay. the nançay radio astronomy observatory and associated laboratories are developing the concept of a " super station " for extending the lofar station now installed and operational in nançay. the lofar super station (lss) will increase the number of high sensitivity long baselines, provide short baselines and an alternate core, and be a large standalone instrument. it will operate in the low frequency band of lofar (30–80 mhz) and extend this range to lower frequencies. three key developments for the lss are described here: (i) the design of a specific antenna, and the distribution of such antennas (ii) at small-scale (analog-phased mini array) and (iii) at large-scale (the whole lss).
state-of-the-art of low frequency radio astronomy, relevant antenna systems and international cooperation in ukraine. the low frequency radio astronomy (decameter-meter range, frequencies of 10-300 mhz) currently demonstrates rapid progress all over the world. new generations of large antennas-lofar, lwa, mwa and others – have been created in many countries. at the same time ukrainian radio astronomical systems utr-2 and uran still remain the largest and most informative ones at the lowest frequency range available for the ground-based radio astronomy (below 33 mhz), especially after their radical modernization during the most recent years. a great number of top priority results have been obtained on the basis of these radio telescopes. the results prove a high significance of the low frequency radio astronomy for astrophysics. substantial part of these results have been obtained in the course of many year cooperation between ukraine on one side and france, austria, germany and other countries on the other. creation of new low frequency instruments gurt (ukraine) and lss/nenufar (france) for the wide frequency range of 10-80 mhz opens up new possibilities for research and fruitful cooperation.
mox fuel enrichment prediction in pwr using polynomial models. a dynamic fuel cycle simulation code models all the ingoing and outgoing material flow in all facilities ofa nuclear reactor’s fleet as well as their evolutions through the different nuclear processes (irradiation,decay, chemical separation, etc.). one of the main difficulties encountered when performing such calculationcomes from the fuel fabrication of reprocessed fuel such as mox fuel. indeed, the mox fuel isfabricated using a plutonium base completed with depleted uranium. the amount of plutonium in thefuel will directly impact the neutron multiplication factor and its evolution through irradiation, so theduration to keep the fuel in the reactor. the present paper presents the study of different pwr mox fuelfabrication polynomial models. those models will allow the prediction of the amount of plutoniumneeded to reach a wanted burnup from the plutonium isotopics. after defining a method to generate atraining sample, that is to say the set of fuel depletion calculations used to fit the polynomial models, thispapers will discuss their performances on 3 different applications. on the two tested models, one linearand one quadratic, while the linear model fail to properly describe the amount of plutonium needed, thefuel fabricated, using the quadratic one, reaches the wanted burnup with a discrepancy below 2%.
production of medical isotopes from a thorium target irradiated by light charged particles up to 70 mev. the irradiation of a thorium target by light charged particles (protons and deuterons) leads to the production of several isotopes of medical interest. direct nuclear reaction allows the production of protactinium-230 which decays to uranium-230 the mother nucleus of thorium-226, a promising isotope for alpha radionuclide therapy. the fission of thorium-232 produces fragments of interest like molybdenum-99, iodine-131 and cadmium-115g. we focus our study on the production of these isotopes, performing new cross section measurements and calculating production yields. our new sets of data are compared with the literature and the last version of the talys code.
is there an interest to use deuteron beams to produce non-conventional radionuclides?. with the recent interest on the theranostic approach, there has been a renewed interest for alternative radionuclides in nuclear medicine. they can be produced using common production routes, i.e., using protons accelerated by biomedical cyclotrons or neutrons produced in research reactors. however, in some cases, it can be more valuable to use deuterons as projectiles. in the case of cu-64, smaller quantities of the expensive target material, ni-64, are used with deuterons as compared with protons for the same produced activity. for the sc-44m/sc-44g generator, deuterons afford a higher sc-44m production yield than with protons. finally, in the case of re-186g, deuterons lead to a production yield five times higher than protons. these three examples show that it is of interest to consider not only protons or neutrons but also deuterons to produce alternative radionuclides.
cross section measurements of deuteron induced nuclear reactions on natural titanium up to 34 mev. experimental cross sections for deuteron induced nuclear reactions on natural titanium were measured, using the stacked-foil technique and gamma spectrometry, up to 34mev with beams provided by the arronax cyclotron. the experimental cross section values were monitored using the (nat)ti(d,x)(48)v reaction, recommended by the iaea. the excitation functions for (nat)ti(d,x)(44m,46,47,48)sc are presented and compared with the existing ones and with the talys 1.6 code calculations using default models. our experimental values are in good agreement with data found in the literature. talys 1.6 is not able to give a good estimation of the production cross sections investigated in this work. these production cross sections of scandium isotopes fit with the new coordinated research project (crp) launched by the international atomic energy agency (iaea) to expand the database of monitor reactions.
production of scandium-44m and scandium-44g with deuterons on calcium-44: cross section measurements and production yield calculations. among the large number of radionuclides of medical interest, sc-44 is promising for pet imaging. either the ground-state sc-44g or the metastable-state sc-44m can be used for such applications, depending on the molecule used as vector. this study compares the production rates of both sc-44 states, when protons or deuterons are used as projectiles on an enriched calcium-44 target. this work presents the first set of data for the deuteron route. the results are compared with the talys code. the thick-target production yields of sc-44m and sc-44g are calculated and compared with those for the proton route for three different scenarios: the production of sc-44g for conventional pet imaging, its production for the new 3 γ imaging technique developed at the subatech laboratory and the production of a sc-44m/sc-44g in vivo generator for antibody labelling.
collective intelligence within the framework of continuing professional development: acopé, the example of a community of practice. cooperating with a view to furthering professional development, taking decisions and functioning interoperatively within a community implies working with participants committed to fostering strong relationships, reflective practice, and reaching shared understanding.the collaboration between the acopé educational advisors takes place within this context itself part of the wider framework of a higher education environment with intersectoral, interprofessional, interdisciplinary and inter-regional issues.the acopé educational advisors undertake professional development activities thus creating synergies as a basis for the emergence and the enhancement of collective intelligence and competence.after having explicited these two concepts, the educational advisors present an analysis of determining factors specific to their organization which facilitate situations of collective production. they also put forward characteristic situational variables which could potentially hinder the expression and development of cooperative actions. their analysis leads to broader reflections on the professional development issues generated within the association.
correlations between molecular descriptors from various volatile organic compounds and photocatalytic oxidation kinetic constants. the photocatalytic oxidation of seven typical indoor volatile organic compounds (vocs) are experimentally investigated using novel nanocrystalline tio2 dip-coated catalysts. not only the role of hydrophilicity of the reactants but also other physico-chemical properties and molecular descriptors are studied and related to kinetic and equilibrium constants. the main objective of this work consists in establishing simple relationships that will be useful to deepen the understanding of gas phase heterogeneous photocatalytic mechanisms and for the prediction of degradation rates of these vocs using an indoor air treatment process.
exploiting renewable sources: when green sla becomes a possible reality in cloud computing. while the proliferation of cloud services have greatly impacted our society, how green are these services is yet to be answered. although, demand escalation for green services has grown due to societal awareness, the approaches to provide green services and establish green slas remain oblivious for cloud or infrastructure providers. the main challenge for cloud provider is to manage green slas with their customers while satisfying their business objectives, such as maximizing profits by lowering expenditure for green energy. since, green sla needs to be proposed based on the presence of green energy, the intermittent nature of renewable sources makes it difficult to be achieved. in response, this paper presents a scheme for green energy management in the presence of explicit and implicit integration of renewable energy in data center. more specifically we propose three contributions: i) we introduce the concept of virtualization of green energy to address the uncertainty of green energy availability, ii) we extend the cloud service level agreement (csla) language to support green sla by introducing two new threshold parameters and iii) we introduce greensla algorithm which leverages the concept of virtualization of green energy to provide per interval specific green sla. experiments were conducted with real workload profile from planetlab and server power model from specpower to demonstrate that, green sla can be successfully established and satisfied without incurring higher cost.
measurements of elliptic and triangular flow in high-multiplicity $^{3}$he$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev. we present the first measurement of elliptic ($v_2$) and triangular ($v_3$) flow in high-multiplicity $^{3}$he$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev. two-particle correlations, where the particles have a large separation in pseudorapidity, are compared in $^{3}$he$+$au and in $p$$+$$p$ collisions and indicate that collective effects dominate the second and third fourier components for the correlations observed in the $^{3}$he$+$au system. the collective behavior is quantified in terms of elliptic $v_2$ and triangular $v_3$ anisotropy coefficients measured with respect to their corresponding event planes. the $v_2$ values are comparable to those previously measured in $d$$+$au collisions at the same nucleon-nucleon center-of-mass energy. comparison with various theoretical predictions are made, including to models where the hot spots created by the impact of the three $^{3}$he nucleons on the au nucleus expand hydrodynamically to generate the triangular flow. the agreement of these models with data may indicate the formation of low-viscosity quark-gluon plasma even in these small collision systems.
exclusion of leptophilic dark matter models using xenon100 electronic recoil data. null
the giant radio array for neutrino detection. high-energy neutrino astronomy will probe the working of the most violent phenomena in the universe. the giant radio array for neutrino detection (grand) project consists of an array of $\sim10^5$ radio antennas deployed over $\sim$200000km$^2$ in a mountainous site. it aims at detecting high-energy neutrinos via the measurement of air showers induced by the decay in the atmosphere of $\tau$ leptons produced by the interaction of the cosmic neutrinos under the earth surface. our objective with grand is to reach a neutrino sensitivity of $3\times10^{-11}e^{-2}$gev$^{-1}$cm$^{-2}$s$^{-1}$sr$^{-1}$ above $3 \times10^{16}$ev. this sensitivity ensures the detection of cosmogenic neutrinos in the most pessimistic source models, and about 100 events per year are expected for the standard models. grand would also probe the neutrino signals produced at the potential sources of uhecrs. we show how our preliminary design should enable us to reach our sensitivity goals, and present the experimental characteristics. we assess the possibility to adapt grand to other astrophysical radio measurements. we discuss in this token the technological options for the detector and the steps to be taken to achieve the grand project.
search for event rate modulation in xenon100 electronic recoil data. we have searched for periodic variations of the electronic recoil event rate in the (2-6) kev energy range recorded between february 2011 and march 2012 with the xenon100 detector, adding up to 224.6 live days in total. following a detailed study to establish the stability of the detector and its background contributions during this run, we performed an un-binned profile likelihood analysis to identify any periodicity up to 500 days. we find a global significance of less than 1 sigma for all periods suggesting no statistically significant modulation in the data. while the local significance for an annual modulation is 2.8 sigma, the analysis of a multiple-scatter control sample and the phase of the modulation disfavor a dark matter interpretation. the dama/libra annual modulation interpreted as a dark matter signature with axial-vector coupling of wimps to electrons is excluded at 4.8 sigma.
energy estimation of cosmic rays with the engineering radio array of the pierre auger observatory. the auger engineering radio array (aera) is part of the pierre auger observatory and is used to detect the radio emission of cosmic-ray air showers. these observations are compared to the data of the surface detector stations of the observatory, which provide well-calibrated information on the cosmic-ray energies and arrival directions. the response of the radio stations in the 30 to 80 mhz regime has been thoroughly calibrated to enable the reconstruction of the incoming electric field. for the latter, the energy density is determined from the radio pulses at each observer position and is interpolated using a two dimensional function that takes into account signal asymmetries due to interference between the geomagnetic and charge excess emission components. the spatial integral over the signal distribution gives a direct measurement of the energy transferred from the primary cosmic ray into radio emission in the aera frequency range. we measure 15.8 mev of radiation energy for a 1 eev air shower arriving perpendicularly to the geomagnetic field. this radiation energy -- corrected for geometrical effects -- is used as a cosmic-ray energy estimator. performing an absolute energy calibration against the surface-detector information, we observe that this radio-energy estimator scales quadratically with the cosmic-ray energy as expected for coherent emission. we find an energy resolution of the radio reconstruction of 22% for the data set and 17% for a high-quality subset containing only events with at least five radio stations with signal.
coherent $\psi$(2s) photo-production in ultra-peripheral pb-pb collisions at $\sqrt{s}_{\rm nn}$ = 2.76 tev. we have has performed the first measurement of the coherent $\psi$(2s) photo-production cross section in ultra-peripheral pb-pb collisions at the lhc. this charmonium excited state is reconstructed via the $\psi$(2s) $\rightarrow l^{+}l^{-}$ and $\psi$(2s) $\rightarrow$ j/$\psi \pi^{+}\pi^{-}$ decays, where the j/$\psi$ decays into two leptons. the analysis is based on an event sample corresponding to an integrated luminosity of about 22 $\mu\rm{b}^{-1}$. the cross section for coherent $\psi$(2s) production in the rapidity interval $-0.9&lt;y&lt;0.9$ is $\mathrm{d}\sigma_{\psi{\rm(2s)}}^{\rm coh}/\mathrm{d}y =0.83\pm 0.19\big(\mathrm{\rm{stat}+{\rm syst}}\big)$ mb. the $\psi$(2s) to j/$\psi$ coherent cross section ratio is $0.34^{+0.08}_{-0.07}(\rm{stat}+{\rm syst})$. the obtained results are compared to predictions from theoretical models.
a modelling pearl with sortedness constraints. some constraint programming solvers and constraint modelling languages feature the sort(l, p , s ) constraint, which holds if s is a nondecreasing rearrangement of the list l, the permutation being made explicit by the optional list p. however, such sortedness constraints do not seem to be used much in practice. we argue that reasons for this neglect are that it is impossible to require the underlying sort to be stable, so that sort cannot be guaranteed to be a total-function constraint, and that l cannot contain tuples of variables, some of which form the key for the sort. to overcome these limitations, we introduce the stablekeysort constraint, decompose it using existing constraints, and propose a propagator. this new constraint enables a powerful modelling idiom, which we illustrate by elegant and scalable models of two problems that are otherwise hard to encode as constraint programs.
using finite transducers for describing and synthesising structural time-series constraints. we describe a large family of constraints for structural time series by means of function composition. these constraints are on aggregations of features of patterns that occur in a time series, such as the number of its peaks, or the range of its steepest ascent. the patterns and features are usually linked to physical properties of the time series generator, which are important to capture in a constraint model of the system, i.e. a conjunction of constraints that produces similar time series. we formalise the patterns using finite transducers, whose output alphabet corresponds to semantic values that precisely describe the steps for identifying the occurrences of a pattern. based on that description, we automatically synthesise automata with accumulators, as well as constraint checkers. the description scheme not only unifies the structure of the existing 30 time-series constraints in the global constraint catalogue, but also leads to over 600 new constraints, with more than 100,000 lines of synthesised code.
open scope: a pragmatic javascript pattern for modular instrumentation. we report on our experience instrumenting narcissus, a javascript interpreter written in javascript, to allow the dynamic deployment of dynamic program analyses. instrumenting an interpreter is a cross-cutting change that can affect many parts of the interpreter source code. we propose a simple open scope pattern that minimizes the changes to the interpreter, while allowing us to implement program analyses in their own files, and to compose them dynamically. we apply our pattern to narcissus using standard javascript features, and find that the gain in extensibility offsets a small loss in performance.
towards modular instrumentation of interpreters in javascript. with an initial motivation based on the security of web applications written in javascript, we consider the instrumentation of an interpreter for a dynamic analysis as a crosscutting concern.  we define the instrumentation problem – an extension to the expression problem with a focus on modifying interpreters.  we then illustrate how we can instrument an interpreter for a simple language using only the bare language features provided by javascript.
aspectizing javascript security. in this position paper we argue that aspects are well-suited to describe and implement a range of strategies to make secure javascript-based applications. to this end, we review major categories of approaches to make client-side applications secure and discuss uses of aspects that exist for some of them. we also propose aspect-based techniques for the categories that have not yet been studied. we give examples of applications where aspects are useful as a general means to flexibly express and implement security policies for javascript.
charged jet cross sections and properties in proton-proton collisions at $\sqrt{s}=7$ tev. the differential charged jet cross sections, jet fragmentation distributions, and jet shapes are measured in minimum bias proton-proton collisions at centre-of-mass energy $\sqrt{s}=7$ tev using the alice detector at the lhc. jets are reconstructed from charged particle momenta in the mid-rapidity region using the sequential recombination $k_{\rm t}$ and anti-$k_{\rm t}$ as well as the siscone jet finding algorithms with several resolution parameters in the range $r=0.2$ to $0.6$. differential jet production cross sections measured with the three jet finders are in agreement in the transverse momentum ($p_{\rm t}$) interval $20&lt;p_{\rm t}^{\rm jet,ch}&lt;100$ gev/$c$. they are also consistent with prior measurements carried out at the lhc by the atlas collaboration. the jet charged particle multiplicity rises monotonically with increasing jet $p_{\rm t}$, in qualitative agreement with prior observations at lower energies. the transverse profiles of leading jets are investigated using radial momentum density distributions as well as distributions of the average radius containing 80% ($\langle r_{\rm 80} \rangle$) of the reconstructed jet $p_{\rm t}$. the fragmentation of leading jets with $r=0.4$ using scaled $p_{\rm t}$ spectra of the jet constituents is studied. the measurements are compared to model calculations from event generators (pythia, phojet, herwig). the measured radial density distributions and $\langle r_{\rm 80} \rangle$ distributions are well described by the pythia model (tune perugia-2011). the fragmentation distributions are better described by herwig.
neutrino physics with juno. the jiangmen underground neutrino observatory (juno), a 20 kton multi-purpose underground liquid scintillator detector, was proposed with the determination of the neutrino mass hierarchy as a primary physics goal. it is also capable of observing neutrinos from terrestrial and extra-terrestrial sources, including supernova burst neutrinos, diffuse supernova neutrino background, geoneutrinos, atmospheric neutrinos, solar neutrinos, as well as exotic searches such as nucleon decays, dark matter, sterile neutrinos, etc. we present the physics motivations and the anticipated performance of the juno detector for various proposed measurements. by detecting reactor antineutrinos from two power plants at 53-km distance, juno will determine the neutrino mass hierarchy at a 3-4 sigma significance with six years of running. the measurement of antineutrino spectrum will also lead to the precise determination of three out of the six oscillation parameters to an accuracy of better than 1\%. neutrino burst from a typical core-collapse supernova at 10 kpc would lead to ~5000 inverse-beta-decay events and ~2000 all-flavor neutrino-proton elastic scattering events in juno. detection of dsnb would provide valuable information on the cosmic star-formation rate and the average core-collapsed neutrino energy spectrum. geo-neutrinos can be detected in juno with a rate of ~400 events per year, significantly improving the statistics of existing geoneutrino samples. the juno detector is sensitive to several exotic searches, e.g. proton decay via the $p\to k^++\bar\nu$ decay channel. the juno detector will provide a unique facility to address many outstanding crucial questions in particle and astrophysics. it holds the great potential for further advancing our quest to understanding the fundamental properties of neutrinos, one of the building blocks of our universe.
centrality dependence of pion freeze-out radii in pb-pb collisions at $\sqrt{\mathbf{s_{nn}}}$=2.76 tev. we report on the measurement of freeze-out radii for pairs of identical-charge pions measured in pb--pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev as a function of collision centrality and the average transverse momentum of the pair $k_{\rm t}$. three-dimensional sizes of the system (femtoscopic radii), as well as direction-averaged one-dimensional radii are extracted. the radii decrease with $k_{\rm t}$, following a power-law behavior. this is qualitatively consistent with expectations from a collectively expanding system, produced in hydrodynamic calculations. the radii also scale linearly with $\left&lt; \mathrm{d}n_{\rm ch}/\mathrm{d}\eta \right&gt;^{1/3}$. this behaviour is compared to world data on femtoscopic radii in heavy-ion collisions. while the dependence is qualitatively similar to results at smaller $\sqrt{s_{\rm nn}}$, a decrease in the $r_{\rm out}/r_{\rm side}$ ratio is seen, which is in qualitative agreement with specific predictions from hydrodynamic models. the results provide further evidence for the production of a collective, strongly coupled system in heavy-ion collisions at the lhc.
baryon study in the njl model. we have studied the phase transition between hadronic matter and quark gluon plasma using the nambu and jona-lasinio model. this model allows a low energy description especially for hadronization process occurring during the cooling.
ninth international workshop on the pragmatics of ocl and other textual specification languages. this paper reports on the 9th ocl workshop held at the models conference in 2009. the workshop focused on the challeges of using ocl in a variety of new scenarios (e.g., model verification and validation, code generation, test-driven development, transformations) and application domains (e.g., domain-specific languages, web semantics) in which ocl is now being used due to the increasing popularity of model-driven development processes and the important role ocl play in them. the workshop included sessions with paper presentations and a final round discussion.
event shape engineering for inclusive spectra and elliptic flow in pb-pb collisions at $\sqrt{s_\rm{nn}}=2.76$ tev. we report on results obtained with the event shape engineering technique applied to pb-pb collisions at $\sqrt{s_\rm{nn}}=2.76$ tev. by selecting events in the same centrality interval, but with very different average flow, different initial state conditions can be studied. we find the effect of the event-shape selection on the elliptic flow coefficient $v_2$ to be almost independent of transverse momentum $p_\rm{t}$, as expected if this effect is due to fluctuations in the initial geometry of the system. charged hadron, pion, kaon, and proton transverse momentum distributions are found to be harder in events with higher-than-average elliptic flow, indicating an interplay between radial and elliptic flow.
medium-induced gluon radiation: an update. the theory of radiative parton energy loss in a static qcd medium is updated.we show that for an incoming parton of large energy $e$ undergoing a hard,small angle scattering in the medium rest frame (i.e., $p_\perp /e \ll 1$ with$p_\perp$ the final parton transverse momentum), the medium-induced radiativeenergy loss due to soft rescatterings is proportional to $e$. it arises fromgluon radiation with large formation time $t_f \gg l$, i.e., fully coherentover the size $l$ of the medium. in particular, in a physical (light-cone)gauge, the medium-induced radiation spectrum arises from the interferencebetween initial and final state radiation. this result, rigorously derived toall orders in the opacity expansion, invalidates a common belief that anymedium-induced energy loss through a finite size target should be bounded inthe high-energy limit. we also review the case of a parton suddenly annihilated(created) in the hard process, where the bound on energy loss applies. in thiscase the induced gluon radiation reduces to purely initial (final) stateradiation, and the fully coherent part of the radiation cancels out, leavingonly a contribution from $t_f \lesssim l$. as is well-known, in the high energylimit the resulting parton energy loss is independent of $e$ (neglectinglogarithms) and proportional to $l^2$.
why hadronic resonances and particle unstable states are interesting?. null
s = −1 meson-baryon interaction in hot and dense nuclear matter: chiral symmetry, many-body and unitarization for a road to gsi/fair. null
latest results from epos3 on the production of stable and unstable hadrons. null
support vector machine in prediction of building energy demand using pseudo dynamic approach. building's energy consumption prediction is a major concern in the recent years and many efforts have been achieved in order to improve the energy management of buildings. in particular, the prediction of energy consumption in building is essential for the energy operator to build an optimal operating strategy, which could be integrated to building's energy management system (bems). this paper proposes a prediction model for building energy consumption using support vector machine (svm). data-driven model, for instance, svm is very sensitive to the selection of training data. thus the relevant days data selection method based on dynamic time warping is used to train svm model. in addition, to encompass thermal inertia of building, pseudo dynamic model is applied since it takes into account information of transition of energy consumption effects and occupancy profile. relevant days data selection and whole training data model is applied to the case studies of ecole des mines de nantes, france office building. the results showed that support vector machine based on relevant data selection method is able to predict the energy consumption of building with a high accuracy in compare to whole data training. in addition, relevant data selection method is computationally cheaper (around 8 minute training time) in contrast to whole data training (around 31 hour for weekend and 116 hour for working days) and reveals realistic control implementation for online system as well.
centrality dependence of the nuclear modification factor of charged pions, kaons, and protons in pb-pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev. transverse momentum ($p_{\rm{t}}$) spectra of pions, kaons, and protons up to $p_{\rm{t}} = 20$ gev/$c$ have been measured in pb-pb collisions at $\sqrt{s_{\rm nn}} = 2.76$ tev using the alice detector for six different centrality classes covering 0-80%. the proton-to-pion and the kaon-to-pion ratios both show a distinct peak at $p_{\rm{t}} \approx 3$ gev/$c$ in central pb-pb collisions that decreases towards more peripheral collisions. for $p_{\rm{t}} &gt; 10$ gev/$c$, the nuclear modification factor is found to be the same for all three particle species in each centrality interval within systematic uncertainties of 10-20%. this suggests there is no direct interplay between the energy loss in the medium and the particle species composition in the hard core of the quenched jet. for $p_{\rm{t}} &lt; 10$ gev/$c$, the data provide important constraints for models aimed at describing the transition from soft to hard physics.
search for weakly decaying $\overline{\lambda\mathrm{n}}$ and $\lambda\lambda $ exotic bound states in central pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. we present results of a search for two hypothetical strange dibaryon states, i.e. the h-dibaryon and the possible $\overline{\lambda\mathrm{n}}$ bound state. the search is performed with the alice detector in central (0-10%) pb-pb collisions at $ \sqrt{s_{\rm{nn}}} = 2.76$ tev, by invariant mass analysis in the decay modes $\overline{\lambda\mathrm{n}} \rightarrow \overline{\mathrm{d}} \pi^{+} $ and h-dibaryon $\rightarrow \lambda \mathrm{p} \pi^{-}$. no evidence for these bound states is observed. upper limits are determined at 99% confidence level for a wide range of lifetimes and for the full range of branching ratios. the results are compared to thermal, coalescence and hybrid urqmd model expectations, which describe correctly the production of other loosely bound states, like the deuteron and the hypertriton.
one-dimensional pion, kaon, and proton femtoscopy in pb-pb collisions at $\sqrt{s_{\rm {nn}}}$ =2.76 tev. the size of the particle emission region in high-energy collisions can be deduced using the femtoscopic correlations of particle pairs at low relative momentum. such correlations arise due to quantum statistics and coulomb and strong final state interactions. in this paper, results are presented from femtoscopic analyses of $\pi^{\pm}\pi^{\pm}$, ${\rm k}^{\pm}{\rm k}^{\pm}$, ${\rm k}^{0}_s{\rm k}^{0}_s$, ${\rm pp}$, and ${\rm \overline{p}}{\rm \overline{p}}$ correlations from pb-pb collisions at $\sqrt{s_{\mathrm {nn}}}=2.76$ tev by the alice experiment at the lhc. one-dimensional radii of the system are extracted from correlation functions in terms of the invariant momentum difference of the pair. the comparison of the measured radii with the predictions from a hydrokinetic model is discussed. the pion and kaon source radii display a monotonic decrease with increasing average pair transverse mass $m_{\rm t}$ which is consistent with hydrodynamic model predictions for central collisions. the kaon and proton source sizes can be reasonably described by approximate $m_{\rm t}$-scaling.
forward-central two-particle correlations in p-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev. two-particle angular correlations between trigger particles in the forward pseudorapidity range ($2.5 &lt; |\eta| &lt; 4.0$) and associated particles in the central range ($|\eta| &lt; 1.0$) are measured with the alice detector in p-pb collisions at a nucleon-nucleon centre-of-mass energy of 5.02 tev. the trigger particles are reconstructed using the muon spectrometer, and the associated particles by the central barrel tracking detectors. in high-multiplicity events, the double-ridge structure, previously discovered in two-particle angular correlations at midrapidity, is found to persist to the pseudorapidity ranges studied in this letter. the second-order fourier coefficients for muons in high-multiplicity events are extracted after jet-like correlations from low-multiplicity events have been subtracted. the coefficients are found to have a similar transverse momentum ($p_{\rm t}$) dependence in p-going (p-pb) and pb-going (pb-p) configurations, with the pb-going coefficients larger by about $16\pm6$%, rather independent of $p_{\rm t}$ within the uncertainties of the measurement. the data are compared with calculations using the ampt model, which predicts a different $p_{\rm t}$ and $\eta$ dependence than observed in the data. the results are sensitive to the parent particle $v_2$ and composition of reconstructed muon tracks, where the contribution from heavy flavour decays are expected to dominate at $p_{\rm t}&gt;2$ gev/$c$.
production of light nuclei and anti-nuclei in pp and pb-pb collisions at lhc energies. the production of (anti-)deuteron and (anti-)$^{3}$he nuclei in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev has been studied using the alice detector at the lhc. the spectra exhibit a significant hardening with increasing centrality. combined blast-wave fits of several particles support the interpretation that this behavior is caused by an increase of radial flow. the integrated particle yields are discussed in the context of coalescence and thermal-statistical model expectations. the particle ratios, $^3$he/d and $^3$he/p, in pb-pb collisions are found to be in agreement with a common chemical freeze-out temperature of $t_{\rm chem} \approx 156$ mev. these ratios do not vary with centrality which is in agreement with the thermal-statistical model. in a coalescence approach, it excludes models in which nucleus production is proportional to the particle multiplicity and favors those in which it is proportional to the particle density instead. in addition, the observation of 31 anti-tritons in pb-pb collisions is reported. for comparison, the deuteron spectrum in pp collisions at $\sqrt{s} = 7$ tev is also presented. while the p/$\pi$ ratio is similar in pp and pb-pb collisions, the d/p ratio in pp collisions is found to be lower by a factor of 2.2 than in pb-pb collisions.
$\phi$-meson production at forward rapidity in p-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev and in pp collisions at $\sqrt{s}$ = 2.76 tev. the first measurement of $\phi$-meson production in p-pb collisions at a nucleon-nucleon centre-of-mass energy $\sqrt{s_{\rm nn}}$ = 5.02 tev has been performed with the alice apparatus at the lhc. the $\phi$-mesons have been identified in the dimuon decay channel in the transverse momentum ($p_{\rm t}$) range $1 &lt; p_{\rm t} &lt; 7$ gev/$c$, both in the p-going ($2.03 &lt; y &lt; 3.53$) and the pb-going ($-4.46 &lt; y &lt; -2.96$) directions, where $y$ stands for the rapidity in the nucleon-nucleon centre-of-mass. differential cross sections as a function of transverse momentum and rapidity are presented. the forward-backward asymmetry for $\phi$-meson production is measured for $2.96&lt;|y|&lt;3.53$, resulting in a factor $\sim 0.5$ with no significant $p_{\rm t}$ dependence within the uncertainties. the $p_{\rm t}$ dependence of the $\phi$ nuclear modification factor $r_{\rm ppb}$ exhibits an enhancement up to a factor 1.6 at $p_{\rm t}$ = 3-4 gev/$c$ in the pb-going direction. the $p_{\rm t}$ dependence of the $\phi$-meson cross section in pp collisions at $\sqrt{s}$ = 2.76 tev, which is used to determine a reference for the p-pb results, is also presented here for $1 &lt; p_{\rm t} &lt; 5$ gev/$c$ and $2.5 &lt;y &lt; 4$.
$^{3}_{\lambda}\mathrm h$ and $^{3}_{\bar{\lambda}} \overline{\mathrm h}$ production in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the production of the hypertriton nuclei $^{3}_{\lambda}\mathrm h$ and $^{3}_{\bar{\lambda}} \overline{\mathrm h}$ has been measured for the first time in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev with the alice experiment at lhc energies. the total yield, d$n$/d$y$ $\times \mathrm{b.r.}_{\left( ^{3}_{\lambda}\mathrm h \rightarrow ^{3}\mathrm{he},\pi^{-} \right)} = \left( 3.86 \pm 0.77 (\mathrm{stat.}) \pm 0.68 (\mathrm{syst.})\right) \times 10^{-5}$ in the 0-10% most central collisions, is consistent with the predictions from a statistical thermal model using the same temperature as for the light hadrons. the coalescence parameter $b_3$ shows a dependence on the transverse momentum, similar to the $b_2$ of deuterons and the $b_3$ of $^{3}\mathrm{he}$ nuclei. the ratio of yields $s_3$ = $^{3}_{\lambda}\mathrm h$/($^{3}\mathrm{he}$ $\times \lambda/\mathrm{p}$) was measured to be $s_3$ = 0.60 $\pm$ 0.13 (stat.) $\pm$ 0.21 (syst.) in 0-10% centrality events; this value is compared to different theoretical models. the measured $s_3$ is fully compatible with thermal model predictions. the measured $^{3}_{\lambda}\mathrm h$ lifetime, $ \tau = 181^{+54}_{-39} (\mathrm{stat.}) \pm 33 (\mathrm{syst.})\ \mathrm{ps}$ is compatible within 1$\sigma$ with the world average value.
centrality dependence of inclusive j/$\psi$ production in p-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev. we present a measurement of inclusive j/$\psi$ production in p-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev as a function of the centrality of the collision, as estimated from the energy deposited in the zero degree calorimeters. the measurement is performed with the alice detector down to zero transverse momentum, $p_{\rm t}$, in the backward ($-4.46 &lt; y_{\rm cms} &lt; -2.96$) and forward ($2.03 &lt; y_{\rm cms} &lt; 3.53$) rapidity intervals in the dimuon decay channel and in the mid-rapidity region ($-1.37 &lt; y_{\rm cms} &lt; 0.43$) in the dielectron decay channel. the backward and forward rapidity intervals correspond to the pb-going and p-going direction, respectively. the $p_{\rm t}$-differential j/$\psi$ production cross section at backward and forward rapidity is measured for several centrality classes, together with the corresponding average $p_{\rm t}$ and $p^2_{\rm t}$ values. the nuclear modification factor, $q_{\rm ppb}$, is presented as a function of centrality for the three rapidity intervals, and, additionally, at backward and forward rapidity, as a function of $p_{\rm t}$ for several centrality classes. at mid- and forward rapidity, the j/$\psi$ yield is suppressed up to 40% compared to that in pp interactions scaled by the number of binary collisions. the degree of suppression increases towards central p-pb collisions at forward rapidity, and with decreasing $p_{\rm t}$ of the j/$\psi$. at backward rapidity, the $q_{\rm ppb}$ is compatible with unity within the total uncertainties, with an increasing trend from peripheral to central p-pb collisions.
differential studies of inclusive j/$\psi$ and $\psi$(2s) production at forward rapidity in pb-pb collisions at $\mathbf{\sqrt{{\textit s}_{_{nn}}}}$ = 2.76 tev. the production of j/$\psi$ and $\psi(2s)$ was measured with the alice detector in pb-pb collisions at the lhc. the measurement was performed at forward rapidity ($2.5 &lt; y &lt; 4 $) down to zero transverse momentum ($p_{\rm t}$) in the dimuon decay channel. inclusive j/$\psi$ yields were extracted in different centrality classes and the centrality dependence of the average $p_{\rm t}$ is presented. the j/$\psi$ suppression, quantified with the nuclear modification factor ($r_{\rm aa}$), was studied as a function of centrality, transverse momentum and rapidity. comparisons with similar measurements at lower collision energy and theoretical models indicate that the j/$\psi$ production is the result of an interplay between color screening and recombination mechanisms in a deconfined partonic medium, or at its hadronization. results on the $\psi(2s)$ suppression are provided via the ratio of $\psi(2s)$ over j/$\psi$ measured in pp and pb-pb collisions.
location of distribution centers in a multi-period collaborative distribution network. this paper presents a research study which aims at determining optimal locations of regional distribution centers in a collaborative distribution network. we consider a multi-layered distribution system between a cluster of suppliers from a given region and several thousands customers spread over the whole country. the optimization problem consists of finding the locations of intermediate logistics facilities called regional distribution centers and assigning customers to these facilities according to one year of historical data. the distribution system combines full truckload (ftl) routes and less-than-truckload (ltl) shipments. the use of several rates for transportation, as well as the high impact of seasonality implies that, for each shipping date, the number of ftl routes and the cost of ltl shipments should be precisely evaluated. this problem is modeled as a mixed integer linear problem and used as a decision aiding tool on a real case study related to the distribution of horticultural products in france.
an adaptive large neighborhood search for a full truckload routing problem in public works. this paper presents a truck routing and scheduling problem faced by a public works company. itconsists of optimizing the collection and delivery of materials between sites, using a heterogeneousfleet of vehicles. these flows of materials arise in levelling works and construction of roads networks.as the quantity of demands usually exceeds the capacity of a truck, several trucks are needed tofulfill them. as a result, demands are split into full truckloads. a set of trucks routes are needed toserve a set of demands sharing a set of resources, available at pickup or delivery sites, which can beloaders or asphalt finishers in our application cases. thus, these routes need to be synchronized ateach resource. we propose an adaptive large neighborhood search (alns) to solve this problem.this approach is evaluated on real instances from a public work company in france.
centrality dependence of high-$p_{\rm t}$ d meson suppression in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the nuclear modification factor, $r_{\rm aa}$, of the prompt charmed mesons ${\rm d^0}$, ${\rm d^+}$ and ${\rm d^{*+}}$, and their antiparticles, was measured with the alice detector in pb-pb collisions at a centre-of-mass energy $\sqrt{s_{\rm nn}} = 2.76$ tev in two transverse momentum intervals, $5&lt;p_{\rm t}&lt;8$ gev/$c$ and $8&lt;p_{\rm t}&lt;16$ gev/$c$, and in six collision centrality classes. the $r_{\rm aa}$ shows a maximum suppression of a factor of 5-6 in the 10% most central collisions. the suppression and its centrality dependence are compatible within uncertainties with those of charged pions. a comparison with the $r_{\rm aa}$ of non-prompt ${\rm j}/\psi$ from b meson decays, measured by the cms collaboration, hints at a larger suppression of d mesons in the most central collisions.
molecular modeling of the effects of 40ar recoil in illite particles on their k–ar isotope dating. null
xemis: a liquid xenon detector for medical imaging. a new medical imaging technique based on the precise 3d location of a radioactive source by the simultaneous detection of 3 gamma rays has been proposed by subatech laboratory. to take advantage of this novel technique a detection device based on a liquid xenon compton telescope and a specific (beta(+), gamma) emitter radionuclide, sc-44, are required. a first prototype of a liquid xenon time projection chamber called xemis1 has been successfully developed showing very promising results for the energy and spatial resolutions for the ionization signal in liquid xenon, thanks to an advanced cryogenics system, which has contributed to a high liquid xenon purity with a very good stability and an ultra-low noise front-end electronics (below 100 electrons) operating at liquid xenon temperature. the very positive results obtained with xemis1 have led to the development of a second prototype for small animal imaging. xemis2, which is now under development. to study the feasibility of the 3 gamma imaging technique and optimize the characteristics of the device, a complete monte carlo simulation has been also carried out. a preliminary study shows very positive results for the sensitivity, energy and spatial resolutions of xemis2. (c) 2014 elsevier b.v. all rights reserved.
crystallographic studies of [nife]-hydrogenase mutants: towards consensus structures for the elusive unready oxidized states. catalytically inactive oxidized o2-sensitive [nife]-hydrogenases are characterized by a mixture of the paramagnetic ni-a and ni-b states. upon o2 exposure, enzymes in a partially reduced state preferentially form the unready ni-a state. because partial o2 reduction should generate a peroxide intermediate, this species was previously assigned to the elongated ni-fe bridging electron density observed for preparations of [nife]-hydrogenases known to contain the ni-a state. however, this proposition has been challenged based on the stability of this state to uv light exposure and the possibility of generating it anaerobically under either chemical or electrochemical oxidizing conditions. consequently, we have considered alternative structures for the ni-a species including oxidation of thiolate ligands to either sulfenate or sulfenic acid. here, we report both new and revised [nife]-hydrogenases structures and conclude, taking into account corresponding characterizations by fourier transform infrared spectroscopy (ftir), that the ni-a species contains oxidized cysteine and bridging hydroxide ligands instead of the peroxide ligand we proposed earlier. our analysis was rendered difficult by the typical formation of mixtures of unready oxidized states that, furthermore, can be reduced by x-ray induced photoelectrons. the present study could be carried out thanks to the use of desulfovibrio fructosovorans [nife]-hydrogenase mutants with special properties. in addition to the ni-a state, crystallographic results are also reported for two diamagnetic unready states, allowing the proposal of a revised oxidized inactive ni-su model and a new structure characterized by a persulfide ion that is assigned to an ni-'sox' species.
crystal structure of hydg from carboxydothermus hydrogenoformans: a trifunctional [fefe]-hydrogenase maturase. the structure of the radical s-adenosyl-l-methionine (sam) [fefe]-hydrogenase maturase hydg involved in cn(-) /co synthesis is characterized by two internal tunnels connecting its tyrosine-binding pocket with the external medium and the c-terminal fe4 s4 cluster-containing region. a comparison with a tryptophan-bound nosl structure suggests that substrate binding causes the closing of the first tunnel and, along with mutagenesis studies, that tyrosine binds to hydg with its amino group well positioned for h-abstraction by sam. in this orientation the dehydroglycine (dhg) fragment caused by tyrosine cα-cβ bond scission can readily migrate through the second tunnel towards the c-terminal domain where both cn(-) and co are synthesized. our hydg structure appears to be in a relaxed state with its c-terminal cluster cysx2 cysx22 cys motif exposed to solvent. a rotation of this domain coupled to fe4 s4 cluster assembly would bury its putatively reactive unique fe ion thereby allowing it to interact with dhg.
[nife]-hydrogenases revisited: nickel-carboxamido bond formation in a variant with accrued o2-tolerance and a tentative re-interpretation of ni-si states. [nife]-hydrogenases are well-studied enzymes capable of oxidizing molecular hydrogen and reducing protons. epr and ftir spectroscopic studies have shown that these enzymes can be isolated in several redox states that include paramagnetic oxidized inactive ni-a and ni-b species and a reduced ni-c form. the latter and the diamagnetic respectively more oxidized ni-si and more reduced ni-r forms are generally thought to be involved in the catalytic cycle of [nife]-hydrogenases. with the exception of ni-si, these different stable states have been well characterized. here, based on the crystal structure of a partially reduced desulfovibrio fructosovorans (df) enzyme and data from the literature we propose that at least one of the ni-si sub-states contains an unexpected combination of hydride and sulfenic acid moieties. we have also determined the structure of the less oxygen-sensitive df [nife]-hydrogenase v74c mutant and found that more than half of the active site nickel occupies a novel position, called ni'. in this new position, the metal ion is coordinated by two cysteine thiolates, a bridging species modeled as sh(-) and a main chain carboxamido n atom. the ni' coordination is similar to the one found in ni superoxide dismutase, an enzyme that operates at significantly more positive potentials than [nife]-hydrogenases. we propose that the oxygen-tolerance of the v74c variant results from a high potential stabilization of a ni'(iii) species induced by the change in the metal ion coordination sphere. we also propose that transient ni'(iii) species can rapidly attract successive electrons from the fe4s4 proximal cluster accelerating the reduction of oxygen to water and hydroxide. the naturally occurring oxygen-tolerant [nife]-hydrogenases have an unusual proximal cluster that has been shown to be exceptionally plastic and capable of undergoing two successive one-electron oxidations. this double oxidation is modulated by the migration of one of the iron atoms in the cluster to the main chain where, as fe(iii), it forms a bond with a carboxamido n ligand. like in the df v74c variant the electrons from the proximal cluster help reducing o2 to h2o and oh(-). in conclusion, in both cases a metal-carboxamido bond may explain, at least partially, the observed oxygen tolerance.
an adaptive large neighborhood search for a vehicle routing problem with cross-dock under dock resource constraints. in this work, we study the impact of dock resource constraints on the cost of vrpcd solutions.
virtual machine introspection: techniques and applications. null
efficient handling of synchronization in three vehicle routing problems. this paper presents a synthesis of contributions to the solving of three vehicle routing problemsinvolving synchronization constraints. these problems are: the pickup and delivery problem withtransfers (pdpt), studied during the phd of renaud masson, co-advised with olivier péton [4], thetwo-echelon multiple-trip vehicle routing problem with satellite synchronization (2e-mtvrpss),studied by philippe grangier during his phd, co-advised with michel gendreau and louis-martin rousseau [2], and the heterogeneous full truckload pickup and delivery problem with timewindows and resource synchronization(hftpdptw-rs), under study by axel grimault in hisphd and co-advised with nathalie bostel [3]. all these problems have been solved with an adaptivelarge neighborhood search (alns).a special focus is given to the temporal feasibility evaluation of an insertion which has beenproposed for the pdpt [5] and extended to the other problems. the concept of forward time slack[6] is extended to provide a constant time feasibility test of temporal constraints. experimentsconfirm the solving time reduction provided by the implementation of this test in a meta-heuristic.
reconstruction of the parameters of cosmic ray induced extensive air showers using radio detection and simulation. null
analysis of the quark-gluon plasma by heavy quarks. null
multi-scale and multi-frequency studies of cosmic ray air shower radio signals at the codalema site. since 2003, the nan\c cay radio observatory hosts the codalema experiment, dedicated to the radio detection of cosmic ray induced extensive air showers. after several instrumental upgrades, codalema is now composed of:\begin{itemize}\item{57 self-triggering radio detection stations working in the 20-250~mhz band, spread over 1 km$^2$;}\item{an array of 13 scintillators acting as a particle detector;}\item{a compact array of 10 cabled antennas, triggered by the particle detector, to test the capabilities of a phased antenna cluster to cleverly select air shower events.}\end{itemize}in addition, codalema supports the extasis project, aiming at detecting the low-frequency signal produced by the sudden deceleration of the air shower particles hitting the ground. beside these dedicated arrays, the nan\c cay site will host the nenufar radio telescope (recognized as a ska pathfinder), made of 1824 dual crossed-polarization antennas similar to the codalema ones. all these arrays present different antenna density and extent, and could be operated in a joint mode to record simultaneously the radio signal coming from an air shower. therefore, the upgraded codalema facilities could offer a complete description of the air shower induced electric field at small, medium and large scale, and over an unique and very wide frequency band (from $\sim2$ to $\sim250$~mhz). the use of multi-band detectors combined with composite trigger algorithms could help boosting the radio detection technique as a candidate for a further very large cosmic ray observatory, or in the frame of a large radio telescope such as ska. we describe the current instrumental set-up and the last results obtained, together with the prospective developments of the radio detection technique.
estimation of driver distraction using the prediction error of a cybernetic driver model. null
identification of a linear parameter varying driver model for the detection of distraction. null
broadband active noise control design through nonsmooth hinf synthesis. null
broadband active noise control design through nonsmooth hinfinity synthesis. this paper deals with active control of a broadband noise in a car cabine. it aims to study the achievable performance of such control in the siso feedback case. the main limitations involved, known as waterbed effect, are critical for such problem due to the presence of non-minimum phase zeros. to evaluate the intrinsic limitations due to these non-minimum phase zeros, a multi-objective control synthesis is proposed, allowing to cope with classical specifications (performance and robustness), without pessimism. the control synthesis is based on a h1 criterion to be minimized under some decoupled constraints. it consists in a non-convex and non-smooth optimization problem, for which a local optimum may be efficiently obtained. a particular control structure is considered in order to reduce the number of decision variables, to set relevant bounds on these parameters and to choose appropriateinitial conditions. then the optimization problem is solved using recent results on non-smooth optimization. the whole design process is detailed, including the identification of the synthesis model. the control strategy is then applied to an instrumented cavity, which shares most of the acoustic characteristics of a car cabin. finally, the analysis of the results gives clear conclusions on siso feedback possibilities, and paves the way for an efficient multivariable design case.
effect of event selection on jetlike correlation measurement in $d$+au collisions at $\sqrt{s_{\rm{nn}}}=200$ gev. dihadron correlations are analyzed in $\sqrt{s_{_{\rm nn}}} = 200$ gev $d$+au collisions classified by forward charged particle multiplicity and zero-degree neutral energy in the au-beam direction. it is found that the jetlike correlated yield increases with the event multiplicity. after taking into account this dependence, the non-jet contribution on the away side is minimal, leaving little room for a back-to-back ridge in these collisions.
compiling results from the count of oscimodes, analytical for light flavored mesons and from fortran programs for like (anti-)baryons. material shown in part at the poster session. null
the schrödinger-langevin equation with and without thermal fluctuations. the schrôdinger-langevin (sl) equation is considered as an effective open quantum system formalism suitable for phenomenological applications. we focus on two open issues relative to its solutions. we first show that the madelung/polar transformation of the wavefunction leads to a nonzero friction for the excited states of the quantum subsystem. we then study analytically and numerically the sl equation ability to bring a quantum subsystem to the thermal equilibrium of statistical mechanics. to do so, concepts about statistical mixed states, quantum noises and their production are discussed and a detailed analysis is carried with two kinds of noise and potential.
dynamical evolution of the chiral magnetic effect: applications to the quark-gluon plasma. we study the dynamical evolution of the so-called chiral magnetic effect in an electromagnetic conductor. to this end, we consider the coupled set of corresponding maxwell and chiral anomaly equations, and we prove that these can be derived from chiral kinetic theory. after integrating the chiral anomaly equation over space in a closed volume, it leads to a quantum conservation law of the total helicity of the system. a change in the magnetic helicity density comes together with a modification of the chiral fermion density. we study in fourier space the coupled set of anomalous equations and we obtain the dynamical evolution of the magnetic fields, magnetic helicity density, and chiral fermion imbalance. depending on the initial conditions we observe how the helicity might be transferred from the fermions to the magnetic fields, or vice versa, and find that the rate of this transfer also depends on the scale of wavelengths of the gauge fields in consideration. we then focus our attention on the quark-gluon plasma phase, and analyze the dynamical evolution of the chiral magnetic effect in a very simple toy model. we conclude that an existing chiral fermion imbalance in peripheral heavy ion collisions would affect the magnetic field dynamics, and consequently, the charge dependent correlations measured in these experiments.
heavy quark scattering and quenching in a qcd medium at finite temperature and chemical potential. the heavy quark collisional scattering on partons of the quark gluon plasma (qgp) is studied in a quantum chromodynamics medium at finite temperature and chemical potential. we evaluate the effects of finite parton masses and widths, finite temperature t, and quark chemical potential μq on the different elastic cross sections for dynamical quasiparticles (on- and off-shell particles in the qgp medium as described by the dynamical quasiparticle model “dqpm”) using the leading order born diagrams. our results show clearly the decrease of the qq and gq total elastic cross sections when the temperature and the quark chemical potential increase. these effects are amplified for finite μq at temperatures lower than the corresponding critical temperature tc(μq). using these cross sections we, furthermore, estimate the energy loss and longitudinal and transverse momentum transfers of a heavy quark propagating in a finite temperature and chemical potential medium. accordingly, we have shown that the transport properties of heavy quarks are sensitive to the temperature and chemical potential variations. our results provide some basic ingredients for the study of charm physics in heavy-ion collisions at beam energy scan at rhic and cbm experiment at fair.
flavor dependence of baryon melting temperature in effective models of qcd. we apply the three-flavor (polyakov-)nambu-jona-lasinio model to generate baryons as quark-diquark bound states using many-body techniques at finite temperature. all the baryonic states belonging to the octet and decuplet flavor representations are generated in the isospin-symmetric case. for each state we extract the melting temperature at which the baryon may decay into a quark-diquark pair. we seek for an evidence of the strangeness dependence of the baryon melting temperature as suggested by the statistical thermal models and supported by lattice-qcd results. a clear and robust signal for this claim is found, pointing to a flavor dependence of the hadronic deconfinement temperature.
tomography of the quark-gluon-plasma by charm quarks. we study charm production in ultra-relativistic heavy-ion collisions by using the parton-hadron-string dynamics (phsd) transport approach. the initial charm quarks are produced by the pythia event generator tuned to fit the transverse momentum spectrum and rapidity distribution of charm quarks from fixed-order next-to-leading logarithm (fonll) calculations. the produced charm quarks scatter in the quark-gluon plasma (qgp) with the off-shell partons whose masses and widths are given by the dynamical quasi-particle model (dqpm) which reproduces the lattice qcd equation-of-state in thermal equilibrium. the relevant cross section are calculated in a consistent way by employing the effective propagators and couplings from the dqpm. close to the critical energy density of the phase transition, the charm quarks are hadronized into $d$ mesons through coalescence and/or fragmentation depending on transverse momentum. the hadronized $d$ mesons then interact with the various hadrons in the hadronic phase with cross sections calculated in an effective lagrangian approach with heavy-quark spin symmetry. finally, the nuclear modification factor $\rm r_{aa}$ and the elliptic flow $v_2$ of $d^0$ mesons from phsd are compared with the experimental data from the star collaboration for au+au collisions at $\sqrt{s_{\rm nn}}$ =200 gev. we find that in the phsd the energy loss of $d$ mesons at high $p_t$ can be dominantly attributed to partonic scattering while the actual shape of $\rm r_{aa}$ versus $p_t$ reflects the heavy quark hadronization scenario, i.e. coalescence versus fragmentation. also the hadronic rescattering is important for the $\rm r_{aa}$ at low $p_t$ and enhances the $d$-meson elliptic flow $v_2$.
total absorption spectroscopy study of $^{92}$rb decay: a major contributor to reactor antineutrino flux. the antineutrino spectra measured in recent experiments at reactors are inconsistent with calculations based on the conversion of integral beta spectra recorded at the ill reactor. $^{92}$rb makes the dominant contribution to the reactor spectrum in the 5-8 mev range but its decay properties are in question. we have studied $^{92}$rb decay with total absorption spectroscopy. previously unobserved beta feeding was seen in the 4.5-5.5 region and the gs to gs feeding was found to be 87.5(25)%. the impact on the reactor antineutrino spectra calculated with the summation method is shown and discussed.
quarkonium suppression from coherent energy loss in fixed-target experiments using lhc beams. quarkonium production in proton-nucleus collisions is a powerful tool to disentangle cold nuclear matter effects. a model based on coherent energy loss is able to explain the available quarkonium suppression data in a broad range of rapidities, from fixed-target to collider energies, suggesting cold energy loss to be the dominant effect in quarkonium suppression in p-a collisions. this could be further tested in a high-energy fixed-target experiment using a proton or nucleus beam. the nuclear modification factors of j/$\psi$ and $\upsilon$ as a function of rapidity are computed in p-a collisions at $\sqrt{s}=114.6$ gev, and in p-pb and pb-pb collisions at $\sqrt{s}=72$ gev. these center-of-mass energies correspond to the collision on fixed-target nuclei of 7 tev protons and 2.76 tev lead nuclei available at the lhc.
scintillation efficiency of liquid argon in low energy neutron-argon scattering. experiments searching for weak interacting massive particles with noble gases such as liquid argon require very low detection thresholds for nuclear recoils. a determination of the scintillation efficiency is crucial to quantify the response of the detector at low energy. we report the results obtained with a small liquid argon cell using a monoenergetic neutron beam produced by a deuterium-deuterium fusion source. the light yield relative to electrons was measured for six argon recoil energies between 11 and 120 kev at zero electric drift field.
enhanced gamma-ray emission from neutron unbound states populated in beta decay. total absorption spectroscopy was used to investigate the beta-decay intensity to states above the neutron separation energy followed by gamma-ray emission in 87,88br and 94rb. accurate results were obtained thanks to a careful control of systematic errors. an unexpectedly large gamma intensity was observed in all three cases extending well beyond the excitation energy region where neutron penetration is hindered by low neutron energy. the gamma branching as a function of excitation energy was compared to hauser-feshbach model calculations. for 87br and 88br the gamma branching reaches 57% and 20% respectively, and could be explained as a nuclear structure effect. some of the states populated in the daughter can only decay through the emission of a large orbital angular momentum neutron with a strongly reduced barrier penetrability. in the case of neutron-rich 94rb the observed 4.5% branching is much larger than the calculations performed with standard nuclear statistical model parameters, even after proper correction for fluctuation effects on individual transition widths. the difference can be reconciled introducing an enhancement of one order-of-magnitude in the photon strength to neutron strength ratio. an increase in the photon strength function of such magnitude for very neutron-rich nuclei, if it proved to be correct, leads to a similar increase in the (n,gamma) cross section that would have an impact on r-process abundance calculations.
measurement of jet quenching with semi-inclusive hadron-jet distributions in central pb-pb collisions at ${\sqrt{\bf{s}_{\mathrm {\bf{nn}}}}}$ = 2.76 tev. we report the measurement of a new observable of jet quenching in central pb-pb collisions at $\sqrt{s_{\rm nn}} = 2.76$ tev, based on the semi-inclusive rate of charged jets recoiling from a high transverse momentum (high-$p_{\rm t}$) charged hadron trigger. jets are measured using collinear-safe jet reconstruction with infrared cutoff for jet constituents of 0.15 gev/$c$, for jet resolution parameters $r = 0.2$, 0.4 and 0.5. underlying event background is corrected at the event-ensemble level, without imposing bias on the jet population. recoil jet spectra are reported in the range $20&lt;p_\mathrm{t,jet}^\mathrm{ch}&lt;100$ gev/$c$. reference distributions for pp collisions at $\sqrt{s} = 2.76$ tev are calculated using monte carlo and nlo pqcd methods, which are validated by comparing with measurements in pp collisions at $\sqrt{s} = 7$ tev. the recoil jet yield in central pb-pb collisions is found to be suppressed relative to that in pp collisions. no significant medium-induced broadening of the intra-jet energy profile is observed within 0.5 radians relative to the recoil jet axis. the angular distribution of the recoil jet yield relative to the trigger axis is found to be similar in central pb-pb and pp collisions, with no significant medium-induced acoplanarity observed. large-angle jet deflection, which may provide a direct probe of the nature of the quasi-particles in hot qcd matter, is explored.
delta scaling: how resources scalability/termination can be taken place economically?. cloud computing promises to completely revolutionize the capacity management of resources. the elasticityand the economy of scale are the intrinsic elements that differentiate it from traditional computing paradigm. a good capacity planning method is a necessary factor but not sufficient to fully exploit cloud elasticity. this paper proposes innovative policies for resource management to achieve the optimal balance between capacity and quality of cloud services while supporting cloud technical and conceptual limitations. the main idea is to control finely the scalability and the termination of virtual machines in regards of several criteria such as the lifecycle of the instances (e.g. initialization time) or their cost. the approach was evaluated with a real infrastructure (amazon ec2) and an application testbed. experimental results illustrate the soundness of the proposed approach and the impact of scalability/termination resource policies. using deltascaling, the cost saving of as much as 30% can be achieved while causing the minimum number of violations, as small as 1%.
sla guarantees for cloud services. quality-of-service and sla guarantees are among the major challenges of cloud-based services. in this paper we first present a new cloud model called slaaas — sla aware service. slaaas considers qos levels and sla as first class citizens of cloud-based services. this model is orthogonal to other saas, paas, and iaas cloud models, and may apply to any of them. more specifically we make three contributions: (i) we provide a novel domain specific language that allows to describe qos-oriented sla associated with cloud services; (ii) we present a general control-theoretic approach for managing cloud service sla; (iii) we apply the proposed language and control approach to guarantee sla in various case studies, ranging from cloud-based mapreduce service, to locking service, and higher-level e-commerce service; these case studies successfully illustrate sla management with different qos aspects of cloud services such as performance, dependability, financial energetic costs.
probing the surface reactivity of mineral phases constituting the callovo-oxfordian argilite by isotope exchange method. null
impact of the radiolysis in the repository disposal. null
investigation of astatine chemistry in solution. null
extraction behavior of polonium-210 from hcl and hno3 solution using tributyl phosphate. null
characterization of at- and ato+ species in simple media by high performance ion exchange chromatography coupled to gamma detector. application to astatine speciation in human serum. null
a new route for polonium-210 production from a bismuth-209 target. null
vmplaces: a generic tool to investigate and compare vm placement algorithms. advanced virtual machines placement policies are evaluated either using limited scale in-vivo experiments or ad hoc simulator techniques. these validation methodologies are unsatisfactory. first they do not model precisely enough real production platforms (size, workload representativeness, etc.). second, they do not enable the fair comparison of different approaches.to resolve these issues, we propose vmplaces, a dedicated simulation framework to perform in-depth investigations and fair comparisons of vm placement algorithms. built on top of simgrid, our framework provides programming support to ease the implementation of placement algorithms and runtime support dedicated to load injection and execution trace analysis. it supports a large set of parameters enabling researchers to design simulations representative of a large space of real-world scenarios. we also report on a comparison using vmplaces of three classes of placement algorithms: centralized, hierarchical and fully-distributed ones.
components of the performance assessment: impact of solubility uncertainties as an example. null
probing the slow processes at the interface solid/water by the isotopic exchange method. null
influence of slow processes close to equilibrium on the fate of rn released in the environment. null
long-term fate and transport of fission products and actinides in geosphere. null
slow processes in close-to-equilibrium conditions for radionuclides in water/solid systems of relevance to nuclear waste management. null
behaviour of carbon in zircaloy and  zirconium in solution. null
thorium oxide solubility behavior vs. the surface crystalline state. null
slow processes in close-to-equilibrium conditions for radionuclides in water/solid systems of relevance to nuclear waste management – skin. null
2nd annual workshop proceedings - 7th ec fp - skin. null
sensorscript: a domain-specific language for sensor networks. null
a neural network approach for burn-up calculation and its application to the dynamic fuel cycle code class. dynamic fuel cycle simulation tools calculate nuclei inventories and mass flows evolution in an entire fuel cycle, from the mine to the final disposal. usually, the fuel depletion in reactor is handled by a fuel loading model and a mean cross section predictor. in the case of a pwr–mox, a fuel loading model provides from a plutonium stock the plutonium fraction in the fresh fuel needed to reach a specific burnup. a mean cross section predictor aims to assess isotopic cross sections required for building bateman equations for any fresh fuel composition with a sufficient accuracy and a reasonable computing time. this paper presents a methodology based on neural networks for building a fuel loading model and a cross section predictor for a pwr reactor loaded with mox fuel. the mean error of the plutonium content prediction from the fuel loading model is 0.37%. furthermore, the mean cross section predictor allows completion of the fuel depletion calculation in less than one minute with excellent accuracy. a maximum deviation of 3% on main nuclei is obtained at the end of cycle between inventories calculated from neural networks and from the reference coupled neutron transport/fuel depletion calculation.
modèles et méthodes d'optimisation combinatoire pour la conception de chaînes logistiques et l'optimisation des transports. null
exploration of the metallic character of astatine. null
a new route of production of polonium-210. null
actinide separations using ionic exchange and extraction chromatography resins. null
synthesis of multiple sensitivity constrained controllers for parametric uncertain lti systems. the purpose of this paper is to propose a synthesis method of multiple parametric sensitivity constrained linear quadratic (slq) controllers for a parametric uncertain lti system. system sensitivity to parameter variation, for each controller, is handled through an additional quadratic trajectory parametric sensitivity term in the criterion to be minimized. the controllers are supposed to cover the whole parametric uncertainty while degrading as less as possible the intrinsic robustness properties of each local linear quadratic controller. in that context, it is difficult to ensure the global optimality. hence an efficient particle swarm optimization (pso) based algorithm is provided to find the best partition of the uncertainty set as well as the set of slq controllers.
mixed matrix sign function/dft inversion method for solving parameterdependent riccati equation. this paper proposes a tractable iterative scheme for computing parameter-dependent matrix sign function. it relies upon two results: (i) a fraction expansion of the matrix sign principal padé rational approximation, (ii) a discrete fourier transform (dft) inversion method for polynomial parameter-dependent matrices. the method is used for solving parameter-dependent riccati equations. some illustrative examples, given throughout the paper, demonstrate the effectiveness of the method. an application for extracting the harmonics of current or voltage waveforms confirms the validity of the approach. a realistic control application dealing with a parameter dependent lqr design problem for an airfoil flutter is also presented.
a parametric sensitivity constrained linear quadratic controller. the purpose of this paper is to give a new insight on a suboptimal linear quadratic control taking explicitly into account the parametric uncertainties. system sensitivity to parameter variations is handled through including a quadratic trajectory parametric sensitivity term in the cost functional to be minimized. the paper main contribution is twofold: - using a descriptor system approach, the paper shows that the underlying singular linear-quadratic optimal control problem leads to a non-standard riccati equation. - a solution to the proposed control problem is given based on a connection to the so-called lur'e matrix equations. some examples are given in order to illustrate the interest of the approach.
particle swarm optimization for the multi-level lot sizing. null
tactical planning for public construction sector. null
integration of maintenance in the tactical production planning process under feasibility constraint. this paper deals with the problem of the joint optimization of the master production schedule and maintenance strategy for a manufacturing system. an efficient production planning and maintenance policy will allow to minimize the impacts of the potential random failures and will let the plan to be feasible. we present a modelisation where we take into account a feasibility constraint; the optimization problem is formulated as a linear program. we propose a heuristic algorithm to solve it and we show the impact of the feasibility constraint on different criteria.
planification conjointe de la production et de la maintenance sous contrainte de faisabilité : cas multi-produits. null
dynamic cluster in particle swarm optimization algorithm. null
search for dark photons from neutral meson decays in p+p and d+au collisions at snn−−−−√=200 gev. the standard model (sm) of particle physics is spectacularly successful, yet the measured value of the muon anomalous magnetic moment (g−2)μ deviates from sm calculations by 3.6σ. several theoretical models attribute this to the existence of a “dark photon,” an additional u(1) gauge boson, which is weakly coupled to ordinary photons. the phenix experiment at the relativistic heavy ion collider has searched for a dark photon, u, in π0,η→γe+e− decays and obtained upper limits of o(2×10−6) on u−γ mixing at 90% c.l. for the mass range 30&lt;mu&lt;90 mev/c2. combined with other experimental limits, the remaining region in the u−γ mixing parameter space that can explain the (g−2)μ deviation from its sm value is nearly completely excluded at the 90% confidence level, with only a small region of 29&lt;mu&lt;32 mev/c2 remaining.
integration approaches of forecasting methods selection with inventory management indicators in the case of spare parts supply chain. null
joint optimization of a master production schedule and a preventive maintenance policy. null
a model and bi-objective solution technique for green sustainable supply chain network design. null
coupled transport and chemistry in clay stone studied by advective displacement. experiments and model . null
influence of clay content on hto and 36cl transport properties in callovo-oxfordian clayrock : percolation experiments and modelling. null
collaborative distribution: from the network design to an operational load plan. null
a column generation approach for a pooled network design problem with vehicle management constraints and piecewise linear cost structures. null
multi-directional local search for a sustainable supply chain network design model. 1. the problem consideredthe increasing importance of environmental issues has prompted decisionmakersto incorporate environmental factors into supply chain networkdesign (scnd) models. we propose a bi-objective scnd model to minimizetwo conflicting objectives: the total cost and the environmental impactexpressed by co2 emissions.the logistics network consists of four layers: suppliers, plants, distributioncenters (dcs) and customers. the model considers several possibletransportations modes in the network, each transportation mode havinga lower and upper capacity limitation. moreover, we consider differentcandidate technology levels at the plants and dcs. each technology representsa type of service with associated fixed and variable costs and co2emissions. a higher-level technology may reduce carbon emissions, but islikely to require more investment cost.the model considers co2 emissions caused by all industrial and logisticsoperations as well as transportation. the main issues to be addressedin the sustainable scnd model includes determining the number, location,and technology level at plants and dcs, suitable transportation mode, andproduct flows between facilities.2. solution methodwe solve the corresponding bi-objective mixed integer linear programmingmodel with the multi-directional local search (mdls) framework. the efficiency of this recent framework has been proved on the multiobjectiveknapsack, set packing and orienteering problems, but to the bestof our knowledge, this is the first attempt to solve a facility location problemwith it. the mdls is based on the principle of separately using independentsingle-objective local searches to iteratively improve the pareto setapproximation. the motivation for using this framework is the capabilityof using already implemented single objective optimization components.in our case, we use a large neighborhood search algorithm as single objectivemethod. our algorithm can be decomposed in the three followingsteps:phase 1: look for an initial pareto set approximation. the initialphase of the single objective lns is executed separately for each objective.the output is an initial pareto set approximation.phase 2: intensification around the pareto set approximation. thepareto set approximation is improved by exploring the neighborhoodof all the solutions in this set with a multi-directional local search.phase 3: optimization of product flows. after stabilizing the locationand transportation mode decisions for all pareto set approximationsolutions in phase 2, we determine the optimal product flows by applyingthe simplex algorithm to all solutions in the set.3. computational resultswe assess the performance of our approach through a comparison withthe well-known "-constraint method. in particular, we analyze the paretofronts given by both solutions on a set of 60 generated instances and showthat the efficiency of our approach improves when the instance size grows.
sustainable supply chain network design: an optimization-oriented review. supply chain network design (scnd) models and methods have been the subject of severalrecent literature review surveys, but none of them explicitly includes sustainable development as amain characteristic of the problem considered. the aim of this review is to bridge this gap. the paperanalyzes 86 papers in the field of supply chain network design, covering mathematical models thatinclude economic factors as well as environmental and/or social dimensions.the review is organizedalong four research questions asking i) which environmental and social objectives are included, ii) how are they integrated into the models, iii) which methods and tool are used and finally iv) which industrial applications and contexts are covered in these models.the review finds that there are a number oflimitations to the current research in sustainable scnd. the narrow scope of environmental and socialmeasures in current models should go beyond limited greenhouse gas indicators to broader life-cycleapproaches including new social metrics. the more effective inclusion of uncertainty and risk inmodels with improved multi-objective approaches is also needed. there are also significant gaps in thesectors used to test models limiting more general applicability. the paper concludes with promisingnew avenues of research to more effectively include sustainability into scnd models.
study of the surface reactivity of clay mineral phases by isotope exchange. null
determination of dissolution and precipitation rates of clayey materials by 29si/28si isotopic exchange. effect of temperature. null
key factors to understand in-situ behavior of cs in callovo-oxfordian clay rock. understanding the behavior of 137cs and 135cs in soils and geological formations is of considerable interest in the context of nuclear accidents and nuclear waste repositories. although the clay fraction is known to be responsible for sorption, there are still unanswered questions raised by the literature data concerning (i) the reversibility of the sorption process(es), (ii) the validity of the additivity rule (the overall distribution coefficient (kd) for a radionuclide on a mixture of minerals is predicted from the distribution coefficients measured on individual minerals) and (iii) the validity of model transposition from dispersed systems to consolidated/intact systems. because of these uncertainties, the validity of sorption models at equilibrium under in-situ conditions and for very long-term interaction is still pending. these different issues are studied in the present work for the callovo–oxfordian (cox) clay-rich rock formation, which is under investigation in france as a geological barrier for a long-term nuclear waste repository. the work is based on sorption data measured on thirteen samples of different mineralogy taken from five different boreholes at several depths within the cox sedimentary layer. to our knowledge, it is the most extended cs sorption dataset that has been published for a single clay formation in term of (i) sample locations (and thus natural variability), (ii) sorption conditions (powder dispersed in suspension, compacted powders and intact samples) and (iii) equilibration time (from one week to five years). moreover, for the first time ever, radioactive cs sorption results were compared to the natural distribution of non-radioactive cs isotopes between pore water and the solid phase. the experimental system appeared to be in chemical equilibrium as much as can be expected for an ion-exchange reaction. more particularly, no kinetically-controlled process leading to partial cs irreversibility was observed, in contrary to what was found in the literature for soils. this difference in behavior may be related to the difference in the illite studied, i.e. a soil-type illite which would be more altered than a sedimentary formation-type illite. no decrease in site capacity was observed between dispersed and intact/compacted states. a model based on exchange reactions with cations interacting with illite (frayed edge, type-ii and planar sites) and mixed layer illite–smectite (i/s) (planar sites) using parameters published in the literature enabled the kd variation to be described as a function of cs concentration, the mineralogy of the samples, the change in water composition and the temperature (22–80 °c). our study clearly demonstrates that no frayed edge sites should be considered on the illite fraction of i/s, thus emphasizing the difference of sorption properties between an i/s mixed layer mineral and a corresponding mechanical mix of illite and smectite minerals. the robustness of the model was confirmed by data analysis describing the behavior of naturally-occurring cs in the formation thereby demonstrating the effectiveness of the cs sorption processes in a very long-time period prospective. lastly, the model was used to predict the sorption of trace concentrations of cs in the cox formation on the time-scale relevant for nuclear waste disposal performance assessment. as expected, the retention was significant with kd values ranging from 100 to 2000 l/kg whatever the conditions that were probed and a simulation covering a period of over 105 years could show that the cox formation is an efficient barrier to prevent cs transport from the storage facility to the surrounding environment.
effect of callovo-oxfordian clay rock on the dissolution rate of the son68 simulated nuclear waste glass. long-term storage of high-level nuclear waste glass in france is expected to occur in an engineered barrier system (ebs) located in a subsurface callovo-oxfordian (cox) clay rock formation in the paris basin in northeastern france. understanding the behavior of glass dissolution in the complex system is critical to be able to reliably model the performance of the glass in this complex environment. to simulate this multi-barrier repository scenario in the laboratory, several tests have been performed to measure glass dissolution rates of the simulated high-level nuclear waste glass, son68, in the presence of cox claystone at 90 °c. experiments utilized a high-performance liquid chromatography (hplc) pump to pass simulated bure site cox pore water through a reaction cell containing son68 placed between two cox claystone cores for durations up to 200 days. silicon concentrations at the outlet were similar in all experiments, even the blank experiment with only the cox claystone (∼4 mg/l at 25 °c and ∼15 mg/l at 90 °c). the steady-state ph of the effluent, measured at room temperature, was roughly 7.1 for the blank and 7.3–7.6 for the glass-containing experiments demonstrating the ph buffering capacity of the cox claystone. dissolution rates for son68 in the presence of the claystone were elevated compared to those obtained from flow-through experiments conducted with son68 without claystone in silica-saturated solutions at the same temperature and similar ph values. additionally, through surface examination of the monoliths, the side of the monolith in direct contact with the claystone was seen to have a corrosion thickness 2.5× greater than the side in contact with the bulk glass powder. results from one experiment containing 32si-doped son68 also suggest that the movement of si through the claystone is controlled by a chemically coupled transport with a si retention factor, kd, of 900 ml/g.
235u/238u isotope ratio analysis by la-icp-ms-hr for environmental radioactivity monitoring. null
complexity of a one-to-one meeting scheduling problem with two types of actors. this article deals with a new scheduling problem that arises in the organization of one-to-one meetings in parallel.we first introduce applications of one-to-one meeting scheduling problems. then, we give an overview of therelevant complexity results. we then prove several complexity results for different versions of the problem.
a note on np-hardness of preemptive mean flow-time scheduling for parallel machines. in the paper “the complexity of mean flow time scheduling problems with release times”, by baptiste, brucker, chrobak, dürr, kravchenko and sourd, the authors claimed to prove strong np -hardness of the scheduling problem p|pmtn,rj|∑cj , namely multiprocessor preemptive scheduling where the objective is to minimize the mean flow time. we point out a serious error in their proof and give a new proof of strong np -hardness for this problem.
monitoring de l’utilisation mémoire et cpu par processus, basé sur l’introspection de machines virtuelles. null
sensorscript : un langage de requête dédié, orienté métiers, pour les réseaux de capteurs. null
phytoextraction associated with bioaugmentaton of contaminated soils by caesium. null
anthropogenic tritium in the loire river estuary. null
microbial mobilization of cesium from illite: role of organic acids and siderophores. null
investigation of the  ato(oh)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt; hydrolysed species: relativistic calculations. null
investigating ato&lt;sup&gt;+&lt;/sup&gt;-(oh&lt;sup&gt;−&lt;/sup&gt;)&lt;sub&gt;n&lt;/sub&gt; complexes at the molecular scale using quantum mechanical methods. null
theoretical investigation of the ato&lt;sup&gt;+&lt;/sup&gt; hydrolyzed species in ligand-exchange reactions including solvation effects. null
behavior of titanium alloys (t40, t64) vs. radiolytic corrosion under 4he2+ irradiation. null
radiolytic corrosion of titanium alloys under 4he2+/γ-ray irradiation. null
radiolytic corrosion occurring at the solid/solution interface investigated at subatech: example of uranium. null
first observation of cp violation in b0-&gt;d(*)cp h0 decays by a combined time-dependent analysis of babar and belle data. we report a measurement of the time-dependent cp asymmetry of b0-&gt;d(*)cp h0 decays, where the light neutral hadron h0 is a pi0, eta or omega meson, and the neutral d meson is reconstructed in the cp eigenstates k+ k-, k0s pi0 or k0s omega. the measurement is performed combining the final data samples collected at the y(4s) resonance by the babar and belle experiments at the asymmetric-energy b factories pep-ii at slac and kekb at kek, respectively. the data samples contain ( 471 +/- 3 ) x 10^6 bb pairs recorded by the babar detector and ( 772 +/- 11 ) x 10^6, bb pairs recorded by the belle detector. we measure the cp asymmetry parameters -eta_f s = +0.66 +/- 0.10 (stat.) +/- 0.06 (syst.) and c = -0.02 +/- 0.07 (stat.) +/- 0.03 (syst.). these results correspond to the first observation of cp violation in b0-&gt;d(*)cp h0 decays. the hypothesis of no mixing-induced cp violation is excluded in these decays at the level of 5.4 standard deviations.
bayesian  networks  to  quantify  transition  rates  in  degradation  modeling:  application  to  a  set  of  steel  bridges  in  the  netherlands. &lt;p&gt;&amp;nbsp;bridge lifetime pose an important challenge in terms of maintenance for decision makers&lt;br /&gt;or asset managers. in this regard markov chains have been used successfully in practice as models for&lt;br /&gt;bridge deterioration. however, one limitation of markov chains can be the assessment of the transition&lt;br /&gt;probabilities. in this paper, we propose an approach based on bayesian networks (bns) to quantify the&lt;br /&gt;transition probabilities of the system state. one of the advantages of doing so is that the bn may be&lt;br /&gt;quantified through physical variables linked to the underlying degradation process in an intuitive way&lt;br /&gt;through&amp;nbsp; expert&amp;nbsp; judgment&amp;nbsp; combined&amp;nbsp; with&amp;nbsp; field&amp;nbsp; measurements.&amp;nbsp; in&amp;nbsp; addition,&amp;nbsp; the&amp;nbsp; possibility&amp;nbsp; of&amp;nbsp; using&lt;br /&gt;bayesian inference&amp;nbsp; allows updating the probabilities when observations become available that could&lt;br /&gt;provide different relevant views of the long-term degradation. an application to a hypothetical stock of&lt;br /&gt;steel bridges in the netherlands is presented and illustrates the method.&lt;/p&gt;.
a condition-based maintenance policy based on a probabilistic meta-model in the case of chloride-induced corrosion. &lt;p&gt;maintenance and management policies are usually focused on minimizing the life-cycle cost only. therefore the optimal solution in this context does not necessarily result in a satisfactory long-term structural performance. in this paper, we will present an approach for modeling the degradation of structures and infrastructures for maintenance purposes. the degradation is modeled using probabilistic data-driven state dependent stochastic processes, hereafter called meta-model. this work implements this degradation model into a maintenance framework and carries out two numerical examples in order to show the applicability of meta-models in a maintenance and management optimization context. this paves the road for future work on meta-model updating and maintenance optimization by considering multi-objective optimization policies.&lt;/p&gt;.
systematic study of azimuthal anisotropy in cu$+$cu and au$+$au collisions at $\sqrt{s_{_{nn}}} = 62.4$ and 200~gev. we have studied the dependence of azimuthal anisotropy $v_2$ for inclusive and identified charged hadrons in au$+$au and cu$+$cu collisions on collision energy, species, and centrality. the values of $v_2$ as a function of transverse momentum $p_t$ and centrality in au$+$au collisions at $\sqrt{s_{_{nn}}}$=200~gev and 62.4~gev are the same within uncertainties. however, in cu$+$cu collisions we observe a decrease in $v_2$ values as the collision energy is reduced from 200 to 62.4~gev. the decrease is larger in the more peripheral collisions. by examining both au$+$au and cu$+$cu collisions we find that $v_2$ depends both on eccentricity and the number of participants, $n_{\rm part}$. we observe that $v_2$ divided by eccentricity ($\varepsilon$) monotonically increases with $n_{\rm part}$ and scales as ${n_{\rm part}^{1/3}}$. the cu$+$cu data at 62.4 gev falls below the other scaled $v_{2}$ data. for identified hadrons, $v_2$ divided by the number of constituent quarks $n_q$ is independent of hadron species as a function of transverse kinetic energy $ke_t=m_t-m$ between $0.1&lt;ke_t/n_q&lt;1$~gev. combining all of the above scaling and normalizations, we observe a near-universal scaling, with the exception of the cu$+$cu data at 62.4 gev, of $v_2/(n_q\cdot\varepsilon\cdot n^{1/3}_{\rm part})$ vs $ke_t/n_q$ for all measured particles.
fusion, fission, alpha emission and superheavy element formation and decay within a generalized liquid drop model. new observed phenomena like cluster emission, cold and asymmetric fission of 252cf,  nuclear molecule formation in 24mg, asymmetric fission of intermediate mass nuclei, quasi-fission of heavy dinuclear systems and alpha emission of superheavy nuclei have renewed interest in investigating the fusion-like fission valley which leads rapidly from a quasi-spherical nucleus to quasi-molecular shapes with deep necks and to two touching quasi-spherical fragments. furthermore, the rotational super and hyperdeformed states as well as the very heavy and superheavy elements are and will be formed in the entrance channel of heavy-ion collisions for which the initial configuration is two close quasi-spherical nuclei.            the balance between the repulsive coulomb forces and attractive surface tension forces alone leads in this quasi-molecular shape valley to a potential barrier with an unrealistic  coulomb peak. a proximity energy term must be added to the usual development of the liquid drop model energy to smoothly describe the transition from one-body shapes to two-body compact shapes. this term takes into account the finite-range effects of the nuclear forces in the crevice between the nascent future fragments or the gap between the incoming nuclei. as a consequence we have developed a particular version of the liquid drop model taking into account both the nuclear proximity energy, the mass and charge asymmetry, the rotational energy, the shell and pairing effects and the temperature. a specific quasi-molecular shape sequence derived from elliptic lemniscatoids has been defined to describe within this generalized liquid drop model the entrance channel of nuclear reactions and also the peculiar decay channel through compact and creviced shapes.           in this deformation valley and within this gldm the calculated l-dependent fission and fusion barriers, alpha and cluster radioactivity half-lives and double-humped barriers and half-lives of actinides are in agreement with the available experimental data [1-4]. in this particular deformation valley, double-humped potential barriers begin to appear even macroscopically for heavy nuclei due to the influence of the proximity forces and quasi-molecular rotational isomeric states are formed at intermediate angular momentum during the fusion process of light or medium mass nuclei. [1] g. royer, nucl. phys. a 848, 279 (2010). [2] g. royer, j. gaudillot, phys. rev. c 84, 044602 (2011).[3] g. royer, m. jaffré, d. moreau, phys. rev. c 86, 044326 (2012).[4] x.j. bao, h.f. zhang, g. royer, j.q. li, nucl. phys. a 906, 1 (2013).  .
inclusive, prompt and non-prompt j/$\psi$ production at mid-rapidity in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the transverse momentum ($p_{\rm t}$) dependence of the nuclear modification factor $r_{\rm aa}$ and the centrality dependence of the average transverse momentum $\langle p_{\rm t}\rangle$ for inclusive j/$\psi$ have been measured with alice for pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev in the e$^+$e$^-$ decay channel at mid-rapidity ($|y|&lt;0.8$). the $\langle p_{\rm t}\rangle$ is significantly smaller than the one observed for pp collisions at the same centre-of-mass energy. consistently, an increase of $r_{\rm aa}$ is observed towards low $p_{\rm t}$. these observations might be indicative of a sizable contribution of charm quark coalescence to the j/$\psi$ production. additionally, the fraction of non-prompt j/$\psi$ from beauty hadron decays, $f_{\rm b}$, has been determined in the region $1.5 &lt; p_{\rm t} &lt; 10$ gev/c in three centrality intervals. no significant centrality dependence of $f_{\rm b}$ is observed. finally, the $r_{\rm aa}$ of non-prompt j/$\psi$ is discussed and compared with model predictions. the nuclear modification in the region $4.5 &lt; p_{\rm t} &lt; 10$ gev/c is found to be stronger than predicted by most models.
measurement of charm and beauty production at central rapidity versus charged-particle multiplicity in proton-proton collisions at $\mathbf{\sqrt{{\textit s}}}=7$ tev. prompt d meson and non-prompt j/$\psi$ yields are studied as a function of the multiplicity of charged particles produced in inelastic proton-proton collisions at a centre-of-mass energy of $\sqrt{s}=7$ tev. the results are reported as a ratio between yields in a given multiplicity interval normalised to the multiplicity-integrated ones (relative yields). they are shown as a function of the multiplicity of charged particles normalised to the average value for inelastic collisions (relative charged-particle multiplicity). d$^0$, d$^+$ and d$^{*+}$ mesons are measured in five $p_{\rm t}$ intervals from 1 to 20 gev/$c$ and for $|y|&lt;0.5$ via their hadronic decays. the d-meson relative yield is found to increase with increasing charged-particle multiplicity. for events with multiplicity six times higher than the average multiplicity of inelastic collisions, a yield enhancement of a factor about 15 relative to the multiplicity-integrated yield in inelastic collisions is observed. the yield enhancement is independent of transverse momentum within the uncertainties of the measurement. the d$^0$-meson relative yield is also measured as a function of the relative multiplicity at forward pseudorapidity. the non-prompt j/$\psi$, i.e. the b hadron, contribution to the inclusive j/$\psi$ production is measured in the di-electron decay channel at central rapidity. it is evaluated for $p_{\rm t}&gt;1.3$ gev/$c$ and $|y|&lt;0.9$, and extrapolated to $p_{\rm t}&gt;0$. the fraction of non-prompt j/$\psi$ in the inclusive j/$\psi$ yields shows no dependence on the charged-particle multiplicity at central rapidity. charm and beauty hadron relative yields exhibit a similar increase with increasing charged-particle multiplicity. the measurements are compared to pythia 8, epos 3 and percolation calculations.
scandium complexes : physico-chemical study and evaluation of stability in vitro and in vivo for nuclear medicine application. among the different isotopes of scandium that can be used in nuclear medicine may be mentioned the ⁴⁷sc and ⁴⁴sc. the first decays by emitting an electron associated with a 159 kev gamma can thus be used either for radiotherapy or temp imaging. the ⁴⁴sc (3.97 h) decays in 94.27% in case by emitting a positron, with a γ photon energy equal to 1.157 mev. this isotope is then an ideal candidate for applications in pet imaging. currently, the cyclotron of high energy and high intensity arronax produce ⁴⁴sc and co-produces the isomeric state the ⁴⁴msc(2.44 d). the ⁴⁴msc has properties (eᵧ = 270 kev, 98.8%), which allows to consider its use as a potential in vivo generator. previous work had demonstrated that the dota ligand is most suitable and stable for sc. this thesis aims; make in evidence the feasibility of the in vivo ⁴⁴m/⁴⁴sc generator. initially a procedure was optimized and validated for the production of ⁴⁴m/⁴⁴sc with a high specific activity and chemical purity. radiolabeling of dota conjugated peptides was then developed and optimized. theoretical and experimental studies have been performed in order to demonstrate the feasibility of ⁴⁴m/⁴⁴sc as a potential in vivo generator. finally, in vitro stability studies on radiolabeled ⁴⁴m / ⁴⁴sc complexes were performed, followed by biodistribution studies and pet imaging.
packing curved objects. this paper deals with the problem of packing two-dimensional objects of quite arbitrary shapes including in particular curved shapes (like ellipses) and assemblies of them. this problem arises in industry for the packaging and transport of bulky objects which are not individually packed into boxes, like car spare parts. there has been considerable work on packing curved objects but, most of the time, with specific shapes; one famous example being the circle packing problem. there is much less algorithm for the general case where different shapes can be mixed together. a successful approach has been proposed recently in [martinez et al., 2013] and the algorithm we propose here is an extension of their work. mar-tinez et al. use a stochastic optimization algorithm with a fitness function that gives a violation cost and equals zero when objects are all packed. their main idea is to define this function as a sum of n!/(2!*(n-2)!) elementary functions that measure the overlapping between each pair of different objects. however, these functions are ad-hoc formulas. designing ad-hoc formulas for every possible combination of object shapes can be a very tedious task, which dramatically limits the applicability of their approach. the aim of this paper is to generalize the approach by replacing the ad-hoc formulas with a numerical algorithm that automatically measures the overlapping between two objects. then, we come up with a fully black-box packing algorithm that accept any kind of objects.[martinez et al., 2013] t. martinez, l. vitorino, f. fages, and a. aggoun. on solving mixed shapes packing problems by continuous optimization with the cma evolution strategy. in proceedings of the first brics countries congress on computational intelligence, 2013.
modeling software application front-ends: introducing the open source ifml graphical editor…. front-ends are important parts of software applications. they can change and evolve from occasionally to very often depending on the type of application. thus, it appears more and more important to be able to properly specify and/or represent them (e.g. to facilitate code generation and automation). based on the new interaction flow modeling language (ifml) standard from the omg, the ifml editor (http://ifml.github.io/) provides a complete open source modeling environment for expressing the content, user interaction and control behavior of software applications front-ends (also offering bindings to the persistence and business logical layers).this short talk is going to introduce this new eclipse-based ifml editor, developed with the sirius technology, and to briefly present its main features.
sensitivity-based pole and input-output errors of linear filters as indicators of the implementation deterioration in fixed-point context. input-output or poles sensitivity is widely used to evaluate the resilience of a filter realization to coefficients quantization in an fwl implementation process. however, these measures do not exactly consider the various implementation schemes and are not accurate in general case. this paper generalizes the classical transfer function sensitivity and pole sensitivity measure, by taking into consideration the exact fixed-point representation of the coefficients. working in the general framework of the specialized implicit descriptor representation, it shows how a statistical quantization error model may be used in order to define stochastic sensitivity measures that are definitely pertinent and normalized. the general framework of mimo filters and controllers is considered. all the results are illustrated through an example.
finite wordlength controller realizations using the specialized implicit form. null
a language for the composition of privacy-enforcement techniques. today's large-scale computations, e.g., in the cloud, are subjectto a multitude of risks concerning the divulging and ownership ofprivate data. privacy risks are mainly addressed using a largevariety of encryption-based techniques. however, these are costlyto operate, lead to large aggregates of data that are highlyvaluable attack targets and do not allow to flexibly handlesubsets of such aggregates. furthermore, today's computations haveto ensure privacy properties in the context over highly variableand complex software compositions; however, no general support forthe declarative definition and implementation ofprivacy-preserving applications has been put forward.in this article, we present a compositional approach to thedeclarative and correct composition of privacy-preservingapplications in the cloud. our approach provides language supportfor the compositional definition of encryption- andfragmentation-based privacy-preserving algorithms. this languagecomes equipped with a set of laws that allows us to verify privacyproperties. finally, we introduce implementation support in scalathat ensures certain privacy properties by construction usingadvanced features of scala's type system.
an improved limit to the diffuse flux of ultra-high energy neutrinos from the pierre auger observatory. neutrinos in the cosmic ray flux with energies near 1 eev and above are detectable with the surface detector array of the pierre auger observatory. we report here on searches through auger data from 1 january 2004 until 20 june 2013. no neutrino candidates were found, yielding a limit to the diffuse flux of ultra-high energy neutrinos that challenges the waxman-bahcall bound predictions. neutrino identification is attempted using the broad time-structure of the signals expected in the sd stations, and is efficiently done for neutrinos of all flavors interacting in the atmosphere at large zenith angles, as well as for "earth-skimming" neutrino interactions in the case of tau neutrinos. in this paper the searches for downward-going neutrinos in the zenith angle bins 60∘−75∘ and 75∘−90∘ as well as for upward-going neutrinos, are combined to give a single limit. the 90% c.l. single-flavor limit to the diffuse flux of ultra-high energy neutrinos with an e−2 spectrum in the energy range 1.0×1017 ev - 2.5×1019 ev is e2νdnν/deν&lt;6.4×10−9 gev cm−2 s−1 sr−1.
a two-layer lpv based control strategy for input and state constrained problem: application to energy management. this paper proposes a pragmatic solution to solve input and state constrained control problems. taking benefits from a two-layer hierarchical architecture, in particular by working at a different period at each level, the general idea is to combine an explicit lpv controller ensuring the regulation task, and a predictive control at the upper level to comply with the active constraints.  in practice, the two layers are connected as the external loop drives the varying parameter of the inner loop. the proposed scheme is well suited mainly with control problems whose constraints violation risks may be predicted sufficiently in advance. the energy management of a hybrid vehicle is finally considered to illustrate the applicability.
map-based transparent persistence for very large models. the progressive industrial adoption of model-driven engineering (mde) is fostering the development of large tool ecosystems like the eclipse modeling project. these tools are built on top of a set of base technologies that have been primarily designed for small-scale scenarios, where models are manually developed. in particular, efficient runtime manipulation for large-scale models is an under-studied problem and this is hampering the application of mde to several industrial scenarios.in this paper we introduce and evaluate a map-based persistence model for mde tools. we use this model to build a transparent persistence layer for modeling tools, on top of a map-based database engine. the layer can be plugged into the eclipse modeling framework, lowering execution times and memory consumption levels of other existing approaches. empirical tests are performed based on a typical industrial scenario, model-driven reverse engineering, where very large software models originate from the analysis of massive code bases. the layer is freely distributed and can be immediately used for enhancing the scalability of any existing eclipse modeling tool.
optimization of a city logistics transportation system with mixed passengers and goods. in this paper, we propose a mathematical model and an adaptive large neighborhood search to solve a two{tiered transportation problem arising in the distribution of goods in congested city cores. in the first tier, goods are transported in city buses from a consolidation and distribution center to a set of bus 10 stops. the main idea is to use the buses spare capacity to drive the goods in the city core. in the second tier, final customers are distributed by a fleet of near-zero emissions city freighters. this system requires transferring the goods from buses to city freighters at the bus stops. we model the corresponding optimization problem as a variant of the pickup and delivery problem with transfers and solve it with an adaptive large neighborhood search. to evaluate its results, lower bounds are calculated with a column generation approach. the algorithm is assessed on data sets derived from a field study in the medium-sized city of la rochelle in france.
study of the energy and the radio emission point of cosmic rays. the purpose of the codalema experiment, installed at the nançay radio observatory (france), is to study the radio-detection of ultra-high-energy cosmic rays. distributed over an area of 0,25 km2, the original device uses in coincidence an array of particle detectors and an array of short antennas. a new analysis of the energy reconstruction from radio data obtained with this device is presented. we suggest that an energy resolution of less than 20% can be achieved and that, not only the lorentz force, but also another contribution proportional to all charged particles generated during the shower development, could play a significant role in the amplitude of the electric field measured by the antennas (as an effect of coherence or of charge excess). since 2011, a new array of radio-detectors, consisting of 60 stand-alone and self-triggered stations, has been in deployment over an area of 1.5 km2 around the first device. this new development leads to specific challenges which are discussed in terms of recognition of cosmic rays and reconstruction of the curvature of radio wave fronts. for commonly-used minimization algorithms, we emphasize the importance of the convergence process induced by the minimization ofa non-linear least squares function that affects the results in terms of degeneration of the solutions. we derive a simple method to obtain a satisfactory estimate of the location of the apparent emission source, which mitigates the problems previously.
some recent results of the codalema experiment. codalema is one of the experiments devoted to the detection of ultra high energy cosmic rays by the radio method. the main objective is to study the features of the radio signal induced by the development in the atmosphere of extensive air showers (eas) generated by cosmic rays in the energy range of 10 pev-1 eev . after a brief presentation of the detector features, the main results obtained are reported (emission mechanism, lateral distribution of the electric field, energy calibration, etc.). the first studies of the radio wave front curvature are discussed as new preliminary results.
measurement of the cosmic ray spectrum above 4×1018 ev using inclined events detected with the pierre auger observatory. a measurement of the cosmic-ray spectrum for energies exceeding 4×1018 ev is presented, which is based on the analysis of showers with zenith angles greater than 60∘ detected with the pierre auger observatory between 1 january 2004 and 31 december 2013. the measured spectrum confirms a flux suppression at the highest energies. above 5.3×1018 ev, the "ankle", the flux can be described by a power law e−γ with index γ=2.70±0.02(stat)±0.1(sys) followed by a smooth suppression region. for the energy (es) at which the spectral flux has fallen to one-half of its extrapolated value in the absence of suppression, we find es=(5.12±0.25(stat)+1.0−1.2(sys))×1019 ev.
coherent $\rho^0$ photoproduction in ultra-peripheral pb--pb collisions at $\mathbf{\sqrt{\textit{s}_{\rm nn}}} = 2.76$ tev. we report the first measurement at the lhc of coherent photoproduction of $\rho^0$ mesons in ultra-peripheral pb-pb collisions. the invariant mass and transverse momentum distributions for $\rho^0$ production are studied in the $\pi^+ \pi^-$ decay channel at mid-rapidity. the production cross section in the rapidity range $|y|&lt;0.5$ is found to be $\mathrm{d}\sigma/\mathrm{d}y = 425 \pm 10 \, (\mathrm{stat.})$ $^{+42}_{-50} \, (\mathrm{sys.})$ mb. coherent $\rho^0$ production is studied with and without requirement of nuclear breakup, and the fractional yields for various breakup scenarios are presented. the results are compared with those from lower energies and with model predictions based on the glauber model and the color dipole model. the measured cross section is found to be inconsistent with a scaling of the $\gamma$-nucleon cross section using the glauber model.
evidence for the charge-excess contribution in air shower radio emission observed by the codalema experiment. codalema is one of the pioneer experiments dedicated to the radio detection of ultra high energy cosmic rays (uhecr), located at the radio observatory of nançay (france). the codalema experiment uses both a particle detector array and a radio antenna array. data from both detection systems have been used to determine the ground coordinates of the core of extensive air showers (eas). we discuss the observed systematic shift of the core positions determined with these two detection techniques. we show that this shift is due to the charge-excess contribution to the total radio emission of air showers, using the simulation code selfas. the dependences of the radio core shift to the primary cosmic ray characteristics are studied in details. the observation of this systematic shift can be considered as an experimental signature of the charge excess contribution.
rapidity and transverse-momentum dependence of the inclusive j/$\mathbf{\psi}$ nuclear modification factor in p-pb collisions at $\mathbf{\sqrt{\textit{s}_{nn}}}=5.02$ tev. we have studied the transverse-momentum ($p_{\rm t}$) dependence of the inclusive j/$\psi$ production in p-pb collisions at $\sqrt{s_{\rm nn}} = 5.02$ tev, in three center-of-mass rapidity ($y_{\rm cms}$) regions, down to zero $p_{\rm t}$. results in the forward and backward rapidity ranges ($2.03 &lt; y_{\rm cms} &lt; 3.53$ and $-4.46 &lt;y_{\rm cms}&lt; -2.96$) are obtained by studying the j/$\psi$ decay to $\mu^+\mu^-$, while the mid-rapidity region ($-1.37 &lt; y_{\rm cms} &lt; 0.43$) is investigated by measuring the ${\rm e}^+{\rm e}^-$ decay channel. the $p_{\rm t}$ dependence of the j/$\psi$ production cross section and nuclear modification factor are presented for each of the rapidity intervals, as well as the j/$\psi$ mean $p_{\rm t}$ values. forward and mid-rapidity results show a suppression of the j/$\psi$ yield, with respect to pp collisions, which decreases with increasing $p_{\rm t}$. at backward rapidity no significant j/$\psi$ suppression is observed. theoretical models including a combination of cold nuclear matter effects such as shadowing and partonic energy loss, are in fair agreement with the data, except at forward rapidity and low transverse momentum. the implications of the p-pb results for the evaluation of cold nuclear matter effects on j/$\psi$ production in pb-pb collisions are also discussed.
measurement of pion, kaon and proton production in proton-proton collisions at $\sqrt{s}=7$ tev. the measurement of primary $\pi^{\pm}$, k$^{\pm}$, p and $\overline{p}$ production at mid-rapidity ($|y| &lt;$ 0.5) in proton-proton collisions at $\sqrt{s} = 7$ tev performed with alice (a large ion collider experiment) at the large hadron collider (lhc) is reported. particle identification is performed using the specific ionization energy loss and time-of-flight information, the ring-imaging cherenkov technique and the kink-topology identification of weak decays of charged kaons. transverse momentum spectra are measured from 0.1 up to 3 gev/$c$ for pions, from 0.2 up to 6 gev/$c$ for kaons and from 0.3 up to 6 gev/$c$ for protons. the measured spectra and particle ratios are compared with qcd-inspired models, tuned to reproduce also the earlier measurements performed at the lhc. furthermore, the integrated particle yields and ratios as well as the average transverse momenta are compared with results at lower collision energies.
development and optimization of targets dedicated to innovative radioisotope production for medical research (cu-67, ge-68 / ga-68) at the arronax cyclotron. nuclear medicine uses radioactive isotopes for diagnostic or therapeutic purposes. the activity of nuclear medicine today is made with a small number of radio-isotopes, but there is a demand for access to new isotopes like 68ga (diagnosis) and 67cu (therapy). these two isotopes can be produced on arronax and are the subject of this work.to produce 68ge, a target containing gallium (melting point: 30°c) must be used. during irradiation, gallium melts becoming a very corrosive liquid which causes the appearance of cracks that may destroy the target. to circumvent this problem, we developed a ga/ni alloy which remains solid under irradiation. ga3ni2 alloy, with a melting temperature of 369°c, is obtained by electroplating and was characterized by sem, edx, xrd and icp-oes. a first irradiation was performed to validate the production of 68ge and to inventory co-produced radioactive impurities.to produce 67cu, it is important to know the production cross sections 68zn(p,2p)67cu to optimize the irradiation parameters. data available in the literature show a large dispersion. this is due to the difficulty to separate 67cu and 67ga in the experiment. to improve our understanding of this reaction, we performed a new series of measurements using the "stacked foils" technique and an original chemical separation procedure. from the data obtained, we were able to determine the expected production yield for this reaction.
8be, 12 c, 16 o, ... nuclei and alpha clustering within a generalized liquid drop model. a liquid drop model previously used to describe smoothly the transition between two-(or three) body and one-body shapes in entrance and exit channels of nuclear reactions has been used to determine the potential barriers governing the evolution of the light nuclei : 8be, 12c, 16o, 20ne, 24mg and 32s.
outils de spéciation d'isotopes radioactifs innovants à l'échelle des ultra-traces pour la médecine nucléaire. null
effect of alpha radiation on the physical and chemical properties of silicate glasses. borosilicate glasses are intended to be used for the long-term confinement of high-level nuclear wastes. alpha particles from the minor actinides induce modifications of the glass structure which could deteriorate the efficiency of the confinement. external irradiation with 1 mev he ions and 7 mev au ions were performed in the son68 glass in order to simulate effect of alpha particles and recoils nucleus. dual beam irradiations composed by he+au ions were also investigated in order to simulate both effects of those two kind of particles. to understand the fundamental origin in physico-chemical properties, irradiation were also carried out on a 6 oxides borosilicate glass called international simplified glass (isg) and two commercially available glass planilux and spectrosil 2000, both from saint-gobain. the mechanical properties and chemical durability of each glass were studied as a function of the cumulated dose. results show that both alpha particles and heavy ions lead to variation in hardness, reduced young’s modulus and density. characterization techniques such as raman, rmn, and xps spectroscopy were used to analyze structural modifications induced by radiations. chemical durability of pristine and irradiated glasses was determined by monitoring the release of glass alteration elements b, li, si, mo and cs. the alteration layer was characterized by sem imaging and edx spectroscopy.
radiolytic corrosion occurring at the solid/solution interface investigated at subatech: example of uranium and titanium. null
bottom-up statistical analysis of the energy consumption of french single-family dwellings. implementing effective energy policies in the residential sector requires better understanding of the sources for the dispersion of energy consumption amongst households. bottom-up statistical models have been identified as one major modelling technique, particularly accounting for the diversity of inhabitants behaviours. in the various statistical models of the literature, behaviour characteristics are seldom incorporated in the data set but socioeconomic data are most often used as " proxy " of occupant behaviour. the present study is based on a detailed survey, combining energy billing data with technical, geographical, socioeconomic and behavioural variables. the corresponding sample, despite of its limited size (420 individuals), is representative of french households. a statistical model relating energy consumption to the other variables has been applied, enabling simultaneous use of quantitative and qualitative explanatory factors (ancova, analysis of covariance). the main determinants found for energy consumption in the sector of single-family dwellings in france are, by decreasing order of weights, surface area, type of main heating system, age of the household head and climate zone. we detected that the most influential behaviour variable is night temperature setting reduction. explanation and prediction capacities of the model as the accuracy of the model coefficients are studied and some possibilities of improvement are proposed.
ortho-positronium observation in the double chooz experiment. the double chooz experiment measures the neutrino mixing angle θ13 by detectingreactor ¯νe via inverse beta decay. the positron-neutron space and time coincidenceallows for a sizable background rejection, nonetheless liquid scintillator detectors wouldprofit from a positron/electron discrimination, if feasible in large detector, to suppress theremaining background. standard particle identification, based on particle dependent timeprofile of photon emission in liquid scintillator, can not be used given the identical mass ofthe two particles. however, the positron annihilation is sometimes delayed by the orthopositronium(o-ps) metastable state formation, which induces a pulse shape distortion thatcould be used for positron identification. in this paper we report on the first observation ofpositronium formation in a large liquid scintillator detector based on pulse shape analysisof single events. the o-ps formation fraction and its lifetime were measured, finding thevalues of 44 % ± 12 % (sys.) ± 5 % (stat.) and 3.68 ns ± 0.17 ns (sys.) ± 0.15 ns (stat.)respectively, in agreement with the results obtained with a dedicated positron annihilationlifetime spectroscopy setup.
mass transfer between a gas phase and two non miscible liquid phases.use of the equivalent absorption capacity concept for gas​/liquid​/liquidcontactor design. null
optimization of the volume fraction of an absorbent phase (silicone oil)​and biodegradation kinetics of dmds in a tppb. null
synthesizing realistic cloud workload traces for studying dynamic ressource system management. null
the epoc project: energy proportional and opportunistic computing system. with the emergence of the future internet and the dawning of new it models such as cloud computing, the usage of data centers (dc), and consequently their power consumption, increase dramatically. besides the ecological impact, the energy consumption is a predominant criteria for dc providers since it determines the daily cost of their infrastructure. as a consequence, power management becomes one of the main challenges for dc infrastructures and more generally for large-scale distributed systems. in this paper, we present the epoc project which focuses on optimizing the energy consumption of mono-site dcs connected to the regular electrical grid and to renewable energy sources.
reconnaître les régulations autonomes pour organiser le travail : l’exemple de la gestion de l’absentéisme en ehpad. null
the absenteeism of health care professionals. null
behavior of heptavalent technetium in sulfuric acid under a-irradiation: structural determination of technetium sulfate complexes by x-ray absorption spectroscopy and first principles calculations. null
spectrophotometric study of the behaviour of pertechnetate in trifluoromethanesulfonic acid: effect of alpha irradiation on the stability of tc(vii). this paper is devoted to the stability of pertechnetate in trifluoromethanesulfonic acid (htfms) in presence or absence of alpha irradiation. the irradiations were performed using alpha particles (4he2+) generated by arronax cyclotron with an external beam energy of 70 mev. the stability has been determined by uv-visible spectroscopy. in the absence of alpha irradiation, the results have shown that tc(vii) is reduced in htfms between 4 and 11 m. uv-visible spectroscopy measurements in 4-8 m show the presence of one phase of tc(iv) oxopolymeric species. at 9 m htfms, the change of uv-visible spectrum with respect to lower concentrations suggests the formation of a second new species of tc(iv). at highly concentrated htfms (11 m), the tc(vii) species remains stable. these findings exhibit the formation of reduced tc species by partial thermal decomposition and hydrolysis processes due to the highly exothermic hydration reaction of triflic acid with water. under alpha irradiation, the same reduced tc species as those observed without external irradiation were obtained with higher reduction kinetics. the formation of oxopolymeric technetium at important radiolytic yield resulted from reducing radiolytic products of both water and acid.
measurement of dijet &lt;mml:math altimg="si1.gif" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/xmlschema" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance" xmlns="http://www.elsevier.com/xml/ja/dtd" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/math/mathml" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd"&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant="normal"&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt; in p–pb collisions at &lt;mml:math altimg="si2.gif" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/xmlschema" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance" xmlns="http://www.elsevier.com/xml/ja/dtd" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/math/mathml" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd"&gt;&lt;mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant="normal"&gt;nn&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:msqrt&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;5.02&lt;/mml:mn&gt;&lt;mml:mtext&gt; tev&lt;/mml:mtext&gt;&lt;/mml:math&gt;. a measurement of dijet correlations in p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev with the alice detector is presented. jets are reconstructed from charged particles measured in the central tracking detectors and neutral energy deposited in the electromagnetic calorimeter. the transverse momentum of the full jet (clustered from charged and neutral constituents) and charged jet (clustered from charged particles only) is corrected event-by-event for the contribution of the underlying event, while corrections for underlying event fluctuations and finite detector resolution are applied on an inclusive basis. a projection of the dijet transverse momentum, $k_{\rm ty} = p_\rm{t,jet}^\rm{ch+ne} \; \rm{sin}(\delta\varphi_{\rm{dijet}})$ with $\delta\varphi_{\rm{dijet}}$ the azimuthal angle between a full and charged jet and $p_\rm{t,jet}^\rm{ch+ne}$ the transverse momentum of the full jet, is used to study nuclear matter effects in p-pb collisions. this observable is sensitive to the acoplanarity of dijet production and its potential modification in p-pb collisions with respect to pp collisions. measurements of the dijet $k_{\rm ty}$ as a function of the transverse momentum of the full and recoil charged jet, and the event multiplicity are presented. no significant modification of $k_{\rm ty}$ due to nuclear matter effects in p-pb collisions with respect to the event multiplicity or a pythia8 reference is observed.
characterization of radio transient signals at the pierre auger observatory. after more than a century of studies, one of the challenging questions related to ultra-high energy cosmic rays concerns their nature, which remains unclear. improving the knowledge about the composition of cosmic rays will permit to constrain the models concerning their origins and the production mechanisms in the astrophysical sources. simulations show that, the electric field emitted by the shower is sensitive to its development. this electric-field can be measured with a high duty cycle, and thus is apromising technique to identify an observable sensitive to the nature of the primary cosmic ray. the radio signal is also used to measure its arrival direction and its energy. since 2006, the pierre auger observatory hosts several radio detection arrays of cosmic rays, starting from small size prototypes (rauger, maxima) to achieve a large scale array of 124 radio stations: aera, the auger engineering radio array covering 6 km². these different arrays allow the study of the radio emission during the development of the shower in the mhz domain. aera is deployed in the low energy extension of the pierre auger observatory in order to have a larger statistics. it enables interesting hybrid measurements, with the comparison of radio observable with those obtained with the surface detector (sd) and the fluorescence telescopes close to the array. this thesis is dedicated to the characterization of the radio transient signals detected by rauger and aera. as one of the challenges of the radio detection of air-shower is to remove the anthropic background causing accidental triggering, methods for background rejection and sd-aera coincidences selection have been developed. a study of the correlation between the shower development in the atmosphere (longitudinal profile) and the electric-field measured by the radio stations is also presented. this study shows the relationship between the electric-field and the shower development in the atmosphere and confirms that the radio signal is a powerful tool to study the nature of the ultra-high energy cosmic rays.
charm and prompt photon production with the event generator epos. at the lhc, strong interaction is studied by doing collisions of high energy particles. in the case of nucleus-nucleus collision (lead at the lhc), a new state of matter, called quarks gluons plasma (qgp), is created. the study of this qgp is currently a lively research field. hard probes, like heavy quarks and prompt photons, are produced during early times of collisions done at the lhc. this is why they are ideal probes for the study of the qgp. they will go through and interact with the medium produced by the collision. a comparison with a case without qgp (proton-proton- collision) will allow us to see how hard probes properties are modified by themedium. then, medium properties like temperature and density can be extracted. this study requires a good understanding of hard probes production in proton-proton collisions. the aim of my thesis is the implementation of heavy quarks and prompt photons in the event generator epos (computer code for colliders), for p-p collisions. our final aim is the study of the qgp in pb-pb collisions.
aqueous and water vapour alteration of the son68 glass at low temperature (35-90°c). the son68 glass is initially altered in dynamic mode under silica rich cox water (42 mg/l) at ph8, high s/v ratio (14000 m⁻¹) and at 35, 50 and 90°c. the results showed that the glass alteration seems to be governed by both diffusion and surface reaction process. the residual rate at 90°c is around 10-4 g.m⁻².d⁻¹. the activation energy is about 70 kj.mol⁻¹. the dissolution /precipitation and hydrolysis/condensation mechanisms are responsible for the development of the alteration layer. mg silicates and calcites precipitate at 35 and 50°c, the same phases in addition to powellite and apatite precipitate at 90°c. the results predicted by the model reproduce well experimental data. the glass is then hydrated at temperatures ranging from 35 to 125°c and relative humidity values (rh) between 92 an 99.9%. the glass hydration increases with the temperature and rh, the hydration energy is about 34.2 kj.mol⁻¹. the alteration layers thicknesses vary between 0.3μm at 35°c and 5μm at 125°c. the alteration layer is depleted in (b, li, na) and enriched in (si, al, fe, zn and ni). the secondary phases are calcite, powellite, apatite and tobermorite in adition to a hydration gel. the effect of near field materials on the ²⁹si doped son68 glass alteration was studied. the presence of steel increases the ph and decreases the si and mo concentrations without changing the overall rate of glass corrosion. the si is retained on the steel corrosion products, its concentration in solution seems to be controlled by the clay dissolution. the glass corrosion in the presence of steel and clay at 90°c leads to the formation of magnetite, siderite, ironsilicates, pure silica, iron sulphur (pyrite, troilite,pyrrhotite and mackinawite), calcite, apatite, powellite and mg silicates. the modelling results agree well with the experimental data.
a logical study of program equivalence. proving program equivalence for a functional language with references is a notoriously difficult problem. the goal of this thesis is to propose a logical system in which such proofs can be formalized, and in some cases inferred automatically. in the first part, a generic extension method of dependent type theory is proposed, based on a forcing interpretation seen as a presheaf translation of type theory. this extension equips type theory with guarded recursive constructions, which are subsequently used to reason on higher-order references. in the second part, we define a nominal game semantics for a language with higher-order references. it marries the categorical structure of game semantics with a trace representation of denotations of programs, which can be computed operationally and thus have good modularity properties. using this semantics, we can prove the completeness of kripke logical relations defined in a direct way, using guarded recursive types, without using biorthogonality. such a direct definition requires omniscient worlds and a fine control of disclosed locations. finally, we introduce a temporal logic which gives a framework to define these kripke logical relations. the problem of contextual equivalence is then reduced to the satisfiability of an automatically generated formula defined in this logic, i.e. to the existence of a world validating this formula. under some conditions, this satisfiability can be decided using a smt solver. completeness of our methods opens the possibility of getting decidability results of contextual equivalence for some fragments of the language, by giving an algorithm to build such worlds.
modèle logique avancé (plateforme opencloudware). t. aubonnet, e. madelaine , l. henrio , t. ledoux , y. kouki , p. moreaux , f. pourraz , i. ayadi , n. simoni - modèle logique avancé, date de dépot: 2013/10/18, nb pages 1- 28, (tech. rep.: cedric-13-2871).
effective bond orders from two-step spin-orbit coupling approaches: the i&lt;sub&gt;2&lt;/sub&gt;, at&lt;sub&gt;2&lt;/sub&gt; , io&lt;sup&gt;+&lt;/sup&gt;, and ato&lt;sup&gt;+&lt;/sup&gt; case studies. the nature of chemical bonds in heavy main-group diatomics is discussed from the viewpoint of effective bond orders, which are computed from spin-orbit wave functions resulting from contracted spin-orbit configuration interaction calculations. the reliability of the relativistic correlated wave functions obtained in such two-step spin-orbit coupling frameworks is assessed by benchmark studies of the spectroscopic constants with respect to either experimental data, or state-of-the-art fully relativis-tic correlated calculations. the i&lt;sub&gt;2&lt;/sub&gt;, at&lt;sub&gt;2&lt;/sub&gt; , io&lt;sup&gt;+&lt;/sup&gt;, and ato&lt;sup&gt;+&lt;/sup&gt; species are considered, and differences and similarities between the astatine and iodine elements are highlighted. in particular, we demonstrate that spin-orbit coupling weakens the covalent character of the bond in at&lt;sub&gt;2&lt;/sub&gt; even more than electron correlation, making the consideration of spin-orbit coupling compulsory for discussing chemical bonding in heavy (6&lt;i&gt;p&lt;/i&gt;) main group element systems.
measurement of charged jet production cross sections and nuclear modification in p-pb collisions at $\sqrt{s_\rm{nn}} = 5.02$ tev. charged jet production cross sections in p-pb collisions at $\sqrt{s_{\rm nn}} = 5.02$ tev measured with the alice detector at the lhc are presented. using the anti-$k_{\rm t}$ algorithm, jets have been reconstructed in the central rapidity region from charged particles with resolution parameters $r = 0.2$ and $r = 0.4$. the reconstructed jets have been corrected for detector effects and the underlying event background. to calculate the nuclear modification factor, $r_{\rm ppb}$, of charged jets in p-pb collisions, a pp reference was constructed by scaling previously measured charged jet spectra at $\sqrt{s} = 7$ tev. in the transverse momentum range $20 \le p_{\rm t,ch\ jet} \le 120$ gev/$c$, $r_{\rm ppb}$ is found to be consistent with unity, indicating the absence of strong nuclear matter effects on jet production. major modifications to the radial jet structure are probed via the ratio of jet production cross sections reconstructed with the two different resolution parameters. this ratio is found to be similar to the measurement in pp collisions at $\sqrt{s} = 7$ tev and to the expectations from pythia pp simulations and nlo pqcd calculations at $\sqrt{s_{\rm nn}} = 5.02$ tev.
synthesis and structures of plutonyl nitrate complexes: is plutonium heptavalent in puo&lt;sub&gt;3&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; ?. gas-phase plutonium nitrate anion complexes were produced by electrospray ionization (esi) of a plutonium nitrate solution. the esi mass spectrum included species with all four of the common oxidation states of plutonium: pu(iii), pu(iv), pu(v) and pu(vi). plutonium nitrate complexes were isolated in a quadrupole ion trap and subjected to collision induced dissociation (cid). cid of complexes of the general formula puo&lt;sub&gt;x&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;y&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; resulted in the elimination of no&lt;sub&gt;2&lt;/sub&gt; to produce puo&lt;sub&gt;x+1&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;y−1&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, which in most cases corresponds to an increase in the oxidation state of plutonium. plutonyl species, pu&lt;sup&gt;v&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; and pu&lt;sup&gt;vi&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;3&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, were produced from pu&lt;sup&gt;iii&lt;/sup&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;4&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; and pu&lt;sup&gt;iv&lt;/sup&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;5&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, respectively, by the elimination of two no&lt;sub&gt;2&lt;/sub&gt; molecules. cid of pu&lt;sup&gt;vi&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;3&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; resulted in no&lt;sub&gt;2&lt;/sub&gt; elimination to yield puo&lt;sub&gt;3&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, in which the oxidation state of plutonium could be vii, a known oxidation state in condensed phase but not yet in the gas phase. density functional theory confirmed the nature of pu&lt;sup&gt;v&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; and pu&lt;sup&gt;vi&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;3&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; as plutonyl(v/vi) cores coordinated by bidentate equatorial nitrate ligands. the computed structure of puo&lt;sub&gt;3&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; is essentially a plutonyl(vi) core, pu&lt;sup&gt;vi&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2+&lt;/sup&gt;, coordinated in the equatorial plane by two nitrate ligands and one radical oxygen atom. the computations indicate that in the ground spin-orbit free state of puo&lt;sub&gt;3&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, the unpaired electron of the oxygen atom is antiferromagnetically coupled to the spin-triplet state of the plutonyl core. the results indicate that pu(vii) is not a readily accessible oxidation state in the gas phase, despite that it is stable in solution and solids, but rather that a pu(vi)-o• bonding configuration is favored, in which an oxygen radical is involved.
etude des mécanismes diffusionnels impliqués dans la séparation cinétique co2-ch4 sur des tamis moléculaires carbonés. null
core library for advanced scenario simulation, c.l.a.s.s.: principle &amp; application. the global warming, the increase of world population and the depletion of fossil resourceshave lead us in a major energy crisis. using electronuclear energy could be one of the meansto solve a part of these issues. the way out of this crisis may be enlightened by the study oftransitional scenarios, guiding the political decisions. the reliability of those studies passesthrough the wide variety of the simulation tools and the comparison between them.from this perspective and in order to perform complex electronuclear scenario simulation,the open source core library for advance scenario simulation (class) is being developed.class main asset is its ability to include any kind of reactor, whether the system is innovativeor standard. a reactor is fully described by its evolution database that must contain a set ofdifferent fuel compositions in order to simulate transitional scenarios. class aims at being auseful tool to study scenarios involving generation iv reactors as well as innovative fuel cycles,like the thorium cycle.the following contribution will present in detail the class software. starting with the workingprinciple of this tool, one will explain the working process of the different modules suchas the evolution module. it will be followed by an exhaustive presentation of the uox-moxbases generation procedure. finally a brief analysis of the error made by the class evolutionmodule will be presented.
effect of heterogeneity in plutonium recycling in steady state pwr. the possible delay of decades for the deployment of fourth generation reactors brings upnew issues. countries like france which are storing plutonium for years in prediction ofstarting fbr could have to adapt their plutonium management strategy.we have compared different strategies for plutonium recycling in pwr reactors. we havechosen to limit the scope of this study to pwrs considering the standard uranium cycle,and we investigated the influence of the heterogeneity in the assembly that would containthe recycled plutonium. the comparison of different strategies is made at steady-state.this paper present a specific method developed to allow complete and detailed studiesof equilibrium scenarios and how it has been implemented and integrated inside the opensourcemure package.we will then discuss of the results obtained through this method apply to pwrs loadedwith homogeneous and heterogeneous assemblies using the following criteria: resourceconsumption, pu inventories in the cycle and waste production.
human ficolin-2 recognition versatility extended: an update on the binding of ficolin-2 to sulfated/phosphated carbohydrates. ficolin-2 has been reported to bind to dna and heparin, but the mechanism involved has not been thoroughly investigated. x-ray studies of the ficolin-2 fibrinogen-like domain in complex with several new ligands now show that sulfate and phosphate groups are prone to bind to the s3 binding site of the protein. composed of arg132, asp133, thr136 and lys221, the s3 site was previously shown to mainly bind n-acetyl groups. furthermore, dna and heparin compete for binding to ficolin-2. mutagenesis studies reveal that arg132, and to a lesser extent asp133, are important for this binding property. the versatility of the s3 site in binding n-acetyl, sulfate and phosphate groups is discussed through comparisons with homologous fibrinogen-like recognition proteins.
anthropogenic tritium in the loire river estuary &amp; hydrogen lability in matrices of interest. null
experimental setup for the determination of exchangeable hydrogen in environmental samples using deuterium and tritium. null
transport properties of iodide in a sandy aquifer: hydrogeological modelling and field tracer tests. the release of radioactive iodine into geological media from nuclear waste disposal is an issue that has tobe considered since iodine is a biophilic element. 129i is, with 99tc, one of the two long-lived radionuclidesthat have the highest mobility in radioactive waste disposal. within this context, iodide retardation is stilla matter of debate. a low value of the retardation factor is generally accepted in soils without organicmatter, but the possibility for sorption cannot be completely ruled out. since isotopic exchange with naturallyoccurring iodine is one of the main potential sorption mechanisms, site-specific retention parametersare needed. in the present paper, we study iodide transport in a sandy aquifer. a hydrogeologicalmodel was built to fit deuterium, bromide and iodide breakthrough data from in situ tracer test experiments.within the precision range of the fitting, iodide is excluded from 2.5% of the effective porosity byanionic exclusion and presents a field retention factor (kd) lower than 0.025 l/kg.
approche archéologique et environnementale des premiers peuplements alpins autour du col du petit-saint-bernard (savoie -vallée d’aoste) : un bilan d’étape. null
une économie pastorale dans le nord du vercors : analyse pluridisciplinaire des niveaux néolithiques et protohistoriques de la grande rivoire (sassenage, isère). the neolithic levels of the rock-shelter “la grande rivoire” are composed of a multitude of sedimentary layers of very contrasting colours. sedimentological analyses show that the fine fractions, mainly silty and rather strongly carbonated, have two main origins: on the one hand, an important accumulation of herbivores faeces due to the repeated penning of flocks (presence of calcite spheruliths, high concentrations of organic matter, high rate of phosphate), on the other hand, a large production of ashes from fire kindled directly on the dung or in nearby hearths. animal bones of domestic species found in these dung levels show a large predominancy of goat / sheep over ox and pig. archaeobotanical studies suggest a fodder / litter supply based on leafy and flowering tree branches. the protohistoric levels are different from the neolithic ones: ashy layers are missing whereas natural detritic components are better represented. nevertheless, the analysis of thin sections from these levels shows the lasting presence of spheruliths and phytoliths. this indicates that stabling was during the bronze and iron ages still practiced in the rock-shelter.
risks and prevention of voc adsorption in installation. null
measurement of jet suppression in central pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the transverse momentum ($p_{\rm t}$) spectrum and nuclear modificationfactor ($r_{\rm aa}$) of reconstructed jets in 0-10% and 10-30% central pb-pbcollisions at $\sqrt{s_{\rm nn}}=2.76$ tev were measured. jets werereconstructed from charged and neutral particles, utilizing the alice trackingdetectors and electromagnetic calorimeter (emcal), with the anti-$k_{\rm t}$jet algorithm with a resolution parameter of r=0.2. the jet $p_{\rm t}$ spectraare reported in the pseudorapidity interval of $|{\eta}_{\rm jet}|&amp;lt;0.5$ for$40&amp;lt;p_{\rm t,jet}&amp;lt;120$ gev/$c$ in 0-10% and for $30&amp;lt;p_{\rm t,jet}&amp;lt;100$ gev/$c$in 10-30% collisions. reconstructed jets were required to contain a leadingcharged particle with $p_{\rm t}&amp;gt;5$ gev/$c$ to suppress jets constructed fromthe combinatorial background in pb-pb collisions. the effect of the leadingcharged particle requirement has been studied in both pp and pb-pb collisionsand has been shown to have negligible effects on the $r_{\rm aa}$ within theuncertainties of the measurement. the nuclear modification factor is obtainedby dividing the jet spectrum measured in pb-pb by that in pp collisions scaledby the number of independent nucleon-nucleon collisions estimated using aglauber model. $r_{\rm aa}$ is found to be $0.28\pm0.04$ in 0-10% and$0.35\pm0.04$ in 10-30% collisions, independent of $p_{\rm t,jet}$ within theuncertainties of the measurement. the observed suppression is in fair agreementwith expectations from two model calculations with different approaches to jetquenching.
precision measurements of cosmic ray air showers. null
coherent radio emission from the cosmic ray air shower extinction at the ground level. null
latest upgrades and results from the codalema experiment. null
investigation of extensive air shower properties with the codalema experiment: tackling the challenges of the next generation cosmic ray observatory. null
radiodetection of extensive air showers at the pierre {a}uger observatory. null
radio detection of air showers with aera. null
some possible interpretations from data of the codalema experiment. null
simulation of radio emission from cosmic ray air shower with selfas2. null
autonomous detection and analysis of radio emission from air showers at the pierre auger observatory. null
charge excess signature in the codalema data. interpretation with selfas2. null
first results of the standalone antenna array of the codalema radio detection experiment. null
the tianshan radio experiment for neutrino detection. null
the first generation experiments for cosmic ray radiodetection. null
latest results of the codalema experiment: cosmic rays radio detection in a self trigger mode. null
detection and characterization of the cosmic ray air shower radio emission with the codalema experiment. null
evidence for a geomagnetic effect in the codalema radio data. null
characteristics of high energy cosmic rays observed with codalema: evidence for a geomagnetic radio emission mechanism. null
radiodetection of astronomical phenomena in the cosmic ray dedicated codalema experiment. null
le projet codalema. this chapter presents the methodology of the radio detection of cosmic ray extensive air showers developed for the codalema experiment. we illustrate the performances and results obtained in the first four years of operation of codalema.
codalema : how to detect fast radio transients from cosmic ray air showers. null
radiodetection of extensive air showers : the codalema experiment. null
codalema : first results, new questions (about radiodetection of cosmic ray extensive air showers). null
bilan du travail exp\'erimental sur la radio-d\'etection des gerbes cosmiques de septembre 2001 \`a juin 2003. null
corr\'elation ${e}_0$ vs ${e}_\text{cic}$. null
validity area of spheric reconstruction. null
first detection of radio signals from cosmic ray air showers with a self triggered, fully autonomous system. null
geomagnetic feature of the events observed by the self triggered, fully autonomous radiodetectors system installed at the clf. null
timing accuracy of the coincidences between the autonomous radio stations at the clf and auger sd. null
analysis of the global behavior of the clf autonomous radio stations. understanding electric field and weather effects. null
first threefold detection of a coincidence between the self-triggered radio stations at the clf and auger sd. null
radio spectrum measurements at auger, part 2. null
radio spectrum measurements at auger. null
angular resolution estimation using airplane signals with the new rauger setup. null
the first platinium event: fd, sd and radio. null
airplane signals detected by the rauger radio stations: spectral and time domain characteristics, inter-calibration and polarization studies. null
estimation of the azimuth of cosmic rays using data from a single radio station. null
two methods for rejecting background radio traces in rauger data at the level of a single station (t1 or t2). null
aera central trigger. null
coincidences searches with the kit/buw stations. null
calculating the vector equivalent length of the butterfly antenna from nec2 by simulating the antenna in transmitting mode. null
the radio detection of extensive air showers : tackling the challenges of the next generation cosmic ray observatory. null
technological developments for the auger engineering radio array (aera). null
characterisation of the radio signal emission from extensive air showers using the selfas code. null
investigating the extensive air shower properties using the polarization and frequency features of the radio signals measured by the codalema autonomous station array. null
a dedicated antenna array for radio detection of extended air showers. radio pulses associated with extended air showers (eas) produced in terrestrial atmosphere by high energycosmic rays (uhecr) of energy 10^17 ev and above, are now routinely observed by dedicated radioinstruments on ground. this may offer a new and appealing way for elucidating the nature and origin ofinvolved primary particles, an open question still unsolved.unfortunately, the high occupancy of the electromagnetic spectrum by undesired signals from natural andanthropogenic origins has made unambiguous eas radio detection a challenging problem. former attemptsbased on timing coincidences from several independent radio antennas, or using auxiliary triggering byconventional particle detectors, are still not fully satisfying.we present here a solution based on real time, coherent radio detection by using a small array of 10x2 crosspolarized dipoles, distributed over a 150m x 150m surface area and operated in continuous sky surveyingmode.preliminary results obtained with the new system are briefly reviewed and discussed.the final detection scheme will be achieved by using on line, fast computing software based on a dedicatedunsupervised recognition algorithm.the new array is a part of the codalema experiment located in nançay radio astronomy observatory(france).
new developments around the butterfly antenna. null
study of ultra-high energy cosmic rays through their radio signal in the atmosphere. null
radio emission from the air shower sudden death. null
the pierre auger cosmic ray observatory. the pierre auger observatory, located on a vast, high plain in western argentina, is the world's largest cosmic ray observatory. the objectives of the observatory are to probe the origin and characteristics of cosmic rays above $10^{17}$ ev and to study the interactions of these, the most energetic particles observed in nature. the auger design features an array of 1660 water-cherenkov particle detector stations spread over 3000 km$^2$ overlooked by 24 air fluorescence telescopes. in addition, three high elevation fluorescence telescopes overlook a 23.5 km$^2$, 61 detector infill array. the observatory has been in successful operation since completion in 2008 and has recorded data from an exposure exceeding 40,000 km$^2$ sr yr. this paper describes the design and performance of the detectors, related subsystems and infrastructure that make up the auger observatory.
muons in air showers at the pierre auger observatory: mean number in highly inclined events. we present the first hybrid measurement of the average muon number in air showers at ultra-high energies, initiated by cosmic rays with zenith angles between $62^\circ$ and $80^\circ$. the measurement is based on 174 hybrid events recorded simultaneously with the surface detector array and the fluorescence detector of the pierre auger observatory. the muon number for each shower is derived by scaling a simulated reference profile of the lateral muon density distribution at the ground until it fits the data. a $10^{19}$~ev shower with a zenith angle of $67^\circ$, which arrives at the surface detector array at an altitude of 1450 m above sea level, contains on average $(2.68 \pm 0.04 \pm 0.48\,\mathrm{(sys.)}) \times 10^{7}$ muons with energies larger than 0.3 gev. the logarithmic gain $\mathrm{d}\ln{n_\mu} / \mathrm{d}\ln{e}$ of muons with increasing energy between $4\times 10^{18}$ ev and $5 \times 10^{19}$ ev is measured to be $(1.029\, \pm\, 0.024\, \pm 0.030\,\mathrm{(sys.)})$.
two-pion femtoscopy in p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev. we report the results of the femtoscopic analysis of pairs of identical pions measured in p-pb collisions at $\sqrt{s_{\mathrm{nn}}}=5.02$ tev. femtoscopic radii are determined as a function of event multiplicity and pair momentum in three spatial dimensions. as in the pp collision system, the analysis is complicated by the presence of sizable background correlation structures in addition to the femtoscopic signal. the radii increase with event multiplicity and decrease with pair transverse momentum. when taken at comparable multiplicity, the radii measured in p-pb collisions, at high multiplicity and low pair transverse momentum, are 10-20% higher than those observed in pp collisions but below those observed in a-a collisions. the results are compared to hydrodynamic predictions at large event multiplicity as well as discussed in the context of calculations based on gluon saturation.
forward-backward multiplicity correlations in pp collisions at $\sqrt{s}$=0.9, 2.76 and 7 tev. the strength of forward-backward (fb) multiplicity correlations is measured by the alice detector in proton-proton (pp) collisions at $\sqrt{s}=0.9$, 2.76 and 7 tev. the measurement is performed in the central pseudorapidity region ($|\eta| &lt; 0.8$) for the transverse momentum $p_{\rm t}&gt;0.3$ gev/$c$. two separate pseudorapidity windows of width ($\delta \eta$) ranging from 0.2 to 0.8 are chosen symmetrically around $\eta=0$. the multiplicity correlation strength ($b_{\rm cor}$) is studied as a function of the pseudorapidity gap ($\eta_{\rm gap}$) between the two windows as well as the width of these windows. the correlation strength is found to decrease with increasing $\eta_{\rm gap}$ and shows a non-linear increase with $\delta\eta$. a sizable increase of the correlation strength with the collision energy, which cannot be explained exclusively by the increase of the mean multiplicity inside the windows, is observed. the correlation coefficient is also measured for multiplicities in different configurations of two azimuthal sectors selected within the symmetric fb $\eta$-windows. two different contributions, the short-range (sr) and the long-range (lr), are observed. the energy dependence of $b_{\rm cor}$ is found to be weak for the sr component while it is strong for the lr component. moreover, the correlation coefficient is studied for particles belonging to various transverse momentum intervals chosen to have the same mean multiplicity. both sr and lr contributions to $b_{\rm cor}$ are found to increase with $p_{\rm t}$ in this case. results are compared to pythia and phojet event generators and to a string-based phenomenological model. the observed dependencies of $b_{\rm cor}$ add new constraints on phenomenological models.
on state space representation for linear discrete-time systems in hilbert spaces. for a linear continuous-time control system in hilbert space with state x(t) is associated a discrete-time system where the state variable is z k = (x((k + 1)h) + x(kh))/2, with small h. this allows to introduce a discrete derivative ∆z k = (x((k + 1)h) − x(kh))/h. the obtained discrete-time system has structural properties with a similar formulation as continuous sys-tem. stability is equivalent to the fact that the spectrum of the state oper-ator of discrete-time system is in the left half plane, lyapunov and riccati equation are similar.
non-linear control of a narrow tilting vehicle. — narrow tilting vehicles (ntvs) are the convergence of a car and a motorcycle. they are expected to be the new generation of city cars considering their practical dimensions and lower energy consumption. but considering their height to breadth ratio, in order to maintain lateral stability, ntvs should tilt when cornering. unlike the motorcycle's case, where the driver tilts the vehicle himself, the tilting of an ntv should be automatic. two tilting systems are available; direct and steering tilt control, the combined action of these two systems being certainly the key to improve considerably ntvs dynamic performances. focusing on the lateral dynamic of ntvs, multivariable control strategies based on linear robust control theory, were already proposed in the literature, assuming decoupling with the longitudinal dynamic. in this paper a 4 dof model of the main longitudinal and lateral dynamics is considered, and its differential flatness is demonstrated. the three flat outputs have furthermore a particular physical meaning, making possible the design of a simple external control loop complying with the driver demands.
tools developed by subatech for radiolysis studies in solution and/or at the interface. null
cross section measurements of deuteron induced nuclear reactions on natural tungsten up to 34 mev. 186gre is a β-/γ emitter of great interest for nuclear medicine. it has shown successful results on bone metastases palliation and has similar chemical properties as 99mtc, the most commonly used imaging agent. 186gre is routinely produced using rhenium target in nuclear reactor. higher speciﬁc activity could be obtained using accelerators. in this paper, production cross section values are presented for the natw (d, x)186gre reaction up to 34 mev, using the stacked-foils method and gamma spectrometry. from this data set, the thick target production yield of 186gre is determined and compared with the validated values of the iaea and also with the proton route. the production cross sections of the natw(d, x)183,182g,184m,184g,181re and natw(d, x)187w reactions have also been  determined.  a  good  agreement is found with the literature. our data are compared with the version 1.6 (december 2013) of the talys code which shows discrepancies both on the shape and on the amplitude for these deuteron induced reactions.
spectral assignment for neutral-type systems and moment problems. for a large class of linear neutral-type systems the problem of assigning eigenvalues and eigenvectors is investigated, i.e. finding the system that has the given spectrum and, in some sense, allmost all eigenvectors. the solution of this problem enables vector moment problems to be considered using the construction of a neutral-type system. the exact controllability property of the system obtained gives the solution of the vector moment problem.
entropy: a consolidation manager for clusters. clusters provide powerful computing environments, but in practice much of this power goes to waste, due to the static allocation of tasks to nodes, regardless of their changing computational requirements. consolidation is an approach that migrates tasks within a cluster as their computational requirements change, both to reduce the number of nodes that need to be active and to eliminate temporary overload situations. previous consolidation strategies have relied on task placement heuristics that use only local optimization and typically do not take migration overhead into account. however, heuristics based on only local optimization may miss the globally optimal solution, resulting in unnecessary resource usage, and the overhead for migration may nullify the benefits of consolidation. in this paper, we propose the entropy resource manager for homogeneous clusters, which performs consolidation based on constraint programming and takes migration overhead into account. the use of constraint programming allows entropy to find mappings of tasks to nodes that are better than those found by heuristics based on local optimizations, and that are frequently globally optimal in the number of nodes. because migration overhead is taken into account, entropy chooses migrations that can be implemented efficiently, incurring a low performance overhead.
impact of colloids on uranium transport in groundwater applied to the aube radioactive waste disposal. the presence of colloids, known vectors of radionuclides and chemical contaminantsin groundwater, has been identified in groundwater at the aube radioactivewaste disposal in 2004. this thesis aims to characterize these colloids, and todetermine their potential impact in the transport of uranium, chosen as the elementof interest for this study. the identified 60 nm in diameter clay colloids and thefulvic and humic acids can move in aptian groundwater, as indirectly evidenced bycolumn experiments. a feasibility study of a in situ test has been done througha transport modeling to confirm the colloid mobility at the field scale. using theconditions of the study, the clay colloids do not influence uranium transport. evenwith the greatest concentration assumed on site, they have a very limited impact onthe mobilization of uranium, in the ph range measured on site. on the contrary, theorganic colloids, despite their low concentration, can facilitate uranium transport,the uranyl - organic acid chemical bond being exceptionally strong. therefore theirlow concentration in groundwater makes their impact on uranium mobility equallyinsignificant.
modélisation de molécules et matériaux d'intérêt en radiochimie. null
microbial aerosol filtration: growth and release of bacteria-fungi consortium collected by fibrous filters in different operating conditions. null
centrality dependence of particle production in p-pb collisions at $\sqrt{s_{\rm nn} }$= 5.02 tev. we report measurements of the primary charged particle pseudorapidity density and transverse momentum distributions in p-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev, and investigate their correlation with experimental observables sensitive to the centrality of the collision. centrality classes are defined using different event activity estimators, i.e. charged particle multiplicities measured in three disjunct pseudorapidity regions as well as the energy measured at beam rapidity (zero-degree). the procedures to determine the centrality, quantified by the number of participants ($n_{\rm part}$), or the number of nucleon-nucleon binary collisions ($n_{\rm coll}$), are described. we show that, in contrast to pb-pb collisions, in p-pb collisions large multiplicity fluctuations together with the small range of participants available, generate a dynamical bias in centrality classes based on particle multiplicity. we propose to use the zero-degree energy, which we expect not to introduce a dynamical bias, as an alternative event-centrality estimator. based on zero-degree energy centrality classes, the $n_{\rm part}$ dependence of particle production is studied. under the assumption that the multiplicity measured in the pb-going rapidity region scales with the number of pb-participants, an approximate independence of the multiplicity per participating nucleon measured at mid-rapitity of the number of participating nucleons is observed. furthermore, at high-$p_{\rm t}$ the p-pb spectra are found to be consistent with the pp spectra scaled by $n_{\rm coll}$ for all centrality classes. our results represent valuable input for the study of the event activity dependence of hard probes in p-pb collision and, hence, help to establish baselines for the interpretation of the pb-pb data.
use of fluorescence spectroscopy and voltammetry for the analysis of metal- organic matter interactions in the new caledonia lagoon. fluorescence, polarographic and potentiometric analysis of sea water from the new caledonia lagoon (located south of noumea) allowed the determination of the specific properties of the dissolved and particulate phases of organic matter (om)-metal complexes according to various regions of the lagoon. in particular, om complexes with ni, zn, pb, cu, cd were chosen in this study due to the sensitivity of these complexes to affect biocenosis of the nearby enclosed coral reef as well as their availability to enter the coast from erosion (terrigenous om) or human activities from nickel extraction or pollution from waste sites (anthopogenic om) that exist throughout new caledonia. combined with geochemical modelling, the om-metal complexes analysis allowed the determination of their conditional stability constants which in turn helped in predicting the fate of the metal pollution in the lagoon. for the first time, fluorescence, polarographic and potentiometric techniques combined with geochemical models that employed discrete pka distribution on om enabled the determination of the origin of the om, as either natural or anthopogenic.
antenna development for astroparticle and radioastronomy experiments. an active dipole antenna is in operation since five years at the nançay radio observatory (france) in the codalema experiment. a new version of this active antenna has been developed, whose shape gave its name of "butterfly" antenna. compared to the previous version, this new antenna has been designed to be more efficient at low frequencies, which could permit the detection of atmospheric showers at large distances. despite a size of only 2 m×1 m in each polarization, its sensitivity is excellent in the 30-80 mhz bandwidth. three antennas in dual polarization were installed on the codalema experiment, and four other have been recently installed on the auger area in the scope of the aera project. the main characteristics of the butterfly antenna are detailed with an emphasis on its key features which make it a good candidate for the low frequency radioastronomy and the radio detection of transients induced by high energy cosmic rays.
fuml as an assembly language for model transformation. within a given modeling platform, modeling tools, such as model editors and transformation engines, interoperate efficiently. they are generally written in the same general-purpose language, and use a single modeling framework (i.e., an api to access models). however, interoperability between tools from different modeling platforms is much more problematic.in this paper, we propose to leverage fuml in order to address this issue by providing a common execution language. modeling frameworks can then be abstracted into generic actions that perform elementary operations on models. not only can user models benefit from a unified execution semantics, but modeling tools can too.we support this proposal by showing how it can apply to a model transformation engine. to this end, a prototype compiler from atl to fuml has been built, and is described. finally, we conclude that fuml has some useful properties as candidate common execution language for mde, but lacks some features.
btrplace: flexible vm management in data centers. null
the non-overlapping constraint between objects described by non-linear inequalities. packing 2d objects in a limited space is an ubiquitous problem with many academic and industrial variants. in any case, solving this problem requires the ability to determine where a first object can be placed so that it does not intersect a second, previously placed, object. this subproblem is called the non-overlapping constraint. the complexity of this non-overlapping constraint depends on the type of objects considered. it is simple in the case of rectangles. it has also been studied in the case of polygons. this paper proposes a numerical approach for the wide class of objects described bynon-linear inequalities. our goal here is to calculate the non-overlapping constraint, that is, to describe the set of all positions and orientations that can be assigned to the first object so that intersection with the second one is empty. this is done using a dedicated branch &amp; bound approach. we first show that the non-overlapping constraint can be cast into a minkowski sum, even if we take into account orientation. we derive from this an innercontractor, that is, an operator that removes from the current domain a subset of positions and orientations that necessarily violate the non-overlapping constraint. this inner contractor is then embedded in a sweeping loop, a pruning technique that was only used with discrete domains so far. we finally come up with a branch &amp; bound algorithm that outperforms the generic state-of-the-art solver rsolver.
a model-driven approach to generate external dsls from object-oriented apis. developers in modern general-purpose programming languages cre-ate reusable code libraries by encapsulating them in applications programming interfaces (apis). domain-specific languages (dsls) can be developed as an al-ternative method for code abstraction and distribution, sometimes preferable to apis because of their expressivity and tailored development environment. how-ever the cost of implementing a fully functional development environment for a dsl is generally higher. in this paper we propose dslit, a prototype-tool that, given an existing api, reduces the cost of developing a corresponding dsl by analyzing the api, automatically generating a semantically equivalent dsl with its complete development environment, and allowing for user customization. to build this bridge between the api and dsl technical spaces we make use of exist-ing model-driven engineering (mde) techniques, further promoting the vision of mde as a unifying technical space.
on the exact controllability and observability of neutral type systems. neutral type systems considered in infinite-dimensional hilbert space are analyzed for exact controllability characterization. the approach is based on the problem of moments using a riesz basis of eigenvectors. the duality with observability is inves-tigated. a criterion of exact observability is deduced.
model of dynamic interactions. in robotic-based machining, an interaction between the workpiece and technological tool causes essential deflections that significantly decrease the manufacturing accuracy. relevant compliance errors highly depend on the manipulator configuration and essentially differ throughout the workspace. their influence is especially important for heavy serial robots. to overcome this difficulty this report presents a new technique for compensation of the compliance errors caused by technological process. in contrast to previous works, this technique is based on the non-linear stiffness model and the reduced elasto-dynamic model of the robotic based milling process. the advantages and practical significance of the proposed approach are illustrated by milling with of kuka kr270. it is shown that after error compensation technique significantly increase the accuracy of milling.
tools for the identification of robot stiffness parameters using cad software. this report proposes a cad-based approach for identification  of the elasto-static parameters of the robotic manipulators. the main contributions are in the areas of virtual experiment planning and algorithmic data processing, which allows to obtain the stiffness matrix with required accuracy. in contrast to previous works, the developed technique operates with the deflection field produced by virtual experiments in a cad environment. the proposed approach provides high identification accuracy (about 0.1% for the stiffness matrix element) and is able to take into account the real shape of the link, coupling between rotational/translational deflections and joint particularities. to compute the stiffness matrix, the numerical technique has been developed, and some recommendations for optimal settings of the virtual experiments are given. in order to minimize the identification errors, the statistical data processing technique was applied. the advantages of the developed approach have been confirmed by case studies dealing with the links of parallel manipulator of the orthoglide family, for which the identification errors have been reduced to 0.1%.
simulation results using a robot with flexibilities for machining and welding. the objective of this report is to detail the models used in simulation and the results obtained in simulation for both machining and fsw process.this report contains in a first part the details of modeling flexibilities of serial robots primarily through a model of localized flexibilities. the flexibilities are expressed both in cartesian space and in the joint space and taking into account possible couplings.the second part deals with the dynamic model used in the simulator and the simulation environment. a significant work was to also model the company kuka robot controller. machining processes and fsw are modeled by simple models but reflecting the reality of the behavior.
robot comparison based on local and global indices proposed and related to fsw welding and machining. this deliverable deals with the comparison of robots as a function of local and global indices related to machining operations of metallic and composite parts and friction stir welding. some typical industrial operations are first presented. then, some local and global performances indices are to machining operations of metallic and composite parts and friction stir welding are presented. a new method for the stiffness modeling of serial and parallel manipulators is also introduced. as a matter of fact, some performance indices depend on the stiffness of the manipulator under study. finally, the proposed technique is illustrated by means of the comparison of three degrees of freedom translational manipulators.
comments on: "continuous initial observability of nonlinear delay parabolic equations"  by choi, kwung and park. it is shown that the assumptions of the main result of the cited paper can never be satisfied.
spinodal instability growth in new stochastic approaches. are spinodal instabilities the leading mechanism in the fragmentation of a fermionic system? numerous experimental indications suggest such a scenario and stimulated much effort in giving a suitable description, without being finalised in a dedicated transport model. on the one hand, the bulk character of spinodal behaviour requires an accurate treatment of the one-body dynamics, in presence of mechanical instabilities. on the other hand, pure mean-field implementations do not apply to situations where instabilities, bifurcations and chaos are present. the evolution of instabilities should be treated in a large-amplitude framework requiring fluctuations of langevin type. we present new stochastic approaches constructed by requiring a thorough description of the mean-field response in presence of instabilities. their particular relevance is an improved description of the spinodal fragmentation mechanism at the threshold, where the instability growth is frustrated by the mean-field resilience.
exposure mode study to xenon-133 in a reactor building. the work described in this thesis focuses on the external and internal dose assessment to xenon-133. during the nuclear reactor operation, fission products and radioactive inert gases, as ¹³³xe, are generated and might be responsible for the exposure of workers incase of clad defect.particle monte carlo transport code is adapted inradioprotection to quantify dosimetric quantities.the study of exposure to xenon-133 is conducted byusing monte-carlo simulations based on geant4, ananthropomorphic phantom, a realistic geometry of thereactor building, and compartmental models.the external exposure inside a reactor building isconducted with a realistic and conservative exposurescenario. the effective dose rate and the eye lensequivalent dose rate are determined by monte-carlosimulations. due to the particular emission spectrum ofxenon-133, the equivalent dose rate to the lens of eyesis discussed in the light of expected new eye doselimits.the internal exposure occurs while xenon-133 isinhaled. the lungs are firstly exposed by inhalation, andtheir equivalent dose rate is obtained by monte-carlosimulations. a biokinetic model is used to evaluate theinternal exposure to xenon-133.this thesis gives us a better understanding to thedosimetric quantities related to external and internalexposure to xenon-133. moreover the impacts of thedosimetric changes are studied on the current andfuture dosimetric limits. the dosimetric quantities arelower than the current and future dosimetric limits.
self-decomposable global constraints. scalability becomes more and more critical to decision support technologies. in order to address this issue in constraint programming, we introduce the family of self-decomposable constraints. these constraints can be satisfied by applying their own filtering algorithms on variable subsets only. we introduce a generic framework which dynamically decompose propagation, by filtering over variable subsets. our experiments over the cumulative constraint illustrate the practical relevance of self-decomposition.
optimization of customer orders routing in a collaborative distribution network. this paper presents a sequential approach for the assessment of a multi-layered distribution network     from a cluster of collaborating suppliers to a large set of customers.      the transportation network includes three segments: suppliers routes from suppliers to a consolidation and distribution center,     full truckload routes toward regional distribution centers, and less-than-truckload distribution toward final customers.     in every shipping date, the optimization problem consists of assigning customers to regional distribution centers     and determining the routes of vehicles through the whole distribution network.     this problem is first modeled as a mixed integer linear problem (milp).     then, we propose to decompose it into three smaller milps that are solved sequentially in order to quickly provide a good approximate solution.     the experiments on real data show that the decomposition method provides near optimal solutions within a few minutes while the original model would require hours of calculation.
searches for anisotropies in the arrival directions of the highest energy cosmic rays detected by the pierre auger observatory. we analyze the distribution of arrival directions of ultra-high energy cosmic rays recorded at the pierre auger observatory in 10 years of operation. the data set, about three times larger than that used in earlier studies, includes arrival directions with zenith angles up to 80∘, thus covering from −90∘ to +45∘ in declination. after updating the fraction of events correlating with the active galactic nuclei (agns) in the v\'eron-cetty and v\'eron catalog, we subject the arrival directions of the data with energies in excess of 40 eev to different tests for anisotropy. we search for localized excess fluxes and for self-clustering of event directions at angular scales up to 30∘ and for different threshold energies between 40~eev and 80~eev. we then look for correlations of cosmic rays with celestial structures both in the galaxy (the galactic center and galactic plane) and in the local universe (the super-galactic plane). we also examine their correlation with different populations of nearby extragalactic objects: galaxies in the 2mrs catalog, agns detected by swift-bat, radio galaxies with jets and the centaurus~a galaxy. none of the tests shows a statistically significant evidence of anisotropy. the strongest departures from isotropy (post-trial probability ∼1.4\%) are obtained for cosmic rays with e&gt;58~eev in rather large windows around swift agns closer than 130~mpc and brighter than 1044~erg/s (18∘ radius) and around the direction of centaurus~a (15∘ radius).
large scale distribution of ultra high energy cosmic rays detected at the pierre auger observatory with zenith angles up to 80$^\circ$. we present the results of an analysis of the large angular scale distribution of the arrival directions of cosmic rays with energy above 4 eev detected at the pierre auger observatory including for the first time events with zenith angle between $60^\circ$ and $80^\circ$. we perform two rayleigh analyses, one in the right ascension and one in the azimuth angle distributions, that are sensitive to modulations in right ascension and declination, respectively. the largest departure from isotropy appears in the $e &gt; 8$ eev energy bin, with an amplitude for the first harmonic in right ascension $r_1^\alpha =(4.4 \pm 1.0){\times}10^{-2}$, that has a chance probability $p(\ge r_1^\alpha)=6.4{\times}10^{-5}$, reinforcing the hint previously reported with vertical events alone.
explanation-based large neighborhood search. one of the most well-known and widely used local search techniques for solving optimization problems in constraint programming is the large neigh-borhood search (lns) algorithm. such a technique is, by nature, very flexible and can be easily integrated within standard backtracking procedures. one of its drawbacks is that the relaxation process is quite often problem dependent. several works have been dedicated to overcome this issue through problem independent parameters. nevertheless, such generic approaches need to be carefully parameter-ized at the instance level. in this paper, we demonstrate that the issue of finding a problem independent neighborhood generation technique for lns can be addressed using explanation-based neighborhoods. an explanation is a subset of constraints and decisions which justifies a solver event such as a domain modification or a conflict. we evaluate our proposal for a set of optimization problems. we show that our approach is at least competitive with or even better than state-of-the-art algorithms and can be easily combined with state-of-the-art neighborhoods. such results pave the way to a new use of explanation-based approaches for improving search.
tree-based graph partitioning constraint. combinatorial problems based on graph partitioning enable us to mathematically represent and model many practical applications. mission planning and the routing problems occurring in logistics perfectly illustrate two such examples. nevertheless, these problems are not based on the same partitioning pattern: generally, patterns like cycles, paths, or trees are distinguished. moreover, the practical applications are often not limited to theoretical problems like the hamiltonian path problem, or k-node disjoint path problems. indeed, they usually combine the graph partitioning problem with several restrictions related to the topology of nodes and arcs. the diversity of implied constraints in real-life applications is a practical limit to the resolution of such problems by approaches considering the partitioning problem independently from each additional restriction.this book focuses on constraint satisfaction problems related to tree partitioning problems enriched by several additional constraints that restrict the possible partitions topology. on the one hand, this title focuses on the structural properties of tree partitioning constraints. on the other hand, it is dedicated to the interactions between the tree partitioning problem and classical restrictions (such as precedence relations or incomparability relations between nodes) involved in practical applications.precisely, tree-based graph partitioning constraint shows how to globally take into account several restrictions within one single tree partitioning constraint. another interesting aspect of this book is related to the implementation of such a constraint. in the context of graph-based global constraints, the book illustrates how a fully dynamic management of data structures makes the runtime of filtering algorithms independent of the graph density.
synchronized sweep algorithms for scalable scheduling constraints. this report introduces a family of synchronized sweep based filtering algorithms for handling scheduling problems involving resource and precedence constraints. the key idea is to filter all constraints of a scheduling problem in a synchronized way in order to scale better. in addition to normal filtering mode, the algorithms can run in greedy mode, in which case they perform a greedy assignment of start and end times. the filtering mode achieves a significant speed-up over the decomposition into independent cumulative and precedence constraints, while the greedy mode can handle up to 1 million tasks with 64 resources constraints and 2 million precedences. these algorithms were implemented in both choco and sicstus.
toward sustainable development in constraint programming. null
propagating regular counting constraints. constraints over finite sequences of variables are ubiquitous in sequencing and timetabling. moreover, the wide variety of such constraints in practical applications led to general modelling techniques and generic propagation algorithms, often based on deterministic finite automata (dfa) and their extensions. we consider counter-dfas (cdfa), which provide concise models for regular counting constraints, that is constraints over the number of times a regular-language pattern occurs in a sequence. we show how to enforce domain consistency in polynomial time for atmost and atleast regular counting constraints based on the frequent case of a cdfa with only accepting states and a single counter that can be incremented by transitions. we also prove that the satisfaction of exact regular counting constraints is np-hard and indicate that an incomplete algorithm for exact regular counting constraints is faster and provides more pruning than the existing propagator from [3]. regular counting constraints are closely related to the costregular constraint but contribute both a natural abstraction and some computational advantages.
linking prefixes and suffixes for constraints encoded using automata with accumulators. consider a constraint on a sequence of variables functionally determining a result variable that is unchanged under reversal of the sequence. most such constraints have a compact encoding via an automaton augmented with accumulators, but it is unknown how to maintain domain consistency efficiently for most of them. using such an automaton for such a constraint, we derive an implied constraint between the result variables for a sequence, a prefix thereof, and the corresponding suffix. we show the usefulness of this implied constraint in constraint solving, both by local search and by propagation-based systematic search.
inclusive photon production at forward rapidities in proton-proton collisions at $\sqrt{s}$ = 0.9, 2.76 and 7 tev. the multiplicity and pseudorapidity distributions of inclusive photons have been measured at forward rapidities ($2.3 &lt; \eta &lt; 3.9$) in proton-proton collisions at three center-of-mass energies, $\sqrt{s}=0.9$, 2.76 and 7 tev using the alice detector. it is observed that the increase in the average photon multiplicity as a function of beam energy is compatible with both a logarithmic and a power-law dependence. the relative increase in average photon multiplicity produced in inelastic pp collisions at 2.76 and 7 tev center-of-mass energies with respect to 0.9 tev are 37.2% $\pm$ 0.3% (stat) $\pm$ 8.8% (sys) and 61.2% $\pm$ 0.3% (stat) $\pm$ 7.6% (sys), respectively. the photon multiplicity distributions for all center-of-mass energies are well described by negative binomial distributions. the multiplicity distributions are also presented in terms of kno variables. the results are compared to model predictions, which are found in general to underestimate the data at large photon multiplicities, in particular at the highest center-of-mass energy. limiting fragmentation behavior of photons has been explored with the data, but is not observed in the measured pseudorapidity range.
gbfs: efficient data-sharing on hybrid platforms. towards adding wan-wide elasticity to dfses. applications dealing with huge amounts of data suffer significant performance impacts when they are deployed on top of an hybrid platform (i.e the extension of a local infras- tructure with external cloud resources). more precisely, through a set of preliminary experiments we show that mechanisms which enable on demand extensions of current distributed file systems (dfses) are required. these mechanisms should be able to leverage external storage resources while taking into account the performance constraints imposed by the physical network topology used to interconnect the different sites. our answer to such a challenge is the group based file system proposal (gbfs), a glue providing the elasticity capability for storage resources by federating on demand any posix file systems. although our first prototype is under heavy development, we discuss in this paper the gbfs model and few preliminary but promising results.
a memetic algorithm for the hub location-routing problem. in many logistic systems for less than truckload (ltl) shipments, transportation of goods is made through collection/delivery tours to/from a hub. the design of such a logistic network corresponds to the hub location routing problem (hlrp). hlrp consists in locating hub facilities concentrating flows in order to take advantage of economies of scale and through which flows are to be routed from origins to destinations, and considers also both collection and distribution routes. we present a generic mip formulation of this problem and a solution method based on a genetic algorithm improved by some local searches. computational experiments are presented.
a taxonomy of domain-specific aspect languages. domain-specific aspect languages (dsals) are domain-specific languages (dsls) designed to express crosscutting concerns. compared to dsls, their aspectual nature greatly amplifies the language design space. we structure this space in order to shed light on and compare the different domain-specific approaches to deal with crosscutting concerns. we report on a corpus of 36 dsals covering the space, discuss a set of design considerations and provide a taxonomy of dsal implementation approaches. this work serves as a frame of reference to dsal and dsl researchers, enabling further advances in the field, and to developers as a guide for dsal implementations.
accountability for data protection. null
geometric and elastostatic calibration of robotic manipulator using partial pose measurements. the paper deals with geometric and elastostatic calibration of robotic manipulator using partial pose measurements, which do not provide the end-effector orientation. the main attention is paid to the efficiency improvement of identification procedure. in contrast to previous works, the developed calibration technique is based on the direct measurements only. to improve the identification accuracy, it is proposed to use several reference points for each manipulator configuration. this allows avoiding the problem of non-homogeneity of the least-square objective, which arises in the classical identification technique with the full-pose information (position and orientation). its efficiency is confirmed by the comparison analysis, which deals with the accuracy evaluation of different identification strategies. the obtained theoretical results have been successfully applied to the geometric and elastostatic calibration of serial industrial robot employed in a machining work-cell for aerospace industry.
q-intersection algorithms for constraint-based robust parameter estimation. given a set of axis-parallel n-dimensional boxes, the q-intersection is defined as the smallest box encompassing all the points that belong to at least q boxes. computing the q-intersection is a combinatorial problem that allows us to han-dle robust parameter estimation with a numerical constraint programming approach. the q-intersection can be viewed as a filtering operator for soft constraints that model measure-ments subject to outliers. this paper highlights the equiva-lence of this operator with the search of q-cliques in a graph whose boxicity is bounded by the number of variables in the constraint network. we present a computational study of the q-intersection. we also propose a fast heuristic and a sophisti-cated exact q-intersection algorithm. first experiments show that our exact algorithm outperforms the existing one while our heuristic performs an efficient filtering on hard problems.
beam monitoring and dosimetry tools for radiobiology experiments at the cyclotron arronax. the arronax (accélérateur pour la recherche en radiochimie et oncologie à nantes atlantique) cyclotron in saint herblain - france is a facility delivering alpha particles at 68 mev (1). one of its purposes is to become a platform for radiobiological studies. the radiobiological studies evolvearound two axes: the low energy range (&lt;10mev) in order to optimize radio-immunotherapy (rit) treatments, and the high energy range (30-68 mev) in order to puzzle out the fundamental mechanisms generated by cells in response to ionizing radiations.the arronax platform for radiobiology is currently preparing to use a time lapse fluorescence confocal microscope suitable for the irradiation of cell wells. this platform should contain tools for beam intensity checks to enable accurate and repeatable irradiation conditions and a device to monitor thedelivered dose.
the megapie 1 mw target in support to ads development: status of r&amp;d and design. the megapie project is aimed at designing, building and operating a liquid metal spallation neutron target as a key experiment on the road to an experimental accelerator driven system and to improve the neutron flux at the psi spallation source. the design of the target system has been completed. the target configuration and the operating conditions have been defined and the expected performance assessed. a preliminary safety analysis has been performed considering normal, off-normal and accident conditions and a corresponding report has been submitted to the authorities for licensing. the experience gained up to now shows that megapie may well be the first liquid metal target to be irradiated under high power beam conditions.
experience from the post-test analysis of megapie. the (megawatt pilot experiment) megapie target was successfully irradiated in 2006 at the sinq facility of the paul scherrer institut. during the irradiation a series of measurements to monitor the operation of the target, the thermal hydraulics behavior and the neutronic and nuclear aspects, has been performed. in the post-test analysis phase of the project, the data were analyzed and important information relevant to accelerator-driven systems (ads) was gained, in particular: (i) from the operation of the target several recommendations concern the simplification of the system and the improved reliability; (ii) data from the thermal hydraulic measurements have offered the opportunity to validate the codes used in the design phase; (iii) the neutronic analysis confirm the high performance of a liquid metal target and the importance of the delayed neutron measurements in an ads target; (iv) the nuclear measurements of the gas released gave the opportunity to validate the codes used during the design phase and provided indications for the operation. from the results in these different domains recommendations to further development of ads and heavy liquid metal targets are discussed.
gas production in the megapie spallation target. the megawatt pilot experiment (megapie) project was started in 2000 to design, build, and operate a liquid lead-bismuth eutectic (lbe) spallation neutron target at the power level of 1 mw. the target was irradiated for 4 months in 2006 at the paul scherrer institute in switzerland. gas samples wereextracted in various phases of operation and analyzed by g spectroscopy, leading to the determination of the main radioactive isotopes released from the lbe. comparison with calculations performed using several validated codes (mcnpx2.5.0/cinder’90, fluka/orihet, and snt) yields the ratio between simulated in-target isotope production rates and experimental amounts released at any given time. this work underlines the weak points of spallation models for some released isotopes. also, results provide relevant information for safety and radioprotection in an accelerator-driven system and more particularly for the gas management in a spallation target dedicated to neutron production facilities.
report on the selection of the reference xt-ads target design and specifications. the xt-ads is an experimental accelerator driven system (ads) that is being developed in the framework of the european fp6 eurotrans project. in this deliverable, the specifications of the spallation target and the selection of its reference design are discussed. justification of the design options, in relation to the performance requirements of the xt-ads and the interlinking with the design of the sub-critical core and the primary system, are given.
general synthesis report of the different ads design status. establishment of a catalogue of the r&amp;d needs. this document is a general synthesis report of the different ads design status being designed within the eurotrans integrated project; an fp6 european commission partially funded programme. this project had the goal to demonstrate the possibility of nuclear waste transmutation/burning in accelerator driven systems (ads) at industrial scale.the focus is on a pb-cooled ads for the european facility on industrial scale transmuter (etd/efit) with a back-up solution based on an he cooled ads.as an intermediate step towards this industrial-scale prototype, an experimental transmuter based on ads concept (etd/xt-ads) able to demonstrate both the feasibility of the ads concept and to accumulate experience when using dedicated fuel sub-assemblies or dedicated pins within a mox fuel core has been also studied.the two machines (xt-ads and pb cooled efit) have been designed in a consistent way bringing more credibility to the potential licensing of these plants and with sufficient details to allow definition of the critical issues as regards design, safety and associated technological and basic r&amp;d needs. the different designs fit rather well with the technical objectives fixed at the beginning of the project in consistency with the european roadmap on ads development.for what concerns the accelerator, the superconducting linac has been clearly assessed as the most suitable concept for the three reactors in particular with respect to the stringent requirements on reliability. associated r&amp;d needs have been identified and will be focused on critical components (injector, cryomodule) long term testing.the design of the different ads has been performed in view of what is reasonably achievable pending the completion of r&amp;d programmes. the way the eurotrans integrated project has been organised with other domains than the dm1 design being specifically devoted to r&amp;d tasks in support to the overall etd/efit and etd/xt-ads design tasks has been helpful. the other domains were centred on the assessment of reactivity measurement techniques (dm2 ecats), on the development of u-free dedicated fuels (dm3 aftra), on materials behaviour and heavy liquid metal technology (dm4 demetra) and on nuclear data assessment (dm5 nudatra). pending questions associated to technology gaps have been identified through the different appropriate r&amp;d work programmes and a catalogue of the r&amp;d needs has been established.finally, the work within the eurotrans integrated project has provided an overall assessment of the feasibility at a reasonable cost for an ads based transmutation so that a decision can be taken to launch a detailed design and construction of the intermediate step experimental ads now already launched within the 7th fp programme under the name of common design team (cdt).
a new characterization of relevant intervals for energetic reasoning. energetic reasoning (er) is a powerful filtering algorithm for the cumulative constraint. unfortunately, er is generally too costly to be used in practice. one reason of its bad behavior is that many intervals are considered as relevant, although most of them should be ignored. in the literature, heuristic approaches have been developed in order to reduce the number of intervals to consider, leading to a loss of filtering. in this paper, we provide a sharp characterization that allows to reduce the number of intervals by a factor seven without loss of filtering.
a declarative paradigm for robust cumulative scheduling. this paper investigates cumulative scheduling in uncertain environments, using constraint programming. we present a new declarative characterization of robustness, which preserves solution quality.we highlight the significance of our framework on a crane assignment problem with business constraints.
definition of the accelerator driven system. this report first describes the efit design from a high level. then it analyzes the transmuta-tion capabilities of the core for the minor actinide stream that was selected in work package 1 of this project. next this report describes the impact of the minor actinide loading on the safety parameters of the reactor core, typically the doppler effect and coolant void effect are studied. when the reference system is analyzed, the need to estimate the capacity needed to reach the scenario goals is analyzed. this is done for several units deployment to give an idea on the performance of the second stratum. finally, an exercise has been done to see if the power of the facility could be increased from the reference 400 mwth to 600 mwth, hence increasing the transmutation rate per unit significantly. clearly, this has to be done without safety issues.
final report on the feasibility of the xt-ads spallation target. in the framework of the eurotrans project an experimental accelerator driven system “xt-ads” is being designed. this document is the final deliverable of work package 1.4, the project in which the design of the spallation target for the xt-ads is developed. the present status of the design of a windowless spallation target for the xt-ads is discussed with particular focus on the modifications made as compared to myrrha draft 2. the document is composed of two major parts. after the definition of the design boundary conditions, a first major part deals with the general layout of the spallation target loop with a discussion of the current lay-out options. in the second part the different design support studies that have been performed are described. these include the spallation target zone and spallation loop thermal-hydraulics, system dynamics, beam impact studies, neutronics, mechanical integration and safety analyses. in the last section some conclusions are formulated.
design and r&amp;d support of the xt-ads spallation target. the experimental accelerator-driven system xt-ads is being developed within the framework of the european fp6 project eurotrans. the device will serve a twofold purpose. firstly it will act as an ads concept demonstrator and secondly it will serve as a flexible fast neutron spectrum experimental irradiation tool for materials, fuel materials and radioactive isotopes studies. because of a functional similarity between the xt-ads and the myrrha concept that was developed earlier at sck-cen in mol, belgium, the design file of the latter was chosen as a starting point for the development of the xt-ads.
webexpir: windowless target electron beam experimental irradiation. the windowless target electron beam experimental irradiation (webexpir) program was set-up as part of the myrrha/xt-ads r&amp;d effort on the spallation target design to investigate the interaction of a proton beam with a liquid lead–bismuth eutectic (lbe) free surface. in particular, possible free surface distortion or shockwave effects in nominal conditions and during sudden beam on/off transient situations, as well as possible enhanced evaporation were assessed. an experiment was conceived at the iba tt-1000 rhodotron, where a 7 mev electron beam was used to simulate the high power deposition at the myrrha/xt-ads lbe free surface. the geometry and the lbe flow characteristics in the webexpir set-up were made as representative as possible of the actual situation in the myrrha/xt-ads spallation target. irradiation experiments were carried out at beam currents of up to 10 ma, corresponding to 40 times the nominal beam current necessary to reproduce the myrrha/xt-ads conditions. preliminary analyses show that the webexpir free surface flow was not disturbed by the interaction with the electron beam and that vacuum conditions stayed well within the design specifications.
loose coupling and substitution principle in objet-oriented frameworks for web services. today, the implementation of services (soap and restful models) and of client applications is increasingly based on object-oriented programming languages. thus, object-oriented frameworks for web services are essentially composed with two levels: an object level built over a service level. in this context, two properties could be particularly required in the specification of these frameworks: (i)first a loose coupling between the two levels, which allows the complex technical details of the service level to be hidden at the object level and the service level to be evolved with a minimal impact on the object level, (ii) second, an interoperability induced by the substitution principle associated to subtyping in the object level, which allows to freely convert a value of a subtype into a supertype. in this thesis, first we present the existing weaknesses of object-oriented frameworks related to these two requirements. then, we propose a new specification for object-oriented web service frameworks in order to resolve these problems. as an application, we provide an implementation of our specification in the cxf framework, for both soap and restful models.
an alns for a two echelon vehicle routing problem arising in city logistics. null
an adaptive large neighborhood search for the two-echelon multiple-trip vehicle routing problem with satellite synchronization. null
an adaptive large neighborhood search for a two echelon vehicle routing problem arising in city logistics. null
advanced plutonium management in pwr, complementarity of thorium and uranium. null
a full truckload routing and scheduling problem with split delivery and resource synchronization. null
megapie target design and dimensioning. in a collaborative effort, the megawatt pillot experiment is an initiative to design, build, operate and decommission an exploratory liquid lead-bismuth spallation target for 1 mw of beam power, taking advantage of the existing spallation neutron facility and accelerator complex sinq in psi.after a successful engineering design review in november 2001, the technical review meeting in march 2002 and the first detailed design review in april 2002, the target design is now almost completed, the manufacturing phase is launched and the construction will start in april 2003 after the second detail design review and the ready for manufacturing review in march. the present contribution is not a detailed description of the target design, but gives an overview on some selected issues of relevance before going into manufacturing.this paper is based on the presentation held at the megapie technical review meeting in paris on march 18-19, 2003. the target heat exchanger and the electromagnetic pump system are presented in separate papers of the meeting and will not be detailed in this paper.
evaluation of potential innovative positron-emitting radionuclides for pet applications. the opportunities offered by the construction in nantes (france) of a high energy (70 mev) and high intensity (2 simultaneous 350 μa proton beams and one 30 μa alpha beam) cyclotron (arronax) for radiochemistry and nuclear medicine research has prompted us to reinvestigate the production of several beta+ isotopes. fluorine-18 is widely used in clinical routine practice for pet examination, but fails to achieve all the potential benefits of pet imaging for one major reason: its short half-life is not suited to the long kinetics of large molecules, e.g. antibodies, which may be used for diagnostics (immuno-pet) and radioimmunotherapy. since pet cameras achieve better image quantification than spect cameras, beta+/beta- couples of the same element, which will follow the same metabolic pathways, could be used to monitor radionuclide distribution, and thus to obtain more precise dosimetry, especially in the field of radioimmunotherapy (rit).
estimation of the chooz cores fission rates and associated errors in the framework of the double chooz experiment. the double chooz experiment is designed to search for a non-vanishing mixing angle θ₁₃ characterizing the ability of neutrinos to oscillate. it consists in two identical detectors located respectively at 400 m and 1050 m of the two pressurized water reactors of the chooz nuclear plant in the french ardennes. indeed, nuclear reactor are huge electron antineutrino emitters (about 10²¹ ⊽ₑ/s for a 1gwe reactor). in double chooz, antineutrino sare detected by the inverse beta decay process in the liquid scintillator of the detectors : ⊽ₑ + p −&gt; e⁺ + n. the θ₁₃ parameter can be investigated searching for ⊽ₑ disappearance and ⊽ₑ energy distortion in the far detector with respect to the near detector. the first phase of the experiment during which only the far detector is taking data has started in april 2011. in absence of far detector whose installation will be completed in 2014, a prediction of the non-oscillated antineutrino flux and spectrum shape expected in the far detector is mandatory to measure θ₁₃ . in this manuscript, we present the simulation work performed to predict the fission rates of both chooz cores responsible for the reactor antineutrino flux. in this view, a complete core model has been developed with the mcnp utility for reactor evolution (mure) simulation code. the results of these simulations were used to determine the fission rates and associated systematic errors since the beginning of data taking and led to the first indication for a non-zero θ₁₃ mixing angle in november 2011.
high energy ion beam analysis at arronax. null
measurement of 230pa and 186re production cross sections induced by deuterons at arronax facility. a dedicated program has been launched on production of innovative radionuclides for pet imaging and for beta- and alpha targeted radiotherapy using proton or alpha particles at the arronax cyclotron. since the accelerator is also able to deliver deuteron beams up to 35 mev, we have reconsidered the possibility of using them to produce medical isotopes. two isotopes dedicated to targeted therapy have been considered: 226th, a decay product of 230pa, and 186re. the production cross sections of 230pa and 186re, as well as those of the contaminants created during the irradiation, have been determined by the stacked-foil technique using deuteron beams. experimental values have been quantified using a referenced cross section. the measured cross sections have been used to determine expected production yields and compared with the calculated values obtained using the talys code with default parameters.
ebt2 films response toalpha radiation at 48.3 mev. to advance the development of a radiobiological experimental set-up for alpha particle irradiations at the arronax cyclotron, experiments were performed to get the dose response of gafchomic ebt2 films for alpha particles at 48.3 mev. a system has been developed using a thin monitor copper foil and an x-ray spectrometer to measure the beam intensity and to calculate the delivered dose. on the other hand, the authors have irradiated ebt2 films, with 6-mv x rays, to get the dose response of ebt2 films for photons. the dose response curve for alpha particles shows an effect of polymerisation saturation compared with the dose response curve for photons.
development of a pixe method at high energy with the arronax cyclotron. the high energy pixe (hepixe) method is a multi-elemental non-destructive ion beam analysis technique. it is based on the detection of the x-ray emitted due to the interaction of high energy particle beam with a sample. this technique is fast and allows the analysis of heavy and medium elements in thin (lm), thick (mm) and multilayer samples. at the arronax facility (nantes, france), the hepixe method has been used to determine the composition of natural and synthetic sodalites. photochromic properties of these samples are supposed to come from the trace elements (concentration in the ppm range) present in the samples. taking advantage of the 70 mev proton beam available at our facility, the hepixe methodhas been also used to study multilayer samples. it has been shown that it is possible to determine the composition of each layer, their thicknesses and their depth position by analyzing the recorded x-ray spectra.
232th(d,xn)230,232,233pa cross section measurements. cross sections for the (d,n), (d,2n) and (d,4n) reactions on 232th were measured using the stackedfoil technique with beams provided by the arronax cyclotron. these data are of relevance for the production of radionuclides. the measured cross sections were compared with previous measurements as well as with theoretical calculations using the code talys.
measurement of volatile radionuclides production and release yields followed by a post-irradiation analysis of a pb/bi filled ta target at isolde. a crucial requirement in the development of liquid-metal spallation neutron target is knowledge of the composition and amount of volatile radionuclides that are released from the target during operation. it is also important to know the total amount produced, which could be released if there was an accident. one type is the lead-bismuth eutectic (lbe) target where different radionuclides can be produced following interaction with a high-energy proton beam, notably noble gases (ar, kr, xe isotopes) and other relative volatile isotopes such as hg and at. the results of an irradiation experiment performed at isolde on a lbe target are compared with predictions from the mcnpx code using the latest developments on the li'ege intranuclear cascade model (incl4.6) and the cem03 model. the calculations are able to reproduce the mass distribution of the radioisotopes produced, including the at production, where there is a significant contribution from secondary reactions. subsequently, a post-irradiation examination of the irradiated target was performed. investigations of both the tantalum target structure, in particular the beam window, and the leadbismuth eutectic were performed using several experimental techniques. no sign of severe irradiation damage, previously observed in other isolde targets, was found.
232th(d,4n)230pa cross-section measurements at arronax facility for the production of 230u. introduction: 226th (t1/2 = 31 min) is a promising therapeutic radionuclide since results, published in 2009, showed that it induces leukemia cells death and activates apoptosis pathways with higher efficiencies than 213bi. 226th can be obtained via the 230u α decay. this study focuses on the 230u production using the 232th(d,4n)230pa(β−)230u reaction.methods: experimental cross sections for deuteron-induced reactions on 232th were measured from 30 down to 19 mev using the stacked-foil technique with beams provided by the arronax cyclotron. after irradiation, all foils (targets as well as monitors) were measured using a high-purity germanium detector.results: our new 230pa cross-section values, as well as those of 232pa and 233pa contaminants created during the irradiation, were compared with previous measurements and with results given by the talys code. experimentally, same trends were observed with slight differences in orders of magnitude mainly due to the nuclear data change. improvements are ongoing about the talys code to better reproduce the data fordeuteron-induced reactions on 232th.conclusions: using our cross-section data points from the 232th(d,4n)230pa reaction, we have calculated the thick-target yield of 230u, in bq/μa·h. this value allows now to a full comparison between the different production routes, showing that the proton routes must be preferred.
measurements of 186re production cross section induced by deuterons on natw target at arronax facility. introduction: the arronax cyclotron, acronym for “accelerator for research in radiochemistry and oncology at nantes atlantique” is a new facility installed in nantes, france. a dedicated program has been launched on production of innovative radioisotopes for pet imaging and for β− and α targeted radiotherapy using protons or α particles. since the accelerator is also able to deliver deuteron beams up to 35 mev, we have reconsidered the possibility of using them to produce medical isotopes. indeed, in some cases, the use of deuterons allows higher production yield than protons.methods: 186re is a β− emitter which has chemical properties close to the widely used 99mtc and has been used in clinical trials for palliation of painful bone metastases resulting from prostate and breast cancer. 186re production cross section has been measured between 9 and 23 mev using the arronax deuteron beam and the stacked-foil technique.a novelty in our work is the use of a monitor foil behind each natwtarget foil in order to record efficiently the deuteron incident flux and energies all over the stack relying on the international atomic energy agency (iaea) recommended cross section of the natti(d,x)48v reaction. since a good optimization process is supposed to find the best compromise between production yield and purity of the final product, isotope of interest and contaminants created during irradiation are measured using gamma spectrometry.results: our new sets of data are presented and compared with the existing ones and with results given by the talys code calculations. the thick target yield (tty) has been calculated after the fit of our experimental values and compared with the iaea recommended ones.conclusions: presented values are in good agreement with existing data. the deuteron production route is clearly the best choice with a tty of 7.8 mb/μah at 30 mev compared to 2.4 mbq/μah for proton as projectile at the same energy. the talys code gives satisfactory results for 183,186re isotopes.
study of the single electron charge signals in the xenon100 direct dark matter search experiment. from the observation of the universe, it has been demonstrated that the mass associated to visible matter represents only few percent of its energetic budget, while the remaining part is composed by dark energy, responsible to the cosmological expansion, and by some hidden matter, the dark matter. the likeliest particles family used to describe this dark matter is called wimp (weakly interacting massive particles). that kind of particles could be directly detected by measuring nuclear recoil during an elastic scattering inside a scintillating material. for this, the xenon collaboration has developed a detector consisting in a time projection chamber (tpc) using xenon dual phase (liquid and gas) detector, and placed underground. the different ionization density of nuclear recoils induced by wimps, and electronic recoils induced by b particles or g rays background source, leads to different ratio between both signals, in the liquid and in the gas phase, and is used to discriminate wimps from background. a good knowledge of the ionization signal is strongly required for such a detector. in this context, the xenon100 response to single electron charge signals is investigated. they correspond to very tiny signals emitted in the gas phase by one or few electrons extracted in time coincidence. thanks to this analysis, an innovative method to establish the extraction yield of electrons from the liquid to the gas phase has been drawn, allowing to explore a key information to reject electronic recoils from nuclearones.
computational determination of the dominant triplet population mechanism in photoexcited benzophenone. in benzophenone, intersystem crossing occurs efficiently between s1(nπ*) and the t1 state of dominant nπ* character, leading to excited triplet states after photoexcitation. the transition mechanism between s1(nπ*) and t1 is still a matter of debate, despite several experimental studies. quantum mechanical calculations have been performed in order to asses the relative efficiencies of previously proposed mechanisms, in particular the direct s1 → t1 and indirect s1 → t2(ππ*) → t1 ones. multiconfigurational wave function based methods are used to discuss the nature of the relevant states and also to determine minimum energy paths and conical intersections. it is found that the t1 state has a mixed nπ*/ππ* character and that the t2(ππ*) state acts as an intermediate state between the s1 and t1 states. this result is in line with recent experiments, which suggested a two-step kinetic model to populate the phosphorescent state after photoexcitation [aloïse et al., j. phys. chem. a 2008, 112, 224-231].
technical system and organiation : a question of learning. null
energy dependence of k/π, p/π, and k/p fluctuations in au+au collisions from snn−−−√ = 7.7 to 200 gev. a search for the quantum chromodynamics (qcd) critical point was performed by the star experiment at the relativistic heavy ion collider, using dynamical fluctuations of unlike particle pairs. heavy-ion collisions were studied over a large range of collision energies with homogeneous acceptance and excellent particle identification, covering a significant range in the qcd phase diagram where a critical point may be located. dynamical k/π, p/π, and k/p fluctuations as measured by the star experiment in central 0-5% au+au collisions from center-of-mass collision energies snn−−−√ = 7.7 to 200 gev are presented. the observable νdyn was used to quantify the magnitude of the dynamical fluctuations in event-by-event measurements of the k/π, p/π, and k/p pairs. the energy dependences of these fluctuations from central 0-5% au+au collisions all demonstrate a smooth evolution with collision energy.
elliptic and triangular flow of heavy flavor in heavy-ion collisions. we investigate the elliptic and the triangular flow of heavy mesons in ultrarelativistic heavy-ion collisions at rhic and the lhc. the dynamics of heavy quarks is coupled to the locally thermalized and fluid dynamically evolving quark-gluon plasma. the elliptic flow of $d$ mesons and the centrality dependence measured at the lhc is well reproduced for purely collisional and bremsstrahlung interactions. due to the event-by-event fluctuating initial conditions from the epos2 model, the $d$ meson triangular flow is predicted to be nonzero at $\sqrt{s}=200$ gev and $\sqrt{s}=2.76$ tev. we study the centrality dependence and quantify the contributions stemming from flow of the light bulk event and the hadronization process. the flow coefficients as response to the initial eccentricities behave differently for heavy mesons than for light hadrons due to their inertia. higher-order flow coefficients of heavy flavor become important in order to quantify the degree of thermalization.
closure of market, formalisation of trade, overflow of the actors. the case of computerised fish auction in pays bigouden (france). les auteurs montrent qu'entre l'offre des marins pêcheurs et la demande des mareyeurs, s'intercalent des criées où sont vendus les produits de la pêche artisanale. après avoir décrit les investissements informatiques successifs des chambres de commerce et de l'industrie (cci) en charge de ces marchés pour mettre en forme les enchères qui s'y déroulent conformément à une représentation marchande empruntant beaucoup à la figure du commissaire-priseur walrassien, les auteurs examinent comment les acteurs composent avec ce dispositif. les mareyeurs, en mutualisant par l'entente les coûts de l'incertitude des approvisionnements comme des débouchés, nous montrent que l'informatisation déplace le contenu politique de l'échange. les producteurs, en maximisant les captures tout en minimisant les informations en direction du marché, nous montrent que si le discours économique orthodoxe véhiculé par les cci existe, il ne pénètre pas tous les acteurs de la même façon.
precision muon reconstruction in double chooz. we describe a muon track reconstruction algorithm for the reactor anti-neutrino experiment double chooz. the double chooz detector consists of two optically isolated volumes of liquid scintillator viewed by pmts, and an outer veto above these made of crossed scintillator strips. muons are reconstructed by their outer veto hit positions along with timing information from the other two detector volumes. all muons are fit under the hypothesis that they are through-going and ultrarelativistic. if the energy depositions suggest that the muon may have stopped, the reconstruction fits also for this hypothesis and chooses between the two via the relative goodness-of-fit. in the ideal case of a through-going muon intersecting the center of the detector, the resolution is ~40 mm in each transverse dimension. high quality muon reconstruction is an important tool for reducing the impact of the cosmogenic isotope background in double chooz.
modeling open-flow steam reforming of methanol over cu/zno/al2o3 catalyst in an axisymmetric reactor. null
environmental performance assessment of retrofitting existing coal fired power plants to co-firing with biomass: carbon footprint and emergy approach. null
the role of water molecules of the first solvation shell in modelling ligand-exchange reactions leading to ato+ hydrolyzed species. null
alpha radiolysis of nitric acid and sodium nitrate with 4he2+ beam of 13.5 mev energy. null
performance degradation of geiger-mode apds at cryogenic temperatures. two-phase cryogenic avalanche detectors (crads) with thgem multipliers, optically read out with geiger-mode apds (gapds), were proposed as potential technique for charge recording in rare-event experiments. in this work we report on the degradation of the gapd performance at cryogenic temperatures revealed in the course of the study of two-phase crad in ar, with combined thgem/gapd-matrix multiplier; the gapds recorded secondary scintillation photons from the thgem holes in the near infrared. the degradation effect, namely the loss of the gapd pulse amplitude, depended on the incident x-ray photon flux. the critical counting rate of photoelectrons produced at the 4.4 mm2 gapd, degrading its performance at 87 k, was estimated as 10000 per second. this effect was shown to result from the considerable increase of the pixel quenching resistor of this cpta-made gapd type. though not affecting low-rate rare-event experiments, the observed effect may impose some limitations on the performance of crads with gapd-based optical readout at higher-rate applications.
comprehensive analysis of fusion data well above the barrier. we report on the comprehensive systematics of nearly 400 fusion-evaporation and/or fusion-fission cross-section data for a very large variety of systems over an energy range ∼3a to 155a mev. scaled by the reaction cross section and expressed as a function of the center-of-mass energy per nucleon, the fusion cross section displays a universal behavior. within experimental errors, this behavior does not depend on system mass, mass asymmetry, or system isospin. the deduced homographic functional dependence for complete and summed complete and incomplete fusion excitation functions is derived from basic strong absorption model formulas for reaction cross sections and allows us to draw the main properties of these functions. the limiting energy for the complete fusion and the main characteristics (onset, maximum, and extinction) of the incomplete fusion excitation functions are determined. the complete fusion reaction process disappears around 6.5 mev/nucleon and the incomplete one disappears at about 13 mev/nucleon in the center-of-mass frame. the regularity in fusion data is particularly obvious for the evaporation-residue subset of the data ensemble. adding the fusion-fission data component does not alter the general data trend but somewhat obscures it owing to the larger uncertainty and/or possible normalization problems.
di-hadron correlations with identified leading hadrons in 200 gev au+au and d+au collisions at star. the star collaboration presents new two-dimensional di-hadron correlations with leading hadrons in 200 gev central au+au and minimum bias d+au collisions to explore hadronization mechanisms in the quark gluon plasma. the enhancement of the jet-like yield for leading pions in au+au data with respect to the d+au reference and the absence of enhancement for leading non-pions (protons and kaons) are discussed within the context of quark recombination. the correlated yield at large angles, specifically in the \emph{ridge region}, is significantly higher for leading non-pions than pions. the consistencies of the constituent quark scaling, azimuthal harmonic model and a mini-jet modification model description of the data are tested, providing further constraints on hadronization.
a large neighborhood search heuristic for supply chain network design. many exact or approximate solution techniques have been used to solve facility location problems and more generally supply chain network design problems. yet, the large neighborhood search technique (lns) has almost never been proposed for solving such problems, although it has proven its efficiency and flexibility in solving other complex combinatorial optimization problems. in this paper we propose an lns framework for solving a four-layer single period multi-product supply chain network design problem involving multimodal transport. location decisions for intermediate facilities (e.g. plants and distribution centers) are made using the lns while transportation modes and product flow decisions are determined by a greedy heuristic. as a post-optimization step, we also use linear programming to determine the optimal product flows once the logistics network is fixed. extensive experiments based on generated instances of different sizes and characteristics show the effectiveness of the method compared with a state-of-the-art solver.
spin physics and tmd studies at a fixed-target experiment at the lhc (after@lhc). we report on the opportunities for spin physics and transverse-momentum dependent distribution (tmd) studies at a future multi-purpose fixed-target experiment using the proton or lead ion lhc beams extracted by a bent crystal. the lhc multi-tev beams allow for the most energetic fixed-target experiments ever performed, opening new domains of particle and nuclear physics and complementing that of collider physics, in particular that of rhic and the eic projects. the luminosity achievable with after@lhc using typical targets would surpass that of rhic by more that 3 orders of magnitude in a similar energy region. in unpolarised proton-proton collisions, after@lhc allows for measurements of tmds such as the boer-mulders quark distributions, the distribution of unpolarised and linearly polarised gluons in unpolarised protons. using the polarisation of hydrogen and nuclear targets, one can measure transverse single-spin asymmetries of quark and gluon sensitive probes, such as, respectively, drell-yan pair and quarkonium production. the fixed-target mode has the advantage to allow for measurements in the target-rapidity region, namely at large x^uparrow in the polarised nucleon. overall, this allows for an ambitious spin program which we outline here.
beam-energy and system-size dependence of the space-time extent of the pion emission source produced in heavy ion collisions. two-pion interferometry measurements are used to extract the gaussian radii $r_{{\rm out}}$, $r_{{\rm side}}$, and $r_{{\rm long}}$, of the pion emission sources produced in cu$+$cu and au$+$au collisions at several beam collision energies $\sqrt{s_{_{nn}}}$ at phenix. the extracted radii, which are compared to recent star and alice data, show characteristic scaling patterns as a function of the initial transverse size $\bar{r}$ of the collision systems and the transverse mass $m_t$ of the emitted pion pairs, consistent with hydrodynamiclike expansion. specific combinations of the three-dimensional radii that are sensitive to the medium expansion velocity and lifetime, and the pion emission time duration show nonmonotonic $\sqrt{s_{_{nn}}}$ dependencies. the nonmonotonic behaviors exhibited by these quantities point to a softening of the equation of state that may coincide with the critical end point in the phase diagram for nuclear matter.
production of inclusive $\upsilon$(1s) and $\upsilon$(2s) in p-pb collisions at $\mathbf{\sqrt{s_{{\rm nn}}} = 5.02}$ tev. we report on the production of inclusive $\upsilon$(1s) and $\upsilon$(2s) in p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev at the lhc. the measurement is performed with the alice detector at backward ($-4.46&lt; y_{{\rm cms}}&lt;-2.96$) and forward ($2.03&lt; y_{{\rm cms}}&lt;3.53$) rapidity down to zero transverse momentum. the production cross sections of the $\upsilon$(1s) and $\upsilon$(2s) are presented, as well as the nuclear modification factor and the ratio of the forward to backward yields of $\upsilon$(1s). a suppression of the inclusive $\upsilon$(1s) yield in p-pb collisions with respect to the yield from pp collisions scaled by the number of binary nucleon-nucleon collisions is observed at forward rapidity but not at backward rapidity. the results are compared to theoretical model calculations including nuclear shadowing or partonic energy loss effects.
review on the tc chemistry at subatech in inorganic media (chloride, carbonate) with radiation effect. null
study induced oxidation/reduction of tc in carbonate media by α and γ radiolysis. null
search for patterns by combining cosmic-ray energy and arrival directions at the pierre auger observatory. energy-dependent patterns in the arrival directions of cosmic rays are searched for using data of the pierre auger observatory. we investigate local regions around the highest-energy cosmic rays with $e \geq 6 \cdot 10^{19}$ ev by analyzing cosmic rays with energies above $e = 5 \cdot 10^{18}$ ev arriving within an angular separation of approximately $15{\deg}$. we characterize the energy distributions inside these regions by two independent methods, one searching for angular dependence of energy-energy correlations and one searching for collimation of energy along the local system of principal axes of the energy distribution. no significant patterns are found with this analysis. the comparison of these measurements with astrophysical scenarios can therefore be used to obtain constraints on related model parameters such as strength of cosmic-ray deflection and density of point sources.
thermodynamic and kinetic study of scandium(iii) complexes of dtpa and dota: a step toward scandium radiopharmaceuticals. diethylenetriamine-n,n,n′,n′′,n′′-pentaacetic acid (dtpa) and 1,4,7,10-tetraazacyclododecane-1,4,7,10-tetraacetic acid (dota) scandium(iii) complexes were investigated in the solution and solid state. three 45sc nmr spectroscopic references suitable for aqueous solutions were suggested: 0.1 m sc(clo4)3 in 1 m aq. hclo4 (δsc=0.0 ppm), 0.1 m sccl3 in 1 m aq. hcl (δsc=1.75 ppm) and 0.01 m [sc(ox)4]5− (ox2−=oxalato) in 1 m aq. k2c2o4 (δsc=8.31 ppm). in solution, [sc(dtpa)]2− complex (δsc=83 ppm, ▵ν=770 hz) has a rather symmetric ligand field unlike highly unsymmetrical donor atom arrangement in [sc(dota)]− anion (δsc=100 ppm, ▵ν=4300 hz). the solid-state structure of k8[sc2(ox)7]⋅13 h2o contains two [sc(ox)3]3− units bridged by twice "side-on" coordinated oxalate anion with sc3+ ion in a dodecahedral o8 arrangement. structures of [sc(dtpa)]2− and [sc(dota)]− in [(hguanidine)]2[sc(dtpa)]⋅3 h2o and k[sc(dota)][h6dota]cl2⋅4 h2o, respectively, are analogous to those of trivalent lanthanide complexes with the same ligands. the [sc(dota)]− unit exhibits twisted square-antiprismatic arrangement without an axial ligand (tsa′ isomer) and [sc(dota)]− and (h6dota)2+ units are bridged by a k+ cation. a surprisingly high value of the last dota dissociation constant (pka=12.9) was determined by potentiometry and confirmed by using nmr spectroscopy. stability constants of scandium(iii) complexes (log kscl 27.43 and 30.79 for dtpa and dota, respectively) were determined from potentiometric and 45sc nmr spectroscopic data. both complexes are fully formed even below ph 2. complexation of dota with the sc3+ ion is much faster than with trivalent lanthanides. proton-assisted decomplexation of the [sc(dota)]− complex (τ1/2=45 h; 1 m aq. hcl, 25 °c) is much slower than that for [ln(dota)]− complexes. therefore, dota and its derivatives seem to be very suitable ligands for scandium radioisotopes.
multidimensional dirac strings and the witten index of symcs theories with groups of higher rank. we discuss generalized dirac strings associated with a given lie group. they live in r-dimensional complex space (r being the rank of the group). such strings show up in the effective born-oppenheimer hamiltonian for 3d supersymmetric yang-mills-chern-simons theories, brought up by the gluon loops. we calculate accurately the number of the vacuum states in the effective hamiltonian associated with these strings. we also show that these states are irrelevant for the final symcs vacuum counting. the witten index of symcs theories depends thus only on the strings generated by fermion loops and carrying fractional generalized fluxes.
surface complexation modeling of eu(iii) and phosphate on na-bentonite: binary and ternary adsorption systems. this study aims to investigate and model the adsorption of eu(iii) on bentonite in the presence of phosphate. binary (phosphate/bentonite) and ternary (eu(iii)/phosphate/bentonite) systems were studied as a function of contact time, ph, solid-to-liquid ratio and eu(iii)/phosphate concentration by using a batch experimental method. the adsorption of phosphate on bentonite slightly increased in the ph range of 2.5-6.5, and decreased in the ph range of 6.5-9.4. this adsorption can be quantitatively interpreted by a model considering the formation of three monodentate surface complexes. in the ternary system, a synergistic adsorption was observed in the presence of both phosphate and eu(iii). in addition to the two sub-models describing eu(iii) and phosphate adsorption, the formation of ternary surface complexes had to be considered in order to explain the synergistic effect experimentally observed. the experimental data could be quantitatively explained when eu(iii) (6-point triple bond; length half of m-dashsoeuh2po4+ and 6-point triple bond; length half of m-dashsoeuhpo4) or phosphate (6-point triple bond; length half of m-dashspo4eu+) are the bridged atoms. complementary experiments carried out by x-ray photoelectron spectroscopy suggested that the second case is the most probable. the proposed model can be used in order to predict eu(iii) adsorption on buffer/backfilling material in the presence of phosphate.
c3po: a spontaneous and ephemeral social networking framework for a collaborative creation and publishing of multimedia contents. online social networks have been adopted by a large part of the population, and have become in few years essential communication means and a source of information for journalists. nevertheless, these networks have some drawbacks that make people reluctant to use them, such as the impossibility to claim for ownership of data and to avoid commercial analysis of them, or the absence of collaborative tools to produce multimedia contents with a real editorial value. in this paper, we present a new kind of social networks, namely spontaneous and ephemeral social networks (sesns). sesns allow people to collaborate spontaneously in the production of multimedia documents so as to cover cultural and sport events.
optimization of the volume fraction of an absorbent phase (silicone oil)and biodegradation kinetics of dmds in a tppb. to improve the removal of hydrophobic volatile org. compds. (voc), an integrated process coupling an absorption step and a biol. treatment in a two-phase partitioning bioreactor was considered. a preliminary bibliog. review allowed to select a silicon oil (polydimethylsiloxane),which appeared efficient for both steps, voc absorption and biodegrdn., since this oil was biocompatible, non-biodegradable and had a negligible soly. in water. the purpose of this work was the implementation of this org. phase in the bioreactor. bod5 measurements showed that a ratio ranging between 20 and 30% of silicone oil in water was found to be optimal for the treatment of dmds, in relation with interfacial area considerations. for a small proportion of oil, the emulsion is homogeneous but the interfacial area is small; while for high ratios, the oil is not totally emulsified and hence the transfer appears limiting leading to decreasing substrate availability. [on scifinder(r)].
mass transfer between a gas phase and two non miscible liquid phases.use of the equivalent absorption capacity concept for gas/liquid/liquidcontactor design. a methodol. for the design of absorbers contacting a gas phase loaded with hydrophobic voc and two immiscible liq. phases (air/water/silicone oil) is presented. the design is based on a concept of mass transfer between phases called the "equiv. absorption capacity". it is demonstrated that voc absorption should be restricted to pollutants characterized by an air/silicone oil partition coeff. around 3 to 4 pa.m3.mol-1 or less. in this case, the gas-liq. mass transfer must be carried out between air and pure silicone oil, allowing a minimization of the diam. of the gas/liq. contactor. [on scifinder(r)].
on developing open source mde tools: our eclipse stories and lessons learned. tool development has always been a fundamental activity of software engineering. nowadays, open source is changing the way this is done in many organizations. traditional ways of doing things are progressively enhanced or even sometimes replaced by new organizational schemes, benefiting as much as possible from the properties of open source (os). this is especially true in innovative areas such as model driven engineering (mde) in which new tools are constantly created, developed and disseminated, many of them coming from research teams. this poses some hard questions: what is the actual impact of os in terms of tool development? how to best take advantage of os communities? and what are the opportunities for research teams in this context? capitalizing on experiences in developing mde os tools on top of the eclipse platform and its license model, we try to give some insights on these questions in this paper.
beyond the clouds, how should next generation utility computing infrastructures be designed?. to accommodate the ever-increasing demand for utility computing (uc) resources while taking into account both energy and economical issues, the current trend consists in building larger and larger data centers in a few strategic locations. although such an approach enables to cope with the actual demand while continuing to operate uc resources through centralized software system, it is far from delivering sustainable and efficient uc infrastructures. we claim that a disruptive change in uc infrastructures is required: uc resources should be man-aged differently, considering locality as a primary concern. to this aim, we pro-pose to leverage any facilities available through the internet in order to deliver widely distributed uc platforms that can better match the geographical dispersal of users as well as the unending resource demand. critical to the emergence of such locality-based uc (luc) platforms is the availability of appropriate operat-ing mechanisms. we advocate the implementation of a unified system driving the use of resources at an unprecedented scale by turning a complex and diverse infra-structure into a collection of abstracted computing facilities that is both easy to operate and reliable. by deploying and using such a luc operating system on backbones, our ultimate vision is to make possible to host/operate a large part of the internet by its internal structure itself: a scalable and nearly infinite set of resources delivered by any computing facilities forming the internet, starting from the larger hubs operated by isps, governments and academic institutions to any idle resources that may be provided by end-users.
effective aspects : a typed monadic model to control and reason about aspect interference. aspect-oriented programming (aop) aims to enhance modularity and reusability in software systems by offering an abstraction mechanism to deal with crosscutting concerns. but, in most general-purpose aspect languages aspects have almost unrestricted power, eventually conflicting with these goals. this work presents effective aspects: a novel approach to embed the pointcut/advice model of aop in a statically-typed functional programming language like haskell; along two main contributions. first, we define a monadic embedding of the full pointcut/advicemodel of aop. type soundness is guaranteed by exploiting the underlying type system, in particular phantom types and a new anti-unification type class. in this model aspects are first-class, can be deployed dynamically, and the pointcut language is extensible, therefore combining the flexibility of dynamically-typed aspect languages with the guarantees of a static type system. monads enable us to directly reason about computational effects both in aspects and base programs using traditional monadic techniques. using this we extend the notion of open modules with effects, and also with protected pointcut interfaces to external advising. these restrictions are enforced statically using the type system. also, we adapt the techniques of effectiveadvice to reason about and enforce control flow properties as well as to control effect interference. we show that the parametricity-based approach to effect interference falls short in the presence of multiple aspects and propose a different approach using monad views, a novel technique for handling the monad stack, developed by schrijvers and oliveira. then, we exploit the properties of our model to enable the modular construction of new semantics for aspect scoping and weaving. our second contribution builds upon a powerful model to reason about mixin-based composition of effectful components and their interference, based on equational reasoning, parametricity, and algebraic laws about monadic effects. our contribution is to show how to reason about interference in the presence of unrestricted quantification through pointcuts. we show that global reasoning can be compositional, which is key for the scalability of the approach in the face of large and evolving systems. we prove a general equivalence theorem that is based on a few conditions that can be established, reused, and adapted separately as the system evolves. the theorem is defined for an abstract monadic aop model; we illustrate its use with a simple version of the model just described. this work brings type-based reasoning about effects for the first time in the pointcut/advice model, in a framework that is expressive, extensible and well-suited for development of robust aspect-oriented systems as well as a research tool for new aspect semantics.
influence of gamma irradiation on uranium determination by arsenazo iii in the presence of fe(ii)/fe(iii). arsenazo iii is a widely used reagent for the concentration measurement of uranium and other actinides in aqueous samples. this study indicates that, for routine aqueous samples, due to the strong complexing ability with arsenazo iii, fe(iii) can significantly decrease the uv-vis absorbance of the u(vi)-arsenazo iii complex, whereas the influence of fe(ii) on the absorbance is negligible. however, when fe(ii) is present in a gamma-irradiated u(vi) aqueous sample, it can give rise to the fenton reaction, which produces oxidizing radicals that decompose the subsequently added arsenazo iii, leading to a sharp decrease in the absorbance of the u(vi)-arsenazo iii complex. the decrease in absorbance depends on the iron content and irradiation dose. furthermore, the oxidizing radicals from the fenton reaction induced by gamma irradiation can be continually produced. even if the irradiated solution has been aged for more than one month in the absence of light at room temperature and without the exclusion of oxygen, the reactivity of the radicals did not decrease toward the subsequently added arsenazo iii. this finding demonstrates that the presence of fe(ii) in gamma-irradiated u(vi) aqueous samples can lead to incorrect u(vi) measurement using the arsenazo iii method, and a new method needs to be developed for the quantitative determination of u(vi) in the presence of gamma radiation and ferrous iron.
lower bounds for resource constrained project scheduling problem. we review the most recent lower bounds for the makespan minimization variant of the resource constrained project scheduling problem. lower bounds are either based on straight relaxations of the problems (e.g., single machine, parallel machine relaxations) or on constraint programming and/or linear programming formulations of the problem.
accuracy improvement of robot-based milling using an enhanced manipulator model. the paper is devoted to the accuracy improvement of robot-based milling by using an enhanced manipulator model that takes into account both geometric and elastostatic factors. particular attention is paid to the model parameters identification accuracy. in contrast to other works, the proposed approach takes into account impact of the gravity compensator and link weights on the manipulator elastostatic properties. in order to improve the identification accuracy, the industry oriented performance measure is used to define optimal measurement configurations and an enhanced partial pose measurement method is applied for the identification of the model parameters. the advantages of the developed approach are confirmed by experimental results that deal with the elastostatic calibration of a heavy industrial robot used for milling. the achieved accuracy improvement factor is about 2.4.
compliance error compensation in robotic-based milling. the paper deals with the problem of compliance errors compensation in robotic-based milling. contrary to previous works that assume that the forces/torques generated by the manufacturing process are constant, the interaction between the milling tool and the workpiece is modeled in details. it takes into account the tool geometry, the number of teeth, the feed rate, the spindle rotation speed and the properties of the material to be processed. due to high level of the disturbing forces/torques, the developed compensation technique is based on the non-linear stiffness model that allows us to modify the target trajectory taking into account nonlinearities and to avoid the chattering effect. illustrative example is presented that deals with robotic-based milling of aluminum alloy.
ethylene glycol intercalation in smectites. molecular dynamics simulation studies. intercalation of ethylene glycol in smectites (glycolation) is widely used to discriminate smectites and vermiculites from other clays and among themselves. during this process, ethylene glycol molecules enter into the interlayer spaces of the swelling clays, leading to the formation of two-layer structure (~17 å) in the case of smectites, or one-layer structure (~14 å) in the case of vermiculites. in spite of the relatively broad literature on the understanding/characterization of ethylene glycol/water-clays complexes, the simplified structure of this complex presented by reynolds (1965) is still used in the contemporary x-ray diffraction computer programs, which simulate structures of smectite and illite-smectite. the monolayer structure is only approximated using the assumption of the interlayer cation and ethylene glycol molecules lying in the middle of interlayer spaces. this study was therefore undertaken to investigate the structure of ethylene glycol/water-clays complex in more detail using molecular dynamics simulation. the structural models of smectites were built on the basis of pyrophyllite crystal structure (lee and guggenheim, 1981), with substitution of particular atoms. in most of simulations, the structural model assumed the following composition, considered as the most common in the mixed layer illite-smectites (środoń et al. 2009): exch0.4(si3.96al0.04)(al1.46fe0.17mg0.37)o10(oh)2 atoms of the smectites were described with clayff force field (cygan et al., 2004), while atoms of water and ethylene glycol with flexible spc (berendsen et al., 1981) and opls (jorgensen et al., 1996) force fields, respectively. ewald summation was used to calculate long range coulombic interactions and the cutoff was set at 8.5 å. results of the simulations show that in the two-layer glycolate the content of water is relatively small: up to 0.8 h2o per half of the smectite unit cell (thereafter phuc). clear thermodynamic preference of mono- or two-layer structure of the complex is observed for typical smectite. based on the calculated radial distribution functions, it was confirmed that water and ethylene glycol molecules compete for the coordination sites of the calcium ions in the clay interlayers. it was also found that the differences in the smectite layer charge, charge location, and the type of the interlayer cation affect the ethylene glycol and water packing in the interlayer space and as result have strong influence on the basal spacing and on the structure of complex. varying amounts and ratio of both ethylene glycol and water are, however, the most important factor influencing the extent of the smectite expansion. comparison of two-layer structure obtained from molecular dynamics simulations with previous models leads to the conclusion that the arrangement of ethylene glycol molecules in the interlayers, used in simulations of x-ray diffractograms of clays, should be modified. in contrast to the reynolds (1965) model, the main difference is that, for different location of the clay charge, interlayer ions tend to change their positions. in the case of montmorillonite, calcium ions are located in the middle of the interlayer space, while for beidellite they are located much closer to the clay surface. water in these structures does not form distinct layers but is distributed rather broadly with a tendency to be concentrated close to the smectite surface. one-layer structure of ethylene glycol/water-smectite complex, characteristic of vermiculite was also proposed. references berendsen, h.j.c., postma, j.p.m., van gunsteren, w.f., hermans, j. (1981) interaction models for water in relation to protein hydration. in intermolecular forces; pullman, b., ed.; d. reidel: amsterdam, pp 331. cygan, r. t., liang, j. j., and kalinichev, a. g. (2004) molecular models of hydroxide, oxyhydroxide, and clay phases and the development of a general force field. journal of physical chemistry b, 108, 1255-1266. jorgensen,w.l., maxwell, d.s., tirado-rives, j. (1996) development and testing of the opls all-atom force field on conformational energetics and properties of organic liquids. j. am. chem. soc., 118, 11225-11236. lee j.h., guggenheim s. (1981) single crystal x-ray refinement of pyrophyllite-1tc. american mineralogist, 66, 350-357 reynolds r. c. (1965) an x-ray study of an ethylene glycol-montmorillonite complex. american mineralogist, 50, 990-1001 środoń j., zeelmaekers e., derkowski a. (2009) the charge of component layers of illite-smectite in bentonites and the nature of end-member illite. clays and clay minerals, 57, 650-672.
automatic reconstruction and analysis of security policies from deployed security components. security is a critical concern for any information system. security properties such as confidentiality, integrity and availability need to be enforced in order to make systems safe. in complex environments, where information systems are composed by a number of heterogeneous subsystems, each subsystem plays a key role in the global system security. for the specific case of access-control, access-control policies may be found in several components (databases, networksand applications) all, supposedly, working together. nevertheless since most times these policies have been manually implemented and/or evolved separately they easily become inconsistent. in this context, discovering and understanding which security policies are actually being enforced by the information system comes out as a critical necessity. the main challenge to solve is bridging the gap between the vendor-dependent security features and a higher-level representation that express these policies in a way that abstracts from the specificities of concrete system components, and thus, it´s easier to understand and reason with. this high-level representation would also allow us to implement all evolution/refactoring/manipulation operations on the security policies in a reusable way. in this work we propose such a reverse engineering and integration mechanism for access-control policies. we rely on model-driven technologies to achieve this goal.
surface complexation modeling of eu(iii) adsorption on silica in the presence of fulvic acid. humic substances (hs) substantially affect heavy metal (m) adsorption on mineral surfaces. however, quantitative descriptions of ternary systems involving m, hs and mineral surfaces remain unclear. this study examines adsorption in a model ternary system including eu(iii), fulvic acid (fa) and silica, and describes the adsorption of eu(iii) and fa by combining a double-layer model (dlm) and the stockholm humic model (shm). shm explains the binding of h+ and eu3+ to fa and the dlm for fa and eu(iii) adsorption on silica. experimental results showed that the presence of fa promotes eu(iii) adsorption at acidic ph values, but decreases it at basic ph values, which indicates the formation of ternary surface complexes. modeling calculations have shown that two ternary surface complexes are required to describe the experimental results in which eu3+ acts as a bridge between the surface site and fa. the present study suggests that the discrete-site approach to hs is a promising method for interpreting the adsorption data for m, hs and mineral ternary systems.
depth of maximum of air-shower profiles at the pierre auger observatory. ii. composition implications. using the data taken at the pierre auger observatory between december 2004 and december 2012, we have examined the implications of the distributions of depths of atmospheric shower maximum (xmax), using a hybrid technique, for composition and hadronic interaction models. we do this by fitting the distributions with predictions from a variety of hadronic interaction models for variations in the composition of the primary cosmic rays and examining the quality of the fit. regardless of what interaction model is assumed, we find that our data are not well described by a mix of protons and iron nuclei over most of the energy range. acceptable fits can be obtained when intermediate masses are included, and when this is done consistent results for the proton and iron-nuclei contributions can be found using the available models. we observe a strong energy dependence of the resulting proton fractions, and find no support from any of the models for a significant contribution from iron nuclei. however, we also observe a significant disagreement between the models with respect to the relative contributions of the intermediate components.
depth of maximum of air-shower profiles at the pierre auger observatory. i. measurements at energies above 10^17.8 ev. we report a study of the distributions of the depth of maximum, xmax, of extensive air-shower profiles with energies above 10^17.8 ev as observed with the fluorescence telescopes of the pierre auger observatory. the analysis method for selecting a data sample with minimal sampling bias is described in detail as well as the experimental cross-checks and systematic uncertainties. furthermore, we discuss the detector acceptance and the resolution of the xmax measurement and provide parameterizations thereof as a function of energy. the energy dependence of the mean and standard deviation of the xmax-distributions are compared to air-shower simulations for different nuclear primaries and interpreted in terms of the mean and variance of the logarithmic mass distribution at the top of the atmosphere.
stiffness modeling for perfect and non-perfect parallel manipulators under internal and external loadings. the paper presents an advanced stiffness modeling technique for perfect and non-perfect parallel manipulators under internal and external loadings. particular attention is paid to the manipulators composed of non-perfect serial chains, whose geometrical parameters differ from the nominal ones and do not allow to assemble manipulator without internal stresses that considerably affect the stiffness properties and also change the end-effector location. in contrast to other works, several types of loadings are considered simultaneously: an external force applied to the end-effector, internal loadings generated by the assembling of non-perfect serial chains and external loadings applied to the intermediate points (auxiliary loading due to the gravity forces and relevant compensator mechanisms, etc.). for this type of manipulators, a non-linear stiffness modeling technique is proposed that allows to take into account inaccuracy in the chains and to aggregate their stiffness models for the case of both small and large deflections. advantages of the developed technique and its ability to compute and compensate the compliance errors caused by the considered factors are illustrated by an example that deals with parallel manipulators of the orthoglide family.
potential barriers governing the 12c formation and decay through quasimolecular shapes. the l-dependent potential barriers that govern the 8be and 12c formation and decay through quasimolecular shapes have been determined using a generalized liquid-drop model and adjusted to reproduce the experimental q value. for the ternary channel of 12c, the energies of prolate linear chain configurations and oblate triangular configurations of three α particles have been compared. the triangular shape with three α nuclei in contact allows the experimental rms radius and the negative quadrupole moment of the 12c ground state to be reproduced. the difference between the energies of the minima in the prolate and oblate ternary shape paths is very close to the energy of the excited hoyle state of the 12c nucleus.
adsorption and transport of polymaleic acid on callovo-oxfordian clay stone: batch and transport experiments. dissolved organic matter (dom) can affect the mobility of radionuclides in pore water of clay-rich geological formations, such as those intended to be used for nuclear waste disposal. the present work studies the adsorption and transport properties of a polycarboxylic acid, polymaleic acid (pma, mw = 1.9 kda), on callovo-oxfordian argillite samples (cox). even though this molecule is rather different from the natural organic matter found in clay rock, the study of its retention properties on both dispersed and intact samples allows assessing to which extent organic acids may undergo sorption under natural conditions (ph 7) and what could be the impact on their mobility. pma sorption and desorption were investigated in dispersed systems. the degree of sorption was measured after 1, 8 and 21 days and for a range of pma initial concentrations from 4.5 × 10− 7 to 1.4 × 10− 3 mol.l− 1. the reversibility of the sorption process was estimated by desorption experiments performed after the sorption experiments. at the sorption steady state, the sorption was described by a two-site langmuir model. a total sorption capacity of cox for pma was found to be 1.01×10− 2 mol.kg− 1 distributed on two sorption sites, one weak and one strong. the desorption of pma was incomplete, independently of the duration of the sorption phase. the amount of desorbable pma even appeared to decrease for sorption phases from 1 to 21 days. to describe the apparent desorption hysteresis, two conceptual models were applied. the two-box diffusion model accounted for intraparticle diffusion and more generally for nonequilibrium processes. the two-box first-order non-reversible model accounted for a first-order non-reversible sorption and more generally for kinetically-controlled irreversible sorption processes. the use of the two models revealed that desorption hysteresis was not the result of nonequilibrium processes but was due to irreversible sorption. irreversible sorption on the strong site was completed after 1 day and represented 96% of the total sorption on this site. on the weak site the irreversible uptake was slower and completed only after 16 days but it also dominated the sorption. 85% of the pma sorbed on the weak site was not desorbable after 21 days of sorption. the migration of pma was studied by applying a hydraulic gradient to a clay core inserted in a stainless steel cell. breakthrough of polymaleic acid, simulated with a 1d transport model including the two-box first-order non-reversible model, revealed that the mobility of pma was limited by the same set of reversible/irreversible interactions as observed in the dispersed system. however, to describe efficiently the transport, the total sorption capacity had to be reduced to 33% of the capacity estimated in batch experiments. the irreversible sorption on the weak site was also slower in the intact sample than in the crushed sample. geometrical constraints would therefore affect both the accessibility to the sorption sites and the kinetics of the irreversible sorption process.
searches for large-scale anisotropy in the arrival directions of cosmic rays detected above energy of $10^{19}$ ev at the pierre auger observatory and the telescope array. spherical harmonic moments are well-suited for capturing anisotropy at any scale in the flux of cosmic rays. an unambiguous measurement of the full set of spherical harmonic coefficients requires full-sky coverage. this can be achieved by combining data from observatories located in both the northern and southern hemispheres. to this end, a joint analysis using data recorded at the telescope array and the pierre auger observatory above $10^{19}$ ev is presented in this work. the resulting multipolar expansion of the flux of cosmic rays allows us to perform a series of anisotropy searches, and in particular to report on the angular power spectrum of cosmic rays above $10^{19}$ ev. no significant deviation from isotropic expectations is found throughout the analyses performed. upper limits on the amplitudes of the dipole and quadrupole moments are derived as a function of the direction in the sky, varying between 7% and 13% for the dipole and between 7% and 10% for a symmetric quadrupole.
a mixed passengers-goods transportation network for territories with a low population density. null
a large neighborhood search based heuristic for supply chain network design. facility location problems and more generally supply chain design models have been the subject of a large number of models and exact or approximate solution techniques. yet, the large neighborhood search technique (lns) has almost never been proposed for solving such problems, although it has proven its efficiency and flexibility to solve complex combinatorial optimization problems. in this paper we propose a new solution framework for solving a four-layer single period multi-product supply chain network design problem involving multimodal transport. location decisions for intermediate facilities (e.g. plants and distribution centers) are made using lns while transportation modes and product flow decisions are determined by a greedy heuristic. finally, an a posteriori flow optimization dual simplex procedure is used to finalize the solution. extensive experiments based on randomly generated instances of different sizes and characteristics show the effectiveness of the method compared with a state-of-the-art solver. further research aims at extending the model and solution technique to encompass other advanced supply chain design features.
a two-phase heuristic for full truckload routing and scheduling with split delivery and resource synchronization in public works. this paper presents a two-phase method to solve a routing and scheduling problem that arises in public works. in this problem, the quantity of demands generally exceeds the capacity of a truck. as a result, demands have to be split into full truckloads, taking into account the various capacities in the heterogeneous fleet of vehicles that perform the transportation. full truckload routes have to be designed and scheduled according to construction or loading constraints on pickup and delivery sites. in the first phase, we propose a linear program to split demands into full truckload requests. the second phase is a heuristic that solves a heterogeneous full truckload pickup and delivery problem with time windows and resource synchronization. the method is evaluated on instances from a real case study.
development of the pixe analysis technique at high energy with the arronax cyclotron. particle induced x-ray emission (pixe) is a fast, nondestructive, multi-elemental analysis technique. it is based on the detection of characteristic x-rays due to the interaction of accelerated charged particles with matter. this method is successfully used in various application fields using low energy protons (energies around few mev), reaching a limit of detection of the order the μg/g (ppm). at this low energy, the depth of analysis is limited. at the arronax cyclotron, protons and alpha particles are delivered with energy up to 70 mev, allowing the development of the high energy pixe technique. thanks to these beams, we mainly produce kx-rays, more energetic than the lx-rays used with standard pixe for the heavy elements analysis. thus, in depth analysis in thick materials is achievable. for light element analysis, the pige technique, based on the detection of gamma rays emitted by excited nuclei, may be used in combination with pixe. first of all, we will introduce the characteristics and principles of high energy pixe analysis that we have developed at arronax. then we will detail the performance achieved, particularly in terms of detection limit in various experimental conditions. finally, we present the results obtained for the analysis of multilayer samples and quantification of trace elements in thick samples.
filtering atmostnvalue with difference constraints : application to the shift minimisation personnel task scheduling problem. the problem of minimising the number of distinct values among a set of variables subject to difference constraints occurs in many real-life contexts. this is the case of the shift minimisation personnel task scheduling problem, introduced by krishnamoorthy et. al., which is used as a case study all along this paper. constraint-programming enables to formulate this problem easily, through several alldifferent constraints and a single atmostnvalue constraint. however, the independence of these constraints results in a poor lower bounding, hence a difficulty to prove optimality. this paper introduces a formalism to describe a family of propagators for atmostnvalue . in particular, we provide simple but significant improvement of the state-of-the-art atmostnvalue propagator of bessière et. al., to filter the conjunction of an atmostnvalue constraint and disequalities. in addition, we provide an original search strategy which relies on constraint reification. extensive experiments show that our contribution significantly improves a straightforward model, so that it competes with the best known approaches from operational research.
isolation of flow and nonflow correlations by two- and four-particle cumulant measurements of azimuthal harmonics in &lt;mml:math altimg="si1.gif" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/xmlschema" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance" xmlns="http://www.elsevier.com/xml/ja/dtd" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/math/mathml" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd"&gt;&lt;mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant="normal"&gt;nn&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:msqrt&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;200&lt;/mml:mn&gt;&lt;mml:mtext&gt; &lt;/mml:mtext&gt;&lt;mml:mtext&gt;gev&lt;/mml:mtext&gt;&lt;/mml:math&gt; au+au collisions. a data-driven method was applied to measurements of au+au collisions at $\sqrt{s_{_{\rm nn}}} =$ 200 gev made with the star detector at rhic to isolate pseudorapidity distance $\delta\eta$-dependent and $\delta\eta$-independent correlations by using two- and four-particle azimuthal cumulant measurements. we identified a component of the correlation that is $\delta\eta$-independent, which is likely dominated by anisotropic flow and flow fluctuations. it was also found to be independent of $\eta$ within the measured range of pseudorapidity $|\eta|&lt;1$. the relative flow fluctuation was found to be $34\% \pm 2\% (stat.) \pm 3\% (sys.)$ for particles of transverse momentum $p_{t}$ less than $2$ gev/$c$. the $\delta\eta$-dependent part may be attributed to nonflow correlations, and is found to be $5\% \pm 2\% (sys.)$ relative to the flow of the measured second harmonic cumulant at $|\delta\eta| &gt; 0.7$.
a two-phase method for the shift design and personnel task scheduling problem with equity objective. in this paper, we study the shift design and personnel task scheduling problem with equity objective (sdptsp-e), initially introduced in [11]. this problem consists in designing the shifts of workers and assigning a set of tasks to quali ed workers, so as to maximise the equity between workers. we propose a natural two-phase approach consisting in rst designing shifts and then assigning tasks to workers, and we iterate between these two phases to improve solutions. we compare our experimental results with existing literature and show that our approach outperforms previous known results.
upper bounding in inner regions for global optimization under inequality constraints. in deterministic constrained global optimization, upper bounding the objective function generally resorts to local minimization at the nodes of the branch and bound. the local minimization process is sometimes costly when constraints must be respected. we propose in this paper an alternative approach when the constraints are inequalities or relaxed equalities so that the feasible space has a non-null volume. first, we extract an inner region, i.e., an (entirely feasible) convex polyhedron or box in which all points satisfy the constraints. second, we select a point inside the extracted inner region and update the upper bound with its cost. we use two inner region extraction algorithms implemented in our interval b&amp;b called ibexopt [7]. this upper bounding shows good performance in medium-sized systems proposed in the coconut suite.
maintaining a system subject to uncertain technological evolution. maintenance decisions can be directly affected by the introduction of a new asset on the market, especially when the new asset technology could increase the expected profit. however new technology has a high degree of uncertainty that must be considered such as, e.g., its appearance time on the market, the expected revenue and the purchase cost. in this way, maintenance optimization can be seen as an investment problem where the repair decision is an option for postponing a replacement decision in order to wait for a potential new asset. technology investment decisions are usually based primarily on strategic parameters such as current probability and expected future benefits while maintenance decisions are based on “functional” parameters such as deterioration levels of the current system and associated maintenance costs. in this paper, we formulate a new combined mathematical optimization framework for taking into account both maintenance and replacement decisions when the new asset is subject to technological improvement. the decision problem is modelled as a non-stationary markov decision process. structural properties of the optimal policy and forecast horizon length are then derived in order to guarantee decision optimality and robustness over the infinite horizon. finally, the performance of our model is highlighted through numerical examples.
influence of air handling units (ahu) management of ventilation systems of buildings on microbial aerosols behavior. filtration performances of air handling units (ahu) filters regarding particles and microbial aerosols have been studied, as well as the influence of the ahu operational conditions on behavior of microorganisms collected on the filters. a lab-scale ahu with two successive filtration stages was developed and validated for the study of prototype filters with industrial geometries. three types of filters of different efficiency have been considered : g4, f7 and f9 according to en 779 standard. two configurations of filters were considered: 1) g4 pleated/f7 bag and 2) f7/f9 bag. filters were sequentially clogged by alumina particles which assured a mineral fraction, and then by micronized rice particles which provides the fungi penicillium chrysogenum and assures an organic fraction which acts as a substrate for microorganisms. finally, a microbial aerosol composed by endospores of bacillus subtilis and spores of aspergillus niger was nebulized for filters contamination. after clogging, stops and restarts of ventilation were simulated for different durations (10 days or 6 weeks). during restarts of ventilation, particles and microbial aerosols samplings were performed downstream of the filters. main results are: (i) level of clogging is significantly less important for the 2nd filtration stage than for the first one, (ii) survival of b. subtilis, growth of p. chrysogenum and decline of a.niger on the filters whatever the period of time studied, and (iii) during restarts of ventilation, microbial aerosols releasing was not detected for sampled fraction. moreover, two full-scale ahu were studied during 6 months. one of the ahu studied is equipped with two filters in series: a g4 pleated filter in 1st stage and a f7 bag filter in 2nd stage. this ahu treats the outdoor air to blow it towards the indoor environments. the other one extracts the indoor air to reject it back outdoors. the filters pressure drop, relative humidity and temperature of the air were measured continuously. filters efficiency regarding particles and microbial aerosols were measured once a month. an original methodology for the monthly estimation of the concentration of microorganisms on the filters was implemented. main results are: (i) no significant evolution of the filter pressure drop in 2nd stage, (ii) efficiency of g4 filters are comparable to the prototype filtersone, (iii) efficiency of f7 filters are lower than prototype filters one, which can be explained by differences of filtration velocity between the two scales, (iv) after 6 months of operation, concentration of microorganisms on g4 filter of the ahu of extraction is 10 times higher than the g4 filter one of ahu who treats outdoor air.
a reconfiguration language for virtualized grid infrastructures. the growing needs in computational power to answer to the increasing number of on-line services and the complexity of applications makes it mandatory to build corresponding hardware infrastructures and to share several distributed hardware and software resources thanks to grid computing. to help with optimizing resource utilization, system virtualization is a more and more adopted technique in data centers. however, this software layer adds to the administration complexity of servers and it requires speciﬁc management tools to deal with hypervisor functionalities like live migration. to address this problem, we propose vmscript, a domain speciﬁc language for administration of virtualized grid infrastructures. this language relies on set manipulation and is used to introspect physical and virtual grid architectures thanks to query expressions and notably to modify vm placement on machines.
controlling propagation and search within a constraint solver. constraint programming is often described, idealistically, as a declarative paradigm in which the user describes the problem and the solver solves it. obviously, the reality of constraint solvers is more complex, and the needs in customization of modeling and solving techniques change with the level of expertise of users. this thesis focuses on enriching the arsenal of available techniques in constraint solvers. on the one hand, we study the contribution of an explanation system to the exploration of the search space in the specific context of a local search. two generic neighborhood heuristics which exploit explanations singularly are described. the first one is based on the difficulty of repairing a partially destroyed solution, the second one is based on the non-optimal nature of the current solution. these heuristics discover the internal structure of the problems to build good neighbors for large neighborhood search. they are complementary to other generic neighborhood heuristics, with which they can be combined effectively. in addition, we propose to make the explanation system lazy in order to minimize its footprint. on the other hand, we undertake an inventory of know-how relative to propagation engines of constraint solvers. these data are used operationally through a domain specific language that allows users to customize the propagation schema, providing implementation structures and defining check points within the solver. this language offershigh-level concepts that allow the user to ignore the implementation details, while maintaining a good level of flexibility and some guarantees. it allows the expression of propagation schemas specific to the internal structure of each problem solved. implementation and experiments were carried out in the choco constraint solver, developed in this thesis. this has resulted in a new version of the overall effectiveness and natively explained tool.
closing the door for dark photons as the explanation for the muon g-2 anomaly. the standard model (sm) of particle physics is spectacularly successful, yet the measured value of the muon anomalous magnetic moment (g-2)_\mu deviates from sm calculations by 3.6 sigma. several theoretical models attribute this to the existence of a "dark photon", an additional u(1) gauge boson, which is weakly coupled to ordinary photons. the phenix experiment at the relativistic heavy ion collider has searched for a dark photon, u, in \pi^0,\eta \rightarrow \gamma e^+e^- decays and obtained upper limits on u-\gamma mixing at 90% cl for the mass range 30 &lt; m_u &lt; 90 mev/c^2. combined with other experimental limits, the remaining region in the u-\gamma mixing parameter space that can explain the (g-2)_\mu deviation from its sm value is nearly completely excluded at the 90% confidence level, with only a small region of 30 &lt; m_u &lt; 36 mev/c^2 remaining.
reactor and antineutrino spectrum calculations for the double chooz first phase results. the double chooz reactor oscillation experiment is designed to search for a non-vanishing value of the mixing angle θ13θ13. for the first phase of the experiment with only the far detector running, the reactor electron antineutrino flux is normalized via reactor simulation. for this first phase and from its last results, double chooz observed an evidence for a reactor electron antineutrino disappearance. in 227.93 days of far detector live time, we obtained view the mathml sourcesin22θ13=0.109±0.030(stat)±0.025(syst). this result excludes the no-oscillation hypothesis at 99.8% cl.
the nucifer experiment. in nuclear reactors, a large number of antineutrinos are generated in the decay chains of the fission products; thus a survey of the antineutrino flux could provide valuable information related to the uranium and plutonium content of the core. this application generated interest by the iaea in using antineutrino detectors as a potential safeguard tool. here we present the nucifer experiment, developed in france, by cea and cnrs/in2p3. the design of this new antineutrino detector has focused on safety, size reduction, reliability and high detection efficiency with a good background rejection. the nucifer detector is currently taking data at the osiris research reactor, inside cea-saclay. presently, the ongoing analyses are considering the main sources of background for the antineutrino detection; the first antineutrino result is expected in 2013. a possible contribution to the understanding of the so called "reactor antineutrino anomaly" is also discussed. finally, we present a brief description of the proposed experiments at very short baselines (vsbl) from reactors in france.
determination of the sensitivity of the antineutrino probe for reactor core monitoring. this paper presents a feasibility study of the use of the detection of reactor-antineutrinos view the mathml source(ν¯e) for non proliferation purpose. to proceed, we have started to study different reactor designs with our simulation tools. we use a package called mcnp utility for reactor evolution (mure), initially developed by cnrs/in2p3 labs to study generation iv reactors. the mure package has been coupled to fission product beta decay nuclear databases for studying reactor antineutrino emission. this method is the only one able to predict the antineutrino emission from future reactor cores, which don't use the thermal fission of 235u, 239pu and 241pu. it is also the only way to include off-equilibrium effects, due to neutron captures and time evolution of the fission product concentrations during a reactor cycle. we will present here the first predictions of antineutrino energy spectra from innovative reactor designs (generation iv reactors). we will then discuss a summary of our results of non-proliferation scenarios involving the latter reactor designs, taking into account reactor physics constraints.
the detection of reactor antineutrinos for reactor core monitoring: an overview. there have been new developments in the field of applied neutrino physics during the last decade. the international atomic energy agency (iaea) has expressed interest in the potentialities of antineutrino detection as a new tool for reactor monitoring and has created an ad hoc working group in late 2010 to follow the associated research and development. several research projects are ongoing around the world to build antineutrino detectors dedicated to reactor monitoring, to search for and develop innovative detection techniques, or to simulate and study the characteristics of the antineutrino emission of actual and innovative nuclear reactor designs. we give, in these proceedings, an overview of the relevant properties of antineutrinos, the possibilities of and limitations on their detection, and the status of the development of a variety of compact antineutrino detectors for reactor monitoring.
contribution of recently measured nuclear data to reactor antineutrino energy spectra predictions. the aim of this work is to study the impact of the inclusion of the recently measured β decay properties of the 102,104,105,106,107tc, 105mo, and 101nb nuclei in the calculation of the antineutrino (anti-ν) energy spectra arising after the fissions of the four main fissile isotopes 235,238u, and 239,241pu in pwrs. these β feeding probabilities, measured using the total absorption technique (tas) at the jyfl facility of jyväskylä, have been found to play a major role in the γ component of the decay heat for 239pu in the 4-3000 s range. following the fission product summation method, the calculation was performed using the mcnp utility reactor evolution code (mure) coupled to the experimental spectra built from β decay properties of the fission products taken from evaluated databases. these latest tas data are found to have a significant effect on the pu isotope energy spectra and on the spectrum of 238u showing the importance of their measurement for a better assessment of the reactor anti-ν energy spectrum, as well as importance for fundamental neutrino physics experiments and neutrino applied physics.
total absorption study of beta decays relevant for nuclear applications and nuclear structure. an overview is given of our activities related to the study of the beta decay of neutron rich nuclei relevant for nuclear applications. recent results of the study of the beta decay of 87,88br using a new segmented total absorption spectrometer are presented. the measurements were performed at the igisol facility using trap-assisted total absorption spectroscopy.
an improved nuclear mass formula with a unified prescription for the shell and pairing corrections. an improvedmacroscopic-microscopic nuclear mass formula is presented in which shell and pairing effects are simultaneously evaluated by a procedure similar to strutinsky method. the coefficients of the macroscopic-microscopic mass formula have been adjusted on 2267 experimental atomic masses extracted from the atomic mass evaluation of 2012 (ame2012). same as inthe weizsäcker-skyrme (ws) model, the influence of the nuclear deformation on the macroscopic energy as well as the mirror nuclei constraint istaken into account, and for the sake of the consistency of the model parameters between the macroscopic and the microscopic parts we approximate the isospin-dependent component of the macroscopic energy to the depth of the woods-saxon potential. as a result, the root-mean square (rms) deviation with respect to 2267 measured nuclear masses is 0.493mev. then,based on the fitted formula we predict the remaining 988 nuclei from the ame2012 for which the masses are still unknown or not well-known, and calculate the α-decay energies of seven chains in the superheavy nuclei region with z=117 and 118.
charged-to-neutral correlation at forward rapidity in au+au collisions at $\sqrt{s_{nn}}$=200 gev. event-by-event fluctuations of the ratio of inclusive charged to photon multiplicities at forward rapidity in au+au collision at $\sqrt{s_{nn}}$=200 gev have been studied. dominant contribution to such fluctuations is expected to come from correlated production of charged and neutral pions. we search for evidences of dynamical fluctuations of different physical origins. observables constructed out of moments of multiplicities are used as measures of fluctuations. mixed events and model calculations are used as baselines. results are compared to the dynamical net-charge fluctuations measured in the same acceptance. a non-zero statistically significant signal of dynamical fluctuations is observed in excess to the model prediction when charged particles and photons are measured in the same acceptance. we find that, unlike dynamical net-charge fluctuation, charge-neutral fluctuation is not dominated by correlation due to particle decay. results are compared to the expectations based on the generic production mechanism of pions due to isospin symmetry, for which no significant (&lt;1%) deviation is observed.
measurement of the effective weak mixing angle in $p\bar{p}\rightarrow z/\gamma^{*}\rightarrow e^{+}e^{-}$ events. we present a measurement of the fundamental parameter of the standard model, the weak mixing angle, in $p\bar{p}\rightarrow z/\gamma^{*}\rightarrow e^{+}e^{-}$ events at a center of mass energy of 1.96 tev, using data corresponding to 9.7 fb$^{-1}$ of integrated luminosity collected by the d0 detector at the fermilab tevatron. the effective weak mixing angle is extracted from the forward-backward charge asymmetry as a function of the invariant mass around the z boson pole. the measured value of $\sin^2\theta_{\text{eff}}^{\text{$\ell$}}=0.23146 \pm 0.00047$ is the most precise measurement from light quark interactions to date, with a precision close to the best lep and sld results.
reconstruction of inclined air showers detected with the pierre auger observatory. we describe the method devised to reconstruct inclined cosmic-ray air showers with zenith angles greater than $60^\circ$ detected with the surface array of the pierre auger observatory. the measured signals at the ground level are fitted to muon density distributions predicted with atmospheric cascade models to obtain the relative shower size as an overall normalization parameter. the method is evaluated using simulated showers to test its performance. the energy of the cosmic rays is calibrated using a sub-sample of events reconstructed with both the fluorescence and surface array techniques. the reconstruction method described here provides the basis of complementary analyses including an independent measurement of the energy spectrum of ultra-high energy cosmic rays using very inclined events collected by the pierre auger observatory.
charge transfer complexes and radical cation salts of chiral methylated organosulfur donors. null
determination and speciation of anthropogenic tritium in the loire river estuary (france). the aim of radioecology is to understand the transfer of radionuclides through the ecosystem. it relies strongly on field studies which can provide useful information on the presence of radionuclides in the environment, and their origins (natural and anthropogenic). in this study, the radioactive isotope of hydrogen, i.e. tritium (3h or t), is considered. tritium is a beta emitter with a radioactive half life of 12.3 years. it is present in the environment in three principal forms: tritiated water (hto or tissue free water), organically bound tritium (obt) and tritiated gas (ht). tritiated water is the most abundant chemical form of tritium in the aquatic and terrestrial environment. obt can be subdivided in two fractions: the exchangeable obt refers to tritium atoms that are easily exchanged (e.g. bound to nitrogen, oxygen or sulfur atoms), while the non-exchangeable obt refers to the remaining obt covalently bound to carbon atoms. the non exchangeable hydrogen pool is considered as the only hydrogen fraction that faithfully records the history of environmental tritium seen by living organisms. in this study, mud and water samples from the loire estuary, the outlet of a watershed where several nuclear power plants are located, were analyzed. mud samples were subjected to freeze-drying and combustion as pre treatment in order to recover free hto and total obt. hto and total obt activities ranged between 4 and 26 bq.l-1 and between 10 and 25 bq.l-1 of combustion water, respectively. to estimate the non exchangeable obt activity in these samples, the exchangeable pool of hydrogen within the matrix has to be known. a dedicated experimental set up was thus developped in order to determine the fraction of exchangeable hydrogen (). it consists in a temperature and humidity controlled glove box where different environmental matrixes are exposed to specific atmospheres with fixed h/d (deuterium) or h/t pressure ratios. the calibration phase of the method was perfomed using cellulose matrix.
study of inclusive j/ψ production in pb-pb collisions at √snn=2,76 tev with the alice muon spectrometer at the lhc. the quantum chromodynamics theory predicts the existence of a deconfined state of matter called quark gluon plasma (qgp). experimentally, the formation of a qgp is expected under the extreme conditions of temperature and density reached in ultra-relativisticheavy-ion collisions. many observables were proposed to observe and characterize indirectly such a state of matter. in particular, the phenomena of suppression and (re)combination of the j/ψ meson in the qgp are extensively studied. this thesis presents the analysis of the inclusive production of j/ψ in pb-pb collisions, at a center of mass energy √snn = 2.76 tev, detected with the alice muon spectrometer at the lhc. from the high statistics of events collected during 2011 datataking, the j/ψ nuclear modification factor was measured as a function of transverse momentum, rapidity and collision centrality. the j/ψ mean transverse momentum was also measured as a function of centrality. the predictions of theoretical models, all including a (re)combination contribution, are in good agreement with data. finally, an excess of j/ψ yield at very low transverse momentum (&lt;300 mev/c) with respect to the expected hadronic production was observed fort he first time.
power management in virtualized data centers : form a load scenario to the optimization of the tasks placement. this thesis considers the virtualized it services hosting and makes two contributions. it first proposes a modular system of management aids, to move the virtual machines of the center in order to keep it in a good condition. this system allows in particular to integrate the concept of server power consumption and rules specific to that concept. what's more, its modularity allows to adjust its components to handle larger problems. this thesis proposes also a tool to compare different virtualized centers managers. this tool injects a reproductible load increase scenario in a virtualized infrastructure. the injection of such a scenario is used to evaluate the performance of the system center manager, using performances probes. the language used for this injection is extensible and allows the creation of parameterized scenarios. the contributions of this thesis were presented in two international conferences and a french conference.
effect capabilities for haskell. computational effects complicate the tasks of reasoning about and maintaining software, due to the many kinds of interferences that can occur. while different proposals have been formulated to alleviate the fragility and burden of dealing with specific effects, such as state or exceptions, there is no prevalent robust mechanism that addresses the general interference issue. build- ing upon the idea of capability-based security, we propose effect capabilities as an effective and flexible manner to control monadic effects and their interfer- ences. capabilities can be selectively shared between modules to establish secure effect-centric coordination. we further refine capabilities with type-based per- mission lattices to allow fine-grained decomposition of authority. we provide an implementation of effect capabilities in haskell, using type classes to establish a way to statically share capabilities between modules, as well as to check proper access permissions to effects at compile time. we exemplify how to tame effect interferences using effect capabilities, by treating state and exceptions.
muons in air showers at the pierre auger observatory: measurement of atmospheric production depth. the surface detector array of the pierre auger observatory provides information about the longitudinal development of the muonic component of extensive air showers. using the timing information from the flash analog-to-digital converter traces of surface detectors far from the shower core, it is possible to reconstruct a muon production depth distribution. we characterize the goodness of this reconstruction for zenith angles around 60 deg. and different energies of the primary particle. from these distributions we define x(mu)max as the depth along the shower axis where the production of muons reaches maximum. we explore the potentiality of x(mu)max as a useful observable to infer the mass composition of ultrahigh-energy cosmic rays. likewise, we assess its ability to constrain hadronic interaction models.
towards an open set of real-world benchmarks for model queries and transformations. with the growing size and complexity of systems under design, industry needs a generation of model-driven engineering (mde) tools, especially model query and transformation, with the proven capability to handle large-scale scenarios. while researchers are proposing several technical solutions in this sense, the community lacks a set of shared scalability benchmarks, that would simplify quantitative assessment of advancements and enable cross-evaluation of different proposals. benchmarks in previous work have been synthesized to stress specific features of model management, lacking both generality and industrial validity. in this paper, we initiate an effort to define a set of shared benchmarks, gathering queries and transformations from real-world mde case studies. we make these case available to community evaluation via a public mde benchmark repository.
improving memory efficiency for processing large-scale models. scalability is a main obstacle for applying model-driven engineering to reverse engineering, or to any other activity manipulating large models. existing solutions to persist and query large models are currently ine cient and strongly linked to memory availability. in this paper, we propose a memory unload strategy for neo4emf, a persistence layer built on top of the eclipse modeling framework and based on a neo4j database backend. our solution allows us to partially unload a model during the execution of a query by using a periodical dirty saving mechanism and transparent reloading. our experiments show that this approach enables to query large models in a restricted amount of memory with an acceptable performance.
product line-based customization of e-government documents. content personalization has been one of the major trends in recent document engineering research. the "one docum ent for n users" paradigm is being replaced by the "one user, one document" model, where the content to be delivered to a particular user is generated by some means. this is a very promising approach for e-government, where personalized government services, including document generation, are more and more required by users. in this paper, we introduce a method to the generation of personalized documents called document product lines (dpl). dpl allows generating content in domains with high variability and with high levels of reuse. we describe the basic principles underlying dpl and show its application to the e-government field using the personalized tax statement as case study.
event-by-event mean $p_{\rm t}$ fluctuations in pp and pb-pb collisions at the lhc. event-by-event fluctuations of the mean transverse momentum of charged particles produced in pp collisions at $\sqrt{s}$ = 0.9, 2.76 and 7 tev, and pb-pb collisions at $\sqrt{s_{nn}}$ = 2.76 tev are studied as a function of the charged-particle multiplicity using the alice detector at the lhc. non-statistical fluctuations are observed in all systems. the results in pp collisions show little dependence on collision energy. the monte carlo event generators pythia and phojet are in qualitative agreement with the data. peripheral pb-pb data exhibit a similar multiplicity dependence as that observed in pp. in central pb-pb, the results deviate from this trend, featuring a significant reduction of the fluctuation strength. the results in pb-pb are in qualitative agreement with previous measurements in au-au at lower collision energies and with expectations from models that incorporate collective phenomena.
extending the interaction flow modeling language (ifml) for model driven development of mobile applications front end. front-end design of mobile applications is a complex and multidisciplinary task, where many perspectives intersect and the user experience must be perfectly tailored to the application objectives. however, development of mobile user interactions is still largely a manual task, which yields to high risks of errors, inconsistencies and ine ciencies. in this paper we propose a model-driven approach to mobile application development based on the ifml standard. we propose an extension of the interaction flow modeling language tailored to mobile applications and we describe our implementation experience that comprises the development of automatic code generators for cross-platform mobile applications based on html5, css and javascript optimized for the apache cordova framework. we show the approach at work on a popular mobile application, we report on the application of the approach on an industrial application development project and we provide a productivity comparison with traditional approaches.
neo4emf : when big models are no longer an issue. handling effectively big emf models has often been one of the main barriers that can retain from adopting modeling technologies in very large-scale complex systems. in this talk we present neo4emf, a neo4j-based persistence framework allowing on-demand loading, storage, unloading and actual use of very large emf models. neo4emf provides a no-sql database persistence framework based on neo4j, which is a transactional property-graph database that has proved having a remarkable running speed for connected data operations compared to relational databases. in terms of performance, neo4emf eases data access and storage not only in a manner to reduce time and memory usage but also to allow big emf models to fit into a reduced amount of memory. this is made possible through a lightweight and on-demand loading mechanism. moreover, neo4emf comes with a dirty saving mechanism allowing to store huge chunks of data even with limited memory resources.
impact of 4he2+ radiolysis of water on uo2 corrosion. null
methods to solve multi-skill project scheduling problem. this is a summary of the author's phd thesis supervised by p. martineau and e. néron and defended on 28 november 2006 at the université françois-rabelais de tours. the thesis is written in french, and is available upon request from the author. this work deals with the problem of scheduling a project. the activities of this project requires skills that may not be mastered by all persons involved. first of all, the problem is defined in the introduction part. then we propose different methods to solve it: lower bounds in part 2, different heuristics and meta-heuristics in part 3, and finally a branch-and-bound procedure in the last part.
combination of cdf and do results on the mass of the top quark using up to 9.7 fb$^{-1}$ at the tevatron. we summarize the current top-quark mass measurements from the cdf and do experiments at fermilab. we combine published run i (1992--1996) results with the most precise published and preliminary run ii (2001--2011) measurements based on data corresponding to up to 9.7 fb$^{-1}$ of $p\bar{p}$ collisions. taking correlations of uncertainties into account, and combining the statistical and systematic uncertainties, the resulting preliminary tevatron average mass of the top quark is $m_{top} = 174.34 \pm 0.64 ~gev/c^2$, corresponding to a relative precision of 0.37%.
spin-orbit coupling and physico-chemical properties of molecules and materials. null
a tabu search procedure for multi-skill project scheduling problem.  an appropriate tabu search implementation is designed to solve the resource constrained project scheduling problem. this approach uses well defined move strategies and a structured neighbourhood, defines appropriate tabu status and tenure and takes account of objective function approximation to speed up the search process. a sound understanding of the problem has helped in many ways in designing and enhancing the tabu search methodology. the method uses diversification, intensification and handles infeasibility via strategic oscillation.
a three step decomposition approach for a transportation network design problem with non-linear costs. null
software modernization and cloudification using the artist migration methodology and framework. cloud computing has leveraged new software development and provisioning approaches by changing the way computing, storage and networking resources are purchased and consumed. the variety of cloud offerings on both technical and business level has considerably advanced the development process and established new business models and value chains for applications and services. however, the modernization and cloudification of legacy software so as to be offered as a service still encounters many challenges. in this work, we present a complete methodology and a methodology instantiation framework for the effective migration of legacy software to modern cloud environments.
integrated models for evaluation of local actions for the reduction of greenhouse gases emissions : heatgrid, an energy simulation model for a strategic management of district heating networks. because of the energy flexibility that they offer and their potential to reduce ghg emissions, disctrict heating (dh) networks are a tool of local energy policies in constant progression. their develpment and/or renovation is not only a classic technico-economical question, insofar as interconnected stakeholders of local energy policies, taking into account specific objectives, are concerned by dh networks. in this context, tools which enable these different stakeholders to evaluate actions related to dh networks are essential. they must be helpful for the assessment of renovation actions, the monitoring and the evaluation of performances....among the tools that allow theses evaluations, the modelling approaches are often too specific to a situation, a type of network, a stakeholder... the work of the thesis consists in developing a dh modeling tool that has this desired flexibility. the proposed tool "heatgrid" can model various network architectures. at each time step, the network running is simulated via linear programming formalism. this tool can be applied either at the design stage of a dh or at the operating stage. the model based approach enables the evaluation and comparison of economic, energy and technical aspects of the dh system in different scenarios. several examples are simulated and analyzed in order to illustrate the potential of the model.
quarkonia measurement in the alice upgrades. null
parton energy loss: an update. null
hard probes and the event generator epos. null
dilepton production in pp and aa at sis energies: a challenge for transport and experiment. null
heavy quark quenching and elliptic flow from rhic to lhc: can the experimental results be understood by pqcd?. null
heavy-quark azimuthal correlations in heavy-ion collisions. null
nuclear modification factor and elliptic flow of muons from heavy-flavour hadron decays in pb-pb collisions at sqrt(s_nn)=2.76 tev with alice. null
upgrade of the alice experiment: letter of intent. null
technical design report for the upgrade of the alice inner tracking system. null
the neutron background of the xenon100 dark matter experiment. the xenon100 experiment, installed underground at the laboratori nazionali del gran sasso (lngs), aims to directly detect dark matter in the form of weakly interacting massive particles (wimps) via their elastic scattering off xenon nuclei. this paper presents a study on the nuclear recoil background of the experiment, taking into account neutron backgrounds from ($\alpha$,n) and spontaneous fission reactions due to natural radioactivity in the detector and shield materials, as well as muon-induced neutrons. based on monte carlo simulations and using measured radioactive contaminations of all detector components, we predict the nuclear recoil backgrounds for the wimp search results published by the xenon100 experiment in 2011 and 2012, 0.11$^{+0.08}_{-0.04}$ events and 0.17$^{+0.12}_{-0.07}$ events, respectively, and conclude that they do not limit the sensitivity of the experiment.
first axion results from the xenon100 experiment. we present the first results of searches for axions and axion-like-particles with the xenon100 experiment. the axion-electron coupling constant, $g_{ae}$, has been tested by exploiting the axio-electric effect in liquid xenon. a profile likelihood analysis of 224.6 live days $\times$ 34 kg exposure has shown no evidence for a signal. by rejecting $g_{ae}$, larger than $7.7 \times 10^{-12}$ (90% cl) in the solar axion search, we set the best limit to date on this coupling. in the frame of the dfsz and ksvz models, we exclude qcd axions heavier than 0.3 ev/c$^2$ and 80 ev/c$^2$, respectively. for axion-like-particles, under the assumption that they constitute the whole abundance of dark matter in our galaxy, we constrain $g_{ae}$, to be lower than $1 \times 10^{-12}$ (90% cl) for masses between 5 and 10 kev/c$^2$.
conceptual design and simulation of a water cherenkov muon veto for the xenon1t experiment. xenon is a direct detection dark matter project, consisting of a time projection chamber (tpc) that uses xenon in double phase as a sensitive detection medium. xenon100, located at the laboratori nazionali del gran sasso (lngs) in italy, is one of the most sensitive experiments of its field. during the operation of xenon100, the design and construction of the next generation detector (of ton-scale mass) of the xenon project, xenon1t, is taking place. xenon1t is being installed at lngs as well. it has the goal to reduce the background by two orders of magnitude compared to xenon100, aiming at a sensitivity of $2 \cdot 10^{-47} \mathrm{cm}^{\mathrm{2}}$ for a wimp mass of 50 gev/c$^{2}$. with this goal, an active system that is able to tag muons and their induced backgrounds is crucial. this active system will consist of a water cherenkov detector realized with a water volume $\sim$10 m high and $\sim$10 m in diameter, equipped with photomultipliers of 8 inches diameter and a reflective foil. in this paper we present the design and optimization study for this muon veto water cherenkov detector, which has been carried out with a series of monte carlo simulations, based on the geant4 toolkit. this study showed the possibility to reach very high detection efficiencies in tagging the passage of both the muon and the shower of secondary particles coming from the interaction of the muon in the rock: &gt;99.5% for the former type of events (which represent $\sim$ 1/3 of all the cases) and &gt;70% for the latter type of events (which represent $\sim$ 2/3 of all the cases). in view of the upgrade of xenon1t, that will aim to an improvement in sensitivity of one order of magnitude with a rather easy doubling of the xenon mass, the results of this study have been verified in the upgraded geometry, obtaining the same conclusions.
transport coefficients of heavy quarks around $t_c$ at finite quark chemical potential. the interactions of heavy quarks with the partonic environment at finite temperature $t$ and finite quark chemical potential $\mu_q$ are investigated in terms of transport coefficients within the dynamical quasi-particle model (dqpm) designed to reproduce the lattice-qcd results (including the partonic equation of state) in thermodynamic equilibrium. these results are confronted with those of nuclear many-body calculations close to the critical temperature $t_c$. the hadronic and partonic spatial diffusion coefficients join smoothly and show a pronounced minimum around $t_c$, at $\mu_q=0$ as well as at finite $\mu_q$. close and above $t_c$ its absolute value matches the lqcd calculations for $\mu_q=0$. the smooth transition of the heavy quark transport coefficients from the hadronic to the partonic medium corresponds to a cross over in line with lattice calculations, and differs substantially from perturbative qcd (pqcd) calculations which show a large discontinuity at $t_c$. this indicates that in the vicinity of $t_c$ dynamically dressed massive partons and not massless pqcd partons are the effective degrees-of-freedom in the quark-gluon plasma.
exclusive $\mathrm{j/}\psi$ photoproduction off protons in ultra-peripheral p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev. we present the first measurement at the lhc of exclusive j/$\psi$ photoproduction off protons, in ultra-peripheral proton-lead collisions at $\sqrt{s_{\rm nn}}=5.02$ tev. events are selected with a dimuon pair produced either in the rapidity interval, in the laboratory frame, $2.5.
improved measurements of the neutrino mixing angle $\theta_{13}$ with the double chooz detector. the double chooz experiment presents improved measurements of the neutrino mixing angle $\theta_{13}$ using the data collected in 467.90 live days from a detector positioned at an average distance of 1050 m from two reactor cores at the chooz nuclear power plant. several novel techniques have been developed to achieve significant reductions of the backgrounds and systematic uncertainties with respect to previous publications, whereas the efficiency of the $\bar\nu_{e}$ signal has increased. the value of $\theta_{13}$ is measured to be $\sin^{2}2\theta_{13} = 0.090 ^{+0.032}_{-0.029}$ from a fit to the observed energy spectrum. deviations from the reactor $\bar\nu_{e}$ prediction observed above a prompt signal energy of 4 mev and possible explanations are also reported. a consistent value of $\theta_{13}$ is obtained from a fit to the observed rate as a function of the reactor power independently of the spectrum shape and background estimation, demonstrating the robustness of the $\theta_{13}$ measurement despite the observed distortion.
working on innovation. null
a new consistent vehicle routing problem for the transportation of people with disabilities. n this article, we address a problem of the transportation of people with disabilities where customers are served on an almost daily basis and expect some consistency in the service. we introduce an original model for the time-consistency of the service, based on so-called time-classes. we then define a new multiday vehicle routing problem (vrp) that we call the time-consistent vrp. we address the solution of this new problem with a large neighborhood search heuristic. each iteration of the heuristic requires solving a complex vrp with multiple time windows and no waiting time which we tackle with a heuristic branch-and-price method. computational tests are conducted on benchmark sets and modified real-life instances. results demonstrate the efficiency of the method and highlight the impact of time-consistency on travel costs. © 2014 wiley periodicals, inc. networks, vol. 63(3), 211-224 2014.
absorption of hydrophobic voc in an organic phase. toluene or dimethyldisulfide (dmds)​. a preliminary bibliog. review allowed to select a silicone oil (polydimethylsiloxane, pdms) that seems efficient for voc absorption, as well as for biol. regeneration. the aim of this work was to evaluate the performances of the 1st step of the process, namely the absorption step. the partition coeffs. of the targeted voc in pdms were quantified. the expts. permitted to demonstrate that the voc have more affinity for pdms than for water (partition coeffs. 267 and 36 times higher for toluene and dmds, resp.)​. nevertheless, the measured diffusion coeffs. of the voc in pdms are lower than those measured in water (∼30 times lower)​, which can lead to a lower efficiency of the process. expts. were then carried out in a packed column pilot plant. as expected, efficiency increased with the liq. flow rate. in optimized operating conditions, toluene and dmds are totally absorbed in pdms for inlet gaseous concns. of ∼68 and 84 ppm, resp.
nh3 biofiltration of piggery air. an aboveground pilot-scale biofilter filled with wood chips was tested to treat ammonia emissions from a piggery located in brittany (france). two long-term tests ("summer" and "autumn" experiments) were carried out to improve biofilter applications for agriculture. the influence of climatic conditions on biofilter performance was taken into account. during summer 2012, the biofilter was operated for 74 days at different empty bed residence times (ebrts) from 6 to 15 s. inlet nh3 concentrations were relatively constant (around 15 mg m 3). significant nh3 reductions were achieved at ebrt ¼ 12 s (removal efficiencies, re, ranged between 90 and 100% for loading rates, lr, of around 4 g m 3 h 1). at a lower ebrt (6 s), re dropped to roughly 30e50%. this was due to the dramatic increase in the loading rate (lr up to 12 g m 3 h 1) but the results showed that the change in atmospheric conditions (temperature and relative humidity) also had a significant influence on biofilter performance. it was evidenced that the use of a humidifier upstream of the biofilter must be taken into account for large-scale biofilter design, but only for specific conditions (the spraying of the biofilter having to be carried out exceptionally). during autumn 2012, the biofilter was operated for 116 days at ebrt ¼ 12 s. re were around 80% for lr of around 3 g m 3 h 1. in such autumnal atmospheric conditions, a demister system should be installed upstream of the biofilter in order to avoid water accumulation in the bed material. although biofiltration was suitable for nh3 treatment of piggery air, the need to control accurately the medium moisture content implies that biofilters would not be easily managed by a pig farmer.
kinetic separation of co2 and ch4 on carbon molecular sieves: study of internal diffusion and surface resistance of pure gases and binary gas mixtures. null
soft synthesis of mil-101(cr) for hydrogen storage by adsorption. null
simple method based on the potential theory for buoyancy effect correction of pure gases adsorption and gas mixtures adsorption. null
o2: a novel combined online and offline computing system for the alice experiment after 2018. null
multiplicity dependence of jet-like two-particle correlations in p-pb collisions at $\sqrt{s_{nn}}$ = 5.02 tev. two-particle angular correlations between unidentified charged trigger and associated particles are measured by the alice detector in p-pb collisions at a nucleon-nucleon centre-of-mass energy of 5.02 tev. the transverse-momentum range 0.7 $ &lt; p_{\rm{t}, assoc} &lt; p_{\rm{t}, trig} &lt;$ 5.0 gev/$c$ is examined, to include correlations induced by jets originating from low momen\-tum-transfer scatterings (minijets). the correlations expressed as associated yield per trigger particle are obtained in the pseudorapidity range $|\eta|&lt;0.9$. the near-side long-range pseudorapidity correlations observed in high-multiplicity p-pb collisions are subtracted from both near-side short-range and away-side correlations in order to remove the non-jet-like components. the yields in the jet-like peaks are found to be invariant with event multiplicity with the exception of events with low multiplicity. this invariance is consistent with the particles being produced via the incoherent fragmentation of multiple parton--parton scatterings, while the yield related to the previously observed ridge structures is not jet-related. the number of uncorrelated sources of particle production is found to increase linearly with multiplicity, suggesting no saturation of the number of multi-parton interactions even in the highest multiplicity p-pb collisions. further, the number scales in the intermediate multiplicity region with the number of binary nucleon-nucleon collisions estimated with a glauber monte-carlo simulation.
a targeted search for point sources of eev neutrons. a flux of neutrons from an astrophysical source in the galaxy can be detected in the pierre auger observatory as an excess of cosmic-ray air showers arriving from the direction of the source. to avoid the statistical penalty for making many trials, classes of objects are tested in combinations as nine "target sets", in addition to the search for a neutron flux from the galactic center or from the galactic plane. within a target set, each candidate source is weighted in proportion to its electromagnetic flux, its exposure to the auger observatory, and its flux attenuation factor due to neutron decay. these searches do not find evidence for a neutron flux from any class of candidate sources. tabulated results give the combined p-value for each class, with and without the weights, and also the flux upper limit for the most significant candidate source within each class. these limits on fluxes of neutrons significantly constrain models of eev proton emission from non-transient discrete sources in the galaxy.
a route for polonium 210 production from alpha-particle irradiated bismuth-209 target. a method is proposed for production of polonium-210 via the 209bi(α, 3n)210at nuclear reaction. bombardment of a bismuth-209 target was performed with a 37 mev alpha-particle beam that leads to the production of astatine-210 (t1/2 = 8.1 h), which decays to polonium-210. it is purified from the bismuth target matrix by employing liquid-liquid extraction using tributyl phosphate (tbp) in para-xylene from 7 m hydrochloric acid. back extraction of polonium-210 was performed by 9 m nitric acid. this method allows to purify a tracer amount of po-210 (2.6 * 10-13 mol) from macroscopic amount of bi (2.8 * 10-2 mol).
multi-objective design optimization of the leg mechanism for a piping inspection robot. this paper addresses the dimensional synthesis of an adaptive mechanism of contact points ie a leg mechanism of a piping inspection robot operating in an irradiated area as a nuclear power plant. this studied mechanism is the leading part of the robot sub-system responsible of the locomotion. firstly, three architectures are chosen from the literature and their properties are described. then, a method using a multi-objective optimization is proposed to determine the best architecture and the optimal geometric parameters of a leg taking into account environmental and design constraints. in this context, the objective functions are the minimization of the mechanism size and the maximization of the transmission force factor. representations of the pareto front versus the objective functions and the design parameters are given. finally, the cad model of several solutions located on the pareto front are presented and discussed.
core - corona model analysis of the low energy beam scan at rhic (relativistic heavy ion collider) in brookhaven (usa). the centrality dependence of spectra of identified particles in collisions between ultrarelativistic heavy ions with a center of mass energy ($\sqrt{s}$) of 39 and 11.5 $agev$ is analyzed in the core - corona model. we show that at these energies the spectra can be well understood assuming that they are composed of two components whose relative fraction depends on the centrality of the interaction: the core component which describes an equilibrated quark gluon plasma and the corona component which is caused by nucleons close to the surface of the interaction zone which scatter only once and which is identical to that observed in proton-proton collisions. the success of this approach at 39 and 11.5 $agev$ shows that the physics does not change between this energy and $\sqrt{s}=200~ agev$ for which this model has been developed (aichelin 2008). this presents circumstantial evidence that a quark gluon plasma is also created at center of mass energies as low as 11.5 $agev$.
d mesons in non-central heavy-ion collisions: fluctuating vs. averaged initial conditions. the suppression of d mesons in non-central heavy-ion collisions is investigated. the anisotropy in collisions at finite impact parameter leads to an ordering of all-angle, in- and out-of-plane nuclear modification factors due to the different in-medium path lengths. within our mc@shq+epos model of heavy-quark propagation in the qgp we demonstrate that fluctuating initial conditions lead to an effective reduction of the energy loss of heavy quarks, which is seen in a larger nuclear modification factor at intermediate and high transverse momenta. the elliptic flow at small transverse momenta is reduced.
dynamical collisional energy loss and transport properties of on- and off-shell heavy quarks in vacuum and in the quark gluon plasma. in this study we evaluate the dynamical collisional energy loss of heavy quarks, their interaction rate as well as the different transport coefficients (drag and diffusion coefficients, $\hat{q}$, etc). we calculate these different quantities for i) perturbative partons (on-shell particles in the vacuum with fixed and running coupling) and ii) for dynamical quasi-particles (off-shell particles in the qgp medium at finite temperature $t$ with a running coupling in temperature as described by the dynamical quasi-particles model). we use the perturbative elastic $(q(g) q \rightarrow q (g) q)$ cross section for the first case, and the infrared enhanced hard thermal loop cross sections for the second. the results obtained in this work demonstrate the effects of a finite parton mass and width on the heavy quark transport properties and provide the basic ingredients for an explicit study of the microscopic dynamics of heavy flavors in the qgp - as formed in relativistic heavy-ion collisions - within transport approaches developed previously by the authors.
anti-strange meson-baryon interaction in hot and dense nuclear matter. we present a study of in-medium cross sections and (off-shell) transition rates for the most relevant binary reactions for strange pseudoscalar meson production close to threshold in heavy-ion collisions at fair energies. our results rely on a chiral unitary approach in coupled channels which incorporates the $s$- and $p$-waves of the kaon-nucleon interaction. the formalism, which is modified in the hot and dense medium to account for pauli blocking effects, mean-field binding on baryons, and pion and kaon self-energies, has been improved to implement full unitarization and self-consistency for both the $s$- and $p$-wave interactions at finite temperature and density. this gives access to in-medium amplitudes in several elastic and inelastic coupled channels with strangeness content $s=-1$. the obtained total cross sections mostly reflect the fate of the $\lambda(1405)$ resonance, which melts in the nuclear environment, whereas the off-shell transition probabilities are also sensitive to the in-medium properties of the hyperons excited in the $p$-wave amplitudes [$\lambda$, $\sigma$ and $\sigma^*(1385)$]. the single-particle potentials of these hyperons at finite momentum, density and temperature are also discussed in connection with the pertinent scattering amplitudes. our results are the basis for future implementations in microscopic transport approaches accounting for off-shell dynamics of strangeness production in nucleus-nucleus collisions.
a search for point sources of eev photons. measurements of air showers made using the hybrid technique developed with the fluorescence and surface detectors of the pierre auger observatory allow a sensitive search for point sources of eev photons anywhere in the exposed sky. a multivariate analysis reduces the background of hadronic cosmic rays. the search is sensitive to a declination band from -85{\deg} to +20{\deg}, in an energy range from 10^17.3 ev to 10^18.5 ev. no photon point source has been detected. an upper limit on the photon flux has been derived for every direction. the mean value of the energy flux limit that results from this, assuming a photon spectral index of -2, is 0.06 ev cm^-2 s^-1, and no celestial direction exceeds 0.25 ev cm^-2 s^-1. these upper limits constrain scenarios in which eev cosmic ray protons are emitted by non-transient sources in the galaxy.
o2: a novel combined online and o ine computing system for the alice experiment after 2018. null
production of $\sigma(1385)^{\pm}$ and $\xi(1530)^{0}$ in proton-proton collisions at $\sqrt{s}=$ 7 tev. the production of the strange and double-strange baryon resonances ($\sigma(1385)^{\pm}$, $\xi(1530)^{0}$) has been measured at mid-rapidity ($\left | y \right |&lt;0.5$) in proton-proton collisions at $\sqrt{s}$ = 7 tev with the alice detector at the lhc. transverse momentum spectra for inelastic collisions are compared to qcd-inspired models, which in general underpredict the data. a search for the $\phi(1860)$ pentaquark, decaying in the $\xi\pi$ channel, has been carried out but no evidence is seen.
geometrical patterns for measurement pose selection in calibration of serial manipulators. null
multi-particle azimuthal correlations in p-pb and pb-pb collisions at the cern large hadron collider. measurements of multi-particle azimuthal correlations (cumulants) for charged particles in p-pb at $\sqrt{s_{\rm nn}} = 5.02$ tev and pb-pb at $\sqrt{s_{\rm nn}} = 2.76$ tev collisions are presented. they help address a question if there is evidence for global, flow-like, azimuthal correlations in the p-pb system. comparisons are made to measurements from the larger pb-pb system, where such evidence is established. in particular, the second harmonic two-particle cumulants are found to decrease with multiplicity, characteristic of a dominance of few-particle correlations in p-pb collisions. however, when a $|\delta \eta|$ gap is placed to suppress such correlations, the two-particle cumulants begin to rise at high-multiplicity, indicating the presence of global azimuthal correlations. the pb-pb values are higher than the p-pb values at similar multiplicities. in both systems, the second harmonic four-particle cumulants exhibit a transition from positive to negative values when the multiplicity increases. the negative values allow for a measurement of $v_{2}\{4\}$ to be made, which is found to be higher in pb-pb collisions at similar multiplicities. the second harmonic six-particle cumulants are also found to be higher in pb-pb collisions. in pb-pb collisions, we generally find $v_{2}\{4\} \simeq v_{2}\{6\}\neq 0$ which is indicative of a bessel-gaussian function for the $v_{2}$ distribution. for very high-multiplicity pb-pb collisions, we observe that the four- and six-particle cumulants become consistent with 0. finally, third harmonic two-particle cumulants in p-pb and pb-pb are measured. these are found to be similar for overlapping multiplicities, when a $|\delta\eta| &gt; 1.4$ gap is placed.
turning emergency plans into executable artifacts. on the way to the improvement of emergency plans, we show how a structured specification of the response procedures allows transforming static plans into dynamic, executable entities that can drive the way different actors participate in crisis responses. additionally, the execution of plans requires the definition of information access mechanisms allowing execution engines to provide an actor with all the information resources he or she needs to accomplish a response task. we describe work in progress to improve the saga's plan definition module and plan execution engine to support information-rich plan execution.
molecular modeling of the hydration, the structure, and the mobility of ions and water in the interlayer space and at the surface of a smectitic clay. the study of adsorption and ion mobility in clay minerals is important for a better understanding of many geochemical and environmental processes, as well as to predict the behavior of radionuclides in geological storage conditions. because of their very small size (&lt;2μm), it is not always easy to study clays by using the existing experimental methods and techniques. one alternative to this issue is to use computational molecular modeling to carry out clay studies. in addition to their tiny size, clays minerals also have complex structures, which can appear due to various possibilities in the distribution and arrangement of isomorphic substitutions in their layers. it has been clearly demonstrated that there is a strong correlation between the distribution of substitutions in the clay layers and their properties. however, this remains to be shown regarding the arrangement of the substitutions in the layers of the clay. in this work, computational molecular modeling techniques are used to determine and compare the hydration properties, as well as the structure and mobility of ions (li⁺, na⁺, k⁺, rb⁺, cs⁺, mg²⁺, ca²⁺, sr²⁺, ba²⁺, ni²⁺,uo₂²⁺) and water in the interlayer space of the three models of montmorillonite, that differ from each other by the arrangement of isomorphic substitutions in the clay layers.the adsorption and diffusion of the previously listed cations and water are also studied on the surface of montmorillonite clay and the results are compared to those obtained in the interlayer space both at 298 k and at 363 k. the data generated in this work agree well with experimental observations, and show a more or less significant correlation between the clay model used and the type of property calculated.
flow in proton-nucleus collisions. null
origin of atmospheric aerosols at the pierre auger observatory using studies of air mass trajectories in south america. the pierre auger observatory is making significant contributions towards understanding the nature and origin of ultra-high energy cosmic rays. one of its main challenges is the monitoring of the atmosphere, both in terms of its state variables and its optical properties. the aim of this work is to analyze aerosol optical depth $\tau_{\rm a}(z)$ values measured from 2004 to 2012 at the observatory, which is located in a remote and relatively unstudied area of the pampa amarilla, argentina. the aerosol optical depth is in average quite low - annual mean $\tau_{\rm a}(3.5~{\rm km})\sim 0.04$ - and shows a seasonal trend with a winter minimum - $\tau_{\rm a}(3.5~{\rm km})\sim 0.03$ -, and a summer maximum - $\tau_{\rm a}(3.5~{\rm km})\sim 0.06$ -, and an unexpected increase from august to september - $\tau_{\rm a}(3.5~{\rm km})\sim 0.055$). we computed backward trajectories for the years 2005 to 2012 to interpret the air mass origin. winter nights with low aerosol concentrations show air masses originating from the pacific ocean. average concentrations are affected by continental sources (wind-blown dust and urban pollution), while the peak observed in september and october could be linked to biomass burning in the northern part of argentina or air pollution coming from surrounding urban areas.
software architecture for product lines. null
quarkonium production measurement with the alice detector at the lhc. null
inclusive production of neutral mesons in alice. null
elliptic flow of non-photonic electrons in au+au collisions at $\sqrt{s_{\rm nn}} = $ 200, 62.4 and 39 gev. we present the measurements of elliptic flow ($v_2$) of non-photonic electrons (npe) by the star experiment using 2- and 4-particle correlations, $v_2${2} and $v_2${4}, and the event plane method in au+au collisions at $\sqrt{s_{nn}} = 200$ gev, and $v_2${2} at 62.4 and 39 gev. $v_2${2} and $v_2${4} are non-zero at low and intermediate transverse momentum ($p_t$) at 200 gev, and $v_2${2} is consistent with zero at low $p_t$ at other energies. for au+au collisions at $p_t&lt;1$ gev/c, there is a statistically significant difference between $v_2${2} at 200 gev and $v_2${2} at the two lower beam energies.
electric sensor-based control of underwater robot groups. some fish species use electric sense to navigate efficiently in the turbid waters of confined spaces. this paper presents a first attempt to use this sense to control a group of nonholonomic rigid underwater vehicles navigating in a cooperative way. a leader whose motion is unknown to the others serves as an active agent for its passive neighbor, which perceives the leader's electric field via current measurements and moves in order to follow a trajectory relative to it. then, this passive agent, becomes in its turn the leader for the next agent and so on. sufficient conditions of convergence of the control law are derived for electric current servoing. this is achieved without the explicit knowledge of the location of the agents. some limits on the possible motion of the leader along with the importance of the choice of controlled outputs are demonstrated. switching between different group configurations by following a virtual agent is also described. simulation and experimental results illustrate the theoretical study.
synthesis of an electric sensor based control for underwater multi-agents navigation in a file. thanks to an electro-sensible skin, some species of fish can feel the surrounding electric field generated by them-self or other fish. known under the name of "electric-sense", this ability allows these fish to navigate in confined surroundings. based on a bio-inspired electric sensor, this article presents how this electric sense can be used for the navigation in formation of several underwater vehicles. the formation considered is a file, each vehicle is assumed to follow its predecessor at a given distance. in confined environment, the file formation is interesting since fish can follow the same safe path. being based on the servoing of the electric measurements, these laws do not require the knowledge of the location of the agents. the underwater vehicle studied have non holonomic properties, their forward velocity has no lateral component. depending on the choice of the controlled outputs (combination of electric measures) we will see that path followed by the follower agents can be different and a methodology to choose the output will be defined in order that all the agents follow the leader path in presence of curved motion of the leader. the influence of the number of electrodes is discussed. simulation results illustrate the proposed approach.
electric sensor based control for underwater multi-agents navigation in formation. thanks to an electro-sensible skin, some species of fish can feel the perturbations of a self generated electric field caused by their surroundings variations. known under the name of "electric-sense", this ability allows these fish to communicate and navigate in confined surroundings wetted by turbid waters where vision and sonar cannot work. based on a bio-inspired electric sensor recently proposed in [1], this article presents a first attempt to use electric sense for the navigation in formation of a set of rigid underwater vehicles. the navigation strategy combines some behaviours observed in electric fish as well as a follower-leader strategy well known from multi-robot navigation. being based one the servoing of the electric measurements, these laws do not require the knowledge of the location of the agents. sufficient convergence conditions of the resulting control laws are given. moreover, some limits on the possible motion of the leader are exhibited and the importance of the choice of controlled outputs is discussed too. finally, simulation results illustrate the feasibility of the approach.
results of fission products $\beta$ decay properties measurement performed with a total absorption spectrometer. null
precision measurement of the longitudinal double-spin asymmetry for inclusive jet production in polarized proton collisions at $\sqrt{s}=200$ gev. we report a new high-precision measurement of the mid-rapidity inclusive jet longitudinal double-spin asymmetry, $a_{ll}$, in polarized $pp$ collisions at center-of-mass energy $\sqrt{s}=200$ gev. the star data place stringent constraints on polarized parton distribution functions extracted at next-to-leading order from global analyses of inclusive deep inelastic scattering (dis), semi-inclusive dis, and rhic $pp$ data. the measured asymmetries provide evidence for positive gluon polarization in the bjorken-$x$ region $x&gt;0.05$.
caching web services: aspect orientation to the rescue. null
mixing aspects and components for grid computing. null
constructing component-based extension interfaces in legacy systems code. implementing an extension of a legacy operating system requires knowing what functionalities the extension should provide and how the extension should be integrated with the legacy code. to resolve the first problem, we propose that the use of a component model can make explicit the interface between an extension and legacy code. to resolve the second problem, we propose to augment interface specifications with rewrite rules that integrate support for extensions in the legacy code. we illustrate our approach using extensions that add new scheduling policies to linux and prefetching to the squid web cache. in both cases a small number of rules are sufficient to describe modifications that apply across the implementation of a large legacy system.
improving the effectiveness of web caching. the bandwidth demands on the (world-wide) web continue to grow at an exponential rate. it is thus becoming crucial to provide solutions improving the web latency. in that framework, the most promising low cost solution lies in the use of caches at the level of the clients, network and servers. caching effectiveness then relies upon adequate cache management so as to keep in the cache the web objects that are the most likely to be re-accessed. however, the effectiveness of a single cache remains poor as it is in general no higher than 40%. one way to further improve caching effectiveness is thus to make caches cooperate so as to increase the probability of retrieving an object at the caching level. the cache cooperation protocol must then be such that it induces a negligible load for the network and cooperating caches. this paper presents our solutions to improving the effectiveness of web caching concerning both cache management and cache cooperation. regarding cache management, we propose two novel algorithms that exploit the latest results about web usage, enabling us to undertake replacement decisions that are more accurate than the one taken by existing algorithms. from the standpoint of cooperating caches, we propose a cooperation protocol, which minimizes the associated network bandwidth, processing load, and storage consumption among caches.
observability and controllability for linear neutral type systems. for a large class of linear neutral type systems which include distributed delays we give the duality relation between exact controllability and exact observability. this duality is based on the representation of the abstract adjoint system as a special neutral type system. as a consequence of this duality relation, a characterization of exact observability is obtained. the time of observability is precised.
elliptic flow of identified hadrons in pb-pb collisions at $\sqrt{s_{\rm{nn}}}$ = 2.76 tev. the elliptic flow coefficient ($v_{2}$) of identified particles in pb--pb collisions at $\sqrt{s_\mathrm{{nn}}} = 2.76$ tev was measured with the alice detector at the lhc. the results were obtained with the scalar product method, a two-particle correlation technique, using a pseudo-rapidity gap of $|\delta\eta| &gt; 0.9$ between the identified hadron under study and the reference particles. the $v_2$ is reported for $\pi^{\pm}$, $\mathrm{k}^{\pm}$, $\mathrm{k}^0_\mathrm{s}$, p+$\overline{\mathrm{p}}$, $\mathrm{\phi}$, $\lambda$+$\overline{\mathrm{\lambda}}$, $\xi^-$+$\overline{\xi}^+$ and $\omega^-$+$\overline{\omega}^+$ in several collision centralities. in the low transverse momentum ($p_{\mathrm{t}}$) region, $p_{\mathrm{t}} &lt; 2 $gev/$c$, $v_2(p_\mathrm{t})$ exhibits a particle mass dependence consistent with elliptic flow accompanied by the transverse radial expansion of the system with a common velocity field. the experimental data for $\pi^{\pm}$ and $\mathrm{k}$ are described fairly well by hydrodynamical calculations coupled to a hadronic cascade model (vishnu) for central collisions. however, the same calculations fail to reproduce the $v_2(p_\mathrm{t})$ for p+$\overline{\mathrm{p}}$, $\mathrm{\phi}$, $\lambda$+$\overline{\mathrm{\lambda}}$ and $\xi^-$+$\overline{\xi}^+$. for transverse momentum values larger than about 3 gev/$c$, particles tend to group according to their type, i.e. mesons and baryons. however, the experimental data at the lhc exhibit deviations from the number of constituent quark (ncq) scaling at the level of $\pm$20$\%$ for $p_{\mathrm{t}} &gt; 3 $gev/$c$.
suppression of $\upsilon$(1s) at forward rapidity in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. we report on the measurement of the inclusive $\upsilon$(1s) production in pb-pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev carried out at forward rapidity ($2.5.
effects of compressed solar radiation and weather data : sensitivity analysis of solar energy system models. null
la place des modèles de trafic dans les récentes modélisations des impacts environnementaux des transports : importance de l'explicitation des méthodes et hypothèses. the role of travel demand models in recent modelings of environmental impacts of transport : importance of the explicitation of methods and hypotheses. abstract : the authors present a recent evolution in the use of the classic travel demand models (tdm) to environmental impact assessment of transport, far from its initial target. by comparing previous cases found in the literature (chester, seoul, florence, brisbane and saint-etienne) with their own present works (eval-pdu in nantes), the authors notice reluctantly that their predecessors tend to be evasive on their use of tdm. so, traffic data are little discussed in these works while they constitute one of the main stakes in this kind of study. indeed the hypotheses assumed for traffic modeling are impacting the next steps of the modeling chain (pollutants emission/dispersion). the importance of this first modeling stage excludes the possibility to leave it off the scene.
oscillatory modes of quarks in baryons for 3 quark flavors u,d,s. null
thermal characteristics confronting trace anomaly and intrinsic canonical structure of qcd. null
beam-energy dependence of charge separation along the magnetic field in au+au collisions at rhic. local parity-odd domains are theorized to form inside a quark-gluon-plasma (qgp) which has been produced in high-energy heavy-ion collisions. the local parity-odd domains manifest themselves as charge separation along the magnetic field axis via the chiral magnetic effect (cme). the experimental observation of charge separation has previously been reported for heavy-ion collisions at the top rhic energies. in this paper, we present the results of the beam-energy dependence of the charge correlations in au+au collisions at midrapidity for center-of-mass energies of 7.7, 11.5, 19.6, 27, 39 and 62.4 gev from the star experiment. after background subtraction, the signal gradually reduces with decreased beam energy, and tends to vanish by 7.7 gev. the implications of these results for the cme will be discussed.
measurement of longitudinal spin asymmetries for weak boson production in polarized proton-proton collisions at rhic. we report measurements of single and double spin asymmetries for $w^{\pm}$ and $z/\gamma^*$ boson production in longitudinally polarized $p+p$ collisions at $\sqrt{s} = 510$ gev by the star experiment at rhic. the asymmetries for $w^{\pm}$ were measured as a function of the decay lepton pseudorapidity, which provides a theoretically clean probe of the proton's polarized quark distributions at the scale of the $w$ mass. the results are compared to theoretical predictions, constrained by recent polarized dis measurements, and show a preference for a sizable, positive up antiquark polarization in the range $0.05.
measurement of $k_s^0$ and $k^{*0}$ in $p$$+$$p$, $d$$+$au, and cu$+$cu collisions at $\sqrt{s_{_{nn}}}=200$ gev. the phenix experiment at the relativistic heavy ion collider has performed a systematic study of $k_s^0$ and $k^{*0}$ meson production at midrapidity in $p$$+$$p$, $d$$+$au, and cu$+$cu collisions at $\sqrt{s_{_{nn}}}=200$ gev. the $k_s^0$ and $k^{*0}$ mesons are reconstructed via their $k_s^0 \rightarrow \pi^0(\rightarrow \gamma\gamma)\pi^0(\rightarrow\gamma\gamma)$ and $k^{*0} \rightarrow k^{\pm}\pi^{\mp}$ decay modes, respectively. the measured transverse-momentum spectra are used to determine the nuclear modification factor of $k_s^0$ and $k^{*0}$ mesons in $d$$+$au and cu$+$cu collisions at different centralities. in the $d$$+$au collisions, the nuclear modification factor of $k_s^0$ and $k^{*0}$ mesons is almost constant as a function of transverse momentum and is consistent with unity showing that cold-nuclear-matter effects do not play a significant role in the measured kinematic range. in cu$+$cu collisions, within the uncertainties no nuclear modification is registered in peripheral collisions. in central collisions, both mesons show suppression relative to the expectations from the $p$$+$$p$ yield scaled by the number of binary nucleon-nucleon collisions in the cu$+$cu system. in the $p_t$ range 2--5 gev/$c$, the strange mesons ($k_s^0$, $k^{*0}$) similarly to the $\phi$ meson with hidden strangeness, show an intermediate suppression between the more suppressed light quark mesons ($\pi^0$) and the nonsuppressed baryons ($p$, $\bar{p}$). at higher transverse momentum, $p_t&gt;5$ gev/$c$, production of all particles is similarly suppressed by a factor of $\approx$ 2.
quantitative structure property relationships (qspr) for the adsorption of organic compounds onto activated carbon cloths. comparison between multiple linear regression and neural network. quantitative structure-property relationship (qspr) for the adsorption of organic compounds onto activated carbon cloth: comparison between multiple linear regression and neural network by: brasquet, c; bourges, b; le cloirec, p environmental science &amp; technology volume: 33 issue: 23 pages: 4226-4231 published: dec 1 1999 context sensitive links close abstractclose abstract the adsorption of 55 organic compounds is carried out onto a recently discovered adsorbent, activated carbon cloth. isotherms are modeled using the freundlich classical model, and the large database generated allows qualitative assumptions about the adsorption mechanism. however, to confirm these assumptions, a quantitative structure-property relationship methodology is used to assess the correlation between an adsorbability parameter (expressed using the freundlich parameter k) and topological indices related to the compounds molecular structure (molecular connectivity indices, mci). this correlation is set up by mean of two different statistical tools, multiple linear regression (mlr) and neural network (nn). a principal component analysis is carried out to generate new and uncorrelated variables. it enables the relations between the mci to be analyzed, but the multiple linear regression assessed using the principal components (pcs) has a poor statistical quality and introduces high order pcs, too inaccurate for an explanation of the adsorption mechanism. the correlations are thus set up using the original variables (mci), and both statistical teals, multiple linear regression and neural network, are compared from a descriptive and predictive point of view. to compare the predictive ability of both methods, a test database of 10 organic compounds is used. results show the good descriptive ability of nn compared with that of mlr, with more than 68% variance explained by nn, whereas mlr allows only 44% variance explanation. however, the predictive ability of nn seems to be low, especially when the structure of the test compounds is not well described in the training database. the good descriptive ability of nn is then exploited to carry out a variable analysis using the garson weight partitioning method and to give information about the adsorption process. this study shows that fiat molecules seem to be better adsorbed onto activated carbon fibers than bulky molecules, because of an adsorption which is located between the micrographitic planes of fibers. the adsorption process occurs via an electron donor-acceptor interaction between the surface of the activated carbon fiber(donor) and the solute (acceptor). consequently, the aromatic compounds with electron-withdrawing substituents seem to be favored. furthermore, the lower the solute affinity for the aqueous media, the greater seems to be the adsorption.
the european solar radiation atlas vol.2: database and exploitation software. null
the european solar radiation atlas vol.1: fundamentals and maps. null
climatic data handbook for europe: climatic data for the design of solar energy systems. null
beauty production in pp collisions at $\sqrt{s}$ = 2.76 tev measured via semi-electronic decays. the alice collaboration at the lhc reports measurement of the inclusive production cross section of electrons from semi-leptonic decays of beauty hadrons with rapidity $|y|&lt;0.8$ and transverse momentum 1 &lt; p(t)&lt; 10 gev/c, in pp collisions at root s = 2.76 tev. electrons not originating from semi-electronic decay of beauty hadrons are suppressed using the impact parameter of the corresponding tracks. the production cross section of beauty decay electrons is compared to the result obtained with an alternative method which uses the distribution of the azimuthal angle between heavy-flavour decay electrons and charged hadrons. perturbative qcd predictions agree with the measured cross section within the experimental and theoretical uncertainties. the integrated visible cross section, sigma(b -&gt; e) = 3.47 +/- 0.40(stat)(+1.12)(-1.33)(sys) +/- 0.07(norm) mu b, was extrapolated to full phase space using fixed order plus next-to-leading log (fonll) calculations to obtain the total b (b) over bar production cross section, sigma(b (b) over bar) = 130 +/- 15.1(stat)(+42.1)(-49.8)(sys)(+3.4)(-3.1)(extr) +/- 2.5(norm) +/- 4.4(br) mu b. (c).
measurement of electrons from semileptonic heavy-flavor hadron decays in pp collisions at $\sqrt{s} = 2.76$ tev. the $p_{\rm t}$-differential production cross section of electrons from semileptonic decays of heavy-flavor hadrons has been measured at mid-rapidity in proton-proton collisions at $\sqrt{s} = 2.76$ tev in the transverse momentum range 0.5 &lt; $p_{\rm t}$ &lt; 12 gev/$c$ with the alice detector at the lhc. the analysis was performed using minimum bias events and events triggered by the electromagnetic calorimeter. predictions from perturbative qcd calculations agree with the data within the theoretical and experimental uncertainties.
open heavy-flavour results from alice. null
suppression of $\psi$(2s) production in p-pb collisions at $\sqrt{s_{nn}}$ = 5.02 tev. the alice collaboration has studied the inclusive production of the charmonium state ψ(2s) in proton-lead (p-pb) collisions at the nucleon-nucleon centre of mass energy snn−−−−√ = 5.02 tev at the cern lhc. the measurement was performed at forward (2.03&lt;ycms&lt;3.53) and backward (−4.46&lt;ycms&lt;−2.96) centre of mass rapidities, studying the decays into muon pairs. in this paper, we present the inclusive production cross sections σψ(2s), both integrated and as a function of the transverse momentum pt, for the two ycms domains. the results are compared to those obtained for the 1s vector state (j/ψ), by showing the ratios between the production cross sections, as well as the double ratios [σψ(2s)/σj/ψ]ppb/[σψ(2s)/σj/ψ]pp between p-pb and proton-proton collisions. finally, the nuclear modification factor for inclusive ψ(2s) is evaluated and compared to the measurement of the same quantity for j/ψ and to theoretical models including parton shadowing and coherent energy loss mechanisms. the results show a significantly larger suppression of the ψ(2s) compared to that measured for j/ψ and to models. these observations represent a clear indication for sizeable final state effects on ψ(2s) production.
neutral pion production at midrapidity in pp and pb-pb collisions at $\sqrt{s_{nn}}$ = 2.76 tev. invariant yields of neutral pions at midrapidity in the transverse momentum range $0.6 &lt; p_{t} &lt; 12 gev/c$ measured in pb-pb collisions at $\sqrt{s_{nn}} = 2.76 tev$ are presented for six centrality classes. the pp reference spectrum was measured in the range $0.4 &lt; p_{t} &lt; 10 gev/c$ at the same center-of-mass energy. the nuclear modification factor, $r_{aa}$, shows a suppression of neutral pions in central pb-pb collisions by a factor of up to about $8-10$ for $5 \lesssim p_{t} \lesssim 7 gev/c$. the presented measurements are compared with results at lower center-of-mass energies and with theoretical calculations.
locality-aware cooperation for vm scheduling in distributed clouds. the promotion of distributed cloud computing infrastructures as the next platform to deliver the utility computing paradigm, leads to new virtual machines (vms) scheduling algorithms leveraging peer to peer approaches. although these proposals considerably improve the scalability, leading to the management of hundreds of thousands of vm over thousands of physical machines (pms), they do not consider the network overhead introduced by multi-site infrastructures. this overhead can have a dramatic impact on performance if there is no mechanism for favoring intra-site vs. inter-site manipulations. this paper introduces a new building block designed over a vivaldi over- lay which maximizes efficient collaborations between pms. we combined this mechanism with dvms, a large scale virtual machine scheduler and showed its benefit by discussing several experiments performed on four distinct sites of the grid'5000 testbed. thanks to our proposal and with- out changing the scheduling decision algorithm, the number of inter-site operations has been reduced by 72%. this result provides a glimpse of the promising future of locality properties to improve performance of massive distributed cloud platforms.
transverse momentum dependence of inclusive primary charged-particle production in p-pb collisions at sqrt(snn) = 5.02 tev. the transverse momentum (pt) distribution of primary charged particles is measured at midrapidity in minimum-bias p-pb collisions at sqrt(snn) = 5.02 tev with the alice detector at the lhc in the range 0.15 &lt; pt &lt; 50 gev/c. the spectra are compared to the expectation based on binary collision scaling of particle production in pp collisions, leading to a nuclear modification factor consistent with unity for pt larger than 2 gev/c. the measurement is compared to theoretical calculations and to data in pb-pb collisions at sqrt(snn) = 2.76 tev.
azimuthal anisotropy of d meson production in pb-pb collisions at sqrt(snn) = 2.76 tev. the production of the prompt charmed mesons d0, d+ and d*+ relative to the reaction plane was measured in pb-pb collisions at a centre-of-mass energy per nucleon--nucleon collision of sqrt(snn) = 2.76 tev with the alice detector at the lhc. d mesons were reconstructed via their hadronic decays at central rapidity in the transverse momentum (pt) interval 2-16 gev/c. the azimuthal anisotropy is quantified in terms of the second coefficient v_2 in a fourier expansion of the d meson azimuthal distribution, and in terms of the nuclear modification factor r_aa, measured in the direction of the reaction plane and orthogonal to it. the v_2 coefficient was measured with three different methods and in three centrality classes in the interval 0-50%. a positive v_2 is observed in mid-central collisions (30-50% centrality class), with an mean value of 0.204_{-0.036}^{+0.099},(tot.unc.) in the interval 2 &lt; pt &lt; 6 gev/c, which decreases towards more central collisions (10-30% and 0-10% classes). the positive v_2 is also reflected in the nuclear modification factor, which shows a stronger suppression in the direction orthogonal to the reaction plane for mid-central collisions. the measurements are compared to theoretical calculations of charm quark transport and energy loss in high-density strongly-interacting matter at high temperature. the models that include substantial elastic interactions with an expanding medium provide a good description of the observed anisotropy. however, they are challenged to simultaneously describe the strong suppression of high-pt yield of d mesons in central collisions and their azimuthal anisotropy in non-central collisions.
measurement of visible cross sections in proton-lead collisions at sqrt(snn) = 5.02 tev in van der meer scans with the alice detector. in 2013, the large hadron collider provided proton-lead and lead-proton collisions at the center-of-mass energy per nucleon pair sqrt(snn) = 5.02 tev. van der meer scans were performed for both configurations of colliding beams, and the cross section was measured for two reference processes, based on particle detection by the t0 and v0 detectors, with pseudo-rapidity coverage 4.6 &lt; eta&lt; 4.9, -3.3 &lt; eta &lt; -3.0 and 2.8 &lt; eta &lt; 5.1, -3.7 &lt; eta &lt; -1.7, respectively. given the asymmetric detector acceptance, the cross section was measured separately for the two configurations. the measured visible cross sections are used to calculate the integrated luminosity of the proton-lead and lead-proton data samples, and to indirectly measure the cross section for a third, configuration-independent, reference process, based on neutron detection by the zero degree calorimeters.
accountability for abstract component design. the importance of the services-based market, 62.9% of the world gross domestic product (gdp), triggered an increase in the use of software offered on-line as services (saas). the use of such software usually implies the flow of personal data on-line between several parties. this can make users reluctant to their use. in this work, we consider this issue at the design-time of the software and we propose some foundations for an accountable software design. accountability for a software is a property describing, among other aspects, its liability to end-users for the usage of the data it has been entrusted. we propose to enrich software's component design by accountability obligations using an abstract accountability language (aal). we also define conditions for the well-formedness of an accountable component design and show how they can be checked using a model-checking tool.
modeling and simulating a narrow tilting car using robotics formalism. modeling and simulation are fundamental tools to develop new urban vehicles. the aim of this work is to model and simulate a narrow urban tilting car, which should significantly decrease traffic congestion, pollution, and parking problems. the structure of the vehicle contains closed kinematic chains. the modeling approach is based on the modified denavit and hartenberg description, which is commonly used in robotics, by considering the vehicle as a mobile robot composed of a multibody poly-articulated system in which the terminal links are the wheels. this description allows automatic calculating of the symbolic expressions of the geometric, kinematic, and dynamic models. a simulator is developed with matlab/simulink, and the simulation of different scenarios is performed and analyzed.
toward a rational matrix approximation of the parameter-dependent riccati equation solution. this paper considers the problem of solving parameter-dependent riccati equations. in this paper, a tractable iterative scheme involving mainly additions and multiplications is developed for finding solutions to arbitrary accuracy. it is first presented in the parameter-independent case and then extended to the parametric case. it hinges upon two results: (i) a palindromic quadratic polynomial matrix characterization of the matrix sign and square root functions. (ii) a particular representation of parameter dependent matrices with negative and positive power series with respect to parameters. several numerical examples are given throughout the paper to prove the validity of the proposed results.
a matrix sign function based solution of parameter dependent sylvester equations. this paper focuses on some parameter dependent sylvester equations arising in systems and control theory. the matrices involved are assumed to be i) parameter dependent, ii) not necessarily of the same size, and iii) with possible common eigenvalues depending on the parameter value. a matrix sign function based solution is proposed, considering two main cases: the square coefficient matrices are diagonalizable or block-diagonalizable.
parametric lyapunov equation approach for robust h2 analysis and structured h2 control problems. this paper presents a new framework to deal with robust h2 analysis and structured h2 control problems for linear time-invariant multi-parameter dependent systems. these two problems still hold a special place for practical reasons and will be formulated, in the current paper, as a problem of finding an exact solution of some parametric lyapunov equation. two results are then proposed: i) a direct inversion method of a particular type parameter-dependent matrix. ii) a discrete fourier transform (dft) based inversion method for polynomial parameter dependent matrices. some didactic examples are given, throughout the paper, to illustrate the validity of the proposed results.
a matrix sign function framework for robust stability analysis and parameter-dependent lyapunov and riccati equalities. this paper presents a new framework to deal with robust stability analysis for time-invariant parameter dependent systems. it hinges upon two results: -the matrix sign function integral definition. -a particular representation of parameter-dependent matrices with negative and positive power series with respect to parameters. exact solutions to parameter dependent lyapunov and riccati equalities are derived. several didactic examples are given throughout the paper to prove the validity of the proposed results.
freeze-out radii extracted from three-pion cumulants in pp, p-pb and pb-pb collisions at the lhc. in high-energy collisions, the spatio-temporal size of the particle production region can be measured using the bose-einstein correlations of identical bosons at low relative momentum. the source radii are typically extracted using two-pion correlations, and characterize the system at the last stage of interaction, called kinetic freeze-out. in low-multiplicity collisions, unlike in high-multiplicity collisions, two-pion correlations are substantially altered by background correlations, e.g. mini-jets. such correlations can be suppressed using three-pion cumulant correlations. we present the first measurements of the size of the system at freeze-out extracted from three-pion cumulant correlations in pp, p-pb and pb-pb collisions at the lhc with alice. at similar multiplicity, the invariant radii extracted in p-pb collisions are found to be 5-15% larger than those in pp, while those in pb-pb are 35-55% larger than those in p-pb. our measurements disfavor models which incorporate substantially stronger collective expansion in p-pb as compared to pp collisions at similar multiplicity.
a model-based approach for extracting business rules out of legacy information systems. today's business world is very dynamic and organizations have to quickly adjust their internal policies to follow the market changes. such adjustments must be propagated to the business logic embedded in the organization's information systems, that are often legacy applications not designed to represent and operationalize the business logic independently from the technical aspects of the programming language employed. consequently, the business logic buried in the system must be discovered and understood before being modified. unfortunately, such activities slow down the modification of the system to new requirements settled in the organization policies and threaten the consistency and coherency of the organization business. in order to simplify these activities, we provide amodel-based approach to extract and represent the business logic, expressed as a set of business rules, from the behavioral and structural parts of information systems. we implement such approach for java, cobol and relational database management systems. the proposed approach is based on model driven engineering,that provides a generic and modular solution adaptable to different languages by offering an abstract and homogeneous representation of the system.
optimal pose selection for the identification of geometric and elastostatic parameters of machining robots. the thesis deals with the optimal pose selection for geometric and elastostatic calibration for industrial robots employed in machining of large parts. particular attention is paid to the improvement of robot positioning accuracy after compensation of the geometric and elastostatic errors. to meet the industrial requirements of machining operations, a new approach for calibration experiments design for serial and quasi-serial industrial robots is proposed. this approach is based on a new industry-oriented performance measure that evaluates the quality of calibration experiment plan via the manipulator positioning accuracy after error compensation, and takes into account the particularities of prescribed manufacturing task by introducing manipulator test-poses. contrary to previous works, the developed approach employs an enhanced partial pose measurement method, which uses only direct position measurements from an external device and allows us to avoid the non-homogeneity of relevant identification equations. in order to consider the impact of gravity compensator that creates closed-loop chains, the conventional stiffness model is extended by including in it some configuration dependent elastostatic parameters, which are assumed to be constant for strictly serial robots. corresponding methodology for calibration of the gravity compensator models is also proposed. the advantages of the developed calibration techniques are validated via experimental study, which deals with geometric and elastostatic calibration of a kuka kr-270 industrial robot.
observation of $d^0$ meson nuclear modifications in au+au collisions at $\sqrt{s_{_{\mathrm{nn}}}}$ = 200 gev. we report the first measurement of charmed-hadron ($d^0$) production via the hadronic decay channel ($d^0\rightarrow k^- + \pi^+$) in au+au collisions at $\sqrt{s_{_{\mathrm{nn}}}}$ = 200\,gev with the star experiment. the charm production cross-section per nucleon-nucleon collision at mid-rapidity scales with the number of binary collisions, $n_{bin}$, from $p$+$p$ to central au+au collisions. the $d^0$ meson yields in central au+au collisions are strongly suppressed compared to those in $p$+$p$ scaled by $n_{bin}$, for transverse momenta $p_{t}&gt;3$ gev/$c$, demonstrating significant energy loss of charm quarks in the hot and dense medium. an enhancement at intermediate $p_{t}$ is also observed. model calculations including strong charm-medium interactions and coalescence hadronization describe our measurements.
interplay between local anisotropies in binuclear complexes. a systematic study has been undertaken to determine how local distortions affect the overall (molecular) magnetic anisotropies in binuclear complexes. for this purpose we have applied a series of distortions to two binuclear ni(ii) model complexes and extracted the magnetic anisotropy parameters of multispin and giant-spin model hamiltonians. furthermore, local and molecular magnetic axes frames have been determined. it is shown that certain combinations of local distortions can lead to constructive interference of the local anisotropies and that the largest contribution to the anisotropic exchange does not arise from the second-rank tensor normally included in the multispin hamiltonian, but rather from a fourth-rank tensor. from the comparison of the extracted parameters, simple rules are obtained to maximize the molecular anisotropy by controlling the local magnetic anisotropy, which opens the way to tune the anisotropy in binuclear or polynuclear complexes.
energy management in multi-consumers multi-sources system: a practical framework. the paper develops a methodology to design a practical multi-sources and multi- consumers energy management systems (ems), applicable in particular to transport systems. a global ems optimization problem is formulated underlining generics criterion and constraints. remarking that a modularity property is essential to add easily (i.e. without redesigning the whole problem) new energy consumers (or sources), a new ems structure is presented, splitting the problem into two independent sub-problems. in compensation of sub-optimality, the computing burden is lighten and robustness to energy shortage enhanced. the framework is introduced through the ems design of an hybrid vehicle transporting conditioned merchandises.
co&lt;sub&gt;2&lt;/sub&gt; adsorption in fe&lt;sub&gt;2&lt;/sub&gt;(dobdc): a classical force field parameterized from quantum mechanical calculations. carbon dioxide adsorption isotherms have been computed for the metal−organic framework (mof) fe2(dobdc), where dobdc4− = 2,5- dioxido-1,4-benzenedicarboxylate. a force field derived from quantum mechanical calculations has been used to model adsorption isotherms within a mof. restricted open-shell møller−plesset second-order perturbation theory (romp2) calculations have been performed to obtain interaction energy curves between a co2 molecule and a cluster model of fe2(dobdc). the force field parameters have been optimized to best reproduced these curves and used in monte carlo simulations to obtain co2 adsorption isotherms. the experimental loading of co2 adsorbed within fe2(dobdc) was reproduced quite accurately. this parametrization scheme could easily be utilized to predict isotherms of various guests inside this and other similar mofs not yet synthesized.
greywater treatment by a fluidized bed reactor and impacts related to their use for irrigation of urban green spaces. a level of water quality intended for human consumption does not seem necessary for domestic uses such as irrigation of green spaces. alternative water supplies like the use of greywater (gw) can thus be considered. however, gw contains pathogenic microorganisms and organic compounds which can cause environmental and health risks. as the risks related to recycling are unknown, gw treatment is necessary before reusing. to describe the risks related to gw reuses, the scientific approach performed in this study was to characterize domestic gw in order to select an appropriate treatment. the biological process chosen is an aerobic fluidized bed reactor. as this process has never been developed for gw, an optimization step based on the study of its hydrodynamic behavior and the kinetics of biodegradation of gw was performed. the treatment performances were then determined. the treated gw produced in this study reached the threshold values expected by the french regulation for irrigation of green spaces with treated wastewater. indeed, the cod and the tss obtained in treated gw were respectively 26 mg o2.l-1 and 5.6 mg.l-1. the fluidized bed reactor has been used to treat 144 l.d-1 of gw for 16 months. three lawn plots were irrigated respectively with raw gw, treated gw and tap water asa reference. contrary to the lawn plot irrigated with raw gw, the risk analysis performed in this study has shown no significant difference between the law plot irrigated with treated gw and the one irrigated with tap water. this study shows that treated gw produced from the fluidized bed reactor developed in this experiment can be used for irrigation of green spaces.
imaging current induced magnetic domain wall motion in la0.7sr0.3mno3 nanowires by xmcd-peem. null
beyond c&lt;i&gt;max&lt;/i&gt;: an optimization-oriented framework for constraint-based scheduling. this paper presents a framework taking advantage of both the flexibility of constraint programming and the efficiency of operations research algorithms for solving scheduling problems under various objectives and constraints. built upon a constraint programming engine, the framework allows the use of scheduling global constraints, and it offers, in addition, a modular and simplified way to perform optimality reasoning based on well-known scheduling relaxations. we present a first instantiation on the single machine problem with release dates and lateness minimization. beyond the simplicity of use, the ptimizationoriented framework appears to be, from the experiments, effective for dealing with such a pure problem even without any ad-hoc heuristics.
radio detection of ultra high energy cosmic rays : commissioning and data analysis of an array of autonomous stations. ultra high energy cosmic rays are undoubtedly the product of the most energetic process in the universe. their energy can reach macroscopic values (up to around 10 joules!) and it is converted to kinetic energy of the particles which compose the air shower in the atmosphere. questions about their source and propagation in the interstellar medium are still open, making the field of the astroparticle physics very attractive. the codalema experiment is an instrument devoted to indirect cosmic ray detection through the radio emission induced by charged particles of air showers. this thesis presents results of data analysis since 2010 concerning the latest setup of the experiment, which is composed of 34 autonomous stations. the noise analysis has revealed that autonomous stations are sensitive to several human-made interferences. since, methods for background rejection has been developed with the aim of embedding them into the next generation electronic boards. likewise, the radio emission process taking place in the shower during its development are studied through their polarization pattern. the presence of the two preponderant emission mechanisms (transverse current and charge excess) are highlighted in this data set. a new parametrization of the lateral profile is also suggested.
composing json-based web apis. the development of web apis has become a discipline that companies have to master to succeed in the web. the so-called api economy is pushing companies to provide access to their data by means of web apis, thus requiring web developers to study and integrate such apis into their applications. the exchange of data with these apis is usually performed by using json, a schemaless data format easy for computers to parse and use. while json data is easy to read, its structure is implicit, thus entailing serious problems when integrating apis coming from di erent vendors. web developers have therefore to understand the domain behind each api and study how they can be composed. we tackle this issue by presenting an approach able to both discover the domain of json-based web apis, and identify composition links among them. our approach allows developers to easily visualize what is behind apis and how they can be composed to be used in their applications.
universe polymorphism in coq. universes are used in type theory to ensure consistency by checking that definitions are well-stratified according to a certain hierarchy. in the case of the coq proof assistant, based on the predicative calculus of inductive constructions (pcic), this hierachy is built from an impredicative sort prop and an infinite number of predicative type_i universes. a cumulativity relation represents the inclusion order of uni- verses in the core theory. originally, universes were thought to be floating levels, and definitions to implicitly constrain these levels in a consistent manner. this works well for most theories, however the globality of levels and constraints precludes generic constructions on universes that could work at different levels. universe polymorphism extends this setup by adding local bindings of universes and constraints, supporting generic definitions over universes, reusable at different levels. this provides the same kind of code reuse facilities as ml-style parametric polymorphism. however, the structure and hierarchy of universes is more complex than bare polymorphic type variables. in this paper, we introduce a conservative extension of pcic supporting universe polymorphism and treating its whole hierarchy. this new design supports typical ambiguity and implicit polymorphic generalization at the same time, keeping it mostly transparent to the user. benchmarking the implementation as an extension of the coq proof assistant on real-world examples gives encouraging results.
emf views: dealing with several interrelated emf models. users only need to see some parts of an emf model, others have to get the full model extended with data from another model, and others simply access to a combination of information coming from different models. based on the unquestionable success and usefulness of database views to solve similar problems in the database world, emf views aims to bring the same concepts to the modeling world. this short talk is going to introduce the current version of emf views, notably showing different possible applications of model views such as software developer views, enterprise architect views, view querying or transformation.
improving the scalability of model driven web engineering approaches with runtime transformations. null
an adapter-based approach to co-evolve generated sql in model-to-text transformations. null
on the verification of uml/ocl class diagrams using constraint programming. assessment of the correctness of software models is a key issue to ensure the quality of the final application. to this end, this paper presents an automatic method for the verification of uml class diagrams extended with ocl constraints. our method checks compliance of the diagram with respect to several correctness properties including weak and strong satisfiability or absence of constraint redundancies among others. the method works by translating the uml/ocl model into a constraint satisfaction problem (csp) that is evaluated using state-of-the-art constraint solvers to determine the correctness of the initial model. our approach is particularly relevant to current mda and mdd methods where software models are the primary artifacts of the development process and the basis for the (semi-)automatic code-generation of the final application.
abstract accountability language. usual preventive security mechanisms are not adequate for a world where personal data can be exchanged on-line between different parties and/or stored at multiple jurisdictions. accountability becomes a necessary principle for future computer systems. this is specially critical for the cloud and web applications that collect personal and sensitive data from end users. accountability regards the responsibility and liability (including other attributes) for the data handling performed by a computer system on behalf of an organisation. in case of misconduct (e.g. security breaches, personal data leak, etc.), accountability should imply in remediation and redress actions, as in the real life. contrary to data privacy, which is already supported by several concrete languages, there is currently no language supporting accountability obligations representation. in this work, we provide an abstract language for accountability obligations representation. we analyze two use cases to illustrate the efficiency of our approach in representing accountability obligations in realistic situations.
test data generation for model transformations combining partition and constraint analysis. model-driven engineering (mde) is a software engineering paradigm where models play a key role. in a mde-based development process, models are successively transformed into other models and eventually into the final source code by means of a chain of model transformations. since writing model transformations is an error-prone task, mechanisms to ensure their reliability are greatly needed. one way of achieving this is by means of testing. a challenging aspect when testing model transformations is the generation of adequate input test data. most existing approaches generate test data following a black-box approach based on some sort of partition analysis that exploits the structural features of the source metamodel of the transformation. however, these analyses pay no attention to the ocl invariants of the metamodel or do it very superficially. in this paper, we propose a mechanism that systematically analyzes ocl constraints in the source metamodel in order to fine-tune this partition analysis and therefore, the generation of input test data. our mechanism can be used in isolation, or combined with other black-box or white-box test generation approaches.
formal verification of static software models in mde: a systematic review. context: model-driven engineering (mde) promotes the utilization of models as primary artifacts in all software engineering activities. therefore, mechanisms to ensure model correctness become crucial, specially when applying mde to the development of software, where software is the result of a chain of (semi)automatic model transformations that refine initial abstract models to lower level ones from which the final code is eventually generated. clearly, in this context, an error in the model/s is propagated to the code endangering the soundness of the resulting software. formal verification of software models is a promising approach that advocates the employment of formal methods to achieve model correctness, and it has received a considerable amount of attention in the last few years. objective: the objective of this paper is to analyze the state of the art in the field of formal verification of models, restricting the analysis to those approaches applied over static software models complemented or not with constraints expressed in textual languages, typically the object constraint language (ocl). method: we have conducted a systematic literature review (slr) of the published works in this field, describing their main characteristics. results: the study is based on a set of 48 resources that have been grouped in 18 different approaches according to their affinity. for each of them we have analyzed, among other issues, the formalism used, the support given to ocl, the correctness properties addressed or the feedback yielded by the verification process. conclusions: one of the most important conclusions obtained is that current model verification approaches are strongly influenced by the support given to ocl. another important finding is that in general, current verification tools present important flaws like the lack of integration into the model designer tool chain or the lack of efficiency when verifying large, real-life models.
k*(892)^0 and phi(1020) production in pb-pb collisions at sqrt(snn) = 2.76 tev. the yields of the k*(892) and phi(1020) resonances are measured in pb-pb collisions at sqrt(snn) = 2.76 tev through their hadronic decays using the alice detector. the measurements are performed in multiple centrality intervals at mid-rapidity (|y|&lt;0.5) in the transverse-momentum ranges 0.3.
characterization of the raw gases emitted during the thermal treatment of nanocomposites, and potential impacts on flue gas cleaning systems. as nanocomposites are increasingly used for a wide range of applications, they are expected to end up in waste treatment facilities. thus, it seems appropriate to study at a lab-scale the thermal treatment by incineration of various nanocomposites. our purpose is to identify the main mechanisms of thermal degradation involved when such materials are incinerated, and to fully characterize the raw gases emitted in the combustion chambers, prior implementing fluegas treatment processes (like filtration, etc). with experimental lab-devices, combustion tests are performed and coupled with raw gas analysis to provide a better understanding of the thermal degradation and emission mechanisms.
characterization of aerosols emitted during the incineration of nanocomposites. end-of-life nanocomposites are likely to undergo disposal through thermal treatment like incineration. data from literature indicate that nanocomposites present specific behaviours according to the conditions of thermal treatment [1]. it is thus relevant to understand the mechanisms of emission and thermal degradation of nanocomposites under incineration conditions. thence, the characterization of the effluents during the incineration of nanocomposites, collected downstream the incinerator furnace (or even upstream the flue-gas cleaning systems), provides an insight in the "nanosafety" of this technology [2], useful to cope with nanowastes (wastes containing nanomaterials). at a lab-scale, the emission mechanisms and the thermal degradation mechanisms were determined using various devices: a microcalorimeter pcfc and a fire propagation apparatus (fpa tewarson) coupled with a fourier transformed infrared (ftir); as well as a modified tubular furnace (fig. 1) coupled with a gas analyser. conditions of thermal degradation implemented in those laboratory devices were accurately characterized by evaluating and controlling (whenever possible) the key operational parameters that govern an incineration process, i.e. chiefly: temperature, residence time, air-excess and turbulence. electrical low pressure impactor (elpi) coupled with combustion devices and the morphology was determined using tem observation (transmission electronic microscopy) via a mps (mini-particle-sampler) [3]. our tests were performed on specimens of nanocomposites incorporating different mineral fillers (like sepiolite and halloysite nanoclays) and neat polymer matrices. the characterization of the combustion aerosols from the small-scale thermal degradation of nanocomposites is a first step for determining nano- objects potentially released during incineration process.
the nanofluegas project : characterization of nanoparticulate emissions from the incineration of wastes containing manufactured nanomaterials. the wide development of nanotechnologies requires a consideration of nanosafety during the whole life cycle of products containing nanomaterials. yet, there is neither specialized procedure for the waste management of nano-objects at the end of their life nor associated regulation. the nanofluegas project has been recently launched for three years (2011-2014) to investigate the safe incineration of wastes containing nanomaterials. the main objectives of this project supported by ademe are: (1) to provide a better understanding of the possible mechanisms of nanoparticles release during the combustion of nanowastes and (2) to evaluate the efficiency of existing processes. the initial works of the nanofluegas aim at: (1) identifying deposits containing nanomaterials; (2) selecting three representative nanowastes. the experimental part is twofold: (1) investigate emission mechanisms and characterization of nanomaterials in the smoke during the incineration at pilot and industrial scales; (2) evaluate the efficiency of main available pollution abatement processes, thus providing fully adapted recommendations on procedures and technological features for the whole processing of nanowastes.this communication describes the different steps of the project and presents preliminary results.
characterization of nanoparticulate emissions from the incineration of wastes containing manufactured nanomaterials. null
the nanofluegas project : characterization and reduction of particulate emissions from the incineration of wastes containing manufactured nanomaterials. in view of sustainable innovation, the development of nanotechnologies requires a deep insight in the nanosafety during the whole life cycle of products containing nanomaterials, from production to recycling and final destruction. no peculiar approved procedure exists for the waste management of nanoobjects at their end of life, due to the so far regulatorily unrecognized nano-specificity of such emerging products. the french nanofluegas project (2011-2014), supported by ademe and led by ineris, will examine nanosafety during the final destruction of nanomanufactured products by incineration, i) to understand possible mechanisms of nanoparticle release during combustion, and ii) to evaluate the efficiency of current pollution control devices. this project benefits from the know-how of both the ecole des mines de nantes gepea laboratory, a main academic partner specialized in engineering, and tredi-séché environnemen, a major expert of hazardous wastes incineration. the initial efforts focused on: i) identifying possible sources containing nanomaterials; ii) selecting three representative nanomaterials; and iii) providing small-scale incineration devices as close as possible to real scale installations. the experimental part is twofold. first, a deep insight is provided on the emission mechanisms and on the characterization of nanoparticles in the smokes (particle number, size, chemical composition, morphology...) during incineration at both pilotscale and on industrial units. secondly, the nanofluegas consortium will review the efficiency of main available pollution abatement processes, thus providing fully adapted recommendations on procedures and technological features for the whole processing of nanowastes. herein, we describe the different steps of the project, providing preliminary results.
direct photon measurements with alice. null
neo4emf, a scalable persistence layer for emf models. several industrial contexts require software engineering methods and tools able to handle large -size artifacts. the central idea of abstraction makes model-driven engineering (mde) a promising approach in such contexts, but current tools do not scale to very large models (vlms): already the task of storing and accessing vlms from a persisting support is currently ine cient. in this paper we propose a scalable persistence layer for the de-facto standard mde framework emf. the layer exploits the e ciency of graph databases in storing and accessing graph structures, as emf models are. a preliminary experimentation shows that typical queries in reverse-engineering emf models have good performance on such persistence layer, compared to le-based backends.
enforcing expressive accountability policies. accountability policies for the enforcement of the responsible stewardship of personal data have to support the gathering of information at all levels of the service stack and across different policy domains, for instance, for the retrospective enforcement of transparency and remediation properties. existing approaches to accountability, however, often do not meet these requirements and corresponding implementation support is generally lacking. in this paper we show how expressive policies can be defined in terms of properties that change across boundaries of policy domains, include access to data at different levels of the service stack, and support preventive and retrospective mechanisms for different accountability properties, notably transparency and remediability. furthermore, we present a notion of accountability schemes that support the constructive implementation of accountability policies. finally, we motivate and apply our approach in the context of real-world attacks to oauth-based authorization and authentication schemes.
unusual structure, bonding and properties in a californium borate. the participation of the valence orbitals of actinides in bonding has been debated for decades. recent experimental and computational investigations demonstrated the involvement of 6p, 6d and/or 5f orbitals in bonding. however, structural and spectroscopic data, as well as theory, indicate a decrease in covalency across the actinide series, and the evidence points to highly ionic, lanthanide-like bonding for late actinides. here we show that chemical differentiation between californium and lanthanides can be achieved by using ligands that are both highly polarizable and substantially rearrange on complexation. a ligand that suits both of these desired properties is polyborate. we demonstrate that the 5f, 6d and 7p orbitals are all involved in bonding in a cf(iii) borate, and that large crystal-field effects are present. synthetic, structural and spectroscopic data are complemented by quantum mechanical calculations to support these observations.
fuml as an assembly language for mda. null
pseudo jahn-teller effect, spin-orbit coupling, and electron correlation in the xf&lt;sub&gt;3&lt;/sub&gt; (x=cl, br, i, at) series. null
the v protein of tioman virus is incapable of blocking type i interferon signaling in human cells. the capacity of a virus to cross species barriers is determined by the development of bona fide interactions with cellular components of new hosts, and in particular its ability to block ifn-α/β antiviral signaling. tioman virus (tiov), a close relative of mumps virus (muv), has been isolated in giant fruit bats in southeast asia. nipah and hendra viruses, which are present in the same bat colonies, are highly pathogenic in human. despite serological evidences of close contacts between tiov and human populations, whether tiov is associated to some human pathology remains undetermined. here we show that in contrast to the v protein of muv, the v protein of tiov (tiov-v) hardly interacts with human stat2, does not degrade stat1, and cannot block ifn-α/β signaling in human cells. in contrast, tiov-v properly binds to human stat3 and mda5, and thus interferes with il-6 signaling and ifn-β promoter induction in human cells. because stat2 binding was previously identified as a host restriction factor for some paramyxoviridae, we established stat2 sequence from giant fruit bats, and binding to tiov-v was tested. surprisingly, tiov-v interaction with stat2 from giant fruit bats is also extremely weak and barely detectable. altogether, our observations question the capacity of tiov to appropriately control ifn-α/β signaling in both human and giant fruit bats that are considered as its natural host.
a framework for variable content document generation with multiple actors. context: advances in customization have highlighted the need for tools supporting variable content document management and generation in many domains. current tools allow the generation of highly customized documents that are variable in both content and layout. however, most frameworks are technology-oriented, and their use requires advanced skills in implementationrelated tools, which means their use by end users (i.e. document designers) is severely limited. objective: starting from past and current trends for customized document authoring, our goal is to provide a document generation alternative in which variants are specified at a high level of abstraction and content reuse can be maximized in high variability scenarios. method: based on our experience in document engineering, we identified areas in the variable content document management and generation field open to further improvement. we first classified the primary sources of variability in document composition processes and then developed a methodology, which we called dpl - based on software product lines principles - to support document generation in high variability scenarios. results: in order to validate the applicability of our methodology we implemented a tool - dplfw - to carry out dpl processes. after using this in different scenarios, we compared our proposal with other state-of-the-art tools for variable content document management and generation. conclusion: the dplfw showed a good capacity for the automatic generation of variable content documents equal to or in some cases surpassing other currently available approaches. to the best of our knowledge, dplfw is the only framework that combines variable content and document workflow facilities, easing the generation of variable content documents in which multiple actors play different roles.
reaction-diffusion approach in soft diffraction. we apply the reaction-diffusion (stochastic) approach to the numerical calculation of the elastic amplitude in the reggeon field theory (rft) and its single diffractive cut. fits to the total, integrated and differential elastic cross sections with account of all pomeron loops are reported together with all-loop calculation of the single difraction dissociation cross section.
vacuum structure in 3d supersymmetric gauge theories. this minireview (written on the basis of the talk that the author delivered at the pomeranchuk memorial conference in itep in june 2013 and his original papers written before on this subject) is devoted to the problem of vacuum dynamics in 3d supersymmetric yang-mills-chern-simons theories with and without extra matter multiplets. by analyzing the effective born-oppenheimer hamiltonian in a small spatial box, we calculate the number of vacuum states (the witten index) for these theories and analyze their structure. the results coincide with those derived by other methods.
beam energy dependence of moments of the net-charge multiplicity distributions in au+au collisions at rhic. we report the first measurements of the moments -mean ($m$), variance ($\sigma^{2}$), skewness ($s$) and kurtosis ($\kappa$) -of the net-charge multiplicity distributions at mid-rapidity in au+au collisions at seven energies, ranging from $\sqrt {{s_{\rm nn}}}$=7.7 to 200 gev, as a part of the beam energy scan program at rhic. the moments are related to the thermodynamic susceptibilities of net-charge, which are expected to diverge at the qcd critical point. we compare the products of the moments, $\sigma^{2}/m$, $s\sigma$ and $\kappa\sigma^{2}$ with the expectations from poisson and negative binomial distributions (nbd). the $s\sigma$ values deviate from poisson and are close to nbd baseline, while the $\kappa\sigma^{2}$ values tend to lie between the two. within the present uncertainties, our data do not show clear evidence of non-monotonic behavior as a function of collision energy.
dielectron azimuthal anisotropy at mid-rapidity in au+au collisions at $\sqrt{s_{_{nn}}} = 200$ gev. we report on the first measurement of the azimuthal anisotropy ($v_2$) of dielectrons at mid-rapidity from $\sqrt{s_{_{nn}}} = 200$ gev au+au collisions with the star detector at rhic, presented as a function of transverse momentum ($p_t$) for different invariant mass regions. in the mass region $m_{ee}/!&lt;1.1$ gev/$c^2$ the dielectron $v_2$ measurements are found to be consistent with expectations from $\pi^{0}$, $\eta$, $\omega$ and $\phi$ decay contributions. in the mass region $1.1/!.
beam-energy-dependent two-pion interferometry and the freeze-out eccentricity of pions measured in heavy ion collisions at the star detector. we present results of analyses of two-pion interferometry in au+au collisions at $\sqrt{s_{nn}}$ = 7.7, 11.5, 19.6, 27, 39, 62.4 and 200 gev measured in the star detector as part of the rhic beam energy scan program. the extracted correlation lengths (hbt radii) are studied as a function of beam energy, azimuthal angle relative to the reaction plane, centrality, and transverse mass ($m_{t}$) of the particles. the azimuthal analysis allows extraction of the eccentricity of the entire fireball at kinetic freeze-out. the energy dependence of this observable is expected to be sensitive to changes in the equation of state. a new global fit method is studied as an alternate method to directly measure the parameters in the azimuthal analysis. the eccentricity shows a monotonic decrease with beam energy that is qualitatively consistent with the trend from all model predictions and quantitatively consistent with a hadronic transport model.
methodology of the state approach control. this chapter deals with advanced control methodology based on h2 control design. the controlled system and the exogeneous signals as well are considered through state-space models. the associated h2 problem is non standard (not necessarily stabilizable or detectable). the solution of this extended problem is given, with new conditions making the riccati equation involved solvable. the continuous-time and exactly discretized problems are both considered.
discrete time systems. the chapter deals with some fundamentals about analysis and manipulation of discrete signals and systems, and about discretization of linear systems.
measurement of quarkonium production at forward rapidity in pp collisions at sqrt{s}= 7 tev. the inclusive production cross sections at forward rapidity of j/psi, psi(2s), upsilon(1s) and upsilon(2s) are measured in pp collisions at sqrt{s} = 7 tev with the alice detector at the lhc. the analysis is based in a data sample corresponding to an integrated luminosity of 1.35 pb^-1. quarkonia are reconstructed in the dimuon-decay channel and the signal yields are evaluated by fitting the mu+mu- invariant mass distributions. the differential production cross sections are measured as a function of the transverse momentum pt and rapidity y, over the ranges 0 &lt; pt &lt; 20 gev/c for j/psi, 0 &lt; pt &lt; 12 gev/c for all other resonances and for 2.5 &lt; y &lt; 4. the measured cross sections integrated over pt and y, and assuming unpolarized quarkonia, are: sigma_{j/psi} = 6.69 +- 0.04 +- 0.63 microbarn, sigma_{psiprime} = 1.13 +- 0.07 +- 0.14 microbarn, sigma_{upsilon(1s)} = 54.2 +- 5.0 +- 6.7 nb and sigma_{upsilon(2s)} = 18.4 +- 3.7 +- 2.2 nb, where the first uncertainty is statistical and the second one is systematic. the results are compared to measurements performed by other lhc experiments and to theoretical models.
open bottom states and the anti-b meson propagation in hadronic matter. the interaction and propagation of anti-b mesons with light mesons, n and delta baryons is studied within a unitarized approach based on effective models that are compatible with chiral and heavy-quark symmetries. we find several heavy-quark spin doublets in the open-bottom sectors, where anti-b and anti-b* mesons are present. in the meson sector we find several resonant states, among them, a b0 and a b1 with masses 5530 mev and 5579 mev as well as bs0* and bs1* narrow states at 5748 mev and 5799 mev, respectively. they form two doublets with no experimental identification yet, the first one being the bottom counterpart of the d0(2400) and d1(2430) states, and the second bottom doublet associated to the ubiquitous ds0* (2317) and the ds1 (2460). in the baryon sector, several lambda_b and sigma_b doublets are identified, among them the one given by the experimental lambda_b(5910) and lambda*_b(5921). moreover, one of our states, the sigma_b*(5904), turns out to be the bottom counterpart of the sigma*(1670) and sigma_c*(2549), which is a case for discovery. we finally analyze different transport coefficients for the anti-b meson in hot matter, such as formed in heavy-ion collisions at rhic and lhc. for rhic/lhc energies, the main contribution to the coefficients comes from the interaction of anti-b mesons with pions. however, we also include the effects of baryonic density which might be sizable at temperatures t &lt; 100 mev, as the chemical potential is expected to increase in the last stages of the expansion. we conclude that although the relaxation time decreases with larger baryonic densities, the anti-b meson does not thermalize at rhic/lhc energies, representing an ideal probe for the initial bottom distribution.
azimuthal emission patterns of $k^{+}$ and of $ k^{-} $ mesons in ni + ni collisions near the strangeness production threshold. azimuthal emission patterns of $k^\pm$ mesons have been measured in ni + ni collisions with the fopi spectrometer at a beam kinetic energy of 1.91 a gev. the transverse momentum $p_{t}$ integrated directed and elliptic flow of $k^{+}$ and $k^{-}$ mesons as well as the centrality dependence of $p_{t}$ - differential directed flow of $k^{+}$ mesons are compared to the predictions of hsd and iqmd transport models. the data exhibits different propagation patterns of $k^{+}$ and $k^{-}$ mesons in the compressed and heated nuclear medium and favor the existence of a kaon-nucleon in-medium potential, repulsive for $k^{+}$ mesons and attractive for $k^{-}$ mesons.
natural modeling: retrospective and perspectives an anthropological point of view. is extreme modeling so extreme? we advocate that natural modeling might be a better term. after all, the ultimate goal is to enable modelers to perform their job naturally. in the century of the "disappearing computer", it definitively makes sense to search for non invasive and flexible modeling technologies. this paper considers modeling from an anthropological point of view. a retrospective starting back to the prehistoric age leads to new perspectives for natural modeling in the information age. it is shown (1) that the need for compromises between flexibility and formality is "natural" rather than "extreme", (2) that the languages are emergent by nature, and (3) that natural interfaces should be provided to all stakeholders. we advocate that surface computing, tangible user-interfaces, collaborative modeling and emergent (meta)modeling are future research directions to be investigated in order to make "extreme" modeling just "natural". just as it should be.
isolated photon production measurement in p-p collisions at √s= 7 tev with the alice detector. the high transverse momentum photon production inproton-proton collisions (p-p) is described by perturbativequantum chromodynamics (pqcd). among thesephotons, those produced directly by an energetic partonicinteraction (called direct photons) are of great interestsince their measurement allows to test pqcdpredictions and it allows also the constraint of protonstructure functions. the work of this thesis aims atstudying and measuring direct photons produced in p-pcollisions at 7 tev with the alice detector. the aliceelectromagnetic calorimeter (emcal) is used to achievethis measurement which is based on an isolation procedurethat allows to reduce background coming fromother photon production modes (fragmentation, decay).multiple aspects like emcal data quality, photon identificationas well as spectrum correction and its normalizationare highlighted. finally, the first isolated promptphoton cross-section measured with alice detector ispresented, compared to theoretical predictions and tothe last results from other lhc experiments.
determination of absolute gas adsorption isotherms: simple method based on the potential theory for buoyancy effect correction of pure gas and gas mixtures adsorption. the absolute adsorption isotherms are necessary to correctly evaluate the selectivity of the adsorbent material or to design adsorption processes at high pressure (e.g., h2 purification from syngas processes, removal of acid gas from natural gas,...). the aim of this work is thus to propose an easy method to correct the buoyancy effect of the bulk phase on the adsorbed phase volume during both pure gas and gas mixtures adsorption for pressures up to 10 mpa. the potential theory of adsorption and the dubinin-radushkevich relation are adapted by introducing mixing parameters based on simple berthelot rules. the concept of internal pressure used to characterize the adsorbed phase is also adapted for mixtures. the method is then improved on a commercial activated carbon (ac), when adsorbing pure h2s and ch4, and their mixtures up to 5 mpa. the study points out the importance to carefully consider the buoyancy effect of the bulk phase on the adsorbed phase volume. its impact on the adsorbent material selectivity at high pressures could affect the design and the performances of psa or tsa processes. for example, only considering the excess adsorption data leads to an apparent selectivity 13 % greater than the absolute one for a concentration of 6 ppm of h2s in a ch4 matrix at 5 mpa (298 k) on the ac.
performance of the alice experiment at the cern lhc. alice is the heavy-ion experiment at the cern large hadron collider. the experiment continuously took data during the first physics campaign of the machine from fall 2009 until early 2013, using proton and lead-ion beams. in this paper we describe the running environment and the data handling procedures, and discuss the performance of the alice detectors and analysis methods for various physics observables.
probing the radio emission from air showers with polarization measurements. the emission of radio waves from air showers has been attributed to the so-called geomagnetic emission process. at frequencies around 50 mhz this process leads to coherent radiation which can be observed with rather simple setups. the direction of the electric field induced by this emission process depends only on the local magnetic field vector and on the incoming direction of the air shower. we report on measurements of the electric field vector where, in addition to this geomagnetic component, another component has been observed which cannot be described by the geomagnetic emission process. the data provide strong evidence that the other electric field component is polarized radially with respect to the shower axis, in agreement with predictions made by askaryan who described radio emission from particle showers due to a negative charge-excess in the front of the shower. our results are compared to calculations which include the radiation mechanism induced by this charge-excess process.
application of sensitivity analysis in building energy simulations: combining first- and second-order elementary effects methods. sensitivity analysis plays an important role in the understanding of complex models. it helps to identify the influence of input parameters in relation to the outputs. it can also be a tool to understand the behavior of the model and can then facilitate its development stage. this study aims to analyze and illustrate the potential usefulness of combining first and second-order sensitivity analysis, applied to a building energy model (esp-r). through the example of an apartment building, a sensitivity analysis is performed using the method of elementary effects (also known as the morris method), including an analysis of the interactions between the input parameters (second-order analysis). the usefulness of higher-order analysis is highlighted to support the results of the first-order analysis better. several aspects are tackled to implement the multi-order sensitivity analysis efficiently: interval size of the variables, the management of non-linearity and the usefulness of various outputs.
adding virtualization capabilities to the grid'5000 testbed. null
tio2 photocatalytic oxidation of indoor vocs at ppb levels in a multi-pass dynamic reactor: influence of vocs mixture on reaction intermediates concentrations. null
structure characterization and adsorption properties of pyrolyzed sewage sludge. sewage sludges produced from wastewater treatment plants continue to set environmental problems in terms of volume and way of reuse. thermal treatment of sewage sludge is considered as an attractive method in reducing sludge volume, and at the same time, it produces reusable byproducts. this paper deals with porous carbonaceous materials production from sewage sludge by pyrolysis (or carbonization) process with a goal of different industrial applications. carbonization experiments were carried out on two kinds of sludge, namely viscous liquid sludge and limed sludge by varying carbonization temperature between 400 degrees c to 1000 degrees c. the porous structure and surface chemistry of the materials obtained were characterized using nitrogen adsorption, scanning electron microscopy, elemental analysis, boehm titration, and ph of zero point of charge determination. the results show that basic character of the carbonized residues increases with increasing carbonization temperature. then, carbonization allows specific surface area and pore volumes to be developed. carbonized viscous liquid sludge and carbonized limed sludge are mainly mesoporous in nature, with specific surface areas reaching about 100 m(2) g(-1) and.
production of porous carbonaceous adsorbent from physical activation of sewage sludge: application to wastewater treatment. with an objective of production of carbonaceous sorbent for industrial effluent treatment, physical activation by steam of biological sludge collected from the municipal wastewater treatment plant of nantes (france) was studied and optimised using experimental design. thus, this activation process consists of a carbonisation under n-2 atmosphere at 600 degrees c for 1 h, followed by a thermal oxidation using steam (760 degrees c, 0.5 h, 2.5 l/min). the global mass yield of the process is equal to 38%. the thermal treatment allows a specific surface area of up to 225 m(2)/g to be reached, the porous structure being composed of both micropores and mesopores. the content of acidic surface groups is 0.71 meq/g whereas that of basic surface groups is 0.55 meq/g. the adsorption properties of the sorbent made from sludge are estimated with regard to various pollutants representative of industrial pollution of wastewaters and compared with those of commercial activated carbon. whereas the adsorption capacities of organic micropollutants are quite low because of proportionality to the microporosity, the important mesoporosity of the sorbent leads to interesting properties for macromolecules removal from aqueous solutions, such as dyes (q(m) = 175-200 mg/g). furthermore, the surface functional groups and ca2+ ions within the materials allow high copper ion adsorption capacities of 140 mg/g to be obtained. finally, a techno-economic approach shows that the sludge activation process seems to be economically competitive with regard to incineration.
production of fibrous activated carbons from natural cellulose (jute, coconut) fibers for water treatment applications. different fibrous activated carbons were prepared from natural precursors (jute and coconut fibers) by physical and chemical activation. physical activation consisted of the thermal treatment of raw fibers at 950 degrees c in an inert atmosphere followed by an activation step with co2 at the same temperature. in chemical activation, the raw fibers were impregnated in a solution of phosphoric acid and heated at 900 degrees c in an inert atmosphere. the characteristics of the fibrous activated carbons were determined in the following terms: elemental analysis, pore characteristics, sem observation of the porous surface, and surface chemistry. as the objective of this study was the reuse of waste for industrial wastewater treatment, the adsorption properties of the activated carbons were tested towards pollutants representative of industrial effluents: phenol, the dye acid red 27 and cu2+ ions. chemical activation by phosphoric acid seems the most suitable process to produce fibrous activated carbon from cellulose fiber. this method leads to an interesting porosity (s-bet up to 1500 m(2) g(-1)), which enables a high adsorption capacity for micropollutants like phenol (reaching 181 mg g(-1)). moreover, it produces numerous acidic surface groups, which are involved in the adsorption mechanisms of dyes and metal ions. (c) 2006 elsevier ltd. all rights reserved.
production and characterization of adsorbent materials from an industrial waste. preparation of activated carbon is carried out from an abundant and very cheap waste by-product from wastewater treatment plant: sewage sludge. the first step of preparation consists in a carbonization process under a 10 ml min(-1) nitrogen flow, at 600 degrees c during 1 hour. the second step is a physical activation, performed with carbon dioxide. the experimental conditions of the activation were optimized using experimental design methodology. three factors were studied: activation temperature (from 700 to 900 degrees c), activation duration (from 30 to 120 min) and co2 flow rate (from 0.7 to 2.9 l min(-1)). the porous carbonaceous materials were characterized in terms of physico-chemical and structural properties (specific surface area, pore volumes, surface ph, surface functional groups) and adsorption properties in aqueous and gaseous phase, these characteristics constituting the responses of the experimental design. a surface response methodology enabled to define optimum values for the 3 factors (at 900 degrees c during 30 min for a co2 flow rate of 2.9 l min(-1)) which involve an adsorbent with a specific surface area of 260 m(2) g(-1) and a pore size distribution comprising meso and micropores. adsorption capacities of organic pollutants (phenol, dyes, voc) are proportional to the specific surface area, apart from copper adsorption capacities (up to 80 mg g(-1)) due to an ion-exchange mechanism with ca2+ ions present in the raw material. in order to decrease the high ash content in the produced material (51 wt.%) and thus to improve the pore development, carbonized sludge were washed with an acid (hcl, 3 m) at room temperature before the activation step. this oxidation pre-treatment allowed to reach a 4 10 m(2) g(-1) specific surface area with an ash content of 26.4 wt.%.
preparation of adsorbents from sewage sludge by steam activation for industrial emission treatment. with an objective of production of carbonaceous sorbent for industrial effluent treatment, physical activation by steam of biological sludge collected from the municipal wastewater treatment plant of nantes (france) is studied. this work focuses on the optimization of the carbonization and activation conditions on physico-chemical and adsorptive properties of adsorbent materials produced. the carbonization is performed in a vertical pyrolysis furnace. its duration is fixed at 1 h, based on tga analysis. the carbonization temperature is optimized between 400 and 1000 degrees c to improve the bet specific surface area, the micropore volume and the quantity of surface functional groups of the char. these parameters seem to be improved by a temperature increase, the specific surface area varying from 20 to 96 m(2) g(-1). an intermediate temperature of 600 degrees c is selected, in order to obtain an interesting specific surface area (60 m(2) g(-1)) and satisfying mass yield. the activation conditions are optimized in terms of temperature (750-850 degrees c) and duration (30-90 min). a factorial design is carried out, based on the following responses: mass yield, porosity (bet surface area, mesopore and micropore volume), surface chemistry and adsorption properties for current pollutants (phenol, copper ions, dyes and cov). the activation step shows an improvement of pore and adsorption characteristics of the adsorbent, with specific surface area reaching 230 m(2) g(-1) and equilibrium adsorption capacities of phenol and copper equal to 50 mg g(-1), and 80 mg g(-1), respectively. the adsorption of microorganic compounds and dyes may be related respectively with micro- and meso-porous properties, whereas copper adsorption efficiency is due to an ion-exchange mechanism as demonstrated by exchange with ca2+ ions contained in the raw water sludge. experimental design responses are optimized using surface response methodology and are validated experimentally. finally, techno-economical analysis of the process shows a higher cost than farmland re-use but steam activation of sludge seems to be competitive towards incineration.
preparation and characterization of activated carbon from sewage sludge: carbonization step. sewage sludges produced from wastewater treatment plants continue to create environmental problems in terms of volume and method of valorization. thermal treatment of sewage sludge is considered as an attractive method in reducing sludge volume which at the same time produces reusable by-products. this paper deals with the first step of activated carbon production from sewage sludge, the carbonization step. experiments are carried out on viscous liquid sludge and limed sludge by varying carbonization temperature and heating rate. the results show that carbonized residue properties are interesting for activated carbon production.
photocatalytic oxidation of indoor vocs at ppb levels: kinetics of by-products formation. null
modelling and analysis of permeability of anisotropic compressed non-woven filters. an existing geometrical pore-scale model for flow through isotropic spongelike media is adapted to predict flow through anisotropic non-woven glass fibre filters. model predictions are compared to experimental results for the permeability obtained for a filter under different stages of compression to demonstrate the capability of the model to adjust to changes in porosity. the experimental data used are for a glass fibre paper with a uniform fibre diameter. the input parameters of the pore-scale model are the porosity, fibre diameter and some measure of the anisotropy between the in-plane and normal directions to the paper. correlation between the predictions and the experimental results is satisfactory and provides confidence in the modelling procedure. it is shown that the permeability is very sensitive to changes in the level of anisotropy, i.e. the level of compression of the nonwoven material.
micro-ingredients carry-over during bucket elevator handling in feed industry: influence of process parameters. currently, carry-over level of feed production lines can be accurately defined, but the causes are not identified yet because of a misunderstanding of this phenomenon. on-site studies charged conveying equipment between mixer and pelleting press, and especially bucket elevator, to be responsible for carry-over level increasing. a series of experiments has been carried out on a bucket elevator test bench in order to determine how process parameters influence carry-over phenomenon. product deposits on equipment walls represent, on average, 3.0% of the initial batch mass and contain about 0.2% of micro-ingredient after one tracer batch passing and 0.1% after flushing batch passing. experimental design results showed that parameters acting on micro-ingredient deposited are different from those influencing collected product. therefore, optimal position of tested process parameters will depend on the batch objective regarding its position in the production schedule. practical applicationsthis work allows a new approach of understanding carry-over phenomenon at pilot scale. indeed, process parameters acting on micro-ingredient deposits by a batch n have been differentiated from those acting on micro-ingredient gathering by a batch n+1. by this way, the results of this experimental study brought industrial solutions to control micro-ingredient transfer, considering the batch position in the whole sequencing.
influence on permeability of the structural parameters of heterogeneous porous media. predicting the macroscopic properties of porous media used in treatment processes is a complex task regarding 3-d structures at micro-, meso- and macro-levels. currently, information is scarce concerning the influence, at a microscopic level, of the 3-d structure of fibrous media on the physical laws governing their macroscopic behaviour. nevertheless, the relationship between macroscopic properties (pressure drop, treatment efficiency) and microstructure can be assessed thanks to suitable structure modelling theories. in this context, the present study proposes and compares different methods (mercury porosimetry and image analysis) for the structure characterization at a microscopic level of filtering fibrous media, such as nonwoven and woven fabrics. the results obtained show a porous structure gradient in the thickness of the nonwoven media studied in terms of porosity, pore size and tortuosity factor. moreover, the influence on structural parameters of media compression, when submitted to friction forces exerted by flow during filtration tests, is established. a model for the determination of multi-level pore size distributions from mercury porosimetry data is proposed. the "equivalent pore" model is used to estimate the tortuosity factor. the influence of measured structural parameters on fibrous media permeability is studied in a classical model for flow through fibrous media.
indoor air particulate filtration onto activated carbon fiber media. due to their bad effects on human health and comfort, removing particles and volatile organic compounds from indoor air has become an issue of major interest. in this study, the potential use of five media for particle removal was investigated: a felt, a cloth, and a knitted fabric made entirely of activated carbon fibers, and two prototype nonwovens made of different proportions of activated carbon and glass fibers. dynamic filtration measurements were performed in experimental conditions as representative as possible of indoor air, with alumina particles (modal diameter: 0.37 mu m), at an inlet concentration of 2,500 particles cm(-3) and for two different frontal velocities of air: 0.37 and 0.50 m s(-1). although this medium was not designed for filtration, felt exhibited a high initial filtration efficiency (74%) for a low pressure drop (less than 210 pa). similarly, associating several layers of woven/knitted media in series led to high performances, as it reduced preferential paths for the airflow. finally, prototype nonwovens appeared more efficient than activated carbon felt, but exhibited higher pressure drops.
image analysis: a useful tool for porous media characterization. null
gas phase photocatalytic oxidation of decane at ppb levels: removal kinetics, reaction intermediates and carbon mass balance. this study focuses on the photocatalytic oxidation of decane at ppb levels, close to indoor air conditions. although these concentrations are typical of indoor air volatile organic compound (voc) levels, such conditions have been poorly investigated to date. decane conversion rates higher than 90% were reached within 15 h for the highest initial concentration tested in the operating conditions used. despite this high decane conversion, 18 reaction intermediates were detected in the gas phase. the main compounds, in terms of concentration level in the gas phase, were formaldehyde, acetaldehyde and propanal. the amounts of these compounds in the gas phase were linearly dependent on the initial decane concentration. a reaction pathway is proposed, based on reaction intermediate temporal profiles and the literature. it consists of six main steps describing the oxidation process from decane to co and co2. the influence of the relative humidity level on the diversity and amounts of reaction intermediates was also studied. it was shown that moisture tends to shift the equilibrium of intermediates adsorption toward desorption, resulting in a relative increase in intermediate quantities in the gas phase. however, the monitoring of co and co2 formation highlighted that, at the end of the reaction, a high mineralization rate could be obtained. finally, an overview of the photocatalytic reaction is given through the carbon mass balance determined for various reaction advancements. this approach gives an overall evaluation of the photocatalytic process performance, from primary compound removal to reaction intermediate and mineralization. (c) 2013 elsevier b.v. all rights reserved.
fibrous media plugging modelling for liquid filtration. the aim of this work is to model the plugging curve of fibrous media for depth and surface filtration. the model links the evolution of pressure drop versus the injected mass of pollutants to structural properties of the media. furthermore, thanks to the model, it is possible to predict the fibrous composition of a medium, which conforms to given filtration properties. first, a review of the typical fibrous media for engine oil filtration formed in our laboratory is proposed, and, the measured filtration properties as well as the methods used for the characterization of structural properties of the media during plugging are presented. then, an explanation of the evolution of structural properties is proposed. in the second part, the results of the structural property characterization during plugging are used to develop a model of the pressure drop evolution as a function of the injected mass of pollutants. then, correlation laws linking the model parameters to the structural properties of the medium are given and the model is used to predict the plugging curve of two typical filtering media.
experimental design methodology for the preparation of carbonaceous sorbents from sewage sludge by chemical activation - application to air and water treatments. the objective of this study is to optimize experimental conditions of sorbent preparation from sewage sludge using experimental design methodology. series of carbonaceous sorbents have been prepared by chemical activation with sulfuric acid. the sorbents produced were characterized, and their properties (surface chemistry, porous and adsorptive properties) were analyzed as a function of the experimental conditions (impregnation ratio, activation temperature and time). carbonaceous sorbents developed from sludge allow copper ion, phenol and dyes (acid red 18 and basic violet 4) to be removed from aqueous solution as well as voc from gas phase. indeed, according to experimental conditions, copper adsorption capacity varies from 77 to 83 mg g(-1) , phenol adsorption capacity varies between 41 and 53 mg g 1 and voc adsorption capacities (acetone and toluene) range from 12 to 54 mg g(-1). each response has been described by a second-order model that was found to be appropriate to predict most of the responses in every experimental region. the most influential factors on each experimental design response have been identified. regions in which optimum values of each factor were achieved for preparation of activated carbons suitable for use in wastewater and gas treatments have been determined using response surfaces methodology. in order to have. a high mass yield and to minimize the energetic cost of the process, the following optimal conditions, 1.5 g of h2so4 g(-1) of sludge, 700 degreesc and 145 min are more appropriate for use of activated carbon from sludge in water and gas treatments. (c) 2004 elsevier ltd. all rights reserved.
collection efficiency of a woven filter made of multifiber yarn: experimental characterization during loading and clean filter modeling based on a two-tier single fiber approach. due to their good adsorption capacities, woven activated carbon filters are promising materials for the removal of volatile organic compounds (voc) and odors in air treatment applications. as these fibrous media can easily be implemented in various filtration configurations, their use in a combined filter device for voc and particulate matter (pm) removal is considered. for this purpose, experiments are performed to characterize their collection efficiencies versus particle size and mass loading for the removal of particulate matter smaller than 10 mu m. the specific fiber arrangement, due to multi-fiber yam weaving, prevents the use of initial collection efficiency models developed for non-woven media. this study aims to characterize collection efficiency during loading and to model initial collection efficiency in a woven structure made of multi-fiber yams. (c) 2005 elsevier ltd. all rights reserved.
characteristics of fixed beds packed with anisotropic particles - use of image analysis. fixed beds of anisotropic particles are commonly involved in chemical engineering processes: beds of cylindrical catalyst pellets or beds of wood shavings in paper industry. the characterization of the spatial variations of their structural properties is of great importance for flow modelling purposes. in this work, we intend to produce new data concerning a bed of long cylinder (h/ d = 5.29) and a bed of flat plates (e/a = 0.209). this study focuses on the characterization of top and bottom effects, as well as wall effects. the experimental procedure is based on the study of sections of packed beds consolidated with a resin. the captured images of successive transverse and longitudinal sections of the beds are treated and analysed, thanks to an image analysis software. the local variation of the voidage is determined in both the transverse and the longitudinal directions of the beds. furthermore, mean intercept lengths and mean numbers of particles are measured and discussed. concerning flat plates, an anisotropic ratio calculated from intercept measurements is compared to that previously obtained from pressure drop measurements. the main results of this work concern the enlightening of wall effects and end effects importance for the two studied beds compared to beds of spheres studied in the literature. (c) 2001 elsevier science b.v. all rights reserved.
application of sludge-based carbonaceous materials in a hybrid water treatment process based on adsorption and catalytic wet air oxidation. this paper describes a preliminary evaluation of the performance of carbonaceous materials prepared from sewage sludges (sbcms) in a hybrid water treatment process based on adsorption and catalytic wet air oxidation; phenol was used as the model pollutant. three different sewage sludges were treated by either carbonisation or steam activation, and the physico-chemical properties of the resultant carbonaceous materials (e.g. hardness, bet surface area, ash and elemental content, surface chemistry) were evaluated and compared with a commercial reference activated carbon (pica f22). the adsorption capacity for phenol of the sbcms was greater than suggested by their bet surface area, but less than f22; a steam activated, dewatered raw sludge (sa_draw) had the greatest adsorption capacity of the sbcms in the investigated range of concentrations (\textless0.05 mol l(-1)). in batch oxidation tests, the sbcms demonstrated catalytic behaviour arising from their substrate adsorptivity and metal content. recycling of sa_draw in successive oxidations led to significant structural attrition and a hardened sa_draw was evaluated, but found to be unsatisfactory during the oxidation step. in a combined adsorption oxidation sequence, both the pica carbon and a selected sbcm showed deterioration in phenol adsorption after oxidative regeneration, but a steady state performance was reached after 2 or 3 cycles. (c) 2010 elsevier ltd. all rights reserved.
adsorption phenomena in photocatalytic reactions: the case of toluene, acetone and heptane. nowadays, with the increase in the thermal insulation of buildings, indoor air quality (iaq) has deteriorated, particularly because of the presence of volatile organic compounds (voc). to improve iaq photocatalytic processes can be used. photocatalytic reactions can be broken down into three steps: adsorption of pollutants, chemical reaction, and desorption of water, carbon dioxide and by-products. in this work, the accessibility of the pollutants to the reactive sites of a commercial tio(2)-photocatalyst is studied. firstly, adsorption mechanisms are investigated through toluene adsorption isotherms in batch reactors. in humid air conditions (relative humidity of 50% at 24 degrees c), the classical adsorption models cannot be applied. consequently, a new model, called the "langmuir-multi", is built. it fits the obtained experimental data properly. adsorption equilibrium constants are calculated based on this new model. to understand adsorption mechanisms better, adsorption isotherms are also performed in dry air conditions with toluene and in humid air with acetone and heptane. it is observed that water vapor plays a major role. secondly, photocatalytic reactions are carried out with toluene, acetone and heptane in humid air. the kinetic curves are well-represented by the langmuir-hinshelwood (l-h) equation so that reaction and equilibrium constants can be assessed. the l-h equilibrium constant appears to depend on the type of pollutant and on the affinity between the pollutant and the photocatalyst surface. no correlation is found between the equilibrium constant and light intensity. it is also shown that the calculated l-h equilibrium constants are not equal to those previously obtained in dark conditions and in batch reactors. (c) 2011 elsevier b.v. all rights reserved.
adsorption of toluene onto activated carbon fibre cloths and felts: application to indoor air treatment. due to their bad effects on human health, removing volatile organic compounds from indoor air has become an issue of major interest. in this study, the potential use of six commercial activated carbon felts and cloths for indoor toluene removal was investigated. both batch and dynamic adsorption studies were performed, at toluene concentrations ranging from 21 to 18160 mg m(-3), for an air velocity representative of indoor air treatment (0.37 m s(-1)). batch measurements showed that felts exhibited higher adsorption capacities at equilibrium than cloths at high toluene concentrations, whereas this trend may be inverted at low concentrations. experimental isotherms and kinetics were satisfactorily fitted by the langmuir-freundlich model and the linear driving force model respectively. no main differences between the adsorption kinetics of felts and cloths were reported. dynamic adsorption capacities at saturation appeared to be higher than 120 mg g(-1) for both cloths and felts, irrespective of relative humidity levels and toluene concentrations. the influence of relative humidity on the adsorption capacity of felts was not significant for the higher toluene concentration studied in dynamics (307 mg m(-3)), whereas an increase in relative humidity induced a decrease in adsorption capacity at the lower toluene concentration (38 mg m(.3)). moreover, experimental curves of breakthrough time versus thickness of medium were satisfactorily fitted by the adams-bohart model, and the critical thickness determined by this model appeared to be below 1.3 mm, regardless of the medium or toluene concentration.
about the applicability of commonly used pressure-flow models to plane single-layer filters of activated carbon fabric. the pressure drop through plane single-layer filters of activated carbon fabric was determined experimentally with air and water as working fluids, for mean flow velocities covering the laminar and turbulent regimes in the upstream flow, and under various operating conditions. observational evidence is given that common approaches consisting of linearly relating the pressure drop coefficient and the reciprocal of the particle reynolds number with constant coefficients are inappropriate to fully describe the hydrodynamic behaviour of such systems. these coefficients were found to strongly depend on the nature of the circulating fluid and on the upstream flow regime as a result mainly of the property of fabrics to deform. significant discrepancies are to be expected between their value in actual service conditions and those estimated from standard measurement procedures with the fabric at rest and in the absence of any flow, as is usually done in practice, with direct practical consequences. data analysis using the pressure-flow models of goodings (1964, textile research journal, 34(8), 713-724.) and comiti and renaud (1989, chemical engineering science, 44(7), 1539-1545.) corroborated the tentative conclusions that were reached as regards the effects of changes in the structure of fabrics with fibre deflection onto the hydrodynamic behaviour of single-layer fabric filters. (c) 2000 elsevier science ltd. all rights reserved.
retention of iodide by the callovo-oxfordian formation: an experimental study. null
bayesian updating for road maintenance optimization. pavement structures are subject to several deterioration patterns classified in surface or structural failure modes. structural deteriorations are frequent and lead to heavy and costly maintenance. we restrict our study to the fatigue longitudinal cracks which arise in the underlying layers and growth up to the surface due to traffic repetitive tensile stresses. the characterization of the complete cracking process is very complex because of a large number of covariates and the strong randomness of the environment (climate and traffic loads). moreover, the current maintenance indicator is a cracking percentage of the road section surface. it gives only partial information onto the underlying racking. in such a context, a condition-based maintenance model on a single variable does not allow to guarantee an optimal maintenance decision. in [10], we have proposed a new model for the longitudinal cracking based on a bivariate stochastic process where the joint probability is a function of the current system state. it allows first to propose a new modeling of imperfect maintenance and then to differentiate maintenance according to their own cracking speed. nevertheless, one of the main limits is the difficulty of its implementation in operation. the objective of this work is to deepen the model in [10] for improving its applicability for road maintenance while keeping their theoretical properties and advantages. two directions are developed. first, a new definition of the bivariate deterioration process and the construction of the respective joint probability law based on classical results in bayesian theory are presented. then the derivation of the statistical framework for estimating the associated parameters will be proposed. the second direction is in the modeling of the uncertainty in the maintenance impact onto the cracking process.
improved lighthill fish swimming model for bio-inspired robots - modelling, computational aspects and experimental comparisons. the best known analytical model of swimming was originally developed by lighthill and is known as large amplitude elongated body theory (laebt). recently, this theory has been improved and adapted to robotics through a series of studies [boyer et al., 2008, 2010; candelier et al., 2011] ranging from hydrodynamic modelling to mobile multibody system dynamics. this article marks a further step towards the lighthill theory. the laebt is ap- plied to one of the best bio-inspired swimming robots yet built: the amphibot iii, a modular anguilliform swimming robot. to that end, we apply a newton-euler modelling approach and focus our attention on the model of hydrodynamic forces. this model is numerically in- tegrated in real time by using an extension of the newton-euler recursive forward dynamics algorithm for manipulators to a robot without a fixed base. simulations and experiments are compared on undulatory gaits and turning manoeuvres for a wide range of parameters. the discrepancies between modelling and reality do not exceed 16% for the swimming speed, while requiring only the one-time calibration of a few hydrodynamic parameters. since the model can be numerically integrated in real time, it has significantly superior accuracy com- pared with computational speed ratio, and is, to the best of our knowledge, one of the most accurate models that can be used in real-time. it should provide an interesting tool for the design and control of swimming robots. the approach is presented in a self contained manner, with the concern to help the reader not familiar with fluid dynamics to get insight both into the physics of swimming and the mathematical tools that can help its modelling.
a 4d-sequencing approach for air traffic management. the current air traffic system is forecasted to face strong challenges due to the continuous increase in air traffic demand. hence, there is a need for new types of organization permitting a more efficient air traffic management, with both a high capacity and a high level of safety, and possibly with a reduced environmental impact. in this article, we study a holistic approach, consisting in designing across europe a very organized air traffic system, as opposed to free flight, to reduce costs while maintaining safety. our work is based on the moving point paradigm, initially presented in prot et al, 2010 (using graph concepts to assess the feasibility of a sequenced air traffic flow with low conflict rate, european journal of operational research, 207 (1), pp 184-196). we give theoretical background to design conflict-free routes with high capacity and propose, based on these results, the allocation of aircraft to a new system of air routes, that include both lattices and orthodromic routes. the efficiency of the approach is assessed through simulations based on real data sets representing a full day of traffic over the whole european sky. the numerical results demonstrates a drastic reduction of the conflicts rate compared to the actual commercial routes, with a very limited fuel overconsumption.
model-based testing of global properties on large-scale distributed systems. large-scale distributed systems are becoming commonplace with the large popularity of peer-to-peer and cloud computing. the increasing importance of these systems contrasts with the lack of integrated solutions to build trustworthy software. a key concern of any large-scale distributed system is the validation of global properties, which cannot be evaluated on a single node. thus, it is necessary to gather data from distributed nodes and to aggregate these data into a global view. this turns out to be very challenging because of the system's dynamism that imposes very frequent changes in local values that affect global properties. this implies that the global view has to be frequently updated to ensure an accurate validation of global properties. in this paper, we present a model-based approach to define a dynamic oracle for checking global properties. our objective is to abstract relevant aspects of such systems into models. these models are updated at runtime, by monitoring the corresponding distributed system. we conduce real-scale experimental validation to evaluate the ability of our approach to check global properties. in this validation, we apply our approach to test two open-source implementations of distributed hash tables. the experiments are deployed on two clusters of 32 nodes. the experiments reveal an important defect on one implementation and show clear performance differences between the two implementations. the defect would not be detected without a global view of the system. testing global properties on distributed software consists of gathering data from different nodes and building a global view of the system, where properties are validated. this process requires a distributed test architecture and tools for representing and validating global properties. model-based techniques are an expressive mean for building oracles that validate global properties on distributed systems.
a language support for cloud elasticity management. elasticity is the intrinsic element that differentiates cloud computing from traditional computing paradigm, since it allows service providers to rapidly adjust their needs for resources to absorb the demand and hence guarantee a minimum level of quality of service (qos) that respects the service level agreements (slas) previously defined with their clients. however, due to non-negligible resource initiation time, network fluctuations or unpredictable workload, it becomes hard to guarantee qos levels and sla violations may occur. this paper proposes a language support for cloud elasticity management that relies on csla (cloud service level agreement). csla offers new features such as qos/functionality degradation and an advanced penalty model that allow providers to finely express contracts so that services self-adaptation capabilities are improved and sla violations minimized. the approach was evaluated with a real infrastructure and application testbed. experimental results show that the use of csla makes cloud services capable of absorbing more peaks and oscillations by trading-off the qos levels and costs due to penalties.
a cloud accountability policy representation framework. nowadays we are witnessing the democratization of cloud services. as a result, more and more end- users (individuals and businesses) are using these services for achieving their electronic transactions (shopping, administrative procedures, b2b transactions, etc.). in such scenarios, personal data is generally flowed between several entities and end-users need (i) to be aware of the management, processing, storage and retention of personal data, and (ii) to have necessary means to hold service providers accountable for the usage of their data. in fact, dealing with personal data raises several privacy and accountability issues that must be considered before to promote the use of cloud services. in this paper, we propose a framework for the representation of cloud accountability policies. such policies offer to end-users a clear view of the privacy and accountability obligations asserted by the entities they interact with, as well as means to represent their preferences. this framework comes with two novel accountability policy languages. an abstract one devoted for the representation of preferences/obligations in an human readable fashion. and a concrete one for the mapping to concrete enforceable policies. we motivate our solution with concrete use case scenarios.
experimental investigations on the use of preheated animal fat as fuel in a compression ignition engine. the effect of fuel inlet temperature on performance, emission and combustion characteristics of a diesel engine is evaluated. a single cylinder direct injection diesel engine developing a power output of 2.8 kw at 1500 rev/min is tested using preheated animal fat as fuel. experiments are conducted at the fuel inlet temperatures of 30, 40, 50, 60 and 70 °c. animal fat at low temperature results in higher ignition delay and combustion duration than diesel. preheated animal fat shows reduced ignition delay and combustion duration. peak pressure and rate of pressure rise are found as high with animal fat at high fuel inlet temperatures. heat release pattern shows reduced premixed combustion phase with animal fat as compared to neat diesel at normal temperature. preheating improves the premixed combustion rate. at low temperature, animal fat results in lower smoke emissions than diesel. the maximum smoke density is k=6.5 m−1 with diesel and k=3.6 m−1 with animal fat at 30 °c. preheated animal fat further reduces smoke levels at all temperatures. the smoke level is reduced up to k=1.7 m−1 with preheated animal fat at the temperature of 70 °c. hydrocarbon and carbon monoxide emissions are higher with animal fat at low temperature as compared to diesel. fuel preheating reduces these emissions. \no\ emission is found as low with animal fat at low temperature. fuel preheating results in increased \no\ emission. however, the level is still lower than diesel even at high temperature (i.e. 70 °c). on the whole it is concluded that preheated animal fat can be used in diesel engines with reduced smoke, hydrocarbon and carbon monoxide emissions with no major detoriation in engine performance.
towards improvement of natural gas-diesel dual fuel mode: an experimental investigation on performance and exhaust emissions. abstract the use of natural gas in compression ignition engines as supplement to liquid diesel in a dual fuel combustion mode is a promising technique. in this study, the effect of \df\ (dual fuel) operating mode on combustion characteristics, engine performances and pollutants emissions of an existing diesel engine using natural gas as primary fuel and neat diesel as pilot fuel, has been examined. at moderate and relatively high loads, the results show very interesting behavior of dual fuel operating mode in comparison to conventional diesel, both for engine performance and emissions. it showed a simultaneous reduction of soot and \nox\ species over a large engine operating area. moreover, it showed the possibility to obtain lower \bsfc\ (brake specific fuel consumption) than conventional diesel engine. however, this mode presents some deficits at low loads, especially concerning unburned hydrocarbons and carbon monoxide emissions. understanding those deficiencies is a key of such engines improvement. some suggestions for new measures towards \df\ mode improvement are deduced.
catalytic hydroliquefaction of charcoal \ccb\ (copper, chromium and boron)-treated wood for bio-oil production: influence of \ccb\ salts, residence time and catalysts. abstract thermochemical processes offer a feasible option for wood waste management and the recovery of a variety of useful chemicals. in this paper, hydroliquefaction with the use of catalysts was optimized to provide bio-oil from ccb-treated wood by reducing gaseous emissions of copper, chromium and boron (hazardous materials). in addition, the influence of \ccb\ salts, catalysts (al2o3, na2co3, mgo and caco3) and residence time on the hydroliquefaction process was investigated. for this, hydroliquefaction of charcoal obtained by slow pyrolysis of ccb-treated wood was conducted under hydrogen pressure in presence of tetralin. the results showed that \ccb\ salts and catalysts increase the yield of bio-oil compared to hydroliquefaction of charcoal from untreated wood. it was also observed, that the use of catalysts improves the residence time during the process. among the catalysts employed, al2o3 appears to be the most effective. furthermore na2co3 promotes the formation of gaseous species particularly ch4. analyses of hazardous materials in charcoal residue (coke) illustrate their transfer to the bio-oil with the increase of bio-oil yield and residence time except when al2o3 was using. the bio-oil obtained contains aromatic compounds.
speciation of technetium in acidic media : effect of α radiations. this project is part of the fundamental study of technetium speciation in highly acidic medium. the behaviour of technetium in htfms was carried out in the absence then in the presence of α irradiation. given these two different conditions, spectrophotometric results of tc(vii) reduction are similar. xas analysis indicates the formation of a cyclic dimer of tc(iv) complexed to triflate ligands and formulated astc₂o₂(cf₃so₃)₄(h₂o)₄. this compound is linearized to tciv-o-tciv with the increase of htfms concentration. at high concentration of htfms +98% (11.15 m), the protonated species tco₃(oh)(h₂o)₂ which is formed in the absence of external ionizing radiations, is reduced to the v oxidation state under α irradiation. structural characterization by exafs spectroscopy and dft calculations suggests the formation of monomer species of tc(v)-triflate complexes where [otc(f₃cso₃)₂(h₂o)₂]⁺ and [otc(f₃cso₃)₂(oh)₂]⁻ compounds were proposed. in concentrated h₂so₄ (ch₂so₄ ≥ 12 m), α-radiolysis experiments of tc(vii) were performed in order to compare the radiolytic behaviour of tc(vii) in both comparable media htfms and h₂so₄. xanes studies show that radiolytic reduction of tc(vii) leads to the formation of tc(v)-tc(vii) mixture in h₂so₄ 13 m and just tc(v) in 18 m of h₂so₄. the analysis of exafs spectra is consistent with the formation of [tco(hso₄)₃(h₂o)₂] and [tco(hso₄)₃(h2o)(oh)]⁻ monomer complexes in h₂so₄ 13 m and [tc(hso₄)₃(so₄)(h₂o)] and [tc(hso₄)₃(so₄)(oh)]⁻ species at 18 m of h₂so₄.
dmaas : syntactic, structural and semantic mediation for service composition. service composition is a major advance service-oriented computing brings to enable the development of distributed applications. however, the distributed nature of services hampers their composition with data heterogeneity problems. in this paper, we address these problems with a decentralized mediation-as-a-service architecture that solves data inconsistencies occurring during the composition of business services. as an extension to our previous work that focused on data interpretation problems, we present in this paper a solution to solve data inconsistencies at the syntactic, structural and semantic levels. we show how syntactic, structural and semantic mediation techniques can be combined, and how semantic mediation provides useful information that helps structural and syntactic mediation. we demonstrate how our architecture enables decentralized publication and discovery of mediation services. we motivate our work with a concrete scenario and validate our proposal with experiments.
combustion characterization of natural gas in a lean burn spark-ignition engine. lean burn natural gas fuelled spark-ignition engines are particularly attractive regarding environmental performance. nevertheless, few data exist concerning lean combustion in gas engines in terms of ignition delay, combustion duration and combustion rate. such data are necessary when using thermodynamic models to predict energy and environmental performance. the present study proposes a contribution to such combustion characterization as a function of spark timing, air excess and engine load. this work is based on experimental cylinder pressure measurements in an open chamber engine on the one hand, and on a one-zone thermodynamic model used as a heat release analysis tool on the other hand. results obtained, such as mass fraction burned, ignition delay and combustion duration, allow identification of the parameters used in the wiebe function, a semi-empirical law that is frequently used to describe the combustion in predictive models.
a new methodological approach of sizing and operation optimization for cogeneration by internal combustion engines. the goal of this article is to present a robust and relatively simple to apply mathematical model to determine the optimal size and operation of the main equipment of cogeneration plant with internal combustion engines. the concerned installations are the gas engines and the peak boilers. the presented model optimizes the power of the equipment to be installed, as well as their instantaneous loads. contrary to previous studies on rated parameters, this analysis takes into account continuous load variation in the range of 50% to 100% for the engines and 0% to 100% for the boilers.
thermoeconomic analysis method for cogeneration plants. in this paper, the authors present a unified comparison method for the calculations of thermodynamic efficiencies applied to combined heat and power. three new indicators have been introduced: two energetic structure indexes defined to size up the chp equipment, and a heat recovering ratio that allows to adapt the developed method to open cycles. a comparison between separated solution and prime movers has been studied by using this analysis tool. these indicators have been used to estimate the influence of technical and economical features.
organic acids emissions from natural-gas-fed engines. a natural-gas-fed spark-ignition engine, operating under lean conditions, is used for the study of the organic acids exhaust emissions. these pollutants are collected by passing a sample of exhaust gas into deionised water. the final solution is directly analysed by hplc/uv at 204 nm. only formic acid is emitted in detectable concentration under the experimental conditions used. its concentration decreases with the three engine operating parameters studied. spark advance, volumetric efficiency and fuel/air equivalence ratio. exhaust formic acid concentration is also linked with exhaust oxygen concentration and exhaust temperature. a comparison with other engines (si engines fed with gasoline and compression ignition engines) from bibliographic data proves that natural-gas-fed engines emit less organic acids than the other two types of engines. (c) 2000 elsevier science ltd. all rights reserved.
optimal use of the generated biogas from manure. this paper presents the results of an optimisation algorithm for biogas use. there is considered the case of the big farms. generated biogas from manure is an important energy resource. the goal of the research is to estimate how biogas can cover the energy demand of the farms. special software has been developed for managing the energy, environmental and economy balances. the best approach is to use biogas for producing heat and power. one can assume different heating schedules and electricity needs. energy consumption for hog, poultry, beef and dairy farms can be simulated. the potential of the generated biogas is estimated using specific indicators. the software is able to find the optimal size of the biogas engine and boiler for different cases. the energy production is compared with the needs of the farm. as a result, there are presented financial analysis of valorisation projects for different types of animal farm.
experimental database for a cogeneration gas engine efficiency prediction. a total of 115 operating test points have been investigated on a cogeneration industrial 200 kw, gas engine at ecole des mines de nantes, with load varying between 50 and 100 per cent, spark advance between 8 degrees and 20 degrees crank angle and air excess in the range 1.3-1.7. the analysis provides st mathematical correlation giving the influence of load variation and other parameters on the thermal power, the natural gas inlet power, the electrical efficiency and the global efficiency with an accuracy better than 2 per cent.
thermodynamic analysis of reciprocating compressors. a global model for the thermodynamic analysis of reciprocating compressors is presented. the model is based on five main and four secondary dimensionless physically meaningful parameters. expressions for the volumetric effectiveness, the work per unit mass and the indicated efficiency are derived. the model has been used in order to predict the performance of a reciprocating air compressor under various operating conditions. the model proves to be a very accurate and useful tool to analyse the compressor performance. the relative importance of the various losses and the influence of different parameters on the reciprocating compressor behaviour are discussed. especially the in-cylinder residual mass fraction and the walt to fluid heat transfer influences on the reciprocating compressor performance are highlighted. (c) 2001 editions scientifiques et medicales elsevier sas.
comparison of aerobic standard medium with specific fungal medium for detecting fusarium spp. in blood cultures. in order to compare the performances of a specific fungal medium and a standard aerobic medium for detecting growth of fusarium spp. in blood, simulated blood cultures were performed. for lower inocula (10(2) and 10(3) cfu/ml taken together), fungal growth was detected significantly earlier using the fungal medium. the mean difference in the time to detection between the two media was 22.33 h at 10(2) cfu/ml, with the maximum difference being achieved for fusarium verticilloides at 37.05 h. these in vitro test results suggest fungal medium could be useful for obtaining more rapid blood culture results when evaluating patients at risk for invasive infection with fusarium spp.
a diesel engine global simulation tool - benefits in coolant loop optimization and additional heating systems use. this article focuses on a global simulation software of a car cooling system. the sub-models involved are firstly detailed. a computation of air temperature blown into the car cabin is satisfactory compared to experimental results carried out in wind tunnel. in a second part, an example of an optimized defrosting strategy is described. finally, the impact of several additional heating systems, burners and electrical systems, is studied in term of comfort and fuel consumption. the evaluation of the different technologies relies on a dimensionless indicator which links car cabin comfort and fuel consumption.
assessment of co2 reduction in tri-generation - case studies. tri-generation is a simultaneous production of three types of energy carriers, usually power, heat and cold. this technology can assure a greater efficiency of fuel use, thus a lower level of co2 emissions compare to separate production. the paper presents an analysis of several existing tri-generation plants with regard to co2 emissions. the analysis focuses on determining the level of co2 emissions for each plant and comparing it with the conventional separate production. the method used for calculating co2 emissions proved that tri-generation provide a lower level of co2 emissions compare to separate production.
opportunity of the on-farm biogas plants - subsidies policy for co2 reduction. the aim of the paper is to investigate the opportunity of the cogeneration projects using generated biogas from manure. the authors have proposed a method for sizing different equipments for the biogas valorization chain. the co2 reduction due the biogas use has been calculated considering the fuel savings in cogeneration plant. a part of the article presents different methods for making a biogas plant become feasible. the results are presented as variations of the payback period of the investment in function of the farm's size. sensitivity analyses are made for determining the most important factor influencing the rentability of the biogas plant.
a new indicator for knock detection in gas si engines. determination of knock onset for any engine tuning remains a difficult work for many engine manufacturers. this study investigates different combinations of existing knock indices in order to produce an upgraded indicator, which is easier to calibrate. experiments are conducted on a single-cylinder gas engine bounded to combined heat and power (chp). effects of spark advance, volumetric efficiency and equivalent ratio are studied under constant speed operation. the ratio impo/(mapo x w) (with impo defined as the integral of modulus of pressure oscillations, mapo as the maximum amplitude of pressure oscillations and w as the width of the computational window) is proposed as suitable indice. in any engine setting, it remains constant under no knocking conditions. when knock occurs, a model deduced from dimensionless analysis allows determination of the oversteps of knock limited spark advance from a single impo/(mapo x w) measurement with an accuracy better than 1 ca. knock is then studied for different gas qualities by adding propane or carbon dioxide to the fuel. the results show that there is no significant effect of the fuel composition on the proposed indicator, making the model able to calculate klsa overstep in all the situations. (c) 2002 editions scientifiques et medicales elsevier sas. all rights reserved.
knock prevention of chp engines by addition of n-2 and co2 to the natural gas fuel. this work focuses on the prevention of knock in the case of spark ignition (si) engines supplied by natural gas network. the effects of the addition of two inert gases (n-2 and co2) are experimentally studied. the added volumetric quantities are between 0% and 25% for n-2 and between 0% and 15% for co2. the thermal efficiency and the emissions of the engine are very slightly affected by the addition, whereas a significant increase of the knock limited spark timing (klst) is always measured. a twice-higher augmentation of klst is noted when co2 is added compared to n2 for an equivalent volumetric concentration. the overall augmentation varies between +1 and +6 degreesca depending on engine operation. finally, a law for predicting the klst augmentation implied by the addition of inert gases is deduced from all the measurements. (c) 2003 elsevier science ltd. all rights reserved.
the use of a phase change material within a cylinder wall in order to detect knock in a gas si engine. the present paper studies the possibility to develop a new method of knock detection in a gas si engine. this method is based on the increase in the wall heat flux when knock occurs. it also must be simple enough to be used by industry. in order to achieve this goal, a metallic phase change material is put within the wall cylinder. the melting of the pcm means that knock has occurred and is persistent. the melting of such a phase change material would be easy to detect using industrial measurement tools. in this paper, numerical simulations of unsteady heat transfer across the cylinder wall are presented. unsteady heat transfer from the hot gas to the wall chamber is simulated by a self-developed program. this program allows fixing instantaneous local heat flux values deduced from the literature in case of both normal and knocking combustion. heat transfer across the cylinder wall is solved by the finite volume technique. grid is validated by comparison with analytical results. melting is treated by the voller and prakash model and sodium is chosen as pcm. among all the results, we can notice that an increase in the knock intensity changes the shape of the isothermal curves around and inside the pcm. this leads to an increase in the melting velocity with a higher rate than the increase in the heat flux.
investigation of the oxidation zone in a biomass two-stage downdraft gasifier. the partial combustion in a two-stage downdraft gasifier is critical for the process optimisation. flame temperature and residence time of gas in the hot zone are indeed determining for tar cracking, the major issue in gasification. this paper presents a cfd model of the oxidation zone. chemical reactions, including tar cracking, are treated by the ''edc'' model (homogeneous reactions). fluid flows are turbulent and simulated by the k-epsilon rng turbulence model. the validation of the model is done with the dtu 100 kw two-stage gasifier. results fit satisfactory the data, regarding the temperature profile.
effect of alcohols addition on the performance and emission characteristics of an animal fat emulsion fuelled diesel engine. in this work, the effect of alcohol addition on the performance and emissions of a diesel engine fuelled with animal fat emulsions is studied. a single cylinder air-cooled, direct injection diesel engine is used. tests are conducted using animal fat emulsions with water and alcohols. methanol and ethanol are used to improve the physical properties of emulsions. results show significant improvement in performance and emissions with animal fat emulsions as compared to neat animal fat. smoke levels are drastically reduced with both the emulsions. ethanol animal fat emulsion shows more reduction in smoke levels than methanol emulsion. no significant difference in no emission is noted between the two emulsions. however, the values are very low as compared to neat fat and neat diesel. hydrocarbon and carbon monoxide emissions are found as higher with ethanol emulsions as compared to methanol. however, these emissions are still lower than neat fat and neat diesel. on the whole it is concluded that animal fat emulsions with methanol and ethanol can be used as fuel in a compression ignition engine with improved performance as compared to neat animal fat.
numerical study of micro-explosion delay of water-in-fuel emulsion droplets. spray combustion of water-emulsified fuel induces an effective decrease in total combustion time, carbonaceous residues and nox compared to pure fuel, thanks to the micro-explosion phenomenon. designing emulsion-burning appliances requires to calculate the micro-explosion delay included in total combustion time. this rather technical contribution addresses the problem of how to model the process of microexplosions in emulsions. the authors describe a numerical method to account for thermal conduction effects and analyze its effect upon the microexplosion delay. some of the expressed physical quantities may be useful for other researchers in this field. the evolution of the emulsion droplet (radius and temperature through the time) is faced with experimental micro-explosion delay, for three cases of emulsion droplet (different initial diameters and temperatures). finally, observations resulting from this comparison are discussed.
unsteady heat transfer enhancement around an engine cylinder in order to detect knock. this paper deals with the transient thermal signal around an engine cylinder in order to propose a new and nonintrusive method of knock detection. numerical simulations of unsteady heat transfer through the cylinder and inside the coolant flow are carried out to account for heat flux variations due to normal and knocking combustion. the effect of rib roughened surfaces on thermal signal amplification is investigated. the geometric parameters are fixed at pi/h=70 and w/h=1 with a reynolds number based on hydraulic diameter of 12,000. the results reveal that square ribs give better performance in term of thermal signal amplification within the fluid. an amplification of the temperature variation up to 20 times higher is found. finally, flow analysis shows that amplification depends on the position where the thermal signal is collected.
use of animal fats as ci engine fuel by making stable emulsions with water and methanol. this paper presents a detailed analysis on different properties of two kind of animal fats and their suitability of using them as fuel in diesel engines. initially, the physical and chemical properties (viscosity, low heating value (lhv) and chemical compositions) of animal fats are obtained experimentally. in the second phase of work, the animal fats are modified to form emulsions with water in different quantities. in the last phase, methanol is added to improve the physical properties of emulsions further. on the whole it is concluded that the animal fats can be transformed into a stable biofuel emulsions with improved physical and chemical properties and used as good alternative fuel in diesel engines. an optimum formulation is obtained by mixing 15% of water, 15% of methanol and 2% of surfactant. (c) 2005 elsevier ltd. all rights reserved.
effect of water and methanol fractions on the performance of a ci engine using animal fat emulsions as fuel. the influence of water and methanol (w/m) fractions on the performance of a compression ignition engine fuelled with animal fat (obtained from duck) emulsions is studied. a single cylinder air-cooled, direct injection diesel engine developing a rated brake power output of 2.8 kw at 1500 r/min is tested using diesel neat animal fat, and animal fat emulsions with different fractions of w/m. results show reduced peak pressure with neat animal fat when compared with diesel. animal fat emulsions result in increased peak pressure with increased w/m fractions. ignition delay is quite high with neat animal fat. increase in w/m amounts in the emulsions further increase the ignition delay. heat release pattern shows improved combustion rates with animal fat emulsions at all w/m fractions when compared with neat animal fat. drastic reduction in smoke emission from 3.6 m(-1) with neat animal fat to a minimum of 0.5 m(-1) is achieved with the animal fat emulsion at the maximum w/m fraction. no emission is found to be lower with neat animal fat than with neat diesel. animal fat emulsions further reduce no emission at all w/m fractions. hydrocarbon and carbon monoxide emissions are also reduced significantly with animal fat emulsions at all w/m ratios when compared with neat animal fat. in general, animal fat emulsions with different w/m fractions show considerable reduction in all emissions and improvement in engine performance when compared with neat animal fat. emulsion of 10w10m2s83 (i.e. 10 per cent of water, 10 per cent of methanol, and 2 per cent of span 83 by volume) is found to be the best among the three tested emulsions for optimum performance and emissions.
dimensional modeling of biomass pyrolysis based on a nodal approach. the purpose of the present research is to develop a numerical model for the biomass gasification processes suitable for the dimensioning of industrial installation. the end use of the producer gas being often internal combustion engines systems, the model concentrates on the quantitative and qualitative aspects of the process, with a particular interest in the problem of tar formation and destruction. this study focuses on the first two steps of the process: drying and pyrolysis. the pyrolysis model presented in this paper consists in a coupling between a heat transfer model based on a nodal approach and a chemical model for the thermal decomposition of the pyrolysed material. drying is considered as a sub process included in pyrolysis. the shown results are in good agreement with experimental data.
detection of knock occurrence in a gas si engine from a heat transfer analysis. this paper focuses on the transient thermal signal around an engine cylinder in order to propose a new and non-intrusive method of knock detection. numerical simulations of unsteady heat transfer through the cylinder and inside the coolant flow are performed. the amplification of wall heat transfer due to knock is taken into account by a self-developed program. it enables one to fix the instantaneous heat flux on the internal side of the cylinder. the transient component of the thermal signal is then calculated and analysed. the effect of a machining, consisting of a transverse groove, which reduces the cylinder liner thermal resistance, is investigated. this groove disturbs the turbulent coolant flow, and its shape mainly influences the heat transfer from the wall to the fluid. the numerical results show that the use of a square groove would multiply by three the amplitude of the transient thermal signal recorded within the water compared to a smooth wall case. moreover, the amplitude can be enhanced by using a nanofluid as coolant because of its higher thermal diffusivity. (c) 2005 elsevier ltd. all rights reserved.
a comparative study of different methods of using animal fat as a fuel in a compression ignition engine. this work explores a comparative study of different methods of using animal fat as afuel in a compression ignition engine. a single-cylinder air-cooled, direct-injection diesel engine is used to test the fuels at 100% and 60% of the maximum engine power output conditions. initially, animal fat is tested as fuel at normal temperature. then, it is preheated to 70 degrees c and used as fuel. finally, animal fat is converted into methanol and ethanol emulsions using water and tested as fuel. a drop in cylinder peak pressure, longer ignition delay, and a lower premixed combustion rate are observed with neat animal fat as compared to neat diesel. with fat preheating and emulsions, there is an improvement in cylinder peak pressure and maximum rate of pressure rise. ignition delay becomes longer with both the emulsions as compared to neat fats. however preheating shows shorter ignition delay. improvement in heat release rates is achieved with all the methods as compared to neat fats. at normal temperature, neat animal fat results in higher specific energy consumption and exhaust gas temperature as compared to neat diesel at both power outputs. preheating and emulsions of animal fat show improvement in performance as compared to neat fat. smoke is lower with neat fat as compared to neat diesel. it reduces further with all the methods. at peak power output, the smoke level is found as 0.89 m(-1) with methanol, 0.28 m(-1) with ethanol emulsions, and 1.7 m(-1) with fat preheating, whereas it is 3.7 m(-1) with neat fat and 6.3 m(-1) with neat diesel. methanol and ethanol emulsions significantly reduce no emissions due to the vaporization of water and alcohols. however no increases with fat preheating due to high in-cylinder temperature. higher unburned hydrocarbon and carbon monoxide emissions are found with neat fat as compared to neat diesel at both power outputs. however these emissions are considerably reduced with all the methods. it is finally concluded that adopting emulsification with the animal fat can lead to a reduction in emissions and an improvement in combustion characteristics of a diesel engine.
ethanol animal fat emulsions as a diesel engine fuel - part 1: formulations and influential parameters. this paper focuses on effective solution to improve the combustion of low quality animal fat by making stable emulsions with water. animal fat emulsions are prepared by mixing the fat with water, surfactant and co-surfactant. ethanol is chosen as the co-surfactant because of its dilution ability. span 83 also called sorbitan sesquiolate is used as the surfactant because it well stabilizes and forms stable animal fat emulsions. emulsions and micro-emulsions are prepared for different co-surfactant/surfactant (c/s) ratios. a number of formulations are made and the sauter mean diameter of water droplets are estimated using electron microscope images. results are presented in pseudo ternary diagrams. influence of different parameters affecting the emulsion characteristics are studied experimentally. according to the stability, structure, viscosity, fat content and economical aspects, the optimum emulsion is found as the emulsion with 36.4% of ethanol, 3.6% of span.83, 10% of water and 50% of animal fat by volume. (c) 2006 elsevier ltd. all rights reserved.
numerical investigation of the partial oxidation in a two-stage downdraft gasifier. null
dimensional modelling of wood pyrolysis using a nodal approach. new gasification installations and techniques are being tested today but they all struggle with mainly the same drawbacks such as removal of various pollutants in the producer gas or clogging of material pathways. this work is oriented on developing a new model for the non-oxidative pyrolysis step of a gasification process as a part of a wider research conducted on the overall gasification of wood waste. a batch reactor is modelled by means of nodal modelling, a technique widely used for simple heat transfer processes. additionally to the heat transport inside the batch reactor the model uses a simple and versatile generic chemistry and simplified mass transfer principles. thermal data from modelling is compared with data obtained from an experimental batch pyrolysis reactor using wood sawdust and cutter shavings. experimental and theoretical results regarding thermal phenomena are in good agreement. (c) 2008 elsevier ltd. all rights reserved.
pollution duality in turbocharged heavy duty diesel engine. diesel engine designers are faced with increasingly stringent social demands to reduce emissions while maintaining high performance. several strategies are considered, such as the advanced fuel system, the cooled exhaust gas recirculation (egr), the particulate filter, the no(x) after-treatment, the oxidation catalyst, the advanced control techniques and the alternative combustion. these strategies have been tuned to achieve the lowest engine exhaust gas emissions. the major problem of diesel engine pollution is the no(x) and soot formation. their antagonistic evolution according to the air/fuel ratio is well-known, and requires a good compromise. in this article, a numerical investigation was carried out using the kiva-3v code. the aim deals with the influence of some engine parameters on the performances and the pollutant (no(x)-soot) formation of a turbocharged heavy duty direct injection diesel engine. the numerical simulations were achieved to capture independently the effects of engine operating parameters such as the fuel injection timing, the fuel injection duration, the piston bowl diameter and the egr rate. the obtained results are discussed and some conclusions are developed.
thermo chemical equilibrium modelling of a biomass gasifying process using aspen plus. this article deals with the use of aspen plus to model the thermo chemical processes occurring in wood biomass gasifiers. an original equilibrium gasification model using aspen plus was first built and validate based on existing data of a downdraft gasifier (ddg). the thermo chemical models assume that reactants reach chemical equilibrium. the simulations enabled to predict the composition of the flaming pyrolysis gas as well as the final composition of the producer gas. a parametric study based on air/fuel ratio variation was then conducted. the model was finally adapted to the operating conditions of a staged gasifier developed at the technical university of denmark (dtu). parametric studies were conducted to observe the effect of humidity with a low air injection. aspen plus was shown to be well adapted to model the gasification process (equilibrium), and it was possible to simulate both a ddg and a dtu using the same model. data about the composition of flaming pyrolysis and the exit gas were obtained.
prediction of micro-explosion delay of emulsified fuel droplets. burning a water-in-oil emulsion enables reduction in solid and gaseous pollutants in comparison with neat oil. in the emulsion, heavy fuel-oil and water lie in distinct phases, having a high difference in boiling point (up to 200 k). in an emulsion droplet injected and subsequently heated inside a flame, the internal water droplets are enclosed inside the emulsion and do not systematically vaporise at boiling point. they are known to reach a metastable state, breaking up at a temperature below the spinodal limit of water. from this moment, the surrounding fuel-oil is fragmented into numerous faster and smaller droplets by the suddenly expanding steam. this physical phenomenon is called ''micro-explosion''. this work demonstrates a numerical modelisation of unsteady heat and mass transfer at the surface and inside of the emulsion droplet, and provides a prediction of its micro-explosion delay, using homogeneous nucleation hypothesis. this assumption of homogeneous nucleation for internal water droplets matches the use of a ''drop tower'' experimental facility. finally, comparisons between predicted ranges for micro-explosion delays and experimental delays from literature are discussed, along with combustion parameters (ambient temperature, relative velocity) and combustible emulsion parameters. as a result, the experimental and numerical micro-explosion delays decrease with liquid or ambient temperature and relative velocity, and increase with water content and radius of emulsion droplet. their low average deviation reveals the accuracy of the assumption of homogeneous nucleation in the considered situations. (c) 2008 elsevier masson sas. all rights reserved.
the use of biofuel emulsions as fuel for diesel engines: a review. animal fats/vegetable oils (called biofuels) and their emulsions are quite promising alternative fuels for diesel engines. this article reports a comprehensive study on the use of animal fats/vegetable oils and their emulsions as fuel in compression ignition engines. emulsions preparation method and their effects on engine performance, emission, and combustion characteristics have been studied in detail. information indicates that biofuel emulsions in diesel engines enhanced the combustion efficiency with improved performance as compared to neat fuels. the maximum percentage of water addition to biofuel was found as 30 per cent by volume for maximum efficiency. they reduced no,, smoke, and particulate emissions considerably. emulsions resulted in higher ignition delay as a result of vaporization of water as compared to neat fuels. peak pressure, rate of pressure rise, and premixed combustion rate in the heat release curve were found to be higher when compared to neat oils because of longer ignition delay. further improvements could be achieved by adding oxygenated fuels like methanol, dimethyl carbonate, and cetane number improvers like diethyl ether with biofuels in small quantities. it has also been suggested that dual fuel operation can significantly reduce particulate and no., emissions with biofuels. exhaust gas recirculation can reduce ignition delay considerably with reduced no., emissions. finally, modelling techniques were presented because they can help in in-depth analysis of the combustion process of biofuel emulsions in diesel engines.
a numerical comparison of spray combustion between raw and water-in-oil emulsified fuel. heavy fuel-oils, used engine oils and animal fat can be used as dense, viscous combustibles within industrial boilers. burning these combustibles in the form of an emulsion with water enables to decrease the flame length and the formation of carbonaceous residue, in comparison with raw combustibles. these effects are due to the secondary atomization among the spray, which is a consequence of the micro-explosion phenomenon. this phenomenon acts in a single emulsion droplet by the fast (&lt; 0.1 ms) vaporization of the inside water droplets, leading to complete disintegration of the whole emulsion droplet. first, the present work demonstrates a model of spray combustion of raw fuel. secondly, the spray combustion of water-in-oil emulsified fuel is exposed to the same burning conditions, taking into account the micro-explosion phenomenon. finally, the comparison between the results with and without second atomization shows some similar qualitative tendencies with experimental measurements from the literature.
formulation and combustion of emulsified fuel: the changes in emission of carbonaceous residue. burning dense, viscous combustibles such as heavy fuel-oil as a water-in-oil emulsified combustible enables to decrease the emission of solid carbonaceous residue, in comparison with raw, non-emulsified combustible. this is due to the phenomenon of micro-explosion, meaning the rapid (&lt;0.1 ms) vaporization of the water droplets inside the emulsion, breaking up the initial emulsion droplet into numerous and faster 'daughter-droplets'. the present work is based on a small-scale furnace (300 kw max.) feed with heavy fuel-oil mixed with 10-20% of gasoil, with and without emulsion of water. the emulsification of combustible enables to record a reproducible lowering in emission of carbonaceous residue from the combustion of emulsified fuel, in comparison with raw fuel. this is added to a variation in granulometry of carbonaceous residue, hereby considered as an indicator of second atomization. copyright (c) 2009 john wiley &amp; sons, ltd.
liquid fuel recovery through pyrolysis of polyethylene waste. despite significant advances in recent years, 61% of the plastic waste generated in western europe is still disposed of to landfill. the polyethylene, high and low density (hdpe and ldpe), is the main part of waste plastics. it is proposed a more effective way to valorise these wastes, which appear to be the pyrolysis. this solution has two main advantages: the quantity of waste is reduced up to 99%, depending on plastic composition, while liquid and gaseous fractions with lhv almost like diesel fuel and natural gas will be produced during the process.
biodiesel elaboration from municipal fat wastes. the purposes of this study are to elaborate a diesel engine bio-fuel using animal fat wastes (waf) as raw material, and to identify the effect of principal elements on the process design. in order to accomplish our goals, we studied the transesterification of these waf with methanol and using sulfuric acid (h(2)so(4)) as catalyst. also the physical characteristics of produced bio-fuel were studied and compared with the european standard en 14214. the waf used in this study was collected from the area of bordeaux in france by ctma, a waste treatment station. the effects of: catalyst amount, time of reaction and alcohol to oil molar ratio on the reaction conversion have been established. the waf have a very high acid number (60 mg (koh)/g (fat)), which is equivalent to a content of 30% (wt.) in free fatty acids, and a cinematic viscosity of 18.75 mm(2)/s at 40 degrees c. after processing, we obtained a liquid with a low acidity (0.4 mg (koh)/g (fuel)) and a viscosity of 4 mm2/s at 40 degrees c.
a comprehensive study on performance, emission, and combustion characteristics of a dual-fuel engine fuelled with orange oil and jatropha oil. performance of a single-cylinder, water-cooled, direct-injection diesel engine on dual-fuel operation with jatropha oil (jo) as pilot fuel and orange oil as primary fuel was evaluated. constant load test at different power outputs was conducted at the rated speed of 1500 r/min with varying orange oil quantities. the loads were fixed as 20 per cent, 40 per cent, 60 per cent, 80 per cent, and 100 per cent. in dual-fuel operation with orange oil induction, the thermal efficiency of jo was increased mainly at high power outputs. maximum thermal efficiency with jo was found as 29 per cent at 31 per cent of orange oil induction at 100 per cent load. smoke was reduced significantly with all orange oil induction rates at all power outputs in dual-fuel operation with jo. it was reduced from 4.4 to 3.3 bsu (bosch smoke units) with jo at the maximum efficiency point at 100 per cent load. hc emissions were increased further at all power outputs in the dual-fuel mode with all rates of orange oil induction. dual-fuel operation increased the ignition delay of jo. however, peak pressure and energy release rates were improved in the dual-fuel operation with orange oil induction. in general, dual-fuel operation with orange oil as inducted fuel with jo as pilot fuel showed inferior performance and emissions at part loads. it is concluded that the jo as pilots fuel and orange oil as the inducted fuel could be used in diesel engines with reduced smoke levels and improved thermal efficiencies with no major detoriation in performance.
use of palm oil-based biofuel in the internal combustion engines: performance and emissions characteristics. palm oil (po) was treated using different methods in order to use and test it as fuel in compression ignition (cl) engines. the treatments include po preheated and preparation of po/diesel oil blends, using mixtures of po with waste cooking oil (wco), which are converted into esters by a transesterification process. the purpose of this study is to evaluate the potential of the palm oil-based biofuels to replace diesel oil in cl engines. tests were conducted in a single cylinder, four-stroke, air-cooled, direct injection diesel engine (no engine modifications were required). experiments were initially carried out with diesel oil for providing baseline data. all the tested fuels have a low heating value compared to diesel fuel. a high fraction of po in diesel fuel decreases the heating value of the blend. the brake thermal efficiency increases for the po/diesel blends. hc emissions for all those fuels except for the po/diesel blends are found lower, while co emissions rise for all types of fuels. no(x) emissions are higher at low load, but lower at full load, for the engine fueled with po and lower both at middle and full load for the engine fueled with the esters. (c) 2011 elsevier ltd. all rights reserved.
thermogravimetric investigation and thermal conversion kinetics of typical north african and middle eastern lignocellulosic wastes. the aim of this work was to thermally characterize the renewable lignocellulosic bioresources derived from palm trees in order to highlight their energy potential. pyrolysis and combustion behaviours of date stones (ds) agricultural by-products were tested by thermo-gravimetric analysis, and the main chemical compositions were analyzed. the work has also been conducted to identify their most important physical characteristics. the study of the sizes and heating rate effects constitute the first part of the experimental work. inert atmosphere and three heating rates: 10, 20, and 50 degrees c/min, were applied to various particle sizes of ds. in the second part, tests were carried out in an oxidizing atmosphere (21% o-2) by varying the size of the ds. the kinetic parameters such as pre-exponential factor and activation energy were determined. increasing the particle sizes and the heating rates didn't have an appreciable influence on the global weight losses. however, degradation rates were significant with the porous structure of the ds. weight losses in inert and oxidizing atmospheres were found to occur in two stages (drying and devolatilization) and in three stages (drying, devolatilization, and oxidation of the char).
performance and emissions of diesel engine using bio-fuel derived from waste fish oil. in the present work, waste fish fat from fish processing industry is considered as an energy source for diesel engines. in this regard, catalytic cracking process is considered for this present study. the physical and chemical properties of biofuel are very close to diesel fuel. the experiments were conducted in a single cylinder diesel engine to study the performance, emission and combustion characteristics of biofuel. as a result, fuel undergoes good combustion and hence there is significant improvement in performance and reduction in emissions. experimental results indicate a marginal increase in brake thermal efficiency at all loads compared to diesel fuel. the results show that despite of high nox and co2, the engine has lesser uhc, co and pm than standard diesel fuel. the premixed and diffusion combustion duration is decreased with biofuel compared to diesel fuel. the engine was running smooth at all load conditions with biofuel. it is concluded that the biofuel derived from waste fish fat can be consider as a substitute for diesel fuel.
experimental analysis of biofuel as an alternative fuel for diesel engines. the growth of energy demand and limited fossil fuel resources lead to renewable energy development such as vegetable oils and animal fats or their derivatives. in the present work, the valuation of waste fish fat by the pyrolysis technique with the presence of catalyst to produce biofuel for diesel engines. as a result, fuel undergoes good combustion and hence there is a significant improvement in performance and reduction in emissions. the brake thermal efficiency of neat biofuel is 32.4% at 80% load which is very high compared to neat diesel (29.98%). the combustion duration and ignition delay are decreased with neat biofuel due to high oxygen content and high cetane number of biofuel. the main problem with the use of neat biofuel in diesel engine is high nox emissions at all loads. addition of diesel with biofuel reduces the nox emissions significantly from 917 ppm to 889 ppm at 80% load with an optimum blend of b80d20. there is a slight decrease in brake thermal efficiency and increase in particulate emission with this blend. the overall results show that by adding small quantity of diesel with biofuel decreases the nox emissions significantly and approaches the performance of neat biofuel. (c) 2012 elsevier ltd. all rights reserved.
investigations on a compression ignition engine using animal fats and vegetable oil as fuels. biofuels are a promising alternative to petroleum-based fuels. this paper investigates the performance, combustion, and exhaust emissions of a single cylinder diesel engine operated on baseline diesel and biofuel produced by vegetable oil and processing animal fat. the vegetable oil is called podl20, which is a blend of palm oil and d-limonen in proportion of 80% and 20%, respectively. the second biofuel is synthesized from the animal fat wastes (waf) after transesterification process. both experimental and numerical investigations are achieved in this work. the experiments are conducted at constant engine speed mode (1800 rpm) with applied loads on a wide domain. the cfd code converge is used to simulate the in-cylinder combustion for all the tested fuels. comparative measures of brake thermal efficiency, break specific fuel consumption (bsfc), exhaust gas temperature, volumetric efficiency, and pollution (thc, co2, co, no, nox) are presented and discussed. also, a step is achieved with in-cylinder cfd simulation of biofuel combustion. the obtained results indicate that the combustion characteristics are slightly changed when comparing neat diesel to biofuels. some of the results obtained in this work indicate that waf fuel decreases the total unburned fuel as well as the nitrogen oxides (nox) emissions. the numerical results are in logic agreement with those obtained experimentally, which promotes more detailed investigations and combustion characteristics optimization in forthcoming works. [doi: 10.1115/1.4005660].
effects of biofuel from fish oil industrial residue - diesel blends in diesel engine. the present work aims to produce biofuel from fish oil industrial residue and to test the biofuel in diesel engine. a 4.5 kw at 1500 rpm single cylinder air cooled direct injection diesel engine was used for the present experimental work. the experimental results show that the brake thermal efficiency marginally increases with biofuel from 29.98% (neat diesel) to the maximum of 32.4% with biofuel at 80% of maximum load. also experiments were conducted with different blends of biofuel and diesel (b20 and 840). though the no emissions are high with neat biofuel and blends, the other emissions like co, hc and particulate matter (pm) are decreased. the pm emissions decrease when the percentage biofuel increases in the blend. it reduces from 8271 ng/s with neat diesel to 8137 ng/s with b40. it further reduces to the minimum of 7842 ng/s with neat biofuel. the cylinder peak pressure increases as the biofuel quantity increases in the blend. the rate of premixed combustion increases with neat biofuel and its blends than neat diesel. addition of biofuel with diesel decreases the combustion duration and ignition delay due to higher cetane number of biofuel. (c) 2012 elsevier ltd. all rights reserved.
combination of pyrolysis and hydroliquefaction of ccb-treated wood for energy recovery: optimization and products characterization. in this paper, pyrolysis and hydroliquefaction processes were successively used to convert ccb-treated wood into bio-oil with respect to environment. pyrolysis temperature has been optimized to produce maximum yield of charcoal with a high metal content (cu, cr, and b). the results obtained indicate that the pyrolysis at 300 degrees c and 30 min are the optimal conditions giving high yield of charcoal about 45% which contains up to 94% of cu, 100% of cr and 88% of b. after pyrolysis process, the charcoal has been converted into bio-oil using hydroliquefaction process. the optimization approach for the yield of bio-oil using a complete factorial design with three parameters: charcoal/solvent, temperature and hydrogen pressure was discussed. it is observed that the temperature is the most significant parameter and the optimum yield of bio-oil is around 82%. the metal analysis shows that the metals present in the bio-oil is very negligible. (c) 2012 elsevier ltd. all rights reserved.
influence of impregnation method on metal retention of ccb-treated wood in slow pyrolysis process. in the present work, the effects of copper, chromium and boron on the pyrolysis of wood and their distribution in the pyrolysis products were investigated. for this, the wood has been impregnated with chromium-copper-boron (ccb). in addition, to describe the effects of impregnation method, vacuum-pressure and dipping methods were also conducted. thermogravimetric analysis (tga) results show that an increase in the final residue and decrease in degradation temperature on both methods of treated wood compared to untreated wood. then, slow pyrolysis experiments were carried out in a laboratory reactor. the mass balance of pyrolysis products is confirmed by tga. furthermore, the concentration of metals in the final residue is measured by inductively coupled plasma mass spectroscopy (icp-ms). the results show that the final residue contains more than 45% of the initial amount of metal present in the treated wood. the phenomenon is more pronounced with vacuum-pressure treated wood. the heating values of pyrolysis products were analyzed. the heating value of charcoal obtained from treated and untreated wood is approximately same. but the heating value of tar from untreated wood is higher than the heating value of the tar from treated wood. (c) 2012 elsevier b.v. all rights reserved.
experimental investigations of a gamma stirling engine. the present work deals with the measurement and performance of a gamma stirling engine of 500?w of mechanical shaft power and 600?rpm of maximal revolutions per minute. series of measurements concerning the pressure distribution, temperature evolution, and brake power were performed. the study of the different functioning parameters such as initial charge pressure, engine velocity, cooling water flowrate, and temperature gradient (between the sources of heat) has been analyzed. the engine brake power increases with the initial charge pressure, with the cooling water flow, and with the engine revolutions per minute. the working fluid temperature measurements have been recorded in different locations symmetrically along both regenerator sides. the recorded temperature in regenerator side one is about 252 degrees c and about 174 degrees c in the opposite side (side two). it shows an asymmetric temperature distribution in the stirling engine regenerator; consequently, heat transfer inside this porous medium is deteriorated. copyright (c) 2011 john wiley &amp; sons, ltd.
assessment of liquid fuel (bio-oil) production from waste fish fat and utilization in diesel engine. increased acceptance of climate change induced by human activities and raising oil demand with unsecure deliverance compels the searching for alternative fuels. the problems with environmental degradation due to industrial wastes can be reduced by converting some of them into bio-oil. in the present work, the waste from fish processing industry is converted to bio-oil by catalytic cracking. experiments were conducted in a direct injection diesel engine of 4.5 kw at 1500 rpm. the different test fuels of diesel, fish oil at 75 c, bio-oil ud (undistilled bio-oil), b20d80 (20% bio-oil in fossil diesel), b80d20 (80% bio-oil in fossil diesel) and neat bio-oil were tested to assess the suitability in diesel engines through combustion, emission and performance characteristics. experimental results show that the brake thermal efficiency is marginally higher with neat bio-oil over other test fuels. it is lower with preheated fish oil and it is almost same for both bio-oil and bio-oil ud. nox, hc, co and pm emissions are higher with bio-oil ud compared to bio-oil. pm, co and hc emissions are lower with bio-oil over diesel. nox emissions are lower with bio-oil compared to bio-oil ud but it is still higher than diesel fuel. addition of diesel with bio-oil reduces the nox emissions marginally. intensity of premixed combustion is strong with bio-oil. ignition delay and combustion duration are reduced with bio-oil due to high cetane number and oxygen concentration. bio-oil from waste fish fat by catalytic cracking can be used as a fuel for diesel engines and also the waste to energy may reduce the environmental and climate change issues due to industrial wastes. (c) 2012 elsevier ltd. all rights reserved.
biodiesel production from biomass gasification tar via thermal/catalytic cracking. this paper is devoted to the study of valorization of tar from biomass gasification as a fuel for internal combustion engine. the methods selected were both thermal cracking and catalytic cracking in the presence of zeolite, magnesium oxide, and aluminum oxide catalyst. the chemical composition of the cracking product was analyzed by gas chromatography-mass spectrometry, together with the physico-chemical properties determination (density, viscosity, higher heating value, and acidic value). thermal cracking of biomass gasification tar gave a yield of bio-diesel 73.67 wt.% of feed. the cracking process in the presence of zeolite, magnesium oxide, and aluminum oxide catalysts gave a yield of biodiesel 62-75 wt.%, 55-66 wt.%, 67-71 wt.% respectively. the influence of the type and quantity of catalyst on production yield and properties of the produced bio-oil is highlighted. the produced bio-oil density and heating value were close to the conventional diesel fuel. the viscosity and acidic value were found to be slightly higher than that of conventional diesel fuel. (c) 2012 elsevier b.v. all rights reserved.
optimization of biodiesel production from animal fat residue in wastewater using response surface methodology. animal fat residues (afr) from waste water were used as feedstock to produce biodiesel by a two-step acid-catalyzed process. treatment of the afrs with 5.4% (w/w) of 17 m h2so4 at a methanol/afr ratio of 13:1 (50% w/w) at 60 degrees c converted more than 95% of the triglycerides into fatty acid methyl esters (fames) with an acid value (av) of 1.3 mg(koh)/g(biodiesel). response surface methodology indicated that a lower av cannot be reached using a one-step acid catalyzed process. thus a two-step acid catalyzed process was employed using 3.6% catalyst and 30% methanol for 5 h for the first step and 1.8% catalyst and 10% methanol for i h in the second step, resulting in a yield higher than 98% and an av of 0.3 mg(koh)/g(biodiesel). the product thus conforms to the european norm en14214 concerning biodiesel. (c) 2012 elsevier ltd. all rights reserved.
single zone combustion modeling of biodiesel from wastes in diesel engine. increasing interest in diesel engine technology and the continuous demand of finding alternate fuels and reducing emissions has motivated over the years for the development of numerical models, to provide qualitatively predictive tools for the designers. among the alternative fuels, biodiesel is considered suitable and the most promising fuel for diesel engine. the properties of biodiesel from waste oils are found similar to that of diesel. in this present work, a unique single zone combustion model for diesel fuel and biodiesel was implemented to predict the cylinder pressure for the better understanding of combustion characteristics of different fuels tested in a diesel engine and also to predict the combustion and performance characteristics of the same engine running on different fuels. the single zone model coupled with a triple-wiebe function was performed to simulate heat release between the period of ivc (inlet valve close) and evo (exhaust valve open). this model also includes the submodels of intake and exhaust gases through the valves, ignition delay, burned fuel during the cycle and heat losses through walls to simulate all phases of combustion. the model calibration was performed using data from experiments on diesel fuel and biodiesel from waste cooking oil. later the same model was used to simulate the combustion and the cylinder pressure of engine running on biodiesel derived from animal fat residues. finally, cylinder pressure traces predicted by using single-zone model are compared to experimental pressure traces obtained from a diesel engine fuelled with diesel fuel and biodiesel. (c) 2012 elsevier ltd. all rights reserved.
continuous production of water-in-oil emulsion using micromixers. the formation of emulsions is a critical application that interests many industrial fields. among the various interests to produce emulsions, this work focuses on the emulsification of water in fuels, in order to improve combustion and reduce emission of harmful gases. the manufacturing process of these emulsions must meet a number of constraints such as water fraction, mean droplet size, delivered flow rate and process energy consumption. among possible techniques, this study focuses on the implementation of crossing microchannels. for this purpose, two geometries of the cross section of the channels have been tested. the implementation of several flow configurations has also been investigated. other parameters were varied such as the variation of the ratio of flow rates of lipid phase and water, the nature and content of surfactant. in conclusion, obtaining emulsions of water in oil having a mean droplet size of about 4 lm was possible with several operating conditions. channel geometry and flow pattern have a significant influence on the possibility of forming this type of emulsions. (c) 2012 elsevier ltd. all rights reserved.
modelling of an indirectly heated fixed bed pyrolysis reactor of wood: transition from batch to continuous staged gasification. gasification is today a mature technology and staged processes know an important development. the design of this kind of reactor is still a sensitive work, and numerical simulation, as a flexible and economic tool, has to be developed. the present work concerns the first pyrolysis stage of a staged gasification process and is organised into two parts. the first part deals with the calibration of the effective thermal conductivity of the bed. this is the key parameter in the heat transfer modelling. the calibration is obtained by comparison between experimental and modelling results in batch reactor. the second part is the development of a transient two dimensional model of a continuous indirectly heated fixed bed pyrolysis reactor. the model uses a finite difference formulation to solve mass and energy balances in a cylindrical vessel. the assumptions of local thermal equilibrium, constant particle size and thermally thin particles are considered. the simulations are used to design a pyrolysis reactor in function of the type of biomass, the wall temperature and the direction of gas circulation. the parametric study is presented to highlight the limitations of the external heating regarding the wood conversion rate. (c) 2012 elsevier ltd. all rights reserved.
liquid hydrocarbon fuels from fish oil industrial residues by catalytic cracking. in the present work, catalytic cracking of fish oil industrial residue was investigated to study the effect of temperature, type of catalyst and the heating rate on the yield of organic liquid fraction (olf) and its acid value. the highest bio-oil yield of 72% (wt.) was obtained at temperature range of 300-500 degrees c and heating rate of 10 degrees c/min with the mixture of al2o3 and na2co3 as a catalyst. it was found that the mixture of na2co3 and mgso4 as a catalyst gives lowest acid value of 8.75 mgkoh/goil and 68.1% of olf yield. furthermore, the acid value is reduced to 0.36 mgkoh/goil using na2co3 as an absorbent. the results show that the catalytic cracking process represents a sustainable method to produce bio-oil from fish oil industrial residues with physicochemical characteristics similar to the diesel fuel. copyright (c) 2012 john wiley &amp; sons, ltd.
slow pyrolysis of ccb-treated wood for energy recovery: influence of chromium, copper and boron on pyrolysis process and optimization. this paper investigates the effect of copper, chromium and boron on the slow pyrolysis of ccb-treated wood. a mixture of softwood has been impregnated with several inorganic salts (individually cuso4, kcr(so4)(2), b4na2o7) and with salt of ccb (mixture of cuso4, kcr(so4)(2) and h3bo3). the weight loss of samples is identified by thermogravimetric analysis (tga). the results show a higher mass of final residue and a decrease in degradation temperature on treated wood compared to untreated wood. tga results also show that, the maximum degradation of ccb treated wood occurs at 300 degrees c. in addition, slow pyrolysis experiments were carried out in a laboratory scale reactor. the pyrolysis products were quantified using analytical balance whereas gases were analyzed by gas chromatography. the trends of weight loss obtained by tga and by laboratory scale pyrolysis are similar. furthermore, it is observed that ccb salts inhibit the formation of co and co2 but promote that of h-2 in the second part of this work, yield of solid pyolysis residue (charcoal) with a high metal content (cu, cr, b) has been carried out at 300 degrees c and 370 degrees c during residence time of 20 and 30 mm. metals in charcoal are analyzed using inductively coupled plasma mass spectrometry. taking into account the energy recovery of by-products, slow pyrolysis at 300 degrees c and 30 mm appears to be optimal conditions with a high yield of charcoal about 45% and the element recovery is up to 94% of cu, 100% of cr and 88% of b. (c) 2013 elsevier b.v. all rights reserved.
effect of free fatty acids and short chain alcohols on conversion of waste cooking oil to biodiesel. in this article, the transesterification of three types of waste cooking oil (wco) with methanol and ethanol was studied using alkali catalyzed process. the catalyst used in this study was sodium hydroxide. the effects of temperature, catalyst amount, alcohol to oil ratio, and the time of reaction on the yield were studied. the temperature and the catalyst amount were the most important factors affecting the yield of biodiesel. also the process exhibited some sensitivity to the level of free fatty acids (ffa) in the wco and to the type of alcohol. the yields of methyl esters varied from 97% with the lowest acidity (0.4% ffa wco) to 76% with the highest acidity (3.25% ffa wco). the ethyl esters yields were lower and the difference increased with the level of ffa in the oil, the maximum yield was 95% and 73% with the lowest and the medium acidities respectively and no reaction was registered with the highest one. the chromatographic analysis of the produced biodiesel showed high contents of fatty acid methyl esters varying from 96.5% to 98%. the physical-chemical characteristics of produced biodiesel were studied and compared to the european norm, en 14214.
background-independent measurement of θ13 in double chooz. the oscillation results published by the double chooz collaboration in 2011 and 2012 rely on background models substantiated by reactor-on data. in this analysis, we present a background-model-independent measurement of the mixing angle θ13 by including 7.53 days of reactor-off data. a global fit of the observed antineutrino rates for different reactor power conditions is performed, yielding a measurement of both θ13 and the total background rate. the results on the mixing angle are improved significantly by including the reactor-off data in the fit, as it provides a direct measurement of the total background rate. this reactor rate modulation analysis considers antineutrino candidates with neutron captures on both gd and h, whose combination yields sin2(2θ13)=0.102±0.028(stat.)±0.033(syst.). the results presented in this study are fully consistent with the ones already published by double chooz, achieving a competitive precision. they provide, for the first time, a determination of θ13 that does not depend on a background model.
radiolytic corrosion of uranium dioxide: role of molecular species. n.a.
direct and steering tilt robust control of narrow vehicles. narrow tilting vehicles (ntvs) are the convergence of a car and a motorcycle. they are expected to be the new generation of city cars considering their practical dimensions and lower energy consumption. however, due to their height to breadth ratio, in order to maintain lateral stability, ntvs should tilt when cornering. unlike the motorcycle, where the driver tilts the vehicle himself, the tilting of an ntv should be automatic. two tilting systems are available; direct and steering tilt control, the combined action of these two systems being certainly the key to improve considerably ntv dynamic performances. in this paper, multivariable control tools (h2 methodology) are used to design, in a systematic way, lateral assistance controllers driving dtc, stc or both dtc/stc systems. a three degrees of freedom model of the vehicle is used, as well as a model of the steering signal, leading to a two degrees of freedom low order controller with an efficient feedforward anticipative part. taking advantage of all the available measurements on ntvs, the lateral acceleration is directly regulated. finally, a gain-scheduling solution is provided to make the dtc, stc, and dtc/stc controllers robust to longitudinal speed variations.
fission of actinides through quasimolecular shapes. the potential energy of heavy nuclei has been calculated in the quasimolecular shape path from a generalized liquid drop model including the proximity energy, the charge and mass asymmetries and the microscopic corrections. the potential barriers are multiple-humped. the second maximum is the saddle-point. it corresponds to the transition from compact one-body shapes with a deep neck to two touching ellipsoids. the scission point lies at the end of an energy plateau well below the saddle-point and where the effects of the nuclear attractive forces between two separated fragments vanish. the energy on this plateau is the sum of the kinetic and excitation energies of the fragments. the shell and pairing corrections play an essential role to select the most probable fission path. the potential barrier heights agree with the experimental data and the theoretical half-lives follow the trend of the experimental values. a third peak and a shallow third minimum appear in asymmetric decay paths when one fragment is close to a double magic quasi-spherical nucleus, while the smaller one changes from oblate to prolate shapes.
scalable multi-dimensional resources scheduling constraints. constraint programming is an approach often used to solve combinatorial problems in different application areas. in this thesis we focus on the cumulative scheduling problems. a scheduling problem is to determine the starting dates of a set of tasks while respecting capacity and precedence constraints. capacity constraints affect both conventional cumulative constraints where the sum of the heights of tasks intersecting a given time point is limited, and colored cumulative constraints where the number of distinct colors assigned to the tasks intersecting a given time point is limited. a newly identified challenge for constraint programming is to deal with large problems, usually solved by dedicated algorithms and metaheuristics. for example, the increasing use of virtualized datacenters leads to multi dimensional placement problems of thousand of jobs. scalability is achieved by using a synchronized sweep algorithm over the different cumulative and precedence constraints that allows to speed up convergence to the fix point. in addition, from these filtering algorithms we derive greedy procedures that can be called at each node of the search tree to find a solution more quickly. this approach allows to deal with scheduling problems involving more than one million jobs and 64 cumulative resources. these algorithms have been implemented within choco and sicstussolvers and evaluated on a variety of placement and scheduling problems.
optiplace: designing cloud management with flexible power models through constraint programing. null
beam-energy dependence of directed flow of protons, antiprotons and pions in au+au collisions. rapidity-odd directed flow($v_1$) measurements for charged pions, protons and antiprotons near mid-rapidity ($y=0$) are reported in $\sqrt{s_{nn}} =$ 7.7, 11.5, 19.6, 27, 39, 62.4 and 200 gev au + au collisions as recorded by the star detector at the relativistic heavy ion collider (rhic). at intermediate impact parameters, the proton and net-proton slope parameter $dv_1/dy|_{y=0}$ shows a minimum between 11.5 and 19.6 gev. in addition, the net-proton $dv_1/dy|_{y=0}$ changes sign twice between 7.7 and 39 gev. the proton and net-proton results qualitatively resemble predictions of a hydrodynamic model with a first-order phase transition from hadronic matter to deconfined matter, and differ from hadronic transport calculations.
first demonstration of thgem/gapd-matrix optical readout in a two-phase cryogenic avalanche detector in ar. the multi-channel optical readout of a thgem multiplier coupled to a matrix of 3×3 geiger-mode apds (gapds) was demonstrated in a two-phase cryogenic avalanche detector (crad) in ar. the gapds recorded thgem-hole avalanches in the near infrared (nir) spectral range. at an avalanche charge gain of 160, the yield of the combined thgem/gapd-matrix multiplier amounted to ~80 photoelectrons per 20 kev x-ray absorbed in the liquid phase. a spatial resolution of 2.5 mm (fwhm) has been measured for the impinging x-rays. this technique has potential applications in coherent neutrino-nucleus scattering and in dark matter search experiments.
voltage regulation of a boost converter in discontinuous conduction mode: a simple robust adaptive feedback controller. ideal switches in power converters are typically implemented using unidirectional semiconductor devices that may lead to a new operation mode generically called discontinuous conduction mode (dcm). the dcm arises when the ripple, that is, sustained oscillations of small amplitude, is large enough to cause the polarity of the signal (current or voltage) applied to the switch to reverse. due to the presence of diodes, switches are assumed to operate unidirectionally, but in dcm this unidirectionality assumption is violated. in classicalconverter topologies, dcm appears very frequently in low load operating modes. more interestingly, to achieve high performance some new converters are purposely designed to operate all the time in dcm [1].
towards a unified description of evaporation-residue fusion cross-sections above the barrier. a meticulous study of nearly 300 fusion-evaporation cross-section data reveals that, when properly scaled, fusion excitation function complies with a universal homographic law which is, within experimental errors, reaction system independent. from such complete and summed complete and incomplete fusion excitation functions are extracted the limiting energy for the complete fusion and the main characteristics (onset, maximum and vanishing) of the incomplete fusion. the dywan microscopic transport model correctly predicts the incomplete fusion cross-section for incident energies $\gtrsim15a\ \text{mev}$ and suggests that the nuclear transparency is at the origin of fusion disappearance.
transport coefficients from the nambu-jona-lasinio model for $su(3)_f$. we calculate the shear $\eta(t)$ and bulk viscosities $\zeta(t)$ as well as the electric conductivity $\sigma_e(t)$ and heat conductivity $\kappa(t)$ within the nambu-jona-lasinio model for 3 flavors as a function of temperature as well as the entropy density $s(t)$, pressure $p(t)$ and speed of sound $c_s^2(t)$. we compare the results with other models such as the polyakov-nambu-jona-lasinio (pnjl) model and the dynamical quasiparticle model (dqpm) and confront these results with lattice qcd data whenever available. we find the njl model to have a limited predictive power for the thermodynamic variables and various transport coefficients above the critical temperature whereas the pnjl model and dqpm show acceptable results for the quantities of interest.
semi-classical approach to $j/\psi$ suppression in high energy heavy-ion collisions. we study the heavy quark/antiquark pair dynamics in strongly-coupled quark gluon plasma. a semi-classical approach, based on the wigner distribution and langevin dynamics, is applied to a color screened $c{\bar c}$ pair, in a hydrodynamically cooling fireball, to evaluate the total $j/\psi$ suppression at both rhic and lhc energies. although its limitation is observed, this approach results to a $j/\psi$ suppression of around 0.30 at rhic and 0.25 at lhc.
transverse-energy distributions at midrapidity in $p$$+$$p$, $d$$+$au, and au$+$au collisions at $\sqrt{s_{_{nn}}}=62.4$--200~gev and implications for particle-production models. measurements of the midrapidity transverse energy distribution, $d\et/d\eta$, are presented for $p$$+$$p$, $d$$+$au, and au$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev and additionally for au$+$au collisions at $\sqrt{s_{_{nn}}}=62.4$ and 130 gev. the $d\et/d\eta$ distributions are first compared with the number of nucleon participants $n_{\rm part}$, number of binary collisions $n_{\rm coll}$, and number of constituent-quark participants $n_{qp}$ calculated from a glauber model based on the nuclear geometry. for au$+$au, $\mean{d\et/d\eta}/n_{\rm part}$ increases with $n_{\rm part}$, while $\mean{d\et/d\eta}/n_{qp}$ is approximately constant for all three energies. this indicates that the two component ansatz, $de_{t}/d\eta \propto (1-x) n_{\rm part}/2 + x n_{\rm coll}$, which has been used to represent $e_t$ distributions, is simply a proxy for $n_{qp}$, and that the $n_{\rm coll}$ term does not represent a hard-scattering component in $e_t$ distributions. the $de_{t}/d\eta$ distributions of au$+$au and $d$$+$au are then calculated from the measured $p$$+$$p$ $e_t$ distribution using two models that both reproduce the au$+$au data. however, while the number-of-constituent-quark-participant model agrees well with the $d$$+$au data, the additive-quark model does not.
production of charged pions, kaons and protons at large transverse momenta in pp and pb-pb collisions at sqrt(snn) = 2.76 tev. transverse momentum spectra of pi+/-, k+/- and p(anti-p) up to pt = 20 gev/c at mid-rapidity, |y| ~&lt; 0.8, in pp and pb-pb collisions at sqrt(snn) = 2.76 tev have been measured using the alice detector at the lhc. at intermediate pt (2-8 gev/c) an enhancement of the proton-to-proton ratio, (p + anti-p)/(pi+ + pi-), with respect to pp collisions is observed and the ratio reaches ~0.80 in central pb-pb collisions. the measurement of the nuclear modification factors for pi+/-, k+/- and p(anti-p) indicates that within the systematic and statistical uncertainties they are the same at high pt (&gt; 10 gev/c), suggesting that the chemical composition of leading particles from jets in the medium is similar to that of vacuum jets.
highlights from the pierre auger observatory. the pierre auger observatory is the world's largest cosmic ray observatory. our current exposure reaches nearly 40,000 km$^2$ str and provides us with an unprecedented quality data set. the performance and stability of the detectors and their enhancements are described. data analyses have led to a number of major breakthroughs. among these we discuss the energy spectrum and the searches for large-scale anisotropies. we present analyses of our x$_{max}$ data and show how it can be interpreted in terms of mass composition. we also describe some new analyses that extract mass sensitive parameters from the 100% duty cycle sd data. a coherent interpretation of all these recent results opens new directions. the consequences regarding the cosmic ray composition and the properties of uhecr sources are briefly discussed.
oxygen dayglow observations on mars by spicam ir on mars-express. o2(1δg) dayglow at 1.27 μm reflects the ozone distribution in the martian atmosphere as a result of ozone photolysis by solar uv radiation. spicam ir on mars-express performed continuous observations of the o2 dayglow at limb and nadir from 2004 to 2012 with resolving power of 2200. the results of o2(1δg) observations have been compared with lmd gcm simulation [1-3] to study its seasonal variations and sensitivity to kinetic parameters.
rightcapacity: sla-driven cross-layer cloud elasticity management. cloud computing paradigm has become the solution to provide good service quality and exploit economies of scale. however, the management of such elastic resources, with different quality-of-service (qos) combined with on-demand self-service, is a complex issue. new challenges for elasticity management arise when people look deeper into the cloud characteristics such as non-ignorable instance initiation time and full hour billing model. the main challenge for a saas provider is to determine the best trade-off between profit and end-user satisfaction. this paper proposes rightcapacity, an approach driven by service level agreement (sla) for optimizing the cloud elasticity management (i.e., both elasticity at the application and at the infrastructure levels). we consider cross-layer (application-resource) cloud elasticity. we model cloud application using closed queueing network model taking into account the sla concept and the cloud economic model. our results show that rightcapacity successfully keeps the best trade-off between saas provider profit and end-user satisfaction. using rightcapacity, the cost saving of as much as 30% can be achieved while causing the minimum number of violations, as small as 1%.
estimated costs of implementation of membrane processes for on-site greywater recycling. greywater reuse inside buildings is a possible way to preserve water resources and face up to water scarcity. this study is focused on a technical-economic analysis of greywater treatment by a direct nanofiltration (nf) process or by a submerged membrane bioreactor (smbr) for on-site recycling. the aim of this paper is to analyse the cost of recycled water for two different configurations (50 and 500 inhabitants) in order to demonstrate the relevance of the implementation of membrane processes for greywater recycling, depending on the production capacity of the equipment and the price of drinking water. the first step was to define a method to access the description of the cost of producing recycled water. the direct costs were defined as a sum of fixed costs due to equipment, maintenance and depreciation, and variable costs generated by chemical products and electricity consumptions. they were estimated from an experimental approach and from data found in literature, enabling operating conditions for greywater recycling to be determined. the cost of treated water by a smbr unit with a processing capacity of 500 persons is close to 4.40 m-3, while the cost is 4.81 m-3 with a nf process running in the same conditions. these costs are similar to the price of drinking water in some european countries.
flauncher and dvms -- deploying and scheduling thousands of virtual machines on hundreds of nodes distributed geographically. although live migration of virtual machines has been an active area of research over the past decade, it has been mainly evaluated by means of simulations and small scale deployments. proving the relevance of live migration at larger scales is a technical challenge that requires to be able to deploy and schedule virtual machines. in the last year, we succeeded to tackle such a challenge by conducting experiments with flauncher and dvms, two frameworks that can respectively deploy and schedule thousands of virtual machines over hundreds of nodes distributed geographically across the grid'5000 testbed.
compositional reasoning about aspect interference. oliveira and colleagues recently developed a powerful model to reason about mixin-based composition of effectful components and their interference, exploiting a wide variety of techniques such as equational reasoning, parametricity, and algebraic laws about monadic effects. this work addresses the issue of reasoning about interference with effectful aspects in the presence of unrestricted quantification through pointcuts. while global reasoning is required, we show that it is possible to reason in a compositional manner, which is key for the scalability of the approach in the face of large and evolving systems. we establish a general equivalence theorem that is based on a few conditions that can be established, reused, and adapted separately as the system evolves. interestingly, one of these conditions, local harmlessness, can be proven by a translation to the mixin setting, making it possible to directly exploit previously established results about certain kinds of harmless extensions.
vr4d: an immersive and collaborative experience to improve the interior design process. ergonomics and spatial constraints are important issues to consider during the design process of limited spaces. in this paper, we present a user-centered methodology for designing a new vr tool for collaborative design and evaluation of limited spaces. the system is composed of two communicating tools: a sketch-based application, and a 3d immersive application. using this tool, two collaborating users can perform simultaneously two steps of the design process: the sketching phase, and the organization and evaluation of the 3d space. a preliminary evaluation session, conducted with expert designers to assess the usability and the utility of the system, shows its value.
a preliminary investigation of the isg glass vapor hydration. during the geological disposal of high-level waste, the nuclear glass is expected to be first hydrated in water vapor prior to liquid alteration. in the present work, we investigated the vapor hydration of the international simple glass (isg) at 175°c and different relative humidities (60%, 80% and 98%). the glass hydration was investigated by nuclear reaction analysis (nra) and fourier transform infra-red spectroscopy. the chemical and mineralogical compositions of the alteration products were studied using scanning electron microscopy/energy dispersive x-ray spectroscopy (sem-eds) and μ-raman spectroscopy, respectively. the nra results gave water diffusion coefficients of 2.31-7.34 × 10−21 m2/s, in good agreement with the literature data on borosilicate glasses altered in aqueous media. the glass hydration increased with relative humidity percentage and the sem-eds analysis showed a slight enrichment in si and loss of na in the hydrated glass layer compared with the pristine glass. the hydration rate of the isg glass was little higher than that of the french son68 glass hydrated using water vapor. the corrosion products were analcime, tobermorite, and calcite, which were typical of the son68 glass hydrated in similar conditions.
hazardous dichloromethane recovery in combined temperature and vacuum pressure swing adsorption process. organic vapors emitted from solvents used in chemical and pharmaceutical processes, or from hydrocarbon fuel storage stations at oil terminals, can be efficiently captured by adsorption onto activated carbon beds. to recover vapors after the adsorption step, two modes of regeneration were selected and could be possibly combined: thermal desorption by hot nitrogen flow and vacuum depressurization (vtsa). because of ignition risks, the conditions in which the beds operate during the adsorption and regeneration steps need to be strictly controlled, as well as optimized to maintain good performances. in this work, the optimal conditions to be applied during the desorption step were determined from factorial experimental design (fed), and validated from the process simulation results. the regeneration performances were compared in terms of bed regeneration rate, concentration of recovered volatile organic compounds (voc) and operating costs. as an example, this methodology was applied in case of dichloromethane. it has been shown that the combination of thermal and vacuum regeneration allows reaching 82% recovery of dichloromethane. moreover, the vacuum desorption ended up in cooling the activated carbon bed from 93°c to 63°c and so that it significantly reduces the cooling time before starting a new cycle.
sla-driven cloud elasticity anagement approach. cloud computing promises to completely revolutionize the way to manage resources. thanks to elasticity, resources can be provisioning within minutes to satisfy a required level of quality of service(qos) formalized by service level agreements (slas) between different cloud actors. the main challenge of service providers is to maintain its consumer's satisfaction while minimizing the service costs due to resources fees. for example, from the saas point of view, this challenge can be achieved in ad-hoc manner by allocating/releasing resources based on a set of predefined rules as amazon auto scaling implements it. however, doing it right -in a way that maintains end-users satisfaction while optimizing service cost- is not a trivial task. first, because of the difficulty to profile service performance,the accuracy of capacity planning may be compromised. second, several parameters should be taken into account such as multiple resource types, non-ignorable resource initiation time and iaas billing model. for that purpose, we propose a complete solution for cloud service level management. we first introduce csla (cloud service levelagreement), a specific language to describe sla for cloud services. it finely expresses sla violations via functionality/qos degradationand an advanced penalty model. then, we propose hybridscale, an auto-scaling framework driven by sla. it implements the cloud elasticity in a triple hybrid way : reactive-proactive management, vertical horizontal scaling at cross-layer (application-infrastructure). our solution is experimentally validated on amazon ec2.
modeling the temperature dependence of adsorption equilibriums of voc(s) onto activated carbons. in order to optimize the efficiency of the removal of volatile organic compounds (vocs) by adsorption onto activated carbon beds, process simulations taking into account exothermicity effects are helpful. significant temperature increases may arise in the bed during the voc adsorption cycle, especially when high concentrations have to be treated. consequently, reliable and easy-to-handle isotherms remain a key hurdle to build realistic models. in this study, adsorption models were tested to describe a set of experimental data obtained for three vocs (acetone, ethyl formate, and dichloromethane) adsorbed onto five commercial activated carbons at four different temperatures (20, 40, 60, and 80°c ). a new expression of the freundlich equation [qe=(a1t+a2t2)ce(1/nf)] was shown to be statistically the most efficient to describe the adsorption isotherms of vocs, single or in mixtures. a second-order polynomial temperature-dependence was introduced in this expression. the so-adapted freundlich relationship gave a mean coefficient of determination of 0.97 for single-component adsorption and a correlation coefficient of 0.98 for binary mixtures.
supersymmetric field theory with benign ghosts. we construct a supersymmetric (1+1)-dimensional field theory involving extra derivatives and associated ghosts: the spectrum of the hamiltonian is not bounded from below, neither from above. in spite of that, there is neither classical, nor quantum collapse and unitarity is preserved.
collisional and radiative energy loss of heavy quarks. null
recent results on jet physics from alice at the lhc. null
towards the dynamical study of heavy-flavor quarks in the quark-gluon-plasma. within the aim of a dynamical study of on- and off-shell heavy quarks q in the quark gluon plasma (qgp) - as produced in relativistic nucleus-nucleus collisions - we study the heavy quark collisional scattering on partons of the qgp. the elastic cross sections $\sigma_{q,g-q}$ are evaluated for perturbative partons (massless on-shell particles) and for dynamical quasi-particles (massive off-shell particles as described by the dynamical quasi-particles model "dqpm") using the leading order born diagrams. we demonstrate that the finite width of the quasi-particles in the dqpm has little influence on the cross sections $\sigma_{q,g-q}$ except close to thresholds. we, furthermore, calculate the heavy quark relaxation time as a function of temperature t within the different approaches using these cross sections.
j/$\psi$ roduction in pb-pb collisions at =$\sqrt{s}$ = 2.76 tev in the alice experiment. null
formulation of synthetic greywater as an evaluation tool for wastewater recycling technologies. on-site greywater recycling is one of the main ways of preserving water resources in urban or arid areas. this study aims to formulate model synthetic greywater (sgw) in order to evaluate and compare the performances of several recycling processes on a reproducible effluent. the formulated sgw is composed of septic effluent to provide indicators of faecal contamination, and technical quality chemical products to simulate organic pollution of greywater. to ensure that the sgw developed is representative of household greywater, its analysis was compared to real greywater collected and analysed (rgws) and to real greywater mentioned in previous publications (rgwl). the performance of a direct nanofiltration process with a concentration factor of 87.5% at 35 bar was then tested on both real greywater and sgw. the laboratory experimental results are promising: fluxes and retention rates were high, and similar for both effluents. the permeation flux was higher than 50 l h-1 m-2. retentions greater than 97% for biochemical oxygen demand for 5 days (bod5) and 92% for anionic surfactants were observed. no enterococcus were detected in the two permeates. these results confirm that the model sgw developed in this study shows the same behaviour as real greywater when recycled. thus, the use of this sgw developed in this study was validated for the evaluation of membrane efficiency to treat greywater. this new tool will be a real asset for future studies.
membrane process treatment for greywater recycling: investigations on direct tubular nanofiltration. on-site greywater recycling and reuse is one of the main ways to reduce potable water requirement in urban areas. direct membrane filtration is a promising technology to recycle greywater on-site. this study aimed at selecting a tubular nanofiltration (nf) membrane and its operating conditions in order to treat and reuse greywater in buildings. to do so, a synthetic greywater (sgw) was reconstituted in order to conduct experiments on a reproducible effluent. then, three pci nf membranes (afc30, afc40 and afc80) having distinct molecular weight cut-offs were tested to recycle this sgw with a constant concentration at 25°c at two different transmembrane pressures (20 and 35 bar). the best results were obtained with afc80 at 35 bar: the flux was close to 50 l m-2 h-1, retentions of 95% for chemical oxygen demand and anionic surfactants were observed, and no enterococcus were detected in the permeate. the performances of afc80 were also evaluated on a real greywater: fluxes and retentions were similar to those observed on sgw. these results demonstrate the effectiveness of direct nanofiltration to recycle and reuse greywater.
observation and applications of single-electron charge signals in the xenon100 experiment. the xenon100 dark matter experiment uses liquid xenon in a time projection chamber (tpc) to measure xenon nuclear recoils resulting from the scattering of dark matter weakly interacting massive particles (wimps). in this paper, we report the observation of single-electron charge signals which are not related to wimp interactions. these signals, which show the excellent sensitivity of the detector to small charge signals, are explained as being due to the photoionization of impurities in the liquid xenon and of the metal components inside the tpc. they are used as a unique calibration source to characterize the detector. we explain how we can infer crucial parameters for the xenon100 experiment: the secondary-scintillation gain, the extraction yield from the liquid to the gas phase and the electron drift velocity.
measurement of transverse-single-spin asymmetries for midrapidity and forward-rapidity production of hadrons in polarized p+p collisions at $\sqrt{s}=$200 and 62.4 gev. measurements of transverse-single-spin asymmetries ($a_{n}$) in $p$$+$$p$ collisions at $\sqrt{s}=$62.4 and 200 gev with the phenix detector at rhic are presented. at midrapidity, $a_{n}$ is measured for neutral pion and eta mesons reconstructed from diphoton decay, and at forward rapidities, neutral pions are measured using both diphotons and electromagnetic clusters. the neutral-pion measurement of $a_{n}$ at midrapidity is consistent with zero with uncertainties a factor of 20 smaller than previous publications, which will lead to improved constraints on the gluon sivers function. at higher rapidities, where the valence quark distributions are probed, the data exhibit sizable asymmetries. in comparison with previous measurements in this kinematic region, the new data extend the kinematic coverage in $\sqrt{s}$ and $p_t$, and it is found that the asymmetries depend only weakly on $\sqrt{s}$. the origin of the forward $a_{n}$ is presently not understood quantitatively. the extended reach to higher $p_t$ probes the transition between transverse momentum dependent effects at low $p_t$ and multi-parton dynamics at high $p_t$.
optimal maintenance and replacement decisions under technological change with consideration of spare parts inventories. classical spare parts inventory models assume that the same vintage of technology will be utilized throughout the planning horizon. however, replacement often occurs in the form of a new technology that renders existing spare parts inventories obsolete. this paper aims to study the impact of spare parts inventory on maintenance and replacement decisions under technological change via a markov decision process formulation. the replacement decision is complex in that one must decide with which technology available on the market to replace the current asset. under technological change, the do nothing and repair options have significantly more value as they allow the appearance of even better technologies in the future.
selecting test sensitivity and specificity parameters to optimally maintain a degrading system. null
simultaneous optimization of x-bar control chart and age-based preventive maintenance policies under an economic objective. null
joint-optimization of inventory policies on a multi-product multi-echelon pharmaceutical system with batching and ordering constraints. null
phase transitions of iron sulphides formed by steel microbial corrosion. null
summary of the workshop on multi-parton interactions (mpi@lhc 2012). with short resumes and highlights the discussions in the different working groups of the workshop mpi@lhc 2012 is documented.
epos lhc : test of collective hadronization with lhc data. epos is a monte-carlo event generator for minimum bias hadronic interactions, used for both heavy ion interactions and cosmic ray air shower simulations. since the last public release in 2009, the lhc experiments have provided a number of very interesting data sets comprising minimum bias p-p, p-pb and pb-pb interactions. we describe the changes required to the model to reproduce in detail the new data available from lhc and the consequences in the interpretation of these data. in particular we discuss the effect of the collective hadronization in p-p scattering. a different parametrization of flow has been introduced in the case of a small volume with high density of thermalized matter (core) reached in p-p compared to large volume produced in heavy ion collisions. both parametrizations depend only on the geometry and the amount of secondary particles entering in the core and not on the beam mass or energy. the transition between the two flow regimes can be tested with p-pb data. epos lhc is able to reproduce all minimum bias.
mass transfer coefficients of styrene into water/silicone oil mixtures: new interpretation using the "equivalent absorption capacity" concept. the physical absorption of styrene into water/silicone oil systems at a constant flow rate for mixtures of different compositions (silicone oil volume fraction ϕ = 0%, 1%, 2%, 3%, 4%, 5%, 6%, 8%, and 10%) was investigated in laboratory-scale bubble reactors using a dynamic absorption method. experimental results previously analyzed assuming no contact between gas and silicone oil [10] were reconsidered by applying the "equivalent absorption capacity" concept characterized by the value of the styrene partition coefficient between air and the mixture (hmix). the results indicate that silicone oil addition slightly hinders the styrene mass transfer rate compared to the air/water system. moreover, a dramatic decrease in kla values due to silicone oil addition is observed. in comparison with similar measurements available in the literature, it is noted that this decrease in kla value could be related to the change in the partition coefficient ratio mr = hwater/hoil. two explanations concerning the relationship between the change in a hydrodynamic parameter (kla) and that in a thermodynamic parameter (mr) are proposed. finally, it appears questionable to study the change in kla in terms of silicone oil addition.
pseudo dynamic transitional modeling of building heating energy demand using artificial neural network. this paper presents the building heating demand prediction model with occupancy profile and operational heating power level characteristics in short time horizon (a couple of days) using artificial neural network. in addition, novel pseudo dynamic transitional model is introduced, which consider time dependent attributes of operational power level characteristics and its effect in the overall model performance is outlined. pseudo dynamic model is applied to a case study of french institution building and compared its results with static and other pseudo dynamic neural network models. the results show the coefficients of correlation in static and pseudo dynamic neural network model of 0.82 and 0.89 (with energy consumption error of 0.02%) during the learning phase, and 0.61 and 0.85 during the prediction phase respectively. further, orthogonal array design is applied to the pseudo dynamic model to check the schedule of occupancy profile and operational heating power level characteristics. the results show the new schedule and provide the robust design for pseudo dynamic model. due to prediction in short time horizon, it finds application for energy services company (escos) to manage the heating load for dynamic control of heat production system.
integrating rule-based modelling and constraint programming for solving industrial packing problems. null
the increasing nvalue constraint. this paper introduces the increasing_nvalue constraint, which restricts the number of distinct values assigned to a sequence of variables so that each variable in the sequence is less than or equal to its successor. this constraint is a specialization of the nvalue constraint, motivated by symmetry breaking. propagating the nvalue constraint is known as an np-hard problem. however, we show that the chain of non strict inequalities on the variables makes the problem polynomial. we propose an algorithm achieving generalized arc-consistency in o(σdi) time, where σdi is the sum of domain sizes. this algorithm is an improvement of filtering algorithms obtained by the automaton-based or the slide-based reformulations. we evaluate our constraint on a resource allocation problem.
on matrices, automata, and double counting. matrix models are ubiquitous for constraint problems. many such problems have a matrix of variables m, with the same constraint defined by a finite-state automaton a on each row of m and a global cardinality constraint gcc on each column of m . we give two methods for deriving, by double counting, necessary conditions on the cardinality variables of the gcc constraints from the automaton a. the first method yields linear necessary conditions and simple arithmetic constraints. the second method introduces the cardinality automaton, which abstracts the overall behaviour of all the row automata and can be encoded by a set of linear constraints. we evaluate the impact of our methods on a large set of nurse rostering problem instances.
collisional processes of on-shell and off-shell heavy quarks in vacuum and in the quark-gluon-plasma. we study the heavy quark scattering on partons of the quark gluon plasma (qgp) being especially interested in the collisional (elastic) scattering processes of heavy quarks on quarks and gluons. we calculate the different cross sections for perturbative partons (massless on-shell particles in the vacuum) and for dynamical quasi-particles (off-shell particles in the qgp medium as described by the dynamical quasi-particles model "dqpm") using the leading order born diagrams. our results show clearly the effect of a finite parton mass and width on the perturbative elastic $(q(g) q \rightarrow q (g) q)$ cross sections which depend on temperature $t$, energy density $\epsilon$, the invariant energy $\sqrt{s}$ and the scattering angle $\theta$. our detailed comparisons demonstrate that the finite width of the quasi-particles in the dqpm - which encodes the multiple partonic scattering - has little influence on the cross section for $q q \rightarrow q q$ as well as $g q \rightarrow g q$ scattering except close to thresholds. thus when studying the dynamics of energetic heavy quarks in a qgp medium the spectral width of the degrees-of-freedom may be discarded. we have, furthermore, compared the cross sections from the dqpm with corresponding results from hard-thermal-loop (htl) approaches. the htl inspired models - essentially fixing the regulators by elementary vacuum cross sections and decay amplitudes instead of properties of the qgp at finite temperature - provide quite different results especially w.r.t. the temperature dependence of the $qq$ and $gq$ cross sections (in all settings). accordingly, the transport properties of heavy quarks will be very different as a function of temperature when compared to dqpm results.
sweeping with continuous domains. the geost constraint has been proposed to model and solve discrete placement problems involving multi-dimensional boxes (packing in space and time). the filtering technique is based on a sweeping algorithm that requires the ability for each constraint to compute a forbidden box around a given fixed point and within a surrounding area. several cases have been studied so far, including integer linear inequalities. motivated by the placement of objects with curved shapes, this paper shows how to implement this service for continuous constraints with arbitrary mathematical expressions. the approach relies on symbolic processing and defines a new interval arithmetic.
mde 2.0 : pragmatical formal model verification and other challenges. this document presents a synthesis of the research results conducted in the eld of software veri cation for model-driven engineering (mde). mde is becoming one of the dominant software engineering paradigms in the industry. the main characteristic of mde is the use of software models and model manipulation operations as main artifacts in all software engineering activities. this change of perspective implies that correctness of models (and model manipulation operations) becomes a key factor in the quality of the nal software product. the problem of ensuring software correctness is still considered to be a grand challenge for the software engineering community. at the modellevel, we are still missing a set of tools and methods that helps in the detection of defects and smoothly integrates in existing mde-based tool-chains without an excessive overhead. characteristics of existing tools, which require designer interaction, deep knowledge of formal methods or extensive manual model annotations seriously impair its usability in practice. in this document, we present our pragmatic set of techniques for formal model veri cation to overcome these limitations. we call our techniques pragmatic because they try to nd the best trade-o between completeness of the veri cation and the usability of the process.
systematical calculation of alpha decay half-lives with a generalized liquid drop model. a systematic calculation of α decay half-lives is presented for even-even nuclei between te and z=118 isotopes. the potential energy governing α decay has been determined within a liquid drop model including proximity effects between the α particle and the daughter nucleus and taking into account the experimental q value. the α decay half-lives have been deduced from the wkb barrier penetration probability. the α decay half-lives obtained agree reasonably well with the experimental data.
heavy quark quenching from rhic to lhc: mc@hq generator compared to experiments. null
gluon radiation by heavy quarks. null
heavy-flavor azimuthal correlations of d mesons. observables of heavy-quark azimuthal correlations in heavy-ion collisions are a new and promising tool for the investigation of the in-medium energy loss. we explore the potential of these observables to discriminate the collisional and radiative contributions within a hybrid epos+mc@shq transport approach.
analysing radial flow features in p-pb and p-p collisions at several tev by studying identified particle production in epos3. experimental transverse momentum spectra of identified particles in p-pb collisions at 5.02 tev show many similarities to the corresponding pb-pb results, the latter ones usually being interpreted in term of hydrodynamic flow. we analyse these data using epos3, an event generator based on a 3d+1 viscous hydrodynamical evolution starting from flux tube initial conditions, which are generated in the gribov-regge multiple scattering framework. an individual scattering is referred to as pomeron, identified with a parton ladder, eventually showing up as flux tubes (or strings). each parton ladder is composed of a pqcd hard process, plus initial and final state linear parton emission. nonlinear effects are considered by using saturation scales $q_{s}$, depending on the energy and the number of participants connected to the pomeron in question. we compute transverse momentum ($p_{t}$) spectra of pions, kaons, protons, lambdas, and $\xi$ baryons in p-pb and p-p scattering, compared to experimental data and many other models. in this way we show in a quantitative fashion that p-pb data (and even p-p ones) show the typical ''flow effect'' of enhanced particle production at intermediate $p_{t}$ values, more and more visible with increasing hadron mass.
$j/\psi$ polarization in p+p collisions at $\sqrt{s}$ = 200 gev in star. we report on a polarization measurement of inclusive $j/\psi$ mesons in the di-electron decay channel at mid-rapidity at 2 $&lt; p_t &lt;$ 6 gev/c in p+p collisions at $\sqrt{s}$ = 200 gev. data were taken with the star detector at rhic. the $j/\psi$ polarization measurement should help to distinguish between the different models of the $j/\psi$ production mechanism since they predict different $p_t$ dependences of the $j/\psi$ polarization. in this analysis, $j/\psi$ polarization is studied in the helicity frame. the polarization parameter $\lambda_{\theta}$ measured at rhic becomes smaller towards high $p_t$, indicating more longitudinal $j/\psi$ polarization as $p_t$ increases. the result is compared with predictions of presently available models.
heavy-flavor electron-muon correlations in $p$$+$$p$ and $d$+au collisions at $\sqrt{s_{_{nn}}}$ = 200 gev. we report $e^\pm-\mu^\mp$ pair yield from charm decay measured between midrapidity electrons ($|\eta|&lt;0.35$ and $p_t&gt;0.5$ gev/$c$) and forward rapidity muons ($1.4&lt;\eta&lt;2.1$ and $p_t&gt;1.0$ gev/$c$) as a function of $\delta\phi$ in both $p$$+$$p$ and in $d$+au collisions at $\sqrt{s_{_{nn}}}=200$ gev. comparing the $p$$+$$p$ results with several different models, we find the results are consistent with a total charm cross section $\sigma_{c\bar{c}} =$ 538 $\pm$ 46 (stat) $\pm$ 197 (data syst) $\pm$ 174 (model syst) $\mu$b. these generators also indicate that the back-to-back peak at $\delta\phi = \pi$ is dominantly from the leading order contributions (gluon fusion), while higher order processes (flavor excitation and gluon splitting) contribute to the yield at all $\delta\phi$. we observe a suppression in the pair yield per collision in $d$+au. we find the pair yield suppression factor for $2.7&lt;\delta\phi&lt;3.2$ rad is $j_{da}$ = 0.433 $\pm$ 0.087 (stat) $\pm$ 0.135 (syst), indicating cold nuclear matter modification of $c\bar{c}$ pairs.
overview of mhz air shower radio experiments and results. in this paper, i present a review of the main results obtained in the last 10 years in the field of radio-detection of cosmic-ray air showers in the mhz range. all results from all experiments cannot be reported here so that i will focus on the results more than on the experiments themselves. modern experiments started in 2003 with codalema and lopes. in 2006, small-size autonomous prototypes setup were installed at the pierre auger observatory site, to help the design of the auger engineering radio array (aera). we will discuss the principal aspects of the radio data analysis and the determination of the primary cosmic ray characteristics: the arrival direction, the lateral distribution of the electric field, the correlation with the primary energy, the emission mechanisms and the sensitivity to the composition of the cosmic rays.
impact of water radiolysis on uranium dioxide corrosion. null
radiolytic corrosion of grain boundaries onto the uo2 triso particle surface. this work is dealing with the understandingof the corrosion mechanisms at solid/solutioninterface and taking into account for the4he2+ions irradiation effects on these mechanisms.these corrosion and4he2+ions radiolysis phenomena append at solid/solution interface andwill be studied at a μmetric scale by the raman spectroscopy. moreover, a4he2+ionsirradiation affects a small low volume and allowsus to control the irradiated area (solution,solid or interface). for the solid,the chemical species induced by4he2+ions radiolysis ofwater are reactive and are involved inclassical corrosion mechanisms of uo2. moreover, wewant to study the impact of the4he2+ions radiolysis of waterlayers physisorbed into thesurface onto corrosion mechanisms. that is thereason why we want touse a local irradiation,allowed by the4he2+ions ion beam provided by thearronax cyclotron (e = 64.7 mev).in this work an experimental apparatus will be performed in order to characterizesolid/solution interface at μmetric scale by raman spectroscopy under4he2+ions irradiationprovided by the cyclotron arronax facility.the leaching experiments under irradiationwill be performed for a short time in order tostudy the parameters during the fast instantrelease step. the grain boundaries effect will be studied by the comparison between onetriso particles set (solids with grain boundaries) and one triso particles set previouslywashed by one acid solution (solid without grain boundaries). the role of h2will be studiedby the comparison between experiments under ar or ar/h2atmosphere. the dose rate rangewill be between 0 and 100 gy/min by using the alpha ion beam which let us control the doseset down into the sample. for all these experiments, measurements will be performed by thein situraman spectroscopy during the irradiation in order to follow theformation/consumption of the secondary phases formed onto the solid. the sem will beperformed in order to characterize the grain boundaries and the secondary phases formed bythe leaching/irradiation experiments.the μgc is used to measure the ph2into the irradiationcell to follow the production/consumption of this gaseous species formed by the waterradiolysis and consumedby the leaching process.
role of bacteria on the release of cesium from illite. null
drifting tracers behavior into the groundwater of a french nuclear waste disposal. null
transport coefficients from the nambu-jona-lasinio model for su(3)f. we calculate the shear η(t) and bulk viscosities ζ(t) as well as the electric conductivity σe(t) and heat conductivity κ(t) within the nambu-jona-lasinio model for three flavors as a function of temperature as well as the entropy density s(t), pressure p(t), and speed of sound cs2(t). we compare the results with other models such as the polyakov-nambu-jona-lasinio (pnjl) model and the dynamical quasiparticle model (dqpm) and confront these results with lattice qcd data whenever available. we find the njl model to have a limited predictive power for the thermodynamic variables and various transport coefficients above the critical temperature, whereas the pnjl model and dqpm show acceptable results for the quantities of interest.
supply chain design and distribution planning under supply uncertainty : application to bulk liquid gas distribution. the distribution of liquid gazes (or cryogenic liquids) using bulks and tractors is a particular aspect of a fret distribution supply chain. traditionally, these optimisation problems are treated under certainty assumptions. however, a large part of real world optimisation problems are subject to significant uncertainties due to noisy, approximated or unknown objective functions, data and/or environment parameters. in this research we investigate both robust and stochastic solutions. we study both an inventory routing problem (irp) and a production planning and customer allocation problem. thus, we present a robust methodology with an advanced scenario generation methodology. we show that with minimal cost increase, we can significantly reduce the impact of the outage on the supply chain. we also show how the solution generation used in this method can also be applied to the deterministic version of the problem to create an efficient grasp and significantly improve the results of the existing algorithm. the production planning and customer allocation problem aims at making tactical decisions over a longer time horizon. we propose a single-period, two-stage stochastic model, where the first stage decisions represent the initial decisions taken for the entire period, and the second stage representing the recovery decision taken after an outage. we aim at making a tool that can be used both for decision making and supply chain analysis. therefore, we not only present the optimized solution, but also key performance indicators. we show on multiple real-life test cases that it isoften possible to find solutions where a plant outage has only a minimal impact.
cad-based approach for identification of elasto-static parameters of robotic manipulators. the paper presents an approach for the identification of elasto-static parameters of a robotic manipulator using the virtual experiments in a cad environment. it is based on the numerical processing of the data extracted from the finite element analysis results, which are obtained for isolated manipulator links. this approach allows to obtain the desired stiffness matrices taking into account the complex shape of the links, couplings between rotational/translational deflections and particularities of the joints connecting adjacent links. these matrices are integral parts of the manipulator lumped stiffness model that are widely used in robotics due to its high computational efficiency. to improve the identification accuracy, recommendations for optimal settings of the virtual experiments are given, as well as relevant statistical processing techniques are proposed. efficiency of the developed approach is confirmed by a simulation study that shows that the accuracy in evaluating the stiffness matrix elements is about 0.1%.
identification of geometrical and elastostatic parameters of heavy industrial robots. the paper focuses on the stiffness modeling of heavy industrial robots with gravity compensators. the main attention is paid to the identification of geometrical and elastostatic parameters and calibration accuracy. to reduce impact of the measurement errors, the set of manipulator configurations for calibration experiments is optimized with respect to the proposed performance measure related to the end-effector position accuracy. experimental results are presented that illustrate the advantages of the developed technique.
stiffness modeling of robotic manipulator with gravity compensator. the paper focuses on the stiffness modeling of robotic manipulators with gravity compensators. the main attention is paid to the development of the stiffness model of a spring-based compensator located between sequential links of a serial structure. the derived model allows us to describe the compensator as an equivalent non-linear virtual spring integrated in the corresponding actuated joint. the obtained results have been efficiently applied to the stiffness modeling of a heavy industrial robot of the kuka family.
practical identifiability of the manipulator link stiffness parameters. the paper addresses a problem of the manipulator stiffness modeling, which is extremely important for the precise manufacturing of contemporary aeronautic materials where the machining force causes significant compliance errors in the robot end-effector position. the main contributions are in the area of the elastostatic parameters identification. particular attention is paid to the practical identifiability of the model parameters, which completely differs from the theoretical one that relies on the rank of the observation matrix only, without taking into account essential differences in the model parameter magnitudes and the measurement noise impact. this problem is relatively new in robotics and essentially differs from that arising in geometrical calibration. to solve the problem, several physical and statistical model reduction methods are proposed. they are based on the stiffness matrix sparseness taking into account the physical properties of the manipulator elements and also on the heuristic selection of the practically non-identifiable parameters that employs numerical analyses of the parameter estimates. the advantages of the developed approach are illustrated by an application example that deals with the stiffness modeling of an industrial robot used in aerospace industry.
advanced robot calibration using partial pose measurements. the paper focuses on the calibration of serial industrial robots using partial pose measurements. in contrast to other works, the developed advanced robot calibration technique is suitable for geometrical and elastostatic calibration. the main attention is paid to the model parameters identification accuracy. to reduce the impact of measurement errors, it is proposed to use directly position measurements of several points instead of computing orientation of the end-effector. the proposed approach allows us to avoid the problem of non-homogeneity of the least-square objective, which arises in the classical identification technique with the full-pose information. the developed technique does not require any normalization and can be efficiently applied both for geometric and elastostatic identification. the advantages of a new approach are confirmed by comparison analysis that deals with the efficiency evaluation of different identification strategies. the obtained results have been successfully applied to the elastostatic parameters identification of the industrial robot employed in a machining work-cell for aerospace industry.
robust algorithm for calibration of robotic manipulator model. the paper focuses on the robust identification of geometrical and elastostatic parameters of robotic manipulator. the main attention is paid to the efficiency improvement of the identification algorithm. to increase the identification accuracy, it is proposed to apply the weighted least square technique that employs a new algorithm for assigning of the weighting coefficients. the latter allows taking into account variation of the measurement system precision in different directions and throughout the robot workspace. the advantages of the proposed approach are illustrated by an application example that deals with the elasto-static calibration of industrial robot.
efficiency improvement of measurement pose selection techniques in robot calibration. the paper deals with the design of experiments for manipulator geometric and elastostatic calibration based on the test-pose approach. the main attention is paid to the efficiency improvement of numerical techniques employed in the selection of optimal measurement poses for calibration experiments. the advantages of the developed technique are illustrated by simulation examples that deal with the geometric calibration of the industrial robot of serial architecture.
modelling of the gravity compensators in robotic manufacturing cells. the paper deals with the modeling and identification of the gravity compensators used in heavy industrial robots. the main attention is paid to the geometrical parameters identification and calibration accuracy. to reduce impact of the measurement errors, the design of calibration experiments is used. the advantages of the developed technique are illustrated by experimental results.
pierre auger observatory and telescope array: joint contributions to the 33rd international cosmic ray conference (icrc 2013). joint contributions of the pierre auger and telescope array collaborations to the 33rd international cosmic ray conference, rio de janeiro, brazil, july 2013: cross-calibration of the fluorescence telescopes, large scale anisotropies and mass composition.
the substitution principle in an object-oriented framework for web services: from failure to success. nowadays, services are more and more implemented by using object-oriented frameworks. in this context, two properties could be particularly required in the specification of these frameworks: (i) a loose coupling between the service layer and the object layer, allowing evolution of the service layer with a minimal impact on the object layer, (ii) an interoperability induced by the substitution principle associated to subtyping in the object layer, allowing to freely convert a value of a subtype into a supertype. however, experimenting with the popular cxf framework, we observed some undesirable coupling and interoperability issues, due to the failure of the substitution principle. therefore we propose a new specification of the data binding used to translate data between the object and service layers. we show that if the cxf framework followed the specification, then the substitution principle would be recovered, with all its advantages.
proceedings of the 1st international symposium on non-photorealistic animation and rendering. null
enabling large-scale testing of iaas cloud platforms on the grid'5000 testbed. almost ten years after its premises, the grid'5000 platform has become one of the most complete testbeds for designing or evaluating large-scale distributed systems. initially dedicated to the study of high performance computing, the infrastructure has evolved to address wider concerns related to desktop computing, the internet of services and more recently the cloud computing paradigm. in this paper, we present the latest mechanisms we designed to enable the automated deployment of the major open-source iaas cloudkits (i.e., nimbus, opennebula, cloudstack, and openstack) on grid'5000. providing automatic, isolated and reproducible deployments of cloud environments lets end-users study and compare each solution or simply leverage one of them to perform higher-level cloud experiments (such as investigating map/reduce frameworks or applications).
a strain-specific segment of the rna-dependent rna polymerase of grapevine fanleaf virus determines symptoms in nicotiana species. null
certified calibration of a cable-driven robot using interval contractor programming. in this paper, an interval based approach is proposed to rigorously identify the model parameters of a parallel cable-driven robot. the studied manipulator follows a parallel architecture having 8 cables to control the 6 dofs of its mobile platform. this robot is complex to model, mainly due to the cable behavior. to simplify it, some hypotheses on cable properties (no mass and no elasticity) are done.an interval approach can take into account the maximal error between this model and the real one. this allows us to work with a simplified although guaranteed interval model. in addition, a specific interval operator makes it possible to manage outliers. a complete experiment validates our method for robot parameter certified identification and leads to interesting observations.
dirac operator on complex manifolds and supersymmetric quantum mechanics. we explore a simple n=2 supersymmetric quantum mechanics (sqm) model describing the motion over complex manifolds in external gauge fields. the nilpotent supercharge q of the model can be interpreted as a (twisted) exterior holomorphic derivative, such that the model realizes the twisted dolbeault complex. the sum q+barq can be interpreted as the dirac operator: the standard dirac operator if the manifold is kähler and the dirac operator involving certain particular extra torsions for a generic complex manifold. focusing on the kähler case, we give new simple physical proofs of the two mathematical facts: (i) the equivalence of the twisted dirac and twisted dolbeault complexes and (ii) the atiyah-singer theorem.
vapor hydration of son68 glass from 90 degrees c to 200 degrees c: a kinetic study and corrosion products investigation. corrosion of nuclear waste glass in unsaturated conditions is expected to occur upon the closure of the repository galleries during disposal cell saturation in the proposed french disposal site. the objectives of the present work were to determine the alteration kinetics of the son68 reference in such conditions. vapor hydration tests were conducted using thin, polished son68 glass coupons contained in stainless steel autoclaves. temperatures ranged between 90 °c and 200 °c and the relative humidity (rh) was maintained at 91 ± 1%. additional experiments at 175 °c and 80, 85, 90 and 95% rh were also conducted to assess the role of rh on the glass corrosion rate. the nature and extent of corrosion have been determined by characterizing the reacted glass surface with scanning electron microscopy (sem), transmission electron microscopy (tem), and energy dispersive x-ray spectroscopy (eds). elemental profiling of the glass hydrated at 90 °c was studied by tof-sims. the chemical composition of the external layer depends on experimental conditions. the hydration rate at 90 °c (tof-sims analysis) is 10 × higher than the generally accepted final rate of son68 in water at 90 °c (~ 10− 4 g m− 2 d− 1). this may indicate that the glass hydration process cannot be simulated by experiments in aqueous solution with a high s/v ratio. subsequent leaching (corrosion in an aqueous solution) of samples weathered in water vapor showed dissolution rate values higher than those of pristine glass. this result indicates that mobile elements are trapped within the alteration products during the hydration step and it gives insight into mobility variations of the considered elements.
combining finite and continuous solvers towards a simpler solver maintenance. combining efficiency with reliability within cp systems is one of the main concerns of cp developers. this paper presents a simple and efficient way to connect choco and ibex, two cp solvers respectively specialised on finite and continuous domains. this enables to take advantage of the most recent advances of the continuous community within choco while saving development and maintenance resources, hence ensuring a better software quality.
the waveform digitiser of the double chooz experiment: performance and quantisation effects on photomultiplier tube signals. we present the waveform digitiser used in the double chooz experiment. we describe the hardware and the custom-built firmware specifically developed for the experiment. the performance of the device is tested with regards to digitising low light level signals from photomultiplier tubes and measuring pulse charge. this highlights the role of quantisation effects and leads to some general recommendations on the design and use of waveform digitisers.
toward the autonomous radio-detection of ultra high energy cosmic rays with codalema. the codalema experiment aims to study the radio-detection of ultra high energy cosmic rays in the energy range of 1017 ev. spread over an area of 0.25 km2, the original device hosted at nançay (france) has mainly benefited of an array of short dipoles, connected by cables up to a centralized acquisition room. since 2010, a major evolution has been initiated to add 60 autonomous radio-detection stations, covering a surface of 1.5 km2. this enlarged configuration should help refine the studies and serve as a bench test for the mastery of autonomous detection. the main characteristics of this new mode of operation is presented in the light of recent results obtained by the original codalema setup.
net-baryon-, net-proton-, and net-charge kurtosis in heavy-ion collisions within a relativistic transport approach. we explore the potential of net-baryon, net-proton and net-charge kurtosis measurements to investigate the properties of hot and dense matter created in relativistic heavy-ion collisions. contrary to calculations in a grand-canonical ensemble we explicitly take into account exact electric and baryon charge conservation on an event-by-event basis. this drastically limits the width of baryon fluctuations. a simple model to account for this is to assume a grand-canonical distribution with a sharp cut-off at the tails. we present baseline predictions of the energy dependence of the net-baryon, net-proton and net-charge kurtosis for central (b≤2.75 fm) pb+pb/au+au collisions from e lab=2a gev to snn−−−√=200 gev from the urqmd model. while the net-charge kurtosis is compatible with values around zero, the net-baryon number decreases to large negative values with decreasing beam energy. the net-proton kurtosis becomes only slightly negative for low snn−−−√ .
adsorption of several actinide (th-u) and lanthanide (la-eu-yb) ions by myobacterium smegmatis. adsorption measurements of several actinide [thorium (th), uranium (u)] and lanthanide [lanthanum (la), europium (eu), ytterbium (yb)] cations by mycobacterium smegmatis showed that sorption kinetics followed a three-phase pattern. for 5% (w/w) bacterial suspensions at ph 1, maximum cation biosorption per gram dry biomass corresponded to 170 mumol th4+ and 187 mumol uo22+. adsorption of all cations studied obeyed the brunauer-emmett-teller isotherm, which assumes multilayer binding at constant energy. plots for the scatchard model showed the existence of at least two types of cation complexation site, with strong and weak affinity and negative cooperation. th4+ was preferentially adsorbed with respect to the other cations, although all species appeared to compete for the same sites independently of bacterial viability. adsorption of these cations was accompanied by partial release of magnesium from the cell wall, indicating that exchange reactions occurred at magnesium (mg)-bonding sites.
selective biosorption of thorium ions by an immobilized mycobacterial biomass. null
selective biosorption of lanthanide (la, eu) ions by mycobacterium smegmatis. the ability of mycobacterium smegmatis to adsorb lanthanide (lanthanum (la), europium (eu)) cations from aqueous solutions was studied. adsorption isotherms of lanthanum ions showed that the adsorption capacity was higher for wet biomass than for dry biomasses (37 degrees c and 70 degrees c). the biosorption capacity on cells dried at 70 degrees c is reduced. potentiometric titrations revealed the existence of at least two types of acidic functions in the cell wall, with strong and weak affinity. the weak acidic groups became inaccessible at a drying temperature of 70 degrees c. lanthanum and europium adsorption by mycobacterium smegmatis obeyed the freundlich and langmuir isotherm relations. eu3+ was preferentially adsorbed with respect to the lanthanum cations. the extended langmuir equation enabled a preliminary theoretical approach of the multicomponent adsorption phenomenon.
m+/h+ ion exchange behavior of the phosphoantimonic acids hnsbnp2o3n+5 center dot xh(2)o (n = 1,3) for m = cs and other alkali metal ions. the sorptions of cesium and of other alkali metal ions have been studied by batch techniques; the data are well fitted using the langmuir equation. the values of the various thermodynamic parameters associated to the ion exchange are reported (free energies, enthalpies, entropies, distribution coefficients kd, selectivity coefficients kc) and discussed; the ion selectivity, at infinite exchange on hsbp2o8 (called h-1) and on h3sb3p2o14 (called h-3) varies according to the sequence cs &gt; rb &gt; k &gt; na. the very negative values of delta g(cs/h)degrees, and delta g(rb/h)degrees are indicative of a preferential adsorption of cs+ and rb+ higher in h-3 than in h, by about one order of magnitude at low concentration level. the selectivity coefficients for cs+, rb+ on h-1 vary linearly with the fractional exchange (x) over bar(m) in solid (kielland plot) suggesting a unique site of exchange. the kielland plot for h-3 can be described in terms of a multisite ion exchange model (two sites) in agreement with the number of sites in the crystalline structure of k-3 (homologous compound). radiotracers have been used to estimate the parameters kd at low concentration levels. the results are discussed on the basis of the thermodynamic data together with the structures. (c) 1998 elsevier science b.v. all rights reserved.
selective biosorption of lanthanide (la, eu, yb) ions by pseudomonas aeruginosa. the ability of pseudomonas aeruginosa to adsorb selectively la3+, eu3+, and yb3+ from aqueous solution was investigated. the lanthanide biosorption equilibrium obeyed the brunauer-emmett-teller isotherm model, indicating multilayer adsorption. determined levels of maximum adsorption capacities were 397 mu mol/g far lanthanum, 290 mu mol/g for europium and 326 mu mol/g for ytterbium (+/-10%). the results indicated that there were about 100 preferential sites for lanthanum per g of dry biomass. experiments with mixed-cation solutions showed that the sequence of preferential biosorption was eu3+ = yb3+ &gt; la3+. biomasses dried at 37 and 70 degrees c showed the same selective behavior as wet biomass. inert microbial biomass dried at 37 degrees c appeared to be the most efficient farm for experimental use. the uptake of lanthanide by p. aeruginosa cells was not affected by the presence of sodium, potassium, calcium, chloride, sulfate and nitrate ions. aluminum was a strong inhibitor of lanthanide ions biosorption. 87% of the total al3+ was removed from the 3 mm solution, whereas only 8%, 20% and 3% of the total la3+, eu3+, and yb3+, respectively, were sorbed from 3 mm solutions. the results suggested that cells of pseudomonas aeruginosa may find promising applications for removal and separation of lanthanide ions from aqueous effluents.
selective biosorption of lanthanide (la, eu, yb) ions by an immobilized bacterial biomass. the removal of metallic ions la3+, eu3+ and yb3+ from aqueous solution by immobilized biomass of pseudomonas aeruginosa was investigated in batch and column reactors. batch studies consisted in kinetic measurements for lanthanum adsorption by biomass-chitosan beads. results did not show a significant effect of the presence of bacteria into chitosan matrix on the lanthanum uptake. then, laboratory scale fixed-bed column experiments were carried out using biomass-entrapped polyacrylamide gel beads, which contained approximately 48% (dry weight basis) of biomass. the lanthanum sorption was dependent on the superficial liquid velocity based on empty column in the range 0.76-2.29 m.h(-1). the removal of lanthan decations (2 mm) at ph 5.0 and 0.76 m/h was 198 mu mol.g(-1) (dry biomass) for lanthanum, 167 mu mol.g(-1) for europium and 192 mu mol.g(-1) for ytterbium (+/-10%). these results are of the order of 1.7-2 times lower than those observed in batch systems with free bacterial cells. column experiments with mixed-cation solutions showed the following sequence of preferential biosorption: eu3+ &gt; yb3+ &gt; la3+.
characterization of lanthanide ions binding sites in the cell wall of pseudomonas aeruginosa. earlier studies have shown that pseudomonas aeruginosa can adsorb selectively la3+, eu3+, and yb3+ from aqueous solution. these bacterial cells may find promising applications for removal and separation of lanthanide ions from contaminated effluents. in this work, potentiometric titrations and time-resolved laser-induced fluorescence spectroscopy were used to determine the binding sites of the biomass and, consequently, to elucidate the underlying mechanisms involved in the biosorption of lanthanide ions. around 90 +/- 5% of the adsorbed lanthanum was easily desorbed with an edta 0.1 m solution. in most instances, lanthanides seemed to concentrate extracellularly. the diversity of potential metal-binding groups was revealed by potentiometric titrations of the biomass. the amount of strong and weaker acidic functional groups in the wet biomass was estimated at 0.24 +/- 0.04 acid 0.86 +/- 0.02 mequiv/g, respectively. time-resolved laser-induced fluorescence spectroscopy on europium-loaded p. aeruginosa biomass suggests that europium binding occurs mostly through carboxyl and phosphate groups.
factors influencing the biosorption of gadolinium by micro-organisms and its mobilisation from sand. the present work was devoted to the study of the biosorption capacities of various microbial species (bacillus subtilis, pseudomonas aeruginosa, ralstonia metallidurans ch34 previously alcaligenes eutrophus ch34, mycobacterium smegmatis, saccharomyces cerevisiae) for ions of the lanthanide gadolinium (gd3+). the uptake by sand of this element was also measured. saturation curves and scatchard models were established for all biosorbants used in this work. the results enabled us to determine the binding affinities and the maximum capacities for biosorption of gd3+, which ranged from 350 micromol g(-1) for b. subtilis to 5.1 micromol g(-1) for s. cerevisiae. this study demonstrated the usefulness of optimisation of experimental conditions in biosorption investigations. experimental results showed that biosorption could be influenced by the growth stage and by the composition of the growth medium of microbial cells. finally, particular attention was given to the transfer of gadolinium ions from a loaded sand to a bacterial suspension.
removal of metal ions from aqueous solution on low cost natural polysaccharides - sorption mechanism approach. the fixation of cu2+, ni2+ and pb2+ on sugar beer pulp, a low-cost material, has been studied. after a simple treatment of the sorbent, the results have shown that the mode of fixation depends on the metal. the key part of the mode of fixation is attributed to ion exchange, while a non-negligible part of adsorption could sometimes occur. as far as lead ions are concerned, 25% of their fixation capacities are due to adsorption mechanism at ph 4. the overall uptake is at a maximum at ph 6 and gives up to 60 mg g(-1) for pb2+, 30 mg g(-1) for cu2+ and 12 mg g(-1) for ni2+, which seems to be removed exclusively by ion exchange. these capacities increase with the ph. diffusion models have been tested on samples of different particle size, and they have suggested a transfer to the intraparticular sites. (c) 2000 elsevier science b.v. all rights reserved.
contribution of biosorption to the behavior of radionuclides in the environment. the biosorption capacities for cesium, technetium, uranium and nickel was determined. various microbial strains, ph and mineral composition of medium (clay and concrete waters) were tested. for the cesium a column experiment with a biofilm was realised.
natural radioactivity in phosphates, phosphogypsum and natural waters in morocco. the contents of natural radionuclides (uranium. actinium and thorium series) were measured in sedimentary phosphate rock samples using high-resolution gamma spectrometry. data obtained for uranium content (ppm) were compared with the results obtained by a method based on the measurements using solid-state nuclear track detectors (ssntd) in the same samples. the potential leaching of radionuclides from sedimentary phosphate rock during the industrial production of the phosphoric acid was studied. the process of leaching of the radioisotopes from phosphogypsum was discussed. a method for the direct alpha counting of ra-226 thin source. elaborated by the deposition of ra from aqueous solutions on manganese oxides film deposited on polyvinyl support, have been developed and applied for the determination of ra-226 in natural water samples. the results show that only the water sample from the mine area reveals the presence of ra-226 at a level of about 0.2 bql(-1) (c) 2001 elsevier science ltd. all rights reserved.
sorption of selenite in a multi-component system using the "dialysis membrane" method. abstract 79se is a potentially mobile long-lived fission product, which may make a dominant contribution to the long-term radiation exposure resulting from deep geological disposal of radioactive waste. its mobility is affected by sorption on minerals. selenium sorption processes have been studied mainly by considering interaction with a single mineral surface. in the case of multi-component systems (e.g. soils), it is difficult to predict the radioelement behaviour only from the mineral constituents. this study contributes to the understanding of multi-component controls of se concentrations towards predicting se behaviour in soils after migration from a disposal site. this goal was approached by measuring selenite sorption on mono and multi-phase systems physically separated by dialysis membranes. to the best of the authors' knowledge, very few studies have used dialysis membranes to study the sorption competition of selenite between several mineral phases. other workers have used this method to study the sorption of pesticides on montmorillonite in the presence of dissolved organic matter. indeed, this method allows measurement of individual kd in a system composed of several mineral phases. dialysis membranes allowed (i) determination of the competition of two mineral phases for selenite sorption (ii) and determination of the role of humic acids (has) on selenite sorption in oxidising conditions. experimental results at ph 7.0 show an average se(iv) sorption distribution coefficient (kd) of approximately 125 and 9410 l kg−1 for bentonite and goethite, respectively. the average kd for goethite decreases to 613 l kg−1 or 3215 l kg−1 in the presence of bentonite or ha, respectively. for bentonite, the average kd decreases slightly in the presence of goethite (60 l kg−1) and remains unchanged in the presence of ha. the experimental data were successfully modelled with a surface complexation model using the phreeqc geochemical code. the drastic decrease in se(iv) sorption on goethite in a multi-phase system is attributed to competition with dissolved silica released by bentonite. as with si the ha compete with se for sorption sites on goethite.
hybrid materials for catalysis? design of new phosphonate-based supported catalysts for the hydrogenation of ketones under hydrogen pressure. supported rhodium. and iridium. 2,2 ' -bipyridine complexes were prepared for the hydrogenation of aromatic ketones under hydrogen pressure. the key feature of the immobilization process is the functionalization of the 2,2 ' -bipyridine unit with two phosphonic acid moieties, thus allowing the covalent grafting of the catalytic complex onto in situ generated titanium oxide particles. a very good catalytic activity is observed with the resulting materials that compares well with the homogeneous parent system and gives evidence that the major part of the catalytic sites are readily accessible. moreover, the catalyst can be reused, and no significant rhodium. leaching is observed.
determination of radium-226 in aqueous solutions by alpha-spectrometry. the new european legislation imposes a lower threshold for radioactivity in drinking water.. this requires the development of more sensitive and reliable analytical methods. this work presents an improved a-spectrometric technique to determine the radium-226 activity in aqueous solution relying on the radium adsorption onto a thin manganese oxide layer followed by a-measurement. the preparation of the mno2 deposit has been optimized as well as the radium adsorption conditions. detection threshold and limit of 5 and 10 mbq .l-1, respectively, with a 10% (95% confidence) uncertainty are currently reached. this paper reports on the overall technique and on its application to assess the radium-226 activity in 28 french mineral waters. in addition, the gross alpha- and beta -activities have been evaluated using proportional counting while the uranium concentrations were derived from icpms.
comparison of the fixation of several metal ions onto a low-cost biopolymer. in the present work, sugar beet pulp, a common waste from the sugar refining industry, was studied in the removal of metal ions from aqueous solutions. the ability of this cheap biopolymer to sorb several metals namely pb2+, cu2+, zn2+, cd2+ and ni2+ in aqueous solutions was investigated. the metal fixation capacities of the sorbent were determined according to operating conditions and the fixation mechanisms were identified. the biopolymer has shown high elimination rates and interesting metal fixation capacities. a pseudo-second-order kinetic model was tested to investigate the adsorption mechanisms. the kinetic parameters of the model were calculated and discussed. for 8 x 10(-4) m initial metal concentration, the initial sorption rates (v(0)) ranged from 0.063 mmol.g(-1).min(-1) 1 for pb2+ to 0.275 mmol.g(-1). min(-1) for ni2+ ions, with the order: ni2+ &gt; cd2+ &gt; zn2+ &gt; cu2+ &gt; pb2+. the equilibrium data fitted well with the langmuir model and showed the following affinity order of the material: pb2+ &gt; cu2+ &gt; zn2+ &gt; cd2+ &gt; ni2+. then, the kinetic and equilibrium parameters calculated q(m) and v(0) were tentatively correlated to the properties of the metals. finally, equilibrium experiments in multimetallic systems were performed to study the competition of the fixation of pb2+, zn2+ and ni2+ cations. in all cases, the metal fixation onto the biopolymer was found to be favourable in multicomponent systems. based on these results, it is demonstrated that this biosorbent represents a low-cost solution for the treatment of metal-polluted wastewaters.
study of the ion exchange selectivity of layered titanosilicate na-3(na,h)ti2o2[si2o6](2) center dot 2h(2)o, am-4, for strontium. this paper describes for the first time the sorption behaviour of a titanosilicate, am-4, that exhibits an extremely high affinity for strontium in neutral and alkaline media. the ion exchange selectivity of this layered titanosilicate is explained in relation to the crystalline structure. the simple approach which consists in studying carefully the distribution of formal charges of oxygens in the anionic framework and calculating a normalized bond length in order to measure the ion stability in its site has enabled us to account for the am-4 performances for the strontium remediation in competition with alkali and alkaline earth metal ions, the sr uptake amounts as a function of the ph were determined, they can be correlated to the formal charges of si-o-nbo. the effect of sodium, potassium, calcium and magnesium on the sr2+ ions sorption was also studied in binary solutions sr2+/na+, sr2+/k+, sr2+/ca2+, sr2+/mg2+. the determination of the equilibrium constant sr2+/2na(+) and of the ion competitive effects with adsorption data have shown that ca2+ is the most efficient to reduce the uptake of sr2+. the preliminary kd values provide an indication that this exchanger may act as a sorber for groundwater and nuclear remediation applications.
fixed-bed study for lanthanide (la, eu, yb) ions removal from aqueous solutions by immobilized pseudomonas aeruginosa: experimental data and modelization. a fixed-bed study was carried out by using cells of pseudomonas aeruginosa immobilized in polyacrylamide gel as a biosorbent for the removal of lanthanide (la, eu, yb) ions from aqueous solutions. the effects of superficial liquid velocity based on empty column, particle size, influent concentration and bed depth on the lanthanum breakthrough curves were investigated. immobilized biomass effectively removed lanthanum from a 6 mm solution with a maximum adsorption capacity of 342 mumolg(-1) (+/-10%) corresponding closely to that observed in earlier batch studies with free bacterial cells. the bohart and adams sorption model was employed to determine characteristic parameters useful for process design. results indicated that the immobilized cells of p. aeruginosa enable removal of lanthanum, europium and ytterbium ions from aqueous effluents with significant and similar maximum adsorption capacities. experiments with a mixed cation solution showed that the sequence of preferential biosorption was eu3+ greater than or equal to yb3+ &gt; la3+. around 96 +/- 4% of the bound lanthanum was desorbed from the column and concentrated by eluting with a 0.1 m edta solution. the feasibility of regenerating and reusing the biomass through three adsorption/desorption cycles was suggested. neural networks were used to model breakthrough curves performed in the dynamic process. the ability of this statistical tool to predict the breakthrough times was discussed. (c) 2002 elsevier science ltd. all rights reserved.
adsorption of several metal ions onto a low-cost biosorbent: kinetic and equilibrium studies. null
modeling of single and competitive metal adsorption onto a natural polysaccharide. sugar beet pulp, a common agricultural waste, was studied in the removal of metal ions from aqueous solutions. potentiometric titrations were used to characterize the surface acidity of the polysaccharide. the acid properties of the material can be described by invoking three distinct types of surface functional groups with the intrinsic acidity constants (pk(a)(int)) values 3.43 +/- 0.1, 6.05 +/- 0.05, and 7.89 +/- 0.1, respectively. the contents of each functional group (i.e., the carboxyl and phenol moieties) were also determined. then, a simple surface complexation model with the diffuse layer model successfully described the sorption of several metal ions (cu2+, zn2+, cd2+, and ni2+) onto the polysaccharide under various experimental,conditions: ph ranging from 2 to 5.5, ionic strength from 0.01 to 0.1 m, metal concentration between 10(-4) and 10(-3) m, for a constant sorbent concentration equal to 2.5 g.l-1. it was observed experimentally that the affinity of the polysaccharide was in the sequence of cu2+ &gt; zn2+ &gt; cd2+ &gt; ni2+. predictions of sorption in binary-metal systems based on single-metal data fits represented competitive sorption data reasonably well.
ni(ii) and cu(ii) binding properties of native and modified sugar beet pulp. kthe abilities of native and modified sugar beet pulps to remove ni(ii) and cu(ii) ions from aqueous solutions were compared. their preparation by chemical treatments (saponification, hot 0.05 m hcl and cold 0.05 m naoh extractions) is described. the sugar composition, which was strongly affected during these modifications, is discussed in terms of metal sorption efficiencies. ne influence of these modifications was also evaluated by comparing the content of the functional groups determined by potentiometric titration, and the rate and extent of cu(ii) and ni(ii) uptakes onto the raw and modified materials. nickel and copper sorptions were fast and complete within 30 min and the kinetic parameters were calculated using a second order model. the equilibrium data fitted well with the langmuir model and showed the affinity order of the materials for the metal ions. the base-extracted pulp and saponified pulp exhibited the highest ni(ii) and cu(h) ion uptakes among the materials tested. (c) 2002 elsevier science ltd. all rights reserved.
academic evaluation protocol for monitoring the usage modalities in automatic control laboratory: real vs. remote. this article describes an academic evaluation protocol (aep) that has been designed and implemented in order for monitoring different kinds of modalities that can be used to carry out an automatic control laboratory. for this proposed, these types of patterns lab-works have been related as: real laboratory (rl), remote laboratory (r@l) and real plus remote laboratory (rl+r@l). twelve (12) groups of students from a control course at the universidad de los andes have been selected to carry out the presented testing, at the same time that, they have been placed into the evaluated patterns lab-works trying to keep uniform the academic features of performance for each involved student-groups in order to reduce the uncertainly in the analysis of the results obtained from the applied aep. to estimate how a specific lab-work modality impact the development of an experimental practice, the parameter average utilization time and also the abet-indicators have been employed for this propose. the results from this pedagogical instrument are analyzed applying the anova test, a descriptive statistical technique and the wilcoxon testing, concluding that, the student-groups who developed the experimental practice on rl and rl+r@l modalities, perform better in the development of the automatic control laboratory than the student-groups who only work on the remote system.
a multi-user remote academic laboratory system. this article describes the development, implementation and preliminary operation assessment of multiuser network architecture to integrate a number of remote academic laboratories for educational purposes on automatic control. through the internet, real processes or physical experiments conducted at the control engineering laboratories of four universities are remotely operated. through an internet connection to the manager administration server, a remote experiment to design and test a modeling and control algorithm can be performed. the suggested network architecture is based on the singlet-server model and uses a database server to record important information that helps create a new remote experiment, including a graphical user interface (applet) developed with easy java simulation, which allows the simple integration of new processes to the manager administrator. results of a real-physical-process remote manipulation through the proposed network architecture are presented as well as results of an academic pilot test conducted to measure functional aspects related to the operation of the remote system when carrying out remote-laboratory work.
valid inequalities and dominance rules for speed-dating scheduling linear models. null
system-size dependence of open-heavy-flavor production in nucleus-nucleus collisions at $\sqrt{s_{_{nn}}}$=200 gev. the phenix collaboration at the relativistic heavy ion collider has measured open heavy flavor production in cu$+$cu collisions at $\sqrt{s_{_{nn}}}$=200 gev through the measurement of electrons at midrapidity that originate from semileptonic decays of charm and bottom hadrons. in peripheral cu$+$cu collisions an enhanced production of electrons is observed relative to $p$$+$$p$ collisions scaled by the number of binary collisions. in the transverse momentum range from 1 to 5 gev/$c$ the nuclear modification factor is $r_{aa}$$\sim$1.4. as the system size increases to more central cu$+$cu collisions, the enhancement gradually disappears and turns into a suppression. for $p_t&gt;3$ gev/$c$, the suppression reaches $r_{aa}$$\sim$0.8 in the most central collisions. the $p_t$ and centrality dependence of $r_{aa}$ in cu$+$cu collisions agree quantitatively with $r_{aa}$ in $d+$au and au$+$au collisions, if compared at similar number of participating nucleons $\langle n_{\rm part} \rangle$.
wastewater reuse in on-site wastewater treatment: bacteria and virus movement in unsaturated flow through sand filter. in on-site wastewater treatment plants, effluents are pre-treated by septic tank and treated by soil infiltration or sand filtration systems, with unsaturated flow conditions. these systems remove efficiently carbon, nitrogen and suspended solids. but for microbial pollution, the treatment efficiency depends on the hydrodynamic behaviour and filtering media characteristics. contamination of superficial water and groundwater due to pathogenic viruses and pathogenic bacteria is responsible for many diseases. the objective of this study is to approach the mechanisms and operating conditions to control bacteria and virus release in the environment. experiments were carried out on reactors of different length packed with sand. hydraulic load of 90 cm.d(-1) with a pulse periodic flow was used. the influence of chemical composition of the solution on the treatment efficiency has also been studied. for the first time, the residence time distribution (rtd) has been studied using a conservative tracer (ki), to determine the main hydrodynamic parameters. for the second time, the rtd with bacterial and viral tracers (e coli, bacteriophage ms2) was applied, with the aim to define microbial behaviour in filtering media, including adsorption and filtration phenomena. this work allowed us to determine retardation factors according to the hydraulic loads and chemical composition.
mechanisms of cr(iii) and cr(vi) removal from aqueous solutions by sugar beet pulp. the removal of three- and hexavalent chromidium from aqueous solutions using sugar beet pulp as biosorbent substrate was performed. the kinetics of cr(iii) and cr(vi) removal were studied at 20.0 +/- 0.5 degreesc and under various experimental conditions. the cr(iii) ions were adsorbed onto the biosorbent by ion-exchange mechanism with ca2+ cations. the influence of solution ph was found to greatly affect the adsorption efficiency of cr(vi). cr(vi) removal was largely involved in a reduction mechanism with the appearance of cr(iii) ions in the solution.
cadmium and lead adsorption by a natural polysaccharide in mf membrane reactor: experimental analysis and modelling. in the present work, ph2+ and cd2+ adsorption onto a natural polysaccharide has been studied in membrane reactors. the process involves a stirred semi-batch reactor for the adsorption step and a microfiltration (mf) process in order to confine the particles. due to their lower affinity for the biosorbent, cd2+ ions were found to breakthrough the process faster than pb2+ cations. the experimental results showed the technical feasibility of the pilot. a mass balance model based on the langmuir equilibrium isotherm was used to describe the adsorption process. this relation is able to predict experimental data under different operating conditions: the adsorbent and metal concentrations, and the permeate flow rate. based on these results, it is demonstrated that the biosorbent studied represents an interesting low-cost solution for the treatment of metal ions polluted waters. (c) 2003 elsevier science ltd. all rights reserved.
rare earth elements removal by microbial biosorption: a review. this paper reviews published work on the sorption of rare earth elements by microbial biomass. in a first part, the biosorption capacities and the various experimental conditions performed in batch reactor experiments are compared. secondly, sorption modelling generally used in biosorption studies are described. thirdly, the microbial cell wall characteristics of the metallic ion binding sites are considered. from these observations it seems that the important functional groups for metallic ion fixation are the carboxyl and the phosphate moieties. moreover, the competing effect of various ions like aluminium, iron, glutamate, sulphate etc. is described. finally, some adsorption results of the rare earth elements in dynamic reactors are presented.
formulation of biofiltration packing materials: essential nutriment release for the biological treatment of odorous emissions. biofiltration is a current biological gas treatment technology extensively used for the treatment of polluted air. the advantages of this cleaning technique are high - superficial area best-suited for poor water solubility compounds treatment, ease of operation and low operating costs. it involves a filter bed serving both as carrier for the microorganisms and as nutrient supplier. in this study, two types of material were formulated with an organic binder to compare two different nitrogen sources: ammonium phosphate or urea phosphate. the obtained supports were tested in terms of cohesion capacities in water, bulk density moisture retention capacity, dissolving rate of the mineral elements (carbonate, phosphorus, calcium, nitrogen) and for the ph regulation. the results show that the binder gives important cohesion capacities to the material even in drastic conditions (material submerged in water) and allows a low diffused release for different elements. therefore, formulated packing materials present interesting properties for biofiltration processes.
lead removal by a natural polysaccharide in membrane reactors. industrial wastewaters often contain heavy metal ions that are toxic to many living species. therefore, economic treatment methods are investigated, involving the sorption of metal ions onto wastes or natural materials. in the present work, the ability of sugar beet pulp, a common waste of the sugar industry, to remove pb2+ polluted waters is investigated. the kinetic and equilibrium experiments were performed in batch reactor in order to determine the pb2+ adsorption mechanisms onto the polysaccharide. the dynamic studies of pb2+ fixation onto the natural polysaccharide involve an adsorption reactor coupled with microfiltration membrane in order to confine the adsorbent particles. a mass balance model based on the langmuir equilibrium isotherm was used to describe the pb2+ breakthrough curves. this model successfully simulated the entire breakthrough curves whatever the operating conditions used. it provides a useful tool for process simulation and optimisation.
competitive adsorption of metals and organics onto a low cost natural polysaccharide. metals removal onto a low-cost natural polysaccharide is performed. firstly, some specific characteristics of adsorbent are determined. potentiometric titrations of the sorbent have been realised. the values of the point of zero net proton charge (pznpc) and the cation exchange capacity (cec) deduced from these experiments have given respectively ph=5 and 0.575 meq g(-1). secondly, sorption of cations from single metallic solution, have been performed and the equilibrium fixation capacities are 0.37 mmol g(-1) for pb2+, 0.28 mmol g(-1) for cu2+ and 0.2 mmol g(-1) for ni2+. in multi-metals solutions of equimolar concentration, ni2+ ions present the greatest fixation decrease in the presence of the two other cations (-61%), pb2+ and cu2+ seem to compete similarly. in the presence of a constant organic load composed of either benzaldehyde, benzoic acid or phenol and expressed as 100 mg l(-1) total organic carbon (toc), benzoic acid induces the largest reduction of the copper equilibrium fixation capacity (-30%).
interactions of natural aminated polymers with different species of arsenic at low concentrations: application in water treatment. in the present work, the interactions between the amine functionnal groups present in chitosan, a natural polysaccharide and different species of inorganic arsenic are studied. depending of the n-deacetylation rate, chitosan provides amine functions that could be protonated and shows interesting affinities to adsorb oxyanions of arsenic in solution. two species, arsenate (asv) and arsenite (asiii), have been tested at ph 5, and commercial chitosan and chitin were used. kinetics have been carried out at two initial concentrations (50 and 500 mu g/l) and different temperatures fixed between 4 to 40 degrees c. the results have shown the reaction is very fast, and consequently, the equilibrium times are short (30 min in the best case). experimental data are well fitted with a first order kinetic model. in a second part, isotherms have been performed with an as concentration range of 10 to 500 mu g/l and 0.5 g/l of biosorbent. maximum adsorption capacities, deduced from the langmuir model, range between 260 mu g/g at 40 degrees c and 730 mu g/g at 4 degrees c. finally the fixation mechanism could be described by an ion exchange reaction between the protonated amine moities of the chitosan and the arsenate anion in solution.
centrality, rapidity and transverse momentum dependence of j/psi suppression in pb-pb collisions at sqrt(snn)=2.76tev. the inclusive j/psi nuclear modification factor raa in pb-pb collisions at sqrt(snn)=2.76tev has been measured by alice as a function of centrality in the e+e- decay channel at mid-rapidity |y| &lt; 0.8 and as a function of centrality, transverse momentum and rapidity in the u+u- decay channel at forward-rapidity 2.5 &lt; y &lt; 4.the j/psi yields measured in pb-pb are suppressed compared to those in pp collisions scaled by the number of binary collisions.the raa integrated over a centrality range corresponding to 90% of the inelastic pb-pb cross section is 0.72 +- 0.06 (stat.) +- 0.10 (syst.) at mid-rapidity and 0.57 +- 0.01 (stat.) +- 0.09 (syst.) at forward-rapidity.at low transverse momentum, significantly larger values of raa are measured at forward-rapidity compared to measurements at lower energy.these features suggest that a contribution to the j/psi yield originates from charm quarks (re)combination in the deconfined partonic medium.
mineral resource assessment: compliance between emergy and exergy respecting odum's hierarchy concept. in this paper, authors suggest to combine the exergoecology and the emergy concept in order to evaluate mineral resources, taking into account their abundance, their chemical and physical properties and the impact of their extraction. the first proposition of this work is to consider that every group of mineral, dispersed in the earth's crust, is a co-product of the latter. the specific emergies of dispersed minerals are, then, inversely proportional to their abundance. the results comply with the material hierarchy as the specific emergy of a dispersed mineral rise with its scarcity. the second is an emergy evaluation model based on the chemical and concentration exergy of the mineral, its condition in the mine and its abundance. this model permits to assess the decline of mineral reserves and its impact on the ecosystem. the dispersed specific emergy of 42 main commercially used minerals has been calculated. furthermore, the emergy decrease of some australian mineral reserves has been studied, as well as the land degradation of us copper mines.
jets, bulk matter, and their interaction in heavy ion collisions at the lhc. null
charm and beauty searches using electron-d0 azimuthal correlations and microvertexing techniques in star experiment at rhic. null
status of the nucifer experiment. null
the neutron background of the xenon100 dark matter search experiment. null
effect of organic solvents on oxygen mass transfer in multiphase systems: application to bioreactors in environmental protection. the absorption of oxygen in aqueous-organic solvent emulsions was studied in a laboratory-scale bubble reactor at a constant gas flow rate. the organic and the gas phases were dispersed in the continuous aqueous phase. volumetric mass transfer coefficients (k(l)a) of oxygen between air and water were measured experimentally using a dynamic method. it was assumed that the gas phase contacts preferentially the water phase. it was found that addition of silicone oils hinders oxygen mass transfer compared to air-water systems whereas the addition of decane, hexadecane and perfluorocarbon pfc40 has no significant influence. by and large, the results show that, for experimental conditions (organic liquid hold-up &lt;= 10% and solubility ratio &lt;= 10), the k(l)a values of oxygen determined in binary air-water systems can be used for multiphase (gas-liquid-liquid) reactor design with applications in environmental protection (water and air treatment processes). (c) 2006 elsevier b.v. all rights reserved.
bacteria removal in septic effluent: influence of biofilm and protozoa. numerous biological, physical and chemical parameters are involved in the retention and removal of bacteria in wastewater treatment systems. biological parameters, such as biofilms and protozoa grazing activity, are often mentioned but few studies provide a better understanding of their influence. in this study, the effect of bacterivorous protozoa on pathogenic indicator bacteria removal was investigated in septic effluent and in the presence of a biofilm coating glass slides. endogenous bacteria from septic effluent were quantified. first, bacteria removal was compared between septic effluents treated or not with an inhibitor of protozoa (cycloheximide). the mortality rates were 10 times lower in treated effluent (96 cfu ml(-1) d(-1)) than in untreated effluent (1100 cfu ml(-1) d(-1)). secondly, the efficiency of bacteria removal was studied (i) with a biofilm surface and active protozoa, (ii) with a biofilm surface and inactivated protozoa, (iii) with a clean surface. protozoa in the presence of a biofilm were responsible for 60% of bacteria removal. biofilm without protozoa and a clean surface each removed similar quantities of bacteria. grazing by protozoa could be an important biological mechanism for bacterial elimination in wastewater treatment systems. (c) 2006 elsevier ltd. all rights reserved.
mass transfer coefficients of styrene and oxygen into silicone oil emulsions in a bubble reactor. the absorption of oxygen and styrene in water-silicone oil emulsions was independently studied in laboratory-scale bubble reactors at a constant gas flow rate for the whole range of emulsion compositions (0-10% v/v). the volumetric mass transfer coefficients to the emulsions were experimentally measured using a dynamic absorption method. it was assumed that the gas phase contacts preferentially the water phase. in the case of oxygen absorption, it was found that the addition of silicone oil hinders oxygen mass transfer compared to an air-water system. decreases in k(l)a(oxygen) of up to 25% were noted. such decreases in the oxygen mass transfer coefficient, which imply longer aeration times to transfer oxygen, could represent a limiting step in biotechnological processes strongly dependent on oxygen concentration. nevertheless, as the large affinity of silicone oil for oxygen enables greater amounts of oxygen to be transferred from the gas phase, it appears that the addition of more than 5% silicone oil should be beneficial to increase the oxygen transfer rate. in the case of styrene absorption, it was established that the volumetric mass transfer coefficient based on the emulsion volume is roughly constant with the increase in the emulsion composition. in spite of the relatively high cost of silicone oil, water-silicone oil emulsions remain relevant to treat low-solubility volatile organic compounds, such as styrene, in low-concentration gas streams. (c) 2006 elsevier ltd. all rights reserved.
wood bark as packing material in a biofilter used for air treatment. biotechnology has been applied to find green and low cost environmental processes. in the waste gas treatments (odours and volatile organic compounds voc) one of the main biological systems used is biofilters. this technology works at normal operating conditions of temperature and pressure, and therefore it is relatively cheap with high efficiencies when the waste gas is characterized by high flow and low pollutant concentration. the aim of this work is to use wood barks (pinus) as packing material in the biofilter. for this purpose, the influence of various parameters such as residence time of the gas and pollutant loads on removal efficiencies was studied for a biofilter pilot unit. ethanol, methyl ethyl ketone, dichloromethane and toluene were used as pollutant compounds, because they are representative of both volatile organic compounds. packing material stability and good biodegradation performances were found.
biofiltration of volatile organic compounds by a fluidized bed of sawdust. in this study, the use of a natural material, sawdust, in a fluidised biofiltrer has been considered. the performance of the biofiltration of ethanol and toluene was estimated in the presence of the native microorganisms of the material and also after the addition, and a period of acclimatization, of external microorganisms. modifications of the physical and biological characteristics of the material were studied in order to better understand the process. the influence of biofilter shutdown periods was also considered to evaluate the effect of a period of inactivity on subsequent performances. this study shows that a significant degradation of the pollutants is obtainable provided that the following steps are performed: seeding with activated sludge, introduction of nutrients, and control of the changes in the material characteristics and the bed moisture. during the operation of the fluidized bed biofilter, the moisture of the bed had an important effect on the biofilter performance, but was rather difficult to control because of its dependence on the ambient and inlet air temperatures, which changed during the day and the seasons. during the tests with the batch of sawdust particles used as delivered without any sludge enrichment, a reduction in abatement performances was measured with time. in the case of ethanol alone, for a concentration of 0.02 g.m-3, abatement decreased from 24% to 18% then to 7%. the partial or complete addition of sawdust particles previously activated with sludge significantly enhanced the performance of the biofilter, both for ethanol and toluene pollution. abatements of ethanol of 85% and 60% were achieved when the sawdust particles were activated by sludge. in contrast, a 5-week shutdown of the reactor produced a decrease in abatement, either by a loss of microorganism efficiency during their ''starvation'' or by their destruction.
antibacterial effects of chitosan powder: mechanisms of action. chitosan, the deacetylated derivative of chitin, is a natural d-glucosamine polymer that can be extracted from the shells of seafood such as prawns crabs and lobsters. it can be used as a flocculent, plant disease resistant promoter, anti-cancer agent, wound healing promotion agent and antimicrobial agent. the aim of this paper is the study of the interaction between chitosan powder and various kinds of pathogen microorganisms potentially present in water. first of all, physico-chemical characterisations of chitin and chitosan powder were performed. the deacetylation yields were 35 %, 60 % and 80 +/- 10 %. the experimental studies focused on the measurements of the mortality constant rate for various bacterial strains, escherichia coli, pseudomonas aeruginosa, enterococcus faecalis and staphylococcus saprophyticus. an explanation of the antibacterial mechanisms is proposed involving the cell wall disruption due to free amino groups present in chitosan.
sfgp 2007 - microbial growth onto filter media used in air treatment devices. this work deals with microbial growth onto filter media and focuses on the ability of microbial communities to proliferate onto filter media. two microorganism types are studied: microorganisms from activated sludge of wastewater treatment plant (sm) and a toluene specific consortium (tsc). the filter media considered for this study contain activated carbon fibres (acf), combined volatile organic compounds (voc), particles treatment purposes, activated carbon fibres felt (acff) and activated carbon and cellulose fibres felt (ac(2)f(2)). using a static growth procedure during 10 days under 100 % relative humidity, artificially contaminated filters are submitted to microbial colonisation. the final concentration of microorganisms per gram of filter have been assessed using a method developed in the lab, based on filter protein content assay. the average surface charge of inocula and filter's fibres are measured to assess the influence of microorganisms adhesion on contamination. the influence of soot particles on tsc proliferation onto ac(2)f(2) filter is then studied. zeta measures enable the assessment of the implication of soot in microorganisms adhesion onto filter fibres. consequences of microbial contamination on filter permeability and downstream particles released have then been assessed in a filtration device. results demonstrate a better resistance of ac(2)f(2) to microbial colonisation. however, sm have more difficulties to proliferate on acff than tsc, whereas sm colonise easier ac(2)f(2) than tsc. charge surface assay has defined an optimal electrostatic compatibility for tsc and ac(2)f(2) and a minimum for sm and acff. when soot is added to tsc solution before introduction in ac(2)f(2), high contamination shapes were observed whereas only a slight one occur without soot addition. zeta potential measures show favourable charge conditions for adhesion of soot on ac(2)f(2) fibres and tsc on soot particles. the soot may thus have played an interface role in microbial adhesion onto media. this means that electrostatic compatibility between particles is a good approach for assessing microbial adhesion onto filters but could not explain the whole mechanism of microbial proliferation. other parameters like nutrition preferences are certainly involved. the contamination have induced filter characteristics modification. permeability have decreased until 20 % with microbial concentration. colonised filters have released up to 450 microbial particles/cm3. such a release was only observed when ac(2)f(2) was contaminated with fungi using spore as the reproduction vector (inoculum containing tsc and soot).
packing material formulation for odorous emission biofiltration. in biological gas treatment, like biofiltration of volatile organic compounds or odorous substances, the microbial nutritional needs could be a key factor of the process. the aim of this work is to propose a new packing material able to provide the lacking nutrients. in the first part of this study, two kinds of material composed of calcium carbonate, an organic binder and two different nitrogen sources, 3 ammonium phosphate and urea phosphate (up), were compared. the new supports present bulk densities between 0.88 and 1.15 g cm(-3) moisture retention capacities close to 50% and 70%, and water cohesion capacities greater than six months for the material with 20% binder. in the second part, oxygen consumption measurements in liquid experiments show that these packing materials could enhance bacterial growth compared to pine bark or pozzolan and have no inhibitory effect. the biodegradation of different substrates (sodium sulfide and ammonia) and the support colonization by the biomass were evaluated. finally, up 20 was chosen and tested in a hydrogen sulfide or ammoniac biofiltration process. this showed that, for h2s concentrations greater than 100 mg m(-3), up 20 has a real advantage over pine bark or pozzolan. (c) 2067 elsevier ltd. all rights reserved.
biodegradation of endocrine disrupters: case of 17 beta-estradiol and bisphenol a. the biodegradation of 17 beta-estradiol (e2) and bisphenol a (bpa) was compared to that of a reference pollutant, sodium benzoate (sb), known for its high biodegradability. the biodegradation was measured using the sturm test (iso 9439 modified sturm test). the susceptibility of the target pollutants to be degraded by microorganisms of activated sludge from a wastewater treatment plant (wwtp) was evaluated by the production of carbon dioxide (co,). sorption experiments onto inactivated sludge were carried out to assess the contribution of sorption in e2 and bpa removal during biological treatment in a wwtp. e2 was more adsorbed than bpa onto inactivated sludge, probably making it less accessible to assimilation by microorganisms. in fact, e2 was less biodegradable than bpa with 66 % and 74 % of theoretical co2 formation (th-co2) in 28 days, respectively. however, e2 showed faster biodegradation than bpa due to the shorter adaptation time of the microorganisms to start the assimilation. final concentrations were measured and revealed that, under sturm test conditions, e2 was totally removed from the aqueous phase while some traces of bpa were detected. this result could be explained by the lower adsorbability of bpa observed in adsorption experiments onto inactivated sludge. to investigate competition in a bi-component solution, sturm tests were carried out with bpa/sb and e2/sb. moreover, the biodegradation curves obtained did not indicate a toxicity of the target compounds towards microorganisms, which rapidly degraded sb. in the case of bpa/sb, an inflection in the curve confirmed the adaptation time of 4-5 days for bpa to be degraded.
two and three-pion quantum statistics correlations in pb-pb collisions at sqrt(snn)=2.76 tev at the lhc. correlations induced by quantum statistics are sensitive to the spatio-temporal extent as well as dynamics of particle emitting sources in heavy-ion collisions. in addition, such correlations can be used to search for the presence of a coherent component of pion production. two and three-pion correlations of same and mixed-charge are measured at low relative momentum to estimate the coherent fraction of charged pions in pb-pb collisions at sqrt(snn)= 2.76 tev at the lhc with alice. the genuine three-pion quantum statistics correlation is found to be suppressed relative to the two-pion correlation based on the assumption of fully chaotic pion emission.the suppression is observed to decrease with triplet momentum. the observed suppression at low triplet momentum may correspond to a coherent fraction in charged pion emission of 22% +- 12%.
biosorption of cu(ii) from aqueous solution by fucus serratus: surface characterization and sorption mechanisms. in this work, the brown alga fucus serratus (fs) used as a low cost sorbent has been studied for the biosorption of copper(ii) ions in batch reactors. firstly, the characterization of the surface functional groups was performed with two methods: a qualitatively analysis with the study of ft-ir spectrum and a quantitatively determination with potentiometric titrations. from this latter, a total proton exchange capacity of 3.15 mmol g(-1) was extrapolated from the fs previously protonated. this value was similar to the total acidity of 3.56 mmol g(-1) deduced from the gran method. using the single extrapolation method, three kinds of acidic functional groups with three intrinsic pk(a) were determined at 3.5, 8.2 and 9.6. the point of zero net proton charge (pznpc) was found close to ph 6.3. secondly, the biosorption of copper ions was studied. the equilibrium time was about 350 min and the adsorption equilibrium data were well described by the langmuir's equation. the maximum adsorption capacity has been extrapolated to 1.60 mmol g(-1). the release of calcium and magnesium ions was also measured in relation to the copper biosorption. finally, the efficiency of this biosorbent in natural tap water for the removal of copper was also investigated. all these observations indicate that the copper biosorption on fs is mainly based on ion exchange mechanism and this biomass could be then a suitable sorbent for the removal of heavy metals from wastewaters. (c) 2008 elsevier ltd. all rights reserved.
evolution of bacterial community in experimental sand filters: physiological and molecular fingerprints. biofilm development in wastewater treatment system by soil infiltration is often mentioned for its participation to purification efficiency and clogging zone formation. it appears necessary to understand its evolution in order to better control the operation of these systems. the objective of this study was to improve knowledge about the temporal evolution of the biofilm structure in the first centimetres of infiltration system. for this purpose, metabolic fingerprints by biolog ecoplate (tm) and molecular fingerprints by ribosomal intergenic spacer analysis (risa) were carried out on sand, septic effluent and treated effluent samples from two experimental reactors supplied with different hydraulic loads collected at different times. the metabolic capabilities of sand microflora decreased in time. in the same way, molecular structure of the biofilm community changed and converged to similar structure in time. principal components analysis on risa gel revealed a ''buffering effect'' of the sand filter on the genetic structure of the bacterial community from treated effluent. the kinetics of evolution of the both metabolic and genetic fingerprints showed a reduction of the metabolic and genetic potentials of septic and treated effluents for the same times. the population dynamic within the biofilms appears interesting evidence in the comprehension of the operation of the treatment systems.
evaluation of a new packing material for h2s removed by biofiltration. this study aims to evaluate the feasibility of using a new packing material (up20) in treating h2s. three identical laboratory-scale biofilters, filled with, respectively, up20 alone, pine bark, and a configuration made of two layers of pozzolan/up20 (80/20, v/v), were used for critical comparison. various concentrations of h2s (up to 100 ppmv) were used to determine the optimum biofilter performances. the superficial velocity of the polluted gas on each biofilter was 65 m h(-1) (0.018 m s(-1); gas flow rate 0.5 n m(3) h(-1)) corresponding to an empty bed residence time of 57s. changes in elimination capacity, removal efficiency, moisture content, temperature and ph were tracked during 95 days. the pressure drops along each biofilter were also measured by varying the gas flow rate from 0.5 to 4n m(3) h(-1). after 63 days of operation, the loading rate was significantly increased to 10 g m(-3) h(-1) and the up20 biofilter retained a removal efficiency of more than 93%, indicating a strong ability to stimulate microbial activity (compared to 69% for the pine bark biofilter and 74% for the biofilter filled with a configuration of two layers of pozzolan/up20). a michaelis-menten type equation was applied and the maximum removal rate (v-m) and saturation constant stant (k-s) were calculated. v-m was evaluated at 35g h2s m(biofilter)(-3) h(-1) for up20 (14 and 15g h2s m(biofilter)(-3) h(-1) for pine bark and pozzolan/up20, respectively). the saturation constant k-s was 70 ppmv for up20 (18 ppmv for pine bark and 20 ppmv for pozzolan/up20) indicating that the new packing material will be effective in treating large pollutant concentrations. at low concentrations of pollutant, the results suggest that a biofilter with a configuration of two layers of pozzolan/up20 is the most suitable choice for treating h2s. (c) 2008 elsevier b.v. all rights reserved.
natural seaweed waste as sorbent for heavy metal removal from solution. biosorption is a suitable heavy metal remediation technique for the treatment of aqueous effluents of large volume and low pollutant concentration. however, today industrial applications need the selection of efficient low-cost biosorbents. the aim of this work is to investigate brown alga such as fucus serratus (fs) as a low-cost biosorbent, for the fixation of metallic ions, namely cu2+, zn2+, pb2+, ni2+, cd2+ and ce3+, in a batch reactor. biosorption kinetics and isotherms have been performed at ph 5.5. for all of the studied metallic ions, the equilibrium time is about 450 min and a tendency based on the initial sorption rate has been established: ce3+ zn2+ ni2+ cu2+ cd2+ pb2+. the adsorption equilibrium data are well described by the langmuir equation. the sequence of the maximum adsorption capacity is pb2+ cu2+ &gt;&gt; ce3+ ni2+ cd2+ zn2+ and values are ranged between 1.78 and 0.71 mmol g-1. these results indicate that the fs biomass is a suitable biosorbent for the removal of heavy metals from wastewater and can be tested in a dynamic process. the selected pilot process involves a hybrid membrane process: a continuous stirred tank reactor is coupled with a microfiltration immersed membrane, in order to confine the fs particles. a mass balance model is used to describe the adsorption process and the breakthrough curves are correctly modelled. based on these results, it is demonstrated that fs is an interesting biomaterial for the treatment of water contaminated heavy metals.
characterization techniques of packing material colonization in gas biofiltration processes. null
evaluation of innovative packing materials for the biodegradation of h2s: a comparative study. the biofiltration of gas polluted with h2s was carried out using innovative configurations of packing materials (i.e. a new synthetic material called up20, sapwood, peat, pozzolan and pine bark). a comparison of seven different configurations (media alone or in combination) was made based on biofilter performances and pressure drop measurements. biofilters were operated continuously for at least 95 days at a constant flow rate (0.5 n m(3) h(-1) corresponding to a superficial velocity of 65 m h(-1) and an empty bed residence time of 57 s). elimination capacities and removal efficiencies were calculated according to loading rates varying from 0 to 25.59 m(-3) h(-1) (inlet concentration up to 400 mg m(-3)). biofilter performances were modelled and biokinetic constants were calculated using the ottengraf model and a modified michaelis-menten model. in terms of elimination capacity, packing materials can be ordered from the most efficient to the least efficient: peat-up20 in a mixture &gt; peat-up20 in two layers &gt; peat &gt; pozzolan-up20 in two layers &gt; pine bark &gt; sapwood-up20 in two layers &gt; sapwood. a maximal removal rate, v-m, of 55 g m(-3) h(-1) was calculated for biofilters filled with peat-up20 (in a mixture or in two layers) and peat (in comparison, v-m = 8.3 m(-3) h(-1) for a biofilter filled with sapwood). peat is the best material to treat high h2s concentrations and the addition of up20 can significantly increase the removal efficiency. the pozzolan-up20 combination represents an interesting packing material to treat pollutant loading rates up to 5 g m(-3) h(-1) with low pressure drops. for low h2s concentrations, sapwood can be considered as a good support for h2s degradation with pollutant loading rates up to 4 g m(-3) h(-1). (c) 2010 society of chemical industry.
silicone oil: an effective absorbent for the removal of hydrophobic volatile organic compounds. background: hydrophobic volatile organic compounds (vocs), such as toluene, dimethyl sulfide (dms) and dimethyl disulfide (dmds), are poorly soluble in water and classical air treatment processes like chemical scrubbers are not efficient. an alternative technique involving an absorption step in an organic solvent followed by a biodegradation phase was proposed. the solvent must fulfil several characteristics, which are key factors of process efficiency, and a previous study allowed polydimethylsiloxane (or pdms, i.e. silicone oil) to be selected for this purpose. the aim of this paper was to determine some of its characteristics like absorption capacity and velocity performances (henry's constant, diffusivity and mass transfer coefficient), and to verify its non-biodegradability. results: for the three targeted vocs, henry's constants in silicone oil were very low compared to those in water, and solubility was infinite. diffusivity values were found to be in the range 10(-10) to 10(-11) m(2) s(-1) and mass transfer coefficients did not show significant differences between the values in pure water and pure silicone oil, in the range 1.0 x 10(-3) to 4.0 x 10(-3) s(-1) for all the vocs considered. silicone oil was also found to be non-biodegradable, since its biological oxygen demand (bod(5)) value was zero. conclusion: absorption performances of silicone oil towards toluene, dms and dmds were determined and showed that this solvent could be used during the first step of the process. moreover, its low biodegradability and its absence of toxicity justify its use as an absorbent phase for the integrated process being considered. (c) 2010 society of chemical industry.
removal of arsenic(v) onto chitosan: from sorption mechanism explanation to dynamic water treatment process. the aim of this work consists in a feasibility study to understand how arsenate ions could be removed from contaminated water by sorption onto chitosan, a biopolymer extracted from the wastes of the seafood industries. firstly, a batch adsorption study investigates different models, namely langmuir, freundlich, tempkin, and redlich-peterson. the sorption mechanism is shown to be sorption by an electrostatic attraction, with thermodynamic parameters indicating an exothermic and spontaneous reaction. the main influencing parameters are the temperature, the ph and the presence of other ions. secondly, a semi-dynamic membrane process is proposed: a stirred batch reactor is coupled with a microfiltration immersed-membrane. a mass balance model is used to describe the adsorption process and the breakthrough curves are well simulated. (c) 2010 elsevier b.v. all rights reserved.
determination of partition coefficients of three volatile organic compounds (dimethylsulphide, dimethyldisulphide and toluene) in water/silicone oil mixtures. the main objective of this work was to propose a model able to predict the partition coefficient of odorous or toxic gaseous pollutants (dimethylsulphide, dimethyldisulphide and toluene) in water/silicone oil mixtures. experimental measurements using a static headspace method were carried out for pure water (h(voc,water)), for pure silicone oil (h(voc,solvent)) and for mixtures of varying composition (h(voc,mixture)). the dramatic decrease in the partition coefficient (h(voc,mixture)) with oil addition clearly showed a deviation from linearity, which was more pronounced for increasing h(voc,water)/h(voc,solvent) ratios. moreover, experiments using a dynamic absorption method underlined that the absorption capacity of a biphasic water/silicone oil mixture can be classed as the absorption capacity of a pseudo-homogeneous phase whose physical properties (molecular weight and density) can be calculated from the physical properties of water and solvent, and balanced using the ''equivalent absorption coefficients'' h(voc,mixture)/h(voc,water) and h(voc,mixture)/h(voc,solvent). an ''equivalent absorption capacity'' concept is then proposed, which should be useful to design absorption units using two-phase liquid mixtures for the treatment of industrial air loaded with volatile organic compounds. (c) 2010 elsevier b.v. all rights reserved.
laundry water recycling in ship by direct nanofiltration with tubular membranes. the present study deals with the feasibility to implement, on board ship, a direct nanofiltration process in order to treat laundry grey waters and recycle 80% to the inlets of the washing machines. at first, a specific methodology for real laundry grey water production was set up at the laboratory for the purpose. characteristics of the lab-produced grey water are close to those observed on board (ph 7,1300 mg(cod)/l, 80 mg(tss)/l). then, a membrane screening in view of a selection was realised at the lab scale. a direct nanofiltration process (without pre-treatment) on tubular pci-afc80 membrane (35 bar, 25 degrees c, volumic-reduction-factor 5) allowed us to produce a permeate free of micro-organisms and suspended solids and with only 48 mg(cod)/l and 7 mg(toc)/l based on these satisfactory preliminary results, a techno-economic study was conducted (up-scaling) with the aim of producing daily 52 m(3) of recyclable permeate from 65 m(3) of polluted grey water. finally, orders of magnitude, both for energetic consumption and processing costs, are proposed. (c) 2010 elsevier b.v. all rights reserved.
erratum: silicone oil: an effective absorbent for the removal of hydrophobic volatile organic compounds. null
bathroom greywater characterization and potential treatments for reuse. with the emerging crisis of water, greywaters represent a significant resource of water if considering recycling for uses not requiring a drinking water quality. samples of greywaters were taken from a few households. their characterization led to results similar to those in literature. however, they showed a lack of phosphorus in c/n/p ratio. nevertheless, it was shown that, in our study, median was more appropriate than mean. the potential treatment steps studied during this work were sand bed filtration, adsorption onto granular activated carbon (gac), and sanitation by chlorine. the sand bed which was supplied with sequential feedings led to a very good removal of total suspended solids (tss; and consequently of turbidity) as well as to a 30% cod decrease. however, the organic matter withdrawal was more efficient by adsorption onto gac. the chlorination of greywaters was efficient to decrease the microbial population. therefore, following the reclaimed water quality which would be required treatment might imply all steps or just one or two. this kind of low-cost device could thus be implemented for reuse such as irrigation, agricultural need, or urban use.
voc absorption in a countercurrent packed-bed column using water/silicone oil mixtures: influence of silicone oil volume fraction. a calculation procedure to determine the influence of the silicone volume fraction on the physical absorption of vocs in water/silicone oil mixtures is presented (eta(silicone) oil = 5 mpa s). it is based on the ''equivalent absorption capacity'' concept previously developed by dumont et al. (2010)111 and applied to a countercurrent gas-liquid absorber. the calculation procedure is first applied to three vocs: dimethylsulphide (dms), dimethyldisulphide (dmds) and toluene, and then generalised to other vocs. the influence of voc partition coefficients (h(voc,water) and h(voc,solvent)) and the ratio mr = h(voc,water)/h(voc,solvent) is shown. for vocs having a much higher affinity for silicone oil than for water (m(r) &gt; 20 as for dmds and toluene). it is preferable to use pure silicone oil rather than water/silicone oil mixtures for absorption. (c) 2011 elsevier b.v. all rights reserved.
conversion of agricultural residues into activated carbons for water purification: application to arsenate removal. the conversion of two agricultural wastes, sugar beet pulp and peanut hulls, into sustainable activated carbons is presented and their potential application for the treatment of arsenate solution is investigated. a direct and physical activation is selected as well as a simple chemical treatment of the adsorbents. the material properties, such as bet surface areas, porous volumes, elemental analysis, ash contents and ph(pzc), of these alternative carbonaceous porous materials are determined and compared with a commercial granular activated carbon. an adsorption study based on experimental kinetic and equilibrium data is conducted in a batch reactor and completed by the use of different models (intraparticle diffusion, pseudo-second-order, langmuir and freundlich) and by isotherms carried out in natural waters. it is thus demonstrated that sugar beet pulp and peanut hulls are good precursors to obtain activated carbons for arsenate removal.
evolution of microbial aerosol behaviour in heating, ventilating and air-conditioning systems - quantification of staphylococcus epidermidis and penicillium oxalicum viability. the aim of this study was to develop an experimental set-up and a methodology to uniformly contaminate several filter samples with high concentrations of cultivable bacteria and fungi. an experimental set-up allows contaminating simultaneously up to four filters for range of velocities representative of heating, ventilating and air-conditioning systems. the test aerosol was composed of a microbial consortium of one bacterium (staphylococcus epidermidis) and one fungus (penicillium oxalicum) and aerosol generation was performed in wet conditions. firstly, the experimental set-up was validated in regards to homogeneity of the air flows. the bioaerosol was also characterized in terms of number and particle size distribution using two particle counters: optical particle counter grimm (r) 1.109 (optical diameters) and tsi aps 3321 (aerodynamic diameters). moreover, stabilities of the number of particles generated were measured. finally, concentrations of cultivable microorganisms were measured with biosamplers (skc) downstream of the four filters.
potential of aquatic macrophytes as bioindicators of heavy metal pollution in urban stormwater runoff. the concentrations of heavy metals in water, sediments, soil, roots, and shoots of five aquatic macrophytes species (oenanthe sp., juncus sp., typha sp., callitriche sp.1, and callitriche sp.2) collected from a detention pond receiving stormwater runoff coming from a highway were measured to ascertain whether plants organs are characterized by differential accumulations and to evaluate the potential of the plant species as bioindicators of heavy metal pollution in urban stormwater runoff. heavy metals considered for water and sediment analysis were cd, cr, cu, ni, pb, zn, and as. heavy metals considered for plant and soil analysis were cd, ni, and zn. the metal concentrations in water, sediments, plants, and corresponding soil showed that the studied site is contaminated by heavy metals, probably due to the road traffic. results also showed that plant roots had higher metal content than aboveground tissues. the floating plants displayed higher metal accumulation than the three other rooted plants. heavy metal concentrations measured in the organs of the rooted plants increased when metal concentrations measured in the soil increased. the highest metal bioconcentration factors (bcf) were obtained for cadmium and nickel accumulation by typha sp. (bcf = 1.3 and 0.8, respectively) and zinc accumulation by juncus sp. (bcf = 4.8). our results underline the potential use of such plant species for heavy metal biomonitoring in water, sediments, and soil.
assessment of the efficiency of photocatalysis on tetracycline biodegradation. the use of photocatalysis to improve the biodegradability of an antibiotic compound, tetracycline (tc) was investigated. the toxicity of tc and its degradation products were also examined. the sturm test was conducted to assess the biodegradability of by-products formed in the photocatalytic process. the toxicity of tetracycline and its by-products was evaluated using a dehydrogenase inhibition test, which showed a decrease in toxicity during photocatalysis. however, the sturm test results indicated that, like tetracycline, the by-products are not biodegradable. possible structures of these by-products were determined using liquid chromatography-electrospray ionization-tandem mass spectrometry (lc-esi-ms/ms). it was found that, during the photocatalytic process, the tc aromatic ring is not opened and the structure of the identified by-products is quite similar to that of tetracycline. a reaction pathway is proposed. (c) 2012 elsevier b.v. all rights reserved.
phosphate removal from synthetic and real wastewater using steel slags produced in europe. electric arc furnace steel slags (eaf-slags) and basic oxygen furnace steel slags (bof-slags) were used to remove phosphate from synthetic solutions and real wastewater. the main objective of this study was to establish an overview of the phosphate removal capacities of steel slags produced in europe. the influences of parameters, including ph, and initial phosphate and calcium concentrations, on phosphate removal were studied in a series of batch experiments. phosphate removal mechanisms were also investigated via an in-depth study. the maximum capacities of phosphate removal from synthetic solutions ranged from 0.13 to 0.28 mg p/g using eaf-slags and from 1.14 to 2.49 mg p/g using bof-slags. phosphate removal occurred predominantly via the precipitation of ca-phosphate complexes (most probably hydroxyapatite) according to two consecutive reactive phases: first, dissolution of cao-slag produced an increase in ca2+ and oh- ion concentrations; then the ca2+ and oh- ions reacted with the phosphates to form hydroxyapatite. it was found that the release of ca2+ from slag was not always enough to enable hydroxyapatite precipitation. however, our results indicated that the ca2+ content of wastewater represented a further source of ca2+ ions that were available for hydroxyapatite precipitation, thus leading to an increase in phosphate removal efficiencies. (c) 2012 elsevier ltd. all rights reserved.
influence of operating conditions on direct nanofiltration of greywaters: application to laundry water recycling aboard ships. the present study completes a previous work dedicated to the feasibility to implement, on-board ship, a direct nanofiltration process in order to treat laundry greywaters and recycle 80% to the inlets of the washing machines (guilbaud et al., 2010). at present, the study investigates the influence of nanofiltration operating conditions on chemical oxygen demand (cod) rejection rates and permeates fluxes. thus, the ph and temperature of greywater as well as transmembrane pressure have been fixed to 7 or 9, 25 or 40 degrees c and 35 or 40 bar, respectively. afc80 membranes show different cod rejection rates whereas permeate fluxes are quasi similar when the same greywater (ph 7) is nanofiltered at 35 bar and 25 degrees c. amongst all the tested operating conditions, the nanofiltration of greywater (ph 7) on afc80 membrane, at 35 bar and 25 degrees c, allows to obtain the highest cod rejection rate (around 93% at vrf5). the best permeate flux (85.5 l/h/m(2) at vrf5) has been obtained at 40 bar and 40 degrees c. an increase of temperature or pressure above 25 degrees c and 35 bar respectively leads to a drop of cod rejection rates. the ph should be maintained at a value of 7, the initial ph of raw greywater, in order to allow a good cod rejection rate. an economic evaluation of greywater nanofiltration has been investigated. (c) 2012 elsevier b.v. all rights reserved.
biological treatment of a mixture of gaseous sulphur reduced compounds: identification of the total bacterial community's structure. background: this study deals with the potential of biological processes combining biofiltration and a biotrickling filter to treat a mixture of sulphur reduced compounds (src) including dimethylsulfide (dms), dimethyldisulfide (dmds) and hydrogen sulphide (h2s). the first step in this work is to evaluate the influence of ph on src biodegradation in microcosms seeded with planktonic biomass from activated sludge of a rendering facility. in a second step, for each tested ph, evaluation of the influence of ph on total bacterial community diversity and structure has been investigated using denaturing gradient gel electrophoresis (dgge). results: at ph 7 and 5, h2s, dms and dmds were completely removed. in return, the abatement of dms and dmds is low, around 20%, for ph 3 and ph 1 microcosms. the selective pressure imposed in the microcosms (concentration and ph) is sufficient to influence strongly the community diversity and composition. the inoculum community seems to make a significant contribution to the 67-day community in the considered ph microcosms. conclusion: these results suggest that distinct communities from different inocula can achieve high and stable functionality. the results obtained provide relevant information for improving the treatment of different src using biotrickling filter/biofilter combination. copyright (c) 2012 society of chemical industry.
h2s biofiltration using expanded schist as packing material: performance evaluation and packed-bed tortuosity assessment. background: the aim of this work was to test an innovative packing material (expanded schist) for h2s biofiltration in order to determine the packing material performance in terms of elimination capacity, removal efficiency and pressure drop changes. additionally, the changes over time of bed characteristics, especially tortuosity, were evaluated according to porosity measurements. results: schist material can treat large loading rates (up to 30 g.m-3.h-1) with 100% efficiency at an empty bed residence time (ebrt) of 16 s, which is much better than most results reported in the literature. the porosity of the packed bed is around 40% (tortuosity estimated to range from 1.5 to 2.0) which leads to pressure drop measurements in the range of 1080 pa m-1. conclusion: schist is a good material for h2s biofiltration in terms of mechanical stability, removal efficiency and effective treatment of high h2s loading rates. schist is a material that provides the appropriate environment for micro-organisms by itself. this trend should be confirmed over a long period. copyright (c) 2012 society of chemical industry.
sustainable activated carbons from agricultural residues dedicated to antibiotic removal by adsorption. the objectives of this study are to convert at laboratory scale agricultural residues into activated carbons (ac) with specific properties, to characterize them and to test them in adsorption reactor for tetracycline removal, a common antibiotic. two new acs were produced by direct activation with steam from beet pulp (bp-h2o) and peanut hulls (ph-h2o) in environmental friendly conditions. bp-h2o and ph-h2o present carbon content ranged between 78% and 91%, similar bet surface areas (821 and 829 m(2).g(-1) respectively) and ph(pzc) values (9.8). their porosities are different: ph-h2o is mainly microporous (84%) with 0.403 cm(3).g(-1) of total porous volume, whereas bp-h2o develops a mesoporous volume of 0.361 cm(3).g-(1) representing 50% of the total porous volume. two other commercial granular ac carbons (gac1 and gac2) were also characterised and used for comparison. the adsorption study is conducted in batch reactors. two parts can be observed from kinetic decay curves: a very fast concentration decrease during the first 12 h, followed by a slow adsorption. an optimal contact time of 120 h is also deduced from these curves. it is shown as well that adsorption decreases with an increase of ph, indicating that the form preferentially adsorbed is probably the zwitterion form of the tetracycline. from equilibrium isotherms data, two adsorption models have been used: langmuir and freundlich. both of them lead to a very good description of the experimental data. maximum adsorption capacities deduced from the langmuir equation follow the sequence: gac2 (817 mg.g(-1))&gt;bp-h2o (288 mg.g(-1))&gt;gac1 (133 mg.g(-1))&gt;ph-h2o (28 mg.g(-1)). in real spring waters spiked with tc (tetracyclines), adsorption isotherms show that the maximum adsorption capacity of bp-h2o is slightly increased to 309 mg.g(-1) while it is decreased by one third to 550 mg.g(-1) in the case of gac2. this study demonstrates that the production of ac from agricultural residues, at lab-scale, is feasible and leads to genuine activated carbons with different intrinsic properties.
styrene absorption in water/silicone oil mixtures. the absorption of styrene in water/silicone oil systems at a constant flow rate for emulsion compositions ranging from 0% to 20% was investigated using a dynamic absorption method. it was found that the mass transfer for air/water/silicone oil systems is roughly double than those determined for air/water systems whatever the silicone oil percentage (phi). this result, mainly due to the high value of the partition coefficient ratio (m(r) = h-water/h-oil = 257), was in agreement with the enhancement factor models proposed in the literature. considering the volumetric mass transfer coefficient, it was shown that change in k(l)a versus phi depended on the mass transfer model used for its determination. by using the ''equivalent absorption capacity'' concept developed by dumont et al. [18] (cej 162 (2010) 927-934; doi:10.1016/j.cej.2010.06.045), a dramatic decrease in the k(l)a with increasing silicone oil volume fraction was observed in relation to the decrease in the value of the partition coefficient h-mix. conversely, considering a styrene mass transfer pathway in series (gas -&gt; water -&gt; oil), the k(l)a values for gas/water/silicone oil systems were roughly double the k(l)a for gas/water systems and did not depend on the mixture composition. the styrene mass transfer performances were also analyzed using the modeling framework proposed by hernandez et al. [17] (cej 172 (2011) 961-969; doi:10.1016/j.cej.2011.07.008). this model confirmed the ability of a water/silicone oil mixture to increase the styrene mass transfer by a factor of 2 and verified that the change in h-mix versus phi followed the trend predicted by the theory (although the h-mix values determined from this model were significantly lower than the theoretical values). (c) 2012 elsevier b.v. all rights reserved.
steel slag filters to upgrade phosphorus removal in constructed wetlands: two years of field experiments. electric arc furnace steel slag (eaf-slag) and basic oxygen furnace steel slag (bof-slag) were used as filter substrates in two horizontal subsurface flow filters (6 m(3) each) designed to remove phosphorus (p) from the effluent of a constructed wetland. the influences of slag composition, void hydraulic retention time (hrtv), temperature, and wastewater quality on treatment performances were studied. over a period of almost two years of operation, the filter filled with eaf-slag removed 37% of the inlet total p, whereas the filter filled with bof-slag removed 62% of the inlet total p. p removal occurred predominantly via cao-slag dissolution followed by ca phosphate precipitation. p removal efficiencies improved with increasing temperature and hrtv, most probably because this affected the rates of cao-slag dissolution and ca phosphate precipitation. it was observed that long hrtv (&gt;3 days) can cause high ph in the effluents (&gt;9) as a result of excessive cao-slag dissolution. however, at shorter hrtv (1-2 days), ph values were elevated only during the first five weeks and then stabilized below a ph of 9. the kinetics of p removal were investigated employing a first-order equation, and a model for filter design was proposed.
novel fe loaded activated carbons with tailored properties for as(v) removal: adsorption study correlated with carbon surface chemistry. novel fe loaded activated carbons have been prepared from sugar beet pulp (bp) agricultural residues by direct steam activation followed by iron impregnation with or without previous oxidation. the corresponding activated carbons were: bp-h2o, bp-h2o-fe, bp-h2o-h2o2-fe and bp-h2o-mno2-fe. the textural characterization of these tailored activated carbons was based on nitrogen adsorption/desorption isotherms leading to bet surface area values between 741 and 821 m(2)/g and total porous volumes between 0.58 and 0.79 cm(3)/g. elemental analysis and ash content showed that carbon content reached 78% for bp-h2o with 13.6% of ash and decreased to 50% in iron-based materials. bp-h2o and bp-h2o-fe revealed a basic nature with ph(pzc) values of 9.8 and 9 respectively while bp-h2o-h2o2-fe and bp-h2o-mno2-fe had acid ph(pzc) (5.1 and 3.6). their surface chemistry has been investigated by xps analysis and by the quantification of the surface chemical moieties based on boehm's approach. a clear relationship was found between the surface iron content and the strong acidic groups. arsenic (as(v)) adsorption isotherms were performed and langmuir, freundlich, redlich-peterson models were used to describe the experimental data by non-linear regression. it was found that redlich-peterson isotherm provided the best fit and the langmuir adsorption capacities confirmed that the iron-based activated carbons were highly attractive for as(v) removal with capacities up to 17 mg g(-1). finally, it has been shown that the surface iron content determined by xps analysis is very well correlated with langmuir q(m) values (r(2) = 0.982) and with the strong acidic moieties deduced from the boehm's method. (c) 2012 elsevier b.v. all rights reserved.
steady- and transient-state h2s biofiltration using expanded schist as packing material. the performances of three laboratory-scale biofilters (bf1, bf2, bf3) packed with expanded schist for h2s removal were studied at different empty bed residence times (ebrt = 35, 24 and 16 s) in terms of elimination capacity (ec) and removal efficiency (re). bf1 and bf2 were filled with expanded schist while bf3 was filled with both expanded schist and a nutritional material (up20; 12% vol). bf1 and bf3 were inoculated with activated sludge, whereas bf2 was not inoculated. a maximum ec of 42 g m(-3) h(-1) was recorded for bf3 at ebrt = 35 s demonstrating the ability of schist to treat high h2s loading rates, and the ability of up20 to improve h2s removal. michaelis-menten and haldane models were fitted to the experimental elimination capacities while biofilter responses to transient-state conditions in terms of removal efficiency during shock load events were also evaluated for bf1 and bf3.
performances of two macrophytes species in floating treatment wetlands for cadmium, nickel, and zinc removal from urban stormwater runoff. conventional stormwater detention ponds frequently show limitations for dissolved heavy metal removal. floating treatment wetlands (ftws), a variant of constructed wetlands, are considered as a promising technology to improve the quality of urban stormwater runoff. our study aimed at evaluating the treatment performances of ftws for cadmium, nickel, and zinc removal through a pot experiment. two macrophytes species, juncus effusus and carex riparia, were grown during 4 months under three metal concentrations (in micrograms per liter): high (cd, 200; ni, 500; zn, 2,000), low (cd, 10; ni, 10; zn, 40), and control (cd-ni-zn, 0). results showed that both macrophytes species significantly removed dissolved heavy metals. cadmium and nickel accumulations were greater in roots than in shoots for both species. under low metal concentration, maximum accumulation of 0.4 mu g g(-1) dry weight (dw) for cd was observed in the roots of j. effusus. under high metal concentration, accumulations of up to 5 mu g cdg(-1) dw and 62 mu g nig(-1) dw were observed in the roots of j. effusus and up to 73 mu g zng(-1) dw in the roots of c. riparia. although j. effusus and c. riparia are not recognized as metal hyperaccumulators, our study demonstrated that they can achieve high metal uptake when both roots and shoots are harvested.
biological characterization and treatment performances of a compact vertical flow constructed wetland with the use of expanded schist. the use of compact vertical flow constructed wetlands is becoming increasingly popular in france to treat raw domestic wastewater. this system enables the use of a single deep stage rather than two stages and therefore dramatically reduces the capital costs. a compact vertical flow constructed wetland known as ecophyltre (r) was investigated. this system was partially filled with a light expanded schist (mayennite (r)) designed to reduce the surface area to 1.2 m(2) pe(-1). the aim of this work was to highlight relationships between mayennite (r) characteristics with biological activity and treatment performances. after 12 months of operation, the total accumulated dry matter quantity was around 5 kg m(-2) (20% on the surface). between 70% and 80% of the microorganisms respiration was principally measured in the top layer of ecophyltre (r) during the first year of operation. however, biological activity was shared between the two mayennite (r) layers over time. results showed that mayennite (r) retention enabled to kept around 15% water in mass enhancing microorganisms growing. removal performances of this system met the french standards (35 mg tss l(-1); 25 mg bod1(-1); 125 mg cod1(-1)). crown copyright (c) 2012 published by elsevier b.v. all rights reserved.
volumetric mass transfer coefficients characterising voc absorption in water/silicone oil mixtures. the physical absorption of three volatile organic compounds (dimethyldisulphide (dmds), dimethylsulphide (dms) and toluene) in "water/silicone oil" systems at a constant flow rate for mixtures of different compositions (f = 0, 5, 10, 15, 20 and 100%) was investigated using a dynamic absorption method. the results indicate that silicone oil addition leads to a dramatic decrease in kla which can be related to the change in the partition coefficient (hmix). they confirm the results obtained for styrene absorption using another measurement technique [15]. the interpretation of the results using dimensionless ratios kla(f)/kla(f=0%) and kla(f)/kla(f=100%) versus f also confirms the importance of the partition coefficient ratio mr = hwater/hoil in the kla change. moreover, the results obtained for toluene absorption in "air/water/silicone oil" systems (f = 10, 15 and 20%) suggest that the mass transfer pathway is in the order gas→water→oil for these operating conditions.
uv-visible photocatalytic degradation of 17b-estradiol and estrogenic activity assessment. null
biofiltration using peat and a nutritional synthetic packing material: influence of the packing configuration on h2s removal. this study aims to evaluate the feasibility of using a nutritional synthetic material (up20) combined with fibrous peat as a packing material in treating h2s (up to 280ppmv). three identical laboratory-scale biofilters with different packing material configurations (peat only; peat+up20 in a mixture; peat+up20 in two layers) were used to determine the biofilter performances. the superficial velocity of the polluted gas on each biofilter was 65m/h (gas flow rate 0.5nm(3)/h) corresponding to an empty bed residence time=57 s. variations in elimination capacity, removal efficiency, temperature and ph were tracked during 111 d. a removal efficiency of 100% was obtained for loading rates up to 6g/m(3)/h for the biofilter filled with 100% peat, and up to 10g/m(3)/h for both biofilters using peat complemented with up20. for higher loading rates (up to 25.5g/m(3)/h), the configuration of peat-up20 in a mixture provided the best removal efficiencies (around 80% compared to 65% for the configuration of peat-up20 in two layers and 60% for peat only). microbial characterization highlighted that peat is able to provide sulfide-oxidizing bacteria. through kinetic analysis (ottengraf and michaelis-menten models were applied), it appeared that the configuration peat-up20 in two layers (80/20 v/v) did not show significant improvement compared with peat alone. although the configuration of peat-up20 in a mixture (80/20 v/v) offered a real advantage in improving h2s treatment, it was shown that this benefit was related to the bed configuration rather than the nutritional properties of up20.
photocatalytic degradation of endocrine disruptor compounds under simulated solar light. nanostructured titanium materials with high uv-visible activity were synthesized in the collaborative project clean water fp7. in this study, the efficiency of some of these catalysts to degrade endocrine disruptor compounds, using bisphenol a as the model compound, was evaluated. titanium dioxide p25 (aeroxide (r) tio2, evonik degussa) was used as the reference. the photocatalytic degradation was carried out under the uv part of a simulated solar light (280-400 nm) and under the full spectrum of a simulated solar light (200 nm-30 gm). catalytic efficiency was assessed using several indicators such as the conversion yield, the mineralization yield, by-product formation and the endocrine disruption effect of by-products. the new synthesized catalysts exhibited a significant degradation of bisphenol a, with the so-called ect-1023t being the most efficient. the intermediates formed during photocatalytic degradation experiments with ect-1023t as catalyst were monitored and identified. the estrogenic effect of the intermediates was also evaluated in vivo using a chgh-gfp transgenic medaka line. the results obtained show that the formation of intermediates is related to the nature of the catalyst and depends on the experimental conditions. moreover, under simulated uv, in contrast with the results obtained using p25, the by-products formed with ect-1023t as catalyst do not present an estrogenic effect. (c) 2013 elsevier ltd. all rights reserved.
excentric labeling: dynamic neighborhood labeling for data visualization. the widespread use of information visualization is hampered by the lack of effective labeling techniques. an informal taxonomy of labeling methods is proposed. we then describe excentric labeling, a new dynamic technique to label a neighborhood of objects located around the cursor. this technique does not intrude into the existing interaction, it is not computationally intensive, and was easily applied to several visualization applications. a pilot study with eight subjects indicates a strong speed benefit over a zoom interface for tasks that involve the exploration of large numbers of objects. observations and comments from users are presented.
compus: visualization and analysis of structured documents for understanding social life in the 16th century. this article describes the compus visualization system that assists in the exploration and analysis of structured document corpora encoded in xml. compus has been developed for and applied to a corpus of 100 french manuscript letters of the 16th century, transcribed and encoded for scholarly analysis using the recommendations of the text encoding initiative. by providing a synoptic visualization of a corpus and allowing for dynamic queries and structural transformations, compus assists researchers in finding regularities or discrepancies, leading to a higher level analysis of historic source. compus can be used with other richly encoded text corpora as well.
input device selection and interaction configuration with icon. this paper describes icon, a novel editor designed to configure a set of input devices and connect them to actions into a graphical interactive application. icon allows physically challenged users to connect alternative input devices and/or configure their interaction techniques according to their needs. it allows skilled users - graphic designers or musicians for example - to configure any icon aware application to use their favorite input devices and interaction techniques (bimanual, voice enabled, etc.). icon works with java swing and requires applications to describe their interaction styles in terms of icon modules. by using icon, users can adapt more deeply than before their applications and programmers can easily provide extensibility to their applications.
the svgl toolkit: enabling fast rendering of rich 2d graphics. as more and more powerful graphical processors be- come available on mainstream computers, it becomes possible to investigate the design of visually rich and fast interactive applications. in this article, we present s vgl , a graphical toolkit that enables programmers and design- ers of interactive applications to benefit from this power. the toolkit is based on a scene graph which is translated into an optimized display graph. after describing the algorithms used to display the scene, we show that the toolkit is two to fifty times faster than similar toolkits.
influence of natural mobile organic matter on europium retention on bure clay rock. bure clay rock (cr) was chosen as host rock for the french high and intermediate level long lived radioactive waste repository. this choice is mostly explained by the retention ability of the callovo-oxfordian rock (cox). bure clay rock contains natural organic matter (om) that could have an influence on radionuclide retention. the aim of this work is to assess the influence of natural mobile om on the retention of eu on clay rock. eu was chosen as a chemical model for trivalent actinides contained in vitrified waste. three organic molecules were studied: suberic, sorbic and tiglic acids, small organic acids identified in cox pore water. all the experiments were carried out in an environment recreating cox water (ph=7.5 ; i=0.1 mol/l ; pco2 =10^(-2) bar).clay rock sample characterization showed that the sample used in this work was similar to those previously extracted from the area of interest and that it was necessary to maintain ph at 7.5 to avoid altering the clay rock. the eu-om system study indicated that organic acids had no influence on eu speciation in cox water. the eu-cr system experimental study confirmed that retention implied sorption on cr (ceu&lt;6.10^(-6)mol/l) and precipitation in cox water (ceu&gt;6.10-6mol/l). distribution coefficient rd (quantifying sorption) was estimated at 170 ± 30 l/g. this high value is consistent with literature values obtained on clay rocks. the ternary eu-om-cr system study showed a slight increase of sorption in the presence of organic matter. this synergistic effect is very satisfactory in terms of storage security: the presence of small organic acids in clay rock does not question retention properties with respect to europium and trivalent actinides.
anomalous centrality evolution of two-particle angular correlations from au-au collisions at $\sqrt{s_{\rm nn}}$ = 62 and 200 gev. we present two-dimensional (2d) two-particle angular correlations on relative pseudorapidity $\eta$ and azimuth $\phi$ for charged particles from au-au collisions at $\sqrt{s_{\rm nn}} = 62$ and 200 gev with transverse momentum $p_t \geq 0.15$ gev/$c$, $|\eta| \leq 1$ and $2\pi$ azimuth. observed correlations include a {same-side} (relative azimuth $&lt; \pi/2$) 2d peak, a closely-related away-side azimuth dipole, and an azimuth quadrupole conventionally associated with elliptic flow. the same-side 2d peak and away-side dipole are explained by semihard parton scattering and fragmentation (minijets) in proton-proton and peripheral nucleus-nucleus collisions. those structures follow n-n binary-collision scaling in au-au collisions until mid-centrality where a transition to a qualitatively different centrality trend occurs within a small centrality interval. above the transition point the number of same-side and away-side correlated pairs increases rapidly {relative to} binary-collision scaling, the $\eta$ width of the same-side 2d peak also increases rapidly ($\eta$ elongation) and the $\phi$ width actually decreases significantly. those centrality trends are more remarkable when contrasted with expectations of jet quenching in a dense medium. observed centrality trends are compared to {\sc hijing} predictions and to the expected trends for semihard parton scattering and fragmentation in a thermalized opaque medium. we are unable to reconcile a semihard parton scattering and fragmentation origin for the observed correlation structure and centrality trends with heavy ion collision scenarios which invoke rapid parton thermalization. on the other hand, if the collision system is effectively opaque to few-gev partons the observations reported here would be inconsistent with a minijet picture.
first test of lorentz violation with a reactor-based antineutrino experiment. we present a search for lorentz violation with 8249 candidate electron antineutrino events taken by the double chooz experiment in 227.9 live days of running. this analysis, featuring a search for a sidereal time dependence of the events, is the first test of lorentz invariance using a reactor-based antineutrino source. no sidereal variation is present in the data and the disappearance results are consistent with sidereal time independent oscillations. under the standard-model extension (sme), we set the first limits on fourteen lorentz violating coefficients associated with transitions between electron and tau flavor, and set two competitive limits associated with transitions between electron and muon flavor.
supercharges in the hyper-kähler with torsion supersymmetric sigma models. we construct explicitly classical and quantum supercharges satisfying the standard n=4supersymmetryalgebra in the supersymmetric sigma models describing the motion over hyper-kähler with torsion manifolds. one member of the family of superalgebras thus obtained is equivalent to the superalgebra derived and formulated earlier in purely mathematical framework.
the distributed slow control system of the xenon100 experiment. the xenon100 experiment, in operation at the laboratori nazionali del gran sasso (lngs) in italy, was designed to search for evidence of dark matter interactions inside a volume of liquid xenon using a dual-phase time projection chamber. this paper describes the slow control system (scs) of the experiment with emphasis on the distributed architecture as well as on its modular and expandable nature. the system software was designed according to the rules of object-oriented programming and coded in java, thus promoting code reusability and maximum flexibility during commissioning of the experiment. the scs has been continuously monitoring the xenon100 detector since mid 2008, remotely recording hundreds of parameters on a few dozen instruments in real time, and setting emergency alarms for the most important variables.
on ocl-based imperative languages. the object constraint language (ocl) is a well-accepted ingredient in model-driven engineering and accompanying modeling languages such as uml (unified modeling language) and emf (eclipse modeling framework) that support object-oriented software development. among various possibilities, ocl offers the formulation of class invariants and operation contracts in form of pre- and postconditions, and side effect-free query operations. much research has been done on ocl and various mature implementations are available for it. ocl is also used as the foundation for several modeling-specific programming and transformation languages. however, an intrusive way of embedding ocl into these language hampers us when we want to benefit from the existing achievements for ocl. in response to this shortcoming, we propose the language soil (simple ocl-like imperative language), which we implemented in the uml and ocl modeling tool use to amend its declarative model validation features. the expression sub-language of soil is identical to ocl. soil adds imperative constructs for programming in the domain of models. thus by employing ocl and soil, it is possible to describe any operation in a declarative way and in an operational way on the modeling level without going into the details of a conventional programming language. in contrast to other similar approaches, the embedding of ocl into soil is done in a careful, non-intrusive way so that purity of ocl is preserved.
the input configurator toolkit: towards high input adaptability in interactive applications. this article describes icon (input configurator), an input management system that enables interactive applications to achieve a high level of input adaptability. we define input adaptability as the ability of an interactive application to exploit alternative input devices effectively and offer users a way of adapting input interaction to suit their needs. we describe several examples of interaction techniques implemented using icon with little or no support from applications that are hard or impossible to implement using regular gui toolkits.
investigation of eu(iii) immobilization on γ-al2o3 surfaces by combining batch technique and exafs analyses: role of contact time and humic acid. aluminum (hydr)oxides play an important role in the regulation of the composition of soil/water, sediment/water and other natural water systems. in this study, the interactions among eu(iii), humic acid (ha) and γ-al2o3 were investigated using a combination of batch and extended x-ray absorption fine structure (exafs) techniques. experiments were performed with varying contact times (2, 15, 60 and 180 d) at a ph of 6.5 for both the binary γ-al2o3/eu(iii) and the ternary γ-al2o3/ha/eu(iii) systems. in addition, two representative ph values (ph 6.5 for a near-neutral condition and ph 8.5 for an alkalescence condition) were selected to determine the sequestration mechanisms of eu(iii) in the ternary γ-al2o3/ha/eu(iii) systems. to verify the specific binding modes and corresponding chemical species, a coordination geometry calculation and a quantitative comparison between the ha binding site concentration and the initial eu(iii) concentration were conducted along with exafs data analysis. the microstructure and thermodynamic stability of the formed eu(iii) species were dependent on various environmental parameters. for the binary γ-al2o3/eu(iii) systems, quantitative analysis results of exafs spectra suggested the presence of two eu(iii) species within a contact time of 15 d. using a coordination geometry calculation, the reu-al values at ∼3.28 å and ∼3.99 å corresponded to the formation of edge-shared and corner-shared surface complexes, respectively. for samples reacted longer than 15 d, the appearance of an additional eu-eu shell at ∼3.50 å was indicative of a structural rearrangement process, leading to the formation of thermodynamically stable surface polynuclear complexes. for the ternary γ-al2o3/ha/eu(iii) systems, the exafs-derived structural parameters indicated the formation of 1:1 type b ternary complexes and binary corner-shared complexes at ph 6.5 after 2 d. in contrast, the eu(iii) sequestration mechanisms at ph 8.5 were mainly attributed to the formation of 1:2 type a ternary complexes and binary edge-shared complexes. considering the high proton dissociation constant of strong ha phenolic sites (8.8) and the high metal loading in the present study, the weak ha carboxylic sites are predominantly involved in eu(iii) complexation at ph 6.5 and 8.5. the time-dependent variation tendency of the eu(iii) chemical species formed in the ternary systems may arise from eu(iii)-induced ha agglomeration, binding of eu(iii) ions on stronger ha binding sites and migration of eu(iii) ions to less sterically accessible sites in the ha macromolecule structures. the adsorbed ha could accelerate eu(iii) immobilization at the γ-al2o3/water interfaces and could enhance the thermodynamic stability of the formed chemical species. the findings presented in this study could provide important microcosmic information for the prediction of the long-term behaviors of eu(iii) and the relevant ln/an(iii) in a geological environment rich in aluminum hydr(oxides).
what heavy ion can teach us about strange particles and what strange particles can teach us about heavy ions?. we show that heavy ion collisions can reveal properties of k+ in matter, here demonstrated for the kn optical potential, and that at the same time the k+ yield is sensitive to nuclear matter properties, here demonstrated for the hadronic equation of state.
in-medium effects on strangeness production. we discuss the strangeness production close to threshold in heavy-ion collisions based on two independent microscopic transport approaches - hsd and iqmd - employing different in-medium scenarios for the modification of particle properties (strange mesons and hyperons) in the dense and hot medium following either from the chiral models or from a coupled-channel g-matrix approach using a meson-exchange model for strange mesons. the comparison of available kaon, antikaon and λ data with the hsd and iqmd models shows a good agreement for the large majority of observables when incorporating the in-medium effects. the investigation of the reactions with help of transport models reveals the complicated multiple interactions of the strange particles with hadronic matter which shows that strangeness production in heavy-ion collisions is very different from that in elementary interactions. we discuss how a variety of strange particle observables can be used to study the different facets of this interaction (production, rescattering and potential interaction) which finally merge into a comprehensive understanding of these reactions.
flattening 3d objects using silhouettes. one important research area in non-photorealistic rendering is to obtain silhouettes. there are many methods to obtain silhouettes from 3d models and raster structures, but they are limited in their ability to draw stylized silhouettes with complete flexibility. these limitations do not exist in illustration, as each element is plane and the interaction between them can be eliminated locating each one in a different layer. this is the approach that we present in this paper: a 3d model is flatted in plane elements ordered in space, which allows us to draw the silhouettes with total flexibility.
global statistical predictor model for characteristic adsorption energy of organic vapors-solid interaction: use in dynamic process simulation. adsorption of volatile organic compounds (vocs) is one of the best remediation techniques for controlling industrial air pollution. in this paper, a quantitative predictor model for the characteristic adsorption energy (e) of the dubinin-radushkevich (dr) isotherm model has been established with r(2) value of 0.94. a predictor model for characteristic adsorption energy (e) has been established by using multiple linear regression (mlr) analysis in a statistical package minitab. the experimental value of characteristic adsorption energy was computed by modeling the isotherm equilibrium data (which contain 120 isotherms involving five vocs and eight activated carbons at 293, 313, 333, and 353 k) with the gauss-newton method in a statistical package r-stat. the mlr model has been validated with the experimental equilibrium isotherm data points, and it will be implemented in the dynamic adsorption simulation model prosim. by implementing this model, it predicts an enormous range of 1200 isotherm equilibrium coefficients of dr model at different temperatures such as 293, 313, 333, and 353k (each isotherm has 10 equilibrium points by changing the concentration) just by a simple mlr characteristic energy model without any experiments.
recovery comparisons--hot nitrogen vs steam regeneration of toxic dichloromethane from activated carbon beds in oil sands process. the regeneration experiments of dichloromethane from activated carbon bed had been carried out by both hot nitrogen and steam to evaluate the regeneration performance and the operating cost of the regeneration step. factorial experimental design (fed) tool had been implemented to optimize the temperature of nitrogen and the superficial velocity of the nitrogen to achieve maximum regeneration at an optimized operating cost. all the experimental results of adsorption step, hot nitrogen and steam regeneration step had been validated by the simulation model prosim. the average error percentage between the simulation and experiment based on the mass of adsorption of dichloromethane was 2.6%. the average error percentages between the simulations and experiments based on the mass of dichloromethane regenerated by nitrogen regeneration and steam regeneration were 3 and 12%, respectively. from the experiments, it had been shown that both the hot nitrogen and steam regeneration had regenerated 84% of dichloromethane. but the choice of hot nitrogen or steam regeneration depends on the regeneration time, operating costs, and purity of dichloromethane regenerated. a thorough investigation had been made about the advantages and limitations of both the hot nitrogen and steam regeneration of dichloromethane.
different families of volatile organic compounds pollution control by microporous carbons in temperature swing adsorption processes. in this research work, the three different vocs such as acetone, dichloromethane and ethyl formate (with corresponding families like ketone, halogenated-organic, ester) are recovered by using temperature swing adsorption (tsa) process. the vapors of these selected vocs are adsorbed on a microporous activated carbon. after adsorption step, they are regenerated under the same operating conditions by hot nitrogen regeneration. in each case of regeneration, factorial experimental design (fed) tool had been used to optimize the temperature, and the superficial velocity of the nitrogen for achieving maximum regeneration efficiency (r(e)) at an optimized operating cost (op(€)). all the experimental results of adsorption step and hot nitrogen regeneration step had been validated by the simulation model prosim. the average error percentage between the simulation and experiment based on the mass of adsorption of dichloromethane was 3.1%. the average error percentages between the simulations and experiments based on the mass of dichloromethane regenerated by nitrogen regeneration were 4.5%.
dynamics modeling of compliant locomotion : application to flapping flight bio-inspired by insects. the objective of the present work is to model the locomotion dynamics of "soft robots", i.e. compliant mobile multi-body systems. these compliances can be either localized and treated as passive joints of the system, or introduced by distributed flexibilities along the bodies. the dynamics of these systems is modeled in a lagrangian approach based on the mathematical tools developed by the american school of geometric mechanics. from the algorithmic viewpoint, the computation of these dynamic models is based on a recursive and efficient newton-euler algorithm which is extended here to the case of robots equipped with compliant organs. the proposed algorithm is compatible with control, fast simulation and real time robotic applications. it is able to solve the direct external dynamics as well as the inverse internal torque dynamics. the modeling tools and algorithms developed in this thesis are applied to one of the most advanced cases of compliante locomotion i.e. the flapping flight mavs bio-inspired by insects. the nonlinear equations governing the passive deformations of the wing are derived using two different methods. in the first method, we separate the wing movement into a rigid component (which corresponds to the movements of a "floating frame"), and a deformation component. the latter one is parameterized in the floating frame using the assumed modes approach where the wing is considered as an euler-bernoulli beam undergoing flexion and torsion deformations. regarding the second method, the wing movements are no longer separated but directly parameterize dusing rigid finite absolute transformations of a cosserat beam. this method is called galilean or "geometrically exact" because it does not require any approximation apart from the unavoidable spatial and temporal discretizations imposed by numerical resolution of the flight dynamics. in both cases, the aerodynamic forces are taken into account through a simplified analytical model. the resulting models and algorithms are used in the context of the collaborative project (anr) eva to develop a flight simulator, and to design wing prototype.
a generically well-posed h2 control problem for a one shot feedforward and feedback synthesis. in control design, a particular method consists in two main steps. firstly, a global model is defined by embodying both, the system to be controlled and an exosystem (regrouping reference and disturbances models). secondly, a standard h2 (or h1) optimization problem is defined. formerly named the disturbance modeling method in the lqg context, the control goal is to minimize a quadratic index by weighting the output reference error in one side, and the control input in the other. in the case where the exosystem is unstable because e.g. persistent disturbances, this index is not bounded. it is then possible to change the criterion, weighting the ad hoc control input and output deviation to make the problem well posed (comprehensively stabilizable). the current paper deals with the same framework for stable exosystems and unstable ones, and proposes a systematic way to get a well-posed control optimization problem leading to a 2 degree of freedom controller.
describing and generating solutions for the edf unit commitment problem with the modelseeker. null
a parametric propagator for discretely convex pairs of sum constraints. null
a synchronized sweep algorithm for the $k$-dimensional cumulative constraint. null
the energetic reasoning checker revisited. energetic reasoning (er) is a powerful filtering algorithm for the cumulative constraint. unfortunately, er is generally too costly to be used in practice. one reason of its bad behavior is that many intervals are considered as relevant by the checker of er, although most of them should be ignored. in this paper, we provide a sharp characterization that allows to reduce the number of intervals by a factor seven. our experiments show that associating this checker with a time-table filtering algorithm leads to promising results.
j/ψ production at low pt in au + au and cu + cu collisions at snn−−−−√=200 gev with the star detector. the $j/\psi$ $p_t$ spectrum and nuclear modification factor ($r_{\textit{aa}}$) are reported for $p_t &lt; 5$ gev/c and $|y|&lt;1$ from 0-60% central au+au and cu+cu collisions at $\sqrt{s_{_{nn}}} =200$ gev at star. a significant suppression of $p_t$-integrated $j/\psi$ production is observed in central au+au events, with less suppression observed in cu+cu. the $p_t$ dependence of the $r_{\textit{aa}}$ is observed to increase at a higher $p_t$ region. the data are compared with the previously published rhic results. comparing with model calculations, it is found that the invariant yields at low $p_t$ are significantly above hydrodynamic flow predictions but are consistent with models that include color screening and regeneration.
views and program transformations for modular maintenances. maintenance consumes a large part of the cost of software development which makes the optimization of that cost among the important issues in the world of software engineering. in this thesis we aim to optimize this cost by making these maintenances modular. to achieve this goal, we define transformations of program architectures that allow to transform a program to maintain into an architecture that facilitates the maintenance tasks required. we focus on transformation between architectures having dual modularity properties such as composite and visitor designpatterns. in this context, we define an automatic and reversible transformation based on refactoring between a program structured according to the composite structure and its corresponding visitor structure. this transformation is validated by generating a precondition which guarantees statically its success. it is also adapted to take into account the transformation of four variations of composite pattern and it is then applied to jhotdraw program in which these four variations occur. we define also a reversible transformation in the singleton pattern to benefit from optimization by introducing this pattern and flexibility by its suppression according to the requirements of the software user.
aspectual session types. multiparty session types allow the definition of distributed processes with strong communication safety properties. a global type is a choreographic specification of the interactions between peers, which is then projected locally in each peer. well-typed processes behave accordingly to the global protocol specification. multiparty session types are however monolithic entities that are not amenable to modular extensions. also, session types impose conservative requirements to prevent any race condition, which prohibit the uni- form application of extensions at different points in a protocol. in this paper, we describe a means to support modular extensions with aspectual session types, a static pointcut/advice mechanism at the session type level. to support the modular definition of crosscut- ting concerns, we augment the expressivity of session types to al- low harmless race conditions. we formally prove that well-formed aspectual session types entail communication safety. as a result, aspectual session types make multiparty session types more flexible, modular, and extensible.
execution levels for aspect-oriented programming: design, semantics, implementations and applications. in aspect-oriented programming (aop) languages, advice evaluation is usually considered as part of the base program evaluation. this is also the case for certain pointcuts, such as if pointcuts in aspectj, or simply all pointcuts in higher-order aspect languages like aspectscheme. while viewing aspects as part of base level computation clearly distinguishes aop from reflection, it also comes at a price: because aspects observe base level computation, evaluating pointcuts and advice at the base level can trigger infinite regression. to avoid these pitfalls, aspect languages propose ad-hoc mechanisms, which increase the complexity for programmers while being insufficient in many cases. after shed- ding light on the many facets of the issue, this paper proposes to clarify the situation by introducing levels of execution in the programming language, thereby allowing aspects to observe and run at specific, possibly different, levels. we adopt a defensive default that avoids infinite regression, and gives advanced programmers the means to override this default using level-shifting operators. we then study execution levels both in practice and in theory. first, we study the relevance of the issues addressed by execution levels in existing aspect-oriented programs. we then formalize the semantics of execution levels and prove that the default semantics is indeed free of a certain form of infinite regression, which we call aspect loops. finally, we report on existing implementations of execution levels for aspect-oriented extensions of scheme, javascript and java, discussing their implementation techniques and current applications.
effective aspects: a typed monadic embedding of pointcuts and advice. aspect-oriented programming(aop) aims to enhance modularity and reusability in software systems by offering an abstraction mechanism to deal with crosscutting concerns. however, in most general-purpose aspect languages aspects have almost unrestricted power, eventually conflicting with these goals. in this work we present effective aspects: a novel approach to embed the point- cut/advice model of aop in a statically-typed functional programming language like haskell. our work extends effectiveadvice, by oliveira, schrijvers and cook; which lacks quantification, and explores how to exploit the monadic setting in the full pointcut/advice model. type soundness is guaranteed by exploiting the underlying type system, in particular phantom types and a new anti-unification type class. aspects are first-class, can be deployed dynamically, and the pointcut language is extensible, therefore combining the flexibility of dynamically-typed aspect languages with the guarantees of a static type system. monads enables us to directly reason about computational effects both in aspects and base programs using traditional monadic techniques. using this we extend aldrich's notion of open modules with effects, and also with protected pointcut interfaces to external advising. these restrictions are enforced statically using the type system. also, we adapt the techniques of effectiveadvice to reason about and enforce control flow properties. moreover, we show how to control effect interference us- ing the parametricity-based approach of effectiveadvice. however this approach falls short when dealing with interference between multiple aspects. we propose a different approach using monad views, a recently developed technique for han- dling the monad stack. finally, we exploit the properties of our monadic weaver to enable the modular construction of new semantics for aspect scoping and weaving. these semantics also benefit fully from the monadic reasoning mechanisms present in the language. this work brings type-based reasoning about effects for the first time in the pointcut/advice model, in a framework that is both expressive and extensible; thus allowing development of robust aspect-oriented systems as well as being a useful research tool for experimenting with new aspect semantics.
rheological characterization of a thermally unstable bioplastic in injection molding conditions. poly(hydroxy-alcanoates) bioplastics have an unusually small temperature processing window due to their thermal instability, which affects both molecular weight distribution and rheological behavior. the use of a rheometrical device mounted on an injection molding machine has been shown to allow the study of dynamic viscosity variations for very short residence times in the melt (down to a few tens of seconds). moreover, dynamic viscosity can be obtained over a wide range of shear rates (up to 50000 s−1) with both bagley and rabinowitsch corrections. the rheological data obtained for a poly(3-hydroxybutyrate-co-3-hydroxyvalerate), phbv copolymer shows that the reduction in viscosity associated with degradation is very fast and is detectable even at temperatures close to the melting point. size exclusion chromatography analysis of selected samples shows that, except for residence times below 15s, molecular mass distribution is substantially affected, shifting toward lower masses, and with an increase in oligomer compound content. these oligomers may act as plasticizers of the bioplastic. the results also suggest that important thermomechanical degradation takes place during the plasticization/feeding step of injection molding.
visualization of the exothermal voc adsorption in a fixed-bed activated carbon adsorber. activated carbon fixed beds are classically used to remove volatile organic compounds (vocs) present in gaseous emissions. in such use, an increase of local temperature due to exothermal adsorption has been reported; some accidental fires in the carbon bed due to the removal of high concentrations of ketones have been published. in this work, removal of vocs was performed in a laboratory-scale pilot unit. in order to visualize the increase in local temperature, the adsorption front was tracked with a flame ionization detector and the thermal wave was simultaneously visualized with an infrared camera. in extreme conditions, fire in the adsorber and the combustion of activated carbon was achieved during ketone adsorption. data have been extracted from these experiments, including local temperature, front velocity and carbon bed combustion conditions.
three generalizations of the focus constraint. the focus constraint expresses the notion that solutions are concentrated. in practice, this constraint suffers from the rigidity of its semantics. to tackle this issue, we propose three generalizations of the focus constraint. we provide for each one a complete filtering algorithm as well as discussing decompositions.
photon and eta production in p+pb and p+c collisions at sqrt{snn} = 17.4 gev. measurements of direct photon production in p+pb and p+c collisions at $\sqrt{s_\mathrm{nn}} = 17.4\mathrm{gev}$ are presented. upper limits on the direct photon yield as a function of $p_\mathrm{t}$ are derived and compared to the results for pb+pb collisions at $\sqrt{s_\mathrm{nn}} = 17.3$ gev. the production of the $\eta$ meson, which is an important input to the direct photon signal extraction, has been determined in the $\eta \rightarrow 2\gamma$ channel for p+c collisions at $\sqrt{s_\mathrm{nn}} = 17.4\mathrm{gev}$.
branch-and-price approach for the multi-skill project scheduling problem. this work introduces a procedure to solve the multi-skill project scheduling problem (mspsp) (néron and baptista, international symposium on combinatorial, optimization (co'2002), 2002). the mspsp mixes both the classical resource constrained project scheduling problem and the multi-purpose machine model. the aim is to find a schedule that minimizes the completion time (makespan) of a project, composed of a set of activities. in addition, precedence relations and resources constraints are considered. in this problem, resources are staff members that master several skills. thus, a given number of workers must be assigned to perform each skill required by an activity. practical applications include the construction of buildings, as well as production and software development planning. we present a column generation approach embedded within a branch-and-price (b&amp;p) procedure that considers a given activity and time-based decomposition approach. obtained results show that the proposed b&amp;p procedure is able to reach optimal solutions for several small and medium sized instances in an acceptable computational time. furthermore, some previously open instances were optimally solved.
two-phase cryogenic avalanche detectors with thgem and hybrid thgem/gem multipliers operated in ar and ar+n2. two-phase cryogenic avalanche detectors (crads) with gem and thgem multipliers have become an emerging potential technique for charge recording in rare-event experiments. in this work we present the performance of two-phase crads operated in ar and ar+n2. detectors with sensitive area of 10 × 10 cm2, reaching a litre-scale active volume, yielded gains of the order of 1000 with a double-thgem multiplier. higher gains, of about 5000, have been attained in two-phase ar crads with a hybrid triple-stage multiplier, comprising of a double-thgem followed by a gem. the performance of two-phase crads in ar doped with n2 (0.1-0.6%) yielded faster signals and similar gains compared to the operation in two-phase ar. the applicability to rare-event experiments is discussed.
identifying clouds over the pierre auger observatory using infrared satellite data. we describe a new method of identifying night-time clouds over the pierre auger observatory using infrared data from the imager instruments on the goes-12 and goes-13 satellites. we compare cloud identifications resulting from our method to those obtained by the central laser facility of the auger observatory. using our new method we can now develop cloud probability maps for the 3000 km2 of the pierre auger observatory twice per hour with a spatial resolution of ∼2.4 km by ∼5.5 km. our method could also be applied to monitor cloud cover for other ground-based observatories and for space-based observatories.
reactivity of htco4 with methanol in sulfuric acid: tc-sulfate complexes revealed by xafs spectroscopy and first principles calculations. the reaction between htco4 and meoh in 13 m h2so4 was investigated by 99tc nmr, uv-visible and x-ray absorption fine structure (xafs) spectroscopy. experimental results and first principles calculations show the formation of tc(+5) sulfate complexes. the results expand the fundamental understanding of tc in high acid solutions.
structure, energetics, and dynamics of smectite clay interlayer hydration: molecular dynamics and metadynamics investigation of na-hectorite. this paper presents a classical molecular dynamics (md) and metadynamics investigation of the relationships between the structure, energetics, and dynamics of na-hydroxyhectorite and serves to provide additional, molecular-scale insight into the interlayer hydration of this mineral. the computational results support a model for interlayer h2o structure and dynamics based on 2h nmr spectroscopy and indicate that h2o molecules undergo simultaneous fast librational motions about the h2o c2 symmetry axis and site hopping with c3 symmetry with respect to the surface normal. hydration energy minima occur at one-, one-and-one-half-, and two-water-layer hydrates, which for the composition modeled correspond to 3, 5.5, and 10 h2o/na+, respectively. na+ ions are coordinated by basal o atoms (omin) at the lowest hydration levels and by h2o molecules (oh2o) in the two-layer hydrate, and h2o molecules have an average of three h-bonds at the greatest hydration levels. the metadynamics calculations yield activation energies for site hopping of h2o molecules of 6.0 kj/mol for the one-layer structure and 3.3 kj/mol for hopping between layers in the two-layer structure. computed diffusion coefficients for water and na+ are substantially less than in bulk liquid water, as expected in a nanoconfined environment, and are in good agreement with previous results.
on the nature of heptavalent technetium in concentrated nitric and perchloric acid. the speciation of tc(+7) was performed in hclo4 and hno3 by 99-tc nmr, uv-vis and xafs spectroscopy. the speciation of tc(+7) depends on the concentration and strength of the acid. pertechnetic acid, htco4, forms above 8 m hclo4 while in concentrated hno3, [tco4]− is still the predominant species. exafs spectroscopy shows that the structure of htco4 in hclo4 is similar to the one in h2so4. the reactivity of tc(+7) was analyzed in the frame of the partial charge model. the partial charge calculated on the tc atoms (δtc) indicates that htco4 (δtc = +057) is more electrophilic than [tco4]− (δtc = +0.52). the difference in the oxidizing properties between [tco4]− and htco4 is given from the reaction of these species with 12 m hcl(aq). in 13 m sulfuric acid htco4 is reduced to tc(+5) while [tco4]− is not reduced in 6 m h2so4.
aerial pollutants in swine buildings: a review of their characterization and methods to reduce them. the swine industry follows a large increase of meat production since the 1950s causing the development of bigger swine buildings which involves a raise of pollutants emissions. due to recent anthropological pressures concerning the animal welfare, the limitation of neighborhood disturbances and atmospheric pollutions limitations, the livestock farming has to adapt their management methods to reduce or treat the aerial pollutants emissions. through the diversity of livestock barns configurations, their climatic location, their size, and their management, we thus propose hereafter a critical review of the characterizations of these aerial pollutants. this is realized by distinguishing both solids and gaseous emissions and by referencing the measurements methods mainly used to analyze and quantify airborne particles, odorants, and gaseous compounds in the atmosphere of swine buildings. the origins of these pollutants are focused and the sturdiest techniques for concentration measurements are highlighted. finally, we discuss pollutants abatement techniques criticizing their implementation in swine buildings and emphasizing the use of biological ways such as biofiltration for gases and odors treatment.
engaging end-users in the collaborative development of domain-speci c modelling languages. domain-speci c modelling languages (dsmls) are high-level languages specially designed to perform tasks in a particular domain. when developing dsmls, the participation of end-users is normally limited to providing domain knowledge and testing the resulting language prototypes. language developers, which are perhaps not domain experts, are therefore in control of the language development and evolution. this may cause misinterpretations which hamper the development process and the quality of the dsml. thus, it would be bene cial to promote a more active participation of end-users in the development process of dsmls. while current dsml workbenches are mono-user and designed for technical experts, we present a process and tool support for the example-driven, collaborative construction of dsmls in order to engage end-users in the creation of their own languages.
management of stateful firewall misconfiguration. firewall configurations are evolving into dynamic policies that depend on protocol states. as a result, stateful configurations tend to be much more error prone. some errors occur on configurations that only contain stateful rules. others may affect those holding both stateful and stateless rules. such situations lead to configurations in which actions on certain packets are conducted by the firewall, while other related actions are not. we address automatic solutions to handle these problems. permitted states and transitions of connection-oriented protocols (in essence, on any layer) are encoded as automata. flawed rules are identified and potential modifications are provided in order to get consistent configurations. we validate the feasibility of our proposal based on a proof of concept prototype that automatically parses existing firewall configuration files and handles the discovery of flawed rules according to our approach.
towards an access-control metamodel for web content management systems. out-of-the-box web content management systems (wcmss) are the tool of choice for the development of millions of enterprise web sites but also the basis of many web applications that reuse wcms for important tasks like user registration and authentication. this widespread use highlights the importance of their security, as wcmss may manage sensitive information whose disclosure could lead to monetary and reputation losses. however, little attention has been brought to the analysis of how developers use the content protection mechanisms provided by wcmss, in particular, access-control (ac). indeed, once configured, knowing if the ac policy provides the required protection is a complex task as the specificities of each wcms need to be mastered. to tackle this problem, we propose here a metamodel tailored to the representation of wcms ac policies, easing the analysis and manipulation tasks by abstracting from vendor-specific details.
model-driven extraction and analysis of network security policies. firewalls are a key element in network security. they are in charge of filtering the traffic of the network in compliance with a number of access-control rules that enforce a given security policy. in an always-evolving context, where security policies must often be updated to respond to new security requirements, knowing with precision the policy being enforced by a network system is a critical information. otherwise, we risk to hamper the proper evolution of the system and compromise its security. unfortunately, discovering such enforced policy is an error-prone and time consuming task that requires low-level and, often, vendor-specific expertise since firewalls may be configured using different languages and conform to a complex network topology. to tackle this problem, we propose a model-driven reverse engineering approach able to extract the security policy implemented by a set of firewalls in a working network, easing the understanding, analysis and evolution of network security policies.
mde support for enterprise architecture in an industrial context: the teap framework experience. model driven engineering (mde) is often applied to support software engineering processes (i.e., from reverse to forward engineering, including maintenance and/or evolution tasks). however, as promoted by the model driven organization (mdo) initiative, it can also be relevant in more business-oriented and strategic decision-making activities such as enterprise architecture (ea). ea is the process of translating business vision and strategy into effective change by better describing the enterprise's future state and thus enable its evolution. even if several approaches have already proposed different kinds of support to deal with the company's ea, an integrated mde framework combining ea data federation, ea standard adaptation and multiple viewpoint support is still missing. this paper reports on our ongoing experience of building the teap mde framework (based on the togaf standard and smartea tooling) notably addressing these three challenges in an industrial ea context.
artist methodology and framework: a novel approach for the migration of legacy software on the cloud. nowadays cloud computing is considered as the ideal environment for engineering, hosting and provisioning applications. a continuously increasing set of cloud-based solutions is available to application owners and developers to tailor their applications exploiting the advanced features of this paradigm for elasticity, high availability and performance. even though these offerings provide many benefits to new applications, they often incorporate constrains to the modernization and migration of legacy applications by obliging the use of specific development technologies and explicit architectural design approaches. the modernization and adaptation of legacy applications to cloud environments is a great challenge for all involved stakeholders, not only from the technical perspective, but also in business level with the need to adapt the the business processes and models of the modernized application that will be offered from now on, as a service. in this paper we present a novel model-driven approach for the migration of legacy applications in modern cloud environments which covers all aspects and phases of the migration process, as well as an integrated framework that supports all migration process.
parallel execution of atl transformation rules. industrial environments that make use of model-driven engineering (mde) are starting to see the appearance of very large models, made by millions of elements. such models are produced automatically (e.g., by reverse engineering complex systems) or manually by a large number of users (e.g., from social networks). the success of mde in these application scenarios strongly depends on the scalability of model manipulation tools. while parallelization is one of the traditional ways of making computation systems scalable, developing parallel model transformations in a general-purpose language is a complex and error-prone task. in this paper we show that rule-based languages like atl have strong parallelization properties. transformations can be developed without taking into account concurrency concerns, and a transformation engine can automatically parallelize execution. we describe the implementation of a parallel transformation engine for the current version of the atl language and experimentally evaluate the consequent gain in scalability.
migrating legacy software to the cloud with artist. as cloud computing allows improving the quality of software and aims at reducing costs of operating software, more and more software is delivered as a service. however, moving from a software as a product strategy to delivering software as a service hosted in cloud environments is very ambitious. this is due to the fact that managing software modernization is still a major challenge; especially when paradigm shifts, such as moving to cloud environments, are targeted that imply fundamental changes to how software is modernized, delivered, and sold. thus, in addition to technical aspects, business aspects need also to be considered. artist proposes a comprehensive software modernization approach covering business and technical aspects. in particular, artist employs model-driven engineering (mde) techniques to automate the reverse engineering of legacy software and forward engineering of cloud-based software in a way that modernized software truly benefits from targeted cloud environments. therewith, artist aims at reducing the risks, time, and costs of software modernization and lowers the barriers to exploit cloud computing capabilities and new business models.
a research roadmap towards achieving scalability in model driven engineering. null
extracting business rules from cobol: a model-based tool. this paper presents a business rule extraction tool for cobol systems. starting from a cobol program, we derive a model-based representation of the source code and we provide a set of model transformations to identify and visualize the embedded business rules. in particular, the tool facilitates the definition of an application vocabulary and the identification of relevant business variables. in addition, such variables are used as starting point to slice the code in order to identify business rules, that are finally represented by means of textual and graphical artifacts. the tool has been developed as an eclipse plug-in in collaboration with ibm france.
extracting business rules from cobol: a model-based framework. organizations rely on the logic embedded in their information systems for their daily operations. this logic implements the business rules in place in the organization, which must be continuously adapted in response to market changes. unfortunately, this evolution implies understanding and evolving also the underlying software components enforcing those rules. this is challenging because, first, the code implementing the rules is scattered throughout the whole system and, second, most of the time documentation is poor and out-of-date. this is specially true for older systems that have been maintained and evolved for several years (even decades). in those systems, it is not even clear which business rules are enforced nor whether rules are still consistent with the current organizational policies. in this sense, the goal of this paper is to facilitate the comprehension of legacy systems (in particular cobol-based ones) by providing a model driven reverse engineering framework able to extract and visualize the business logic embedded in them.
automating inference of ocl business rules from user scenarios. user scenarios have been advocated as an effective means to capture requirements by describing the system-to-be at the instance or example level. this instance-level information is then used to infer a possible software specification consistent with the provided valid and invalid scenarios. so far existing approaches have often focused on the generation of static models but have omitted the inference of business rules that could complement the static models and improve the precision of the software specification. in this sense this paper provides a first set of invariant inference patterns that are applied on valid and invalid snapshots in order to generate ocl~(object constraint language) integrity constraints that the system should always satisfy. we strengthen the confidence of inferred results based on the user's feedback of generated examples and counterexamples for the considered constraint. the approach is realized with a prolog-based tool that could support the designer to effectively define ocl integrity constraints in a semi-automatic way.
extracting uml/ocl integrity constraints and derived types from relational databases. relational databases usually enforce relevant organizational business rules. this aspect is ignored by current database reverse engineering approaches which only focus on the extraction of the structural part of the conceptual schema. other database elements like triggers, views, column constraints, etc. are not considered by those methods. as a result, the generated conceptual schema is incomplete since integrity constraints and derivation rules enforced by the database are not represented.
implementing constraint relaxation over finite domains using atms. null
best-first search for property maintenance in reactive constraints systems. null
a best first approach for solving over-constrained dynamic problems. null
dynamic domain splitting for numeric csp. null
about avoidable computations in interval methods. null
l'algorithme path-repair. null
maintaining arc-consistency within dynamic backtracking. null
local search with constraint propagation and conflict-based heuristics. in this paper, we introduce a new solving algorithm for constraint satisfaction problems (csp). it performs an overall local search helped with a domain filtering technique to prune the search space. conflicts detected during filtering are used to guide the search. first experiments with a tabu version of the algorithm have shown good results on hard instances of open shop scheduling problems. it competes well with the best highly specialized algorithms.
instantiating and detecting design patterns: putting bits and pieces together. null
no java without caffeine ― a tool for dynamic analysis of java programs. null
fourth international workshop on integration of ai and or techniques in constraint programming for combinatorial optimisation problems (cp-ai-or). null
solving dynamic timetabling problems as dynamic resource constrained project scheduling problems using new constraint programming tools. null
combining constraint programming and local search to design new powerful heuristics. null
visexp: visualizing constraint solver dynamics using explanations. null
decomposition and learning for a hard real-time task allocating problem. null
interactively solving school timetabling problems using extensions of constraint programming. timetabling problems have been frequently studied due to their wide range of applications. however, they are often solved manually because of the lack of appropriate computer tools. although many approaches mainly based on local search or constraint programming seem to have been quite successful in recent years, they are often highly dedicated to specific problems and encounter difficulties to take the dynamic and over-constrained nature of such problems. we were confronted with such an over-constrained and dynamic problem in our institution. this paper deals with a timetabling system based on constraint programming with the use of explanations to offer a dynamic behaviour and to allow automatic relaxations of constraints. our tool has successfully answered the needs of the current planner by providing solutions in a few minutes instead of a week of manual design.we present in this paper the techniques used, the results obtained and a discussion on the effects of the automation of the timetabling process.
constraint programming for software engineering. null
identifying and exploiting problem structures using explanation-based constraint programming. identifying structures in a given combinatorial problem is often a key step for designing efficient search heuristics or for understanding the inherent complexity of the problem. several operations research approaches apply decomposition or relaxation strategies upon such a structure identified within a given problem. the next step is to design algorithms that adaptively integrate that kind of information during search. we claim in this paper, inspired by previous work on impact-based search strategies for constraint programming, that using an explanation-based constraint solver may lead to collect invaluable information on the intimate dynamically revealed and static structures of a problem instance. moreover, we discuss how dedicated or solving strategies (such as benders decomposition) could be adapted to constraint programming when specific relationships between variables are exhibited.
special issue constraint programming. null
vehicle routing problem in mixed flows for reverse logistics: a modeling framework. null
multiobjective optimization and constraint programming. null
subcontractors scheduling on residential buildings construction sites. null
integrating strong local consistencies into constraint solvers. null
hydrophobic voc absorption in two-phase partitioning bioreactors; influence of silicone oil volume fraction on absorber diameter. a methodology to determine the diameter of an absorber contacting a gas phase and two immiscible liquid phases (water/silicone oil mixture) is presented. the methodology is applied to a countercurrent gas/liquid randomly packed column for the absorption of three vocs (toluene, dimethyl sulfide, or dimethyl disulfide). whatever the silicone oil volume fraction, eckert's generalized pressure drop correlation was used. the results present the change in the column diameter through the change in the dimensionless ratio d(ϕ)/d(ϕ=1) versus the silicone oil volume fraction for the same operating conditions. for toluene and dimethyl disulfide, characterized by hvoc,silicone oil values equal to 2.3 and 3.4 pa m3 mol−1, respectively, it is highlighted that it is unwise to use water/silicone oil mixtures for mass transfer. in these cases, the contact between the polluted air and pure silicone oil requires roughly the same amount of silicone oil as for a (90/10 v/v) water/silicone oil mixture, but enables a 2-fold decrease in the column diameter. for dimethyl sulfide, which is characterized by a larger partition coefficient value (hvoc,silicone oil=17.7 pa m3 mol−1), the mass transfer operation should not be considered because large amounts of silicone oil are required (whatever the silicone oil volume fraction), which is not acceptable from an economic point of view. the feasibility of using a bioscrubber for the treatment of hydrophobic pollutants depends mainly on the partition coefficient hvoc,silicone oil. voc absorption in tppb should therefore be restricted to pollutants characterized by a hvoc,silicone oil value of around 3 to 4 pa m3 mol−1 or less. in this case, absorption can be efficiently carried out in a biphasic air/silicone oil system.
optimization of the volume fraction of the napl, silicon oil, and biodegradation kinetics of toluene and dmds in a tppb. the volume ratio between non-aqueous phase liquid (napl) and water was optimised in order to remove the two hydrophobic volatile organic compounds (vocs), toluene and dimethyldisulfide (dmds). biological oxygen demand after 5 days (bod5) measurements showed that ratios ranging from 20 to 30% of silicone oil in water enabled optimal removal of both selected substrates, certainly due to a high adhesion of microorganisms at the interface and a high o2 consumption to metabolize pollutants. the removal of these compounds was then efficiently carried out in batch cultures in a two-phase partitioning bioreactor (tppb) containing 25% silicone oil. if compared to the results obtained in the absence of napl, its presence led to 62 and 107% improvement of the biodegradation rate (global removal rate) for toluene and dmds respectively. in addition silicone oil was a more efficient napl than di-ethylhexyladipate (deha), since biodegradation rates were 0.17 and 0.95 g m3 h−1 for toluene in 25% deha and silicone oil respectively. the use of silicone oil as napl led to removal yields close to 90 and 75% for toluene and dmds respectively, and hence to the complete removal of residual voc if losses by stripping were taken into account, showing the efficiency of tppb containing silicone oil for the removal of these compounds.
efficiency of biological activator formulated material (bafm) for volatile organic compounds removal--preliminary batch culture tests with activated sludge. during biological degradation, such as biofiltration of air loaded with volatile organic compounds, the pollutant is passed through a bed packed with a solid medium acting as a biofilm support. to improve microorganism nutritional equilibrium and hence to enhance the purification capacities, a biological activator formulated material (bafm) was developed, which is a mixture of solid nutrients dissolving slowly in a liquid phase. this solid was previously validated on mineral pollutants: ammonia and hydrogen sulphide. to evaluate the efficiency of such a material for biodegradation of some organic compounds, a simple experiment using an activated sludge batch reactor was carried out. the pollutants (sodium benzoate, phenol, p-nitrophenol and 2-4-dichlorophenol) were in the concentration range 100 to 1200 mg l(-1). the positive impact of the formulated material was shown. the improvement of the degradation rates was in the range 10-30%. this was the consequence of the low dissolution of the nutrients incorporated during material formulation, followed by their consumption by the biomass, as shown for urea used as a nitrogen source. owing to its twofold interest (mechanical resistance and nutritional supplementation), the biological activator formulated material seems to be a promising material. its addition to organic or inorganic supports should be investigated to confirm its relevance for implementation in biofilters.
propagation engine prototyping with a domain specific language. constraint propagation is at the heart of constraint solvers. two main trends co-exist for its implementation: variable-oriented propagation engines and constraint-oriented propagation engines. those two approaches ensure the same level of local consistency but their efficiency (computation time) can be quite different depending on the instance solved. however, it is usually accepted that there is no best approach in general, and modern constraint solvers implement only one. in this paper, we would like to go a step further providing a solver independent language at the modeling stage to enable the design of propagation engines. we validate our proposal with a reference implementation based on the choco solver and the minizinc constraint modeling language.
when is it worthwhile to propagate a constraint? a probabilistic analysis of alldifferent. this article presents new work on analyzing the behaviour of a constraint solver, with a view towards optimization. in constraint programming, the propagation mechanism is one of the key tools for solving hard combinatorial problems. it is based on specific algorithms: propagators, that are called a large number of times during the resolution process. but in practice, these algorithms may often do nothing: their output is equal to their input. it is thus highly desirable to be able to recognize such situations, so as to avoid useless calls. we propose to quantify this phenomenon in the particular case of the alldifferent constraint (bound consistency propagator). our first contribution is the definition of a probabilistic model for the constraint and the variables it is working on. this model then allows us to compute the probability that a call to the propagation algorithm for alldifferent does modify its input. we give an asymptotic approximation of this probability, depending on some macroscopic quantities related to the variables and the domains, that can be computed in constant time. this reveals two very different behaviors depending of the sharpness of the constraint. first experiments show that the approximation allows us to improve constraint propagation behaviour.
coefficients of different macro-microscopic mass formulae from the ame2012 atomic mass evaluation. the coefficients of different possible macro-microscopic mass formulae previously proposed have been adjusted on 2264 experimental atomic masses extracted from the ame2012 atomic mass evaluation [1] assuming n,z⩾8 and the one standard deviation uncertainty on the mass lower than 150 kev. all the formulae include the volume and surface energies, the coulomb energy, the diffuseness correction to the sharp radius coulomb energy, the shell and pairing energies and take into account or not the curvature energy, different forms of the wigner term, a free charge radius, the experimental equivalent rms charge radius or a fixed short central radius. masses of 976 more exotic nuclei are extrapolated from the most accurate formula.
adapting workﬂows using generic schemas: application to the security of business processes. existing approaches to the adaptation of workflows over web services fall short in two respects. first, they only provide, if ever, limited means for taking into account the execution history of a workflow. second, they do not support adaptations that require modifications not only at the service composition level but also at the levels of interceptors and service implementations. this is particular problematic for the enforcement of security properties over workflows: enforcing authorization properties, for instance, frequently requires execution contexts to be defined and modifications to be applied at all these abstraction levels of web services. we present two main contributions in this context. first, we introduce workflow adaptation schemas (was), a new notion of generic protocol-based workflow adapters. was enable the declarative definition of adaptations involving complex service compositions and implementations. second, we present two real-world security issues related to the use of oauth 2.0, a recent and widely used framework for the authorization of resource accesses. as we motivate, these security issues require history-based adaptations over different abstraction levels of services. we then show how to resolve these issues using was.
contraction analysis of nonlinear random dynamical systems. in order to bring contraction analysis into the very fruitful and topical fields of stochastic and bayesian systems, we extend here the theory describes in \cite{lohmiller98} to random differential equations. we propose new definitions of contraction (almost sure contraction and contraction in mean square) which allow to master the evolution of a stochastic system in two manners. the first one guarantees eventual exponential convergence of the system for almost all draws, whereas the other guarantees the exponential convergence in $l_2$ of to a unique trajectory. we then illustrate the relative simplicity of this extension by analyzing usual deterministic properties in the presence of noise. specifically, we analyze stochastic gradient descent, impact of noise on oscillators synchronization and extensions of combination properties of contracting systems to the stochastic case. this is a first step towards combining the interesting and simplifying properties of contracting systems with the probabilistic approach.
energy dependence of moments of net-proton multiplicity distributions at rhic. we report the beam energy (\sqrt s_{nn} = 7.7 - 200 gev) and collision centrality dependence of the mean (m), standard deviation (\sigma), skewness (s), and kurtosis (\kappa) of the net-proton multiplicity distributions in au+au collisions. the measurements are carried out by the star experiment at midrapidity (|y| &lt; 0.5) and within the transverse momentum range 0.4 &lt; pt &lt; 0.8 gev/c in the first phase of the beam energy scan program at the relativistic heavy ion collider. these measurements are important for understanding the quantum chromodynamic (qcd) phase diagram. the products of the moments, s\sigma and \kappa\sigma^{2}, are sensitive to the correlation length of the hot and dense medium created in the collisions and are related to the ratios of baryon number susceptibilities of corresponding orders. the products of moments are found to have values significantly below the skellam expectation and close to expectations based on independent proton and anti-proton production. the measurements are compared to a transport model calculation to understand the effect of acceptance and baryon number conservation, and also to a hadron resonance gas model.
sr-82 purification procedure using chelex-100 resin. 82rb is a positron-emitting radionuclide widely used in nuclear cardiology. one great advantage is its availability through a generator loaded with 82sr. 82sr can be produced in a high energy cyclotron by irradiating rubidium chloride target with proton beam. in this paper, we present an extensive study (elution profiles, effect of the elution flow rate) on the use of chelex-100 resin and ammonia buffer. no significant effect of flow rate was evidenced between 1 and 10 ml/min leading us to propose a purification process which can be easily automated.
an international initiative on long-term behavior of high-level nuclear waste glass. nations using borosilicate glass as an immobilization material for radioactive waste have reinforced the importance of scientific collaboration to obtain a consensus on the mechanisms controlling the long-term dissolution rate of glass. this goal is deemed to be crucial for the development of reliable performance assessment models for geological disposal. the collaborating laboratories all conduct fundamental and/or applied research using modern materials science techniques. this paper briefly reviews the radioactive waste vitrification programs of the six participant nations and summarizes the current state of glass corrosion science, emphasizing the common scientific needs and justifications for on-going initiatives.
studies of di-jets in au+au collisions using angular correlations with respect to back-to-back leading hadrons. jet-medium interactions are studied via a multi-hadron correlation technique (called "2+1"), where a pair of back-to-back hadron triggers with large transverse momentum is used as a proxy for a di-jet. this work extends the previous analysis for nearly-symmetric trigger pairs with the highest momentum threshold of trigger hadron of 5 gev/$c$ with the new calorimeter-based triggers with energy thresholds of up to 10 gev and above. the distributions of associated hadrons are studied in terms of correlation shapes and per-trigger yields on each trigger side. in contrast with di-hadron correlation results with single triggers, the associated hadron distributions for back-to-back triggers from central au+au data at $\sqrt{s_{nn}}$=200 gev show no strong modifications compared to d+au data at the same energy. an imbalance in the total transverse momentum between hadrons attributed to the near-side and away-side of jet-like peaks is observed. the relative imbalance in the au+au measurement with respect to d+au reference is found to increase with the asymmetry of the trigger pair, consistent with expectation from medium-induced energy loss effects. in addition, this relative total transverse momentum imbalance is found to decrease for softer associated hadrons. such evolution indicates the energy missing at higher associated momenta is converted into softer hadrons.
evaluation of discrepancies in tetravalent oxide solubility values by isotopic exchange and its impact on the safety assessment. null
idempotent version of the fréchet contingency array problem. in this paper we study the idempotent version of the so-called fréchet correlation array problem. the problem is studied using an algebraic approach. the major result is that there exists a unique upper bound and several lower bounds. the formula for the upper bound is given. an algorithm is proposed to compute one lower bound. another algorithm is provided to compute all lower bounds, but the number of lower bounds may be a very large number. note that all these results are only based on the distributive lattice property of the idempotent algebraic structure.
electrolocation sensors in conducting water bio-inspired by electric fish. this article presents the first research into designing an active sensor inspired by electric fish. it is notable for its potential for robotics underwater navigation and exploration tasks in conditions where vision and sonar would meet difficulty. it could also be used as a complementary omnidirectional, short range sense to vision and sonar. combined with a well defined engine geometry, this sensor can be modeled analytically. in this article, we focus on a particular measurement mode where one electrode of the sensor acts as a current emitter and the others as current receivers. in spite of the high sensitivity required by electric sense, the first results show that we can obtain a detection range of the order of the sensor length, which suggests that this sensor principle can be used for robotics obstacle avoidance as it is illustrated at the end of the article.
how to take into account potential change of deterioration mode in condition-based maintenance decision rule. classical results in maintenance optimization are based on the characterization of an average system degradation behaviour. the main reasons of such approaches are the robustness of the decisions with respect to the system state and the time horizon. by cons, the ''smoothing'' effect in existing degradation models and the ''generic'' aspect of the decision affect the decision quality as these models do not take into account potential changes in deterioration modes. such consideration can be particularly valuable in a safety context. we propose here a condition-based maintenance approach which adapts the maintenance decision according to the degradation behaviour updated using current state observations. first, we propose to extend the system state definition, usually limited to an indicator of observable degradation by adding information related to the speed of deterioration referred hereafter as potential of degradation or deterioration growth rate. moreover, we propose to take benefit of current observations as well as performed actions to update the degradation laws. more than the model fitness improvement, another advantage of our approach is the proposition of a more realistic modelling of the maintenance impact onto the future behaviour of the system. however, the introduction of the potential of degradation as a new decision parameter does not prejudge an intuitive structure for the decision policy. moreover, as the classical concept of the failure rate, the potential of degradation is non observable. which implies more efforts in both optimization modelling and solution procedure. we highlight the structural properties of the optimization problem which ensure an optimal control limit policy. we will conclude our communication by the illustration of the results of our maintenance model in a pavement management context.
jescala: modular coordination with declarative events and joins. advanced concurrency abstractions overcome the drawbacks of low-level techniques such as locks and monitors, freeing programmers that implement concurrent applications from the burden of concentrating on low-level details. however, with current approaches the coordination logic involved in complex coordination schemas is fragmented into several pieces including join patterns, data emissions triggered in different places of the application, and the application logic that implicitly creates dependencies among channels, hence indirectly among join patterns. we present jescala, a language that captures coordination schemas in a more expressive and modular way by leveraging a seamless integration of an advanced event system with join abstractions. we validate our approach with case studies and provide a first performance assessment.
methodology to estimate methane surface emission in a landfill site. null
note on the swimming of an elongated body in a non-uniform flow. this paper presents an extension of lighthill's large-amplitude elongated-body theory of fish locomotion which enables the effects of an external weakly non-uniform potential flow to be taken into account. to do so, the body is modelled as a kirchhoff beam, made up of elliptical cross-sections whose size may vary along the body, undergoing prescribed deformations consisting of yaw and pitch bending. the fluid velocity potential is decomposed into two parts corresponding to the unperturbed potential flow, which is assumed to be known, and to the perturbation flow. the laplace equation and the corresponding neumann's boundary conditions governing the perturbation velocity potential are expressed in terms of curvilinear coordinates which follow the body during its motion, thus allowing the boundary of the body to be considered as a fixed surface. equations are simplified according to the slenderness of the body and the weakness of the non-uniformity of the unperturbed flow. these simplifications allow the pressure acting on the body to be determined analytically using the classical bernoulli equation, which is then integrated over the body. the model is finally used to investigate the passive and the active swimming of a fish in a kármán vortex street.
a hybrid dynamic model for bio-inspired soft robots - application to a flapping-wing micro air vehicle. the paper deals with the dynamic modeling of bio-inspired robots with soft appendages such as flying insect-like or swimming fish-like robots. in order to model such soft systems, we propose to use the mobile multibody system framework introduced in [1][2][3]. in such a framework, the robot is considered as a tree-like structure of rigid bodies where the evolution of the position of the joints is governed by stress-strain laws or control torques. based on the newton-euler formulation of these systems, we propose a new algorithm able to compute at each step of a time loop both the net and passive joint accelerations along with the control torques supplied by the motors. to illustrate, based on previous work [4], the proposed algorithm is applied to the simulation of the hovering flight of a soft flapping-wing insect-like robot (see the attached video).
using the execo toolbox to perform automatic and reproducible cloud experiments. on cloud testbeds, conducting reproducible and verifiable experiments cannot be done by hand. this paper describes execo, a library providing easy and efficient control of local or remote, standalone or parallel, processes execution, as well as tools designed for scripting distributed computer science experiments or tests on any computing platform, in particular on the grid'5000 testbed. execo usage is illustrated by two experiments exploring the impact on performances of virtual machines collocation and the performances of virtual machines migrations.
adding a live migration model into simgrid, one more step toward the simulation of infrastructure-as-a-service concerns. virtual machine (vm) placement problem has been active research area over the past decade. the research community needs an open simulation framework that can accurately and scalably simulate virtual machine operations including live migrations. however, existing cloud simulation frameworks cannot reproduce live migration behaviors correctly. a naive migration model, not considering memory update operations nor resource sharing contention, can drastically underestimate the duration of a live migration and the size of migration traffic. in this paper, we propose a simulation framework of virtualized distributed systems with the first class support of live migration operations. we developed a resource share calculation mechanism for vms and a live migration model implementing the precopy migration algorithm of qemu/kvm. we extended a widely-used simulation toolkits, simgrid, which allows users to simulate large-scale distributed systems by using user friendly programming api. through experiments, we confirmed that our simulation framework correctly reproduced live migration behaviors of the real world under various conditions. through a first use case, we also confirmed that it is possible to conduct large scale simulations of complex virtualized workloads upon hundred thousands of vms upon thousands of physical machines (pms).
adding virtual machine abstractions into simgrid, a first step toward the simulation of infrastructure-as-a-service concerns. as real systems become larger and more complex, the use of simulator frameworks grows in our research community. by leveraging them, users can focus on the major aspects of their algorithm, run in-siclo experiments (i.e., simulations), and thoroughly analyze results, even for a large-scale environment without facing the complexity of conducting in-vivo studies (i.e., on real testbeds). since nowadays the virtual machine (vm) technology has become a fundamental building block of distributed computing environments, in particular in cloud infrastructures, our community needs a full-fledged simulation framework that enables us to investigate large-scale virtualized environments through accurate simulations. to be adopted, such a framework should provides easy-to-use apis, close to the real ones and preferably fully compatible with those of an existing popular simulation framework. in this paper, we present the current implementation status of a highly-scalable and versatile simulation framework supporting vm environments, extending a widely-used, open-source frame- work, simgrid. our simulation framework allows users to launch hundreds of thousands of vms on their simulation programs and control vms in the same manner as in the real world (e.g., suspend/resume and migrate). users can execute computation and communication tasks on physical machines (pms) and vms through the same simgrid api, which will provide a seamless migration path to iaas simulations for thousands of simgrid users. preliminary validations showed that the resource sharing mechanism of the vm support worked correctly.
a shared " passengers &amp; goods " city logistics system. many strategic planning models have been developed to help decision making in city logistics. such models do not take into account, or very few, the flow of passengers because the considered unit does not have the same nature (a person is active and a good is passive). however, it seems fundamental to gather the goods and the passengers in one model when their respective transports interact with each other. in this context, we suggest assessing a shared passengers &amp; goods city logistics system where the spare capacity of public transport is used to distribute goods toward the city core. we model the problem as a vehicle routing problem with transfers and give a mathematical formulation. then we propose an adaptive large neighborhood search (alns) to solve it. this approach is evaluated on data sets generated following a field study in the city of la rochelle in france.
heavy-quarkonium suppression in p-a collisions from parton energy loss in cold qcd matter. the effects of parton energy loss in cold nuclear matter on heavy-quarkonium suppression in p-a collisions are studied. it is shown from first principles that at large quarkonium energy e and small production angle in the nucleus rest frame, the medium-induced energy loss scales as e. using this result, a phenomenological model depending on a single free parameter is able to reproduce j/psi and upsilon suppression data in a broad xf-range and at various center-of-mass energies. these results strongly support energy loss as the dominant effect in heavy-quarkonium suppression in p-a collisions. predictions for j/psi and upsilon suppression in p-pb collisions at the lhc are made. it is argued that parton energy loss scaling as e should generally apply to hadron production in p-a collisions, such as light hadron or open charm production.
the influence of the openness of an e-learning situation on adult learners'self-regulation. this article presents an empirical research conducted with french speaking adult studying for a diploma. their training took place mainly in e-learning. the goal of this research was to identify and explain the processes of influence existing between two specific dimensions: the degree of openness of the components of the e-learning situation and students' self-regulated behaviors in the management of these components (2). this research was based on the socio-cognitive theory of self-regulation (bandura, 1986; schunk &amp; zimmerman, 2007; zimmerman, 2002) and on a theoretical definition of the notion of "openness" (jézégou (2005). it applied the "actantial model" (greimas, 1966; hiernaux, 1977) for analyzing data collected while using a specific validated instrument of assessment of openness (jézégou, 2010b). the main results of this empirical work are the role played by three psychological dimensions in the influence processes identified. more empirical study is required to confirm their explanatory and validity.
tactical production planning under system availability constraint. null
a preliminary integrated model for optimizing tactical production planning and condition-based maintenance. the inclusion of production and maintenance objectives in an overall decision-making approach remains as well an academic challenge as a response to a real industrial need. based on the observation of mutual dependence between production planning and the stochastic variability of the system production performance due to degradation, we propose in this paper a strategy for optimizing both tactical production planning and maintenance under feasibility constraint. the strategy is driven by an iterative two-unit algorithm. the first unit performs the master production schedule optimization whereas the second unit provides an estimation of the feasibility indicators via a stochastic simulation-based approach. mathematical framework and the proposed iterative algorithm are directly integrated in friendly-user interface software for optimizing the methodology usability in an industrial context.
a particle swarm approach for the mllp. this contribution presents a discrete particle swarm optimization (dpso) approach for the multi-level lot-sizing problem (mllp), which is an uncapacitated lot sizing problem dedicated to materials requirements planning (mrp) systems. the originality of the proposed dpso approach is that it is based on cost modification. by the way, we use pso for that it has been developed: the continuous optimization. each particle of the swarm is represented by a matrix of logistic costs. a sequential approach heuristic, using wagner-whitin algorithm, is then used to determine the associated production planning. the first results obtained are very encouraging. our dpso outperforms the results recently published with other nature-inspired algorithms.
tactical planning for optimal cash flow and value sharing in supply chain. in this paper, we discuss optimisation of cash flow and value sharing in the context of supply chain planning. based on the previous work of (comelli et al., 2007), a generic objective function which allows cash flow optimisation is presented. it could be adapted to any supply chain planning model by using links between physical and financial flow. a mathematical model is also given: its aim is to share the cash flow created by the whole supply chain among the partners thanks to transfer pricing. then, a framework composed with a linking between tactical planning model and the latter, is presented. finally, a real case study is given to illustrate our approach.
discrete particle swarm optimization for the multi-level lot-sizing problem. this paper presents a discrete particle swarm optimization (dpso) approach for the multi-level lot-sizing problem (mllp), which is an uncapacitated lot sizing problem dedicated to materials requirements planning (mrp) systems. the proposed dpso approach is based on cost modification and uses pso in its original form with continuous velocity equations. each particle of the swarm is represented by a matrix of logistic costs. a sequential approach heuristic, using wagner-whitin algorithm, is used to determine the associated production planning. the authors demonstrate that any solution of the mllp can be reached by particles. the sequential heuristic is a subjective function from the particles space to the set of the production plans, which meet the customer's demand. the authors test the dpso scheme on benchmarks found in literature, more specifically the unique dpso that has been developed to solve the mllp.
metaheuristic for the capacitated lot sizing problem: a software tool for mps elaboration. the master production schedule elaboration plays a major part in tactical planning. among mathematical models which deal with the tactical planning, a particular one is dedicated to it: the capacitated lot‐sizing problem. literature about its resolution is huge, but few metaheuristics have been developed in order to solve it: we propose to use optimisation methods based on a simulated annealing: the data encoding are based on a production planning matrix and the neighbourhood system is maked up of several possible moves. we also proposed a bi‐objective function which integrates logistic costs and an evaluation of the degree of the capacities' temporarily leave the set of feasible solutions in order to escape from local minimas. we have tested our optimisation methods on benchmarks from the literature and some best results are outperformed. these methods have been integrated into a software tool.
neutral pion cross section and spin asymmetries at intermediate pseudorapidity in polarized proton collisions at sqrt{s} = 200 gev. the differential cross section and spin asymmetries for neutral pions produced within the intermediate pseudorapidity range 0.8 &lt; {\eta} &lt; 2.0 in polarized proton-proton collisions at sqrt{s} = 200 gev are presented. neutral pions were detected using the endcap electromagnetic calorimeter in the star detector at rhic. the cross section was measured over a transverse momentum range of 5 &lt; p_t &lt; 16 gev/c and is found to be within the scale uncertainty of a next-to-leading order perturbative qcd calculation. the longitudinal double-spin asymmetry, a_ll, is measured in the same pseudorapidity range. this quantity is sensitive to the gluonic contribution to the proton spin, {\delta}g(x), at low bjorken-x (down to x approx 0.01), where it is less constrained by measurements at central pseudorapidity. the measured a_ll is consistent with model predictions. the parity-violating asymmetry, a_l, is also measured and found to be consistent with zero. the transverse single-spin asymmetry, a_n, is measured within a previously unexplored kinematic range in feynman-x and p_t. such measurements may aid our understanding of the on-set and kinematic dependence of the large asymmetries observed at more forward pseudorapidity ({\eta} approx 3) and their underlying mechanisms. the a_n results presented are consistent with a twist-3 model prediction of a small asymmetry within the present kinematic range.
taming the zoo of supersymmetric quantum mechanical models. we show that in many cases nontrivial and complicated supersymmetric quantum mechanical (sqm) models can be obtained from the simple model describing free dynamics in flat complex space by two operations: (i) hamiltonian reduction and (ii) similarity transformation of the complex supercharges. we conjecture that it is true for any sqm model.
centrality and pt dependence of j/psi suppression in proton-nucleus collisions from parton energy loss. the effects of parton energy loss and pt-broadening in cold nuclear matter on the pt and centrality dependence, at various rapidities, of j/psi suppression in p-a collisions are investigated. calculations are systematically compared to e866 and phenix measurements. the very good agreement between the data and the theoretical expectations further supports pt-broadening and the associated medium-induced parton energy loss as dominant effects in j/psi suppression in high-energy p-a collisions. predictions for j/psi (and upsilon) suppression in p-pb collisions at the lhc are given.
response of the xenon100 dark matter detector to nuclear recoils. results from the nuclear recoil calibration of the xenon100 dark matter detector installed underground at the laboratori nazionali del gran sasso (lngs), italy are presented. data from measurements with an external 241ambe neutron source are compared with a detailed monte carlo simulation which is used to extract the energy dependent charge-yield qy and relative scintillation efficiency leff. a very good level of absolute spectral matching is achieved in both observable signal channels - scintillation s1 and ionization s2 - along with agreement in the 2-dimensional particle discrimination space. the results confirm the validity of the derived signal acceptance in earlier reported dark matter searches of the xenon100 experiment.
calculation of the gibbs free energy of solvation and dissociation of hcl in water via monte carlo simulations and continuum solvation models. the gibbs free energy of solvation and dissociation of hydrogen chloride in water is calculated through a combined molecular simulation/quantum chemical approach at four temperatures between t = 300 and 450 k. the gibbs free energy is first decomposed into the sum of two components: the gibbs free energy of transfer of molecular hcl from the vapor to the aqueous liquid phase and the standard-state gibbs free energy of acid dissociation of hcl in aqueous solution. the former quantity is calculated using gibbs ensemble monte carlo simulations using either kohn-sham density functional theory or a molecular mechanics force field to determine the system's potential energy. the latter gibbs free energy contribution is computed using a continuum solvation model utilizing either experimental reference data or micro-solvated clusters. the predicted combined solvation and dissociation gibbs free energies agree very well with available experimental data.
a large neighborhood search for the shift design personnel task scheduling problem with equity. null
filtering atmostnvalue with difference constraints: application to the shift minimisation personnel task scheduling problem. this paper introduces a propagator which filters a conjunction of difference constraints and an atmostnvalue constraint. this propagator is relevant in many applications such as the shift minimisation personnel task scheduling problem, which is used as a case study all along this paper. extensive experiments show that it significantly improves a straightforward cp model, so that it competes with best known approaches from operational research.
a branch-and-cut-and-price approach for the pickup and delivery problem with shuttle routes. the pickup and delivery problem with shuttle routes (pdps) is a special case of the pickup and delivery problem with time windows (pdptw) where the trips between the pickup points and the delivery points can be decomposed into two legs. the first leg visits only pickup points and ends at some delivery point. the second leg is a direct trip - called a shuttle - between two delivery points. this optimization problem has practical applications in the transportation of people between a large set of pickup points and a restricted set of delivery points. this paper proposes three mathematical models for the pdps and a branch- and-cut-and-price algorithm to solve it. the pricing subproblem, an elementary shortest path problem with resource constraints (espprc) is solved with a la- beling algorithm enhanced with efficient dominance rules. three families of valid inequalities are used to strengthen the quality of linear relaxations. the method is evaluated on generated and real-world instances containing up to 193 transportation requests. instances with up to 87 customers are solved to optimality within a computation time of one hour.
the dial-a-ride problem with transfers. the dial-a-ride problem with transfers (darpt) consists in defining a set of routes that satisfy transportation requests of users between a set of pickup points and a set of delivery points, in the presence of ride time constraints. users may change vehicles during their trip. this change of vehicle, called a transfer, is made at specific locations called transfer points. solving the darpt involves modeling and algorithmic difficulties. in this paper we provide a solution method based on an adaptive large neighborhood search (alns) metaheuristic and explain how to check the feasibility of a request insertion. the method is evaluated on real-life and generated instances. experiments show that savings due to transfers can be up to 8\% on real-life instances.
underwater reflex navigation in confined environment based on electric sense. this paper shows how a sensor inspired by an electric fish could be used to help navigate in confined environments. exploiting the morphology of the sensor, the physics of electric interactions, as well as taking inspiration from passive electrolocation in real fish, a set of reactive control laws encoding simple behaviors, such as avoiding any electrically contrasted object, or seeking a set of objects while avoiding others according to their electric properties, is proposed. these reflex behaviors are illustrated on simulations and experiments carried out on a setup dedicated to the study of electric sense. the approach does not require a model of the environment and is quite cheap to implement.
impact of a sulphidogenic environment on the corrosion behavior of carbon steel at 90 °c. the influence of sulphide ion concentration on the behavior of carbon steel in a synthetic solution at 90 °c has been investigated using the methods of weight loss, scanning electron microscopy/energy dispersive x-ray spectroscopy (sem/eds), confocal micro-raman spectroscopy and x-ray diffraction (xrd). corrosion batch experiments were conducted at 90 °c for 1 month with steel coupons immersed in na2s solutions. weight loss measurements revealed that the corrosion layer resistance is strongly dependent on both the sulphide concentration and the physicochemical properties of the corrosion products. in the absence of sulphide ions, a magnetite (fe3o4) corrosion product layer was formed on the steel coupon, while in presence of sulphide ions (1 mg l−1), we observed the formation of a less protective mackinawite corrosion layer. at higher sulphide concentrations (5-15 mg l−1), highly protective pyrrhotite and pyrite are formed, inhibiting the steel corrosion process. thus, it has been suggested that the formation of pyrite and/or pyrrhotite could be a promising strategy to protect against carbon steel corrosion in sulphidogenic media.
modeling and analysis of networked control algorithms using descriptor models. this paper deals with the problem of controllers/filters implementation over networked control systems (ncs). the implementation of a given controller/filter over ncs is not unique, depending on its realization and how it is distributed over several cpu. the point here is to consider a dedicated descriptor model, allowing the essential features of the implementation to be represented. this macroscopic model accounts for the inherent delays or packet losses occurring during communication between subsystems. it makes possible to consider in the same framework, deteriorations induced by implementation of whatever nature: i) computation using a finite word length arithmetic and ii) communication delays over the network. focusing on the last point, the paper shows how to evaluate the resilience of a given realization against internal delays. finally, a filtering problem is considered to show the importance on how the filter is implemented, by using the modeling and analyzing tools proposed.
radiometals for combined imaging and therapy. null
emissivity and conductivity of parton-hadron matter. we investigate the properties of the qcd matter across the deconfinement phase transition. in the scope of the parton-hadron string dynamics (phsd) transport approach, we study the strongly interacting matter in equilibrium as well as the out-of equilibrium dynamics of relativistic heavy-ion collisions. we present here in particular the results on the electromagnetic radiation, i.e. photon and dilepton production, in relativistic heavy-ion collisions and the relevant correlator in equilibrium, i.e. the electric conductivity. by comparing our calculations for the heavy-ion collisions to the available data, we determine the relative importance of the various production sources and address the possible origin of the observed strong elliptic flow $v_2$ of direct photons.
j/psi production and nuclear effects in p-pb collisions at sqrt(snn)=5.02 tev. inclusive j/psi production has been studied with the alice detector in p-pb collisions at sqrt(snn) = 5.02 tev at the cern lhc, in the rapidity domains 2.03 &lt; ycms &lt; 3.53 and -4.46 &lt; ycms &lt; -2.96, down to zero transverse momentum. the j/psi measurement is performed in the muon spectrometer through the mu+ mu- decay mode. in this paper, the j/psi production cross section and the nuclear modification factor r(ppb) for the rapidities under study are presented. while at forward rapidity, corresponding to the proton direction, a suppression of the j/psi yield with respect to binary-scaled pp collisions is observed, in the backward region no suppression is present. the ratio of the forward and backward yields is also shown differentially in rapidity and transverse momentum. theoretical predictions based on nuclear shadowing, as well as on models including, in addition, a contribution from partonic energy loss, are in fair agreement with the experimental results.
beyond the cloud, how should next generation utility computing infrastructures be designed?. to accommodate the ever-increasing demand for utility computing (uc) resources, while taking into account both energy and economical issues, the current trend consists in building larger and larger data centers in a few strategic locations. although such an approach enables to cope with the actual demand while continuing to operate uc resources through centralized software system, it is far from delivering sustainable and efficient uc infrastructures. we claim that a disruptive change in uc infrastructures is required: uc resources should be managed differently, considering locality as a primary concern. we propose to leverage any facilities available through the internet in order to deliver widely distributed uc platforms that can better match the geographical dispersal of users as well as the unending demand. critical to the emergence of such locality-based uc (luc) platforms is the availability of appropriate operating mechanisms. in this paper, we advocate the implementation of a unified system driving the use of resources at an unprecedented scale by turning a complex and diverse infrastructure into a collection of abstracted computing facilities that is both easy to operate and reliable. by deploying and using such a luc operating system on backbones, our ultimate vision is to make possible to host/operate a large part of the internet by its internal structure itself: a scalable and nearly infinite set of resources delivered by any computing facilities forming the internet, starting from the larger hubs operated by isps, government and academic institutions to any idle resources that may be provided by end-users. unlike previous researches on distributed operating systems, we propose to consider virtual machines (vms) instead of processes as the basic element. system virtualization offers several capabilities that increase the flexibility of resources management, allowing to investigate novel decentralized schemes.
multi autonomic management for optimizing energy consumption in cloud infrastructures. as a direct consequence of the increasing popularity of internet and cloud computing services, data centers are amazingly growing and hence have to urgently face energy consumption issues. paradoxically, cloud computing allows infrastructure and applications to dynamically adjust the provision of both physical resources and software services in a pay-per-use manner so as to make the infrastructure more energy efficient and applications more quality of service (qos) compliant. however, optimization decisions taken in isolation at a certain level may indirectly interfere in (or even neutralize) decisions taken at another level, e.g. an application requests more resources to keep its qos while part of the infrastructure is being shutdown for energy reasons. hence, it becomes necessary not only to establish a synergy between cloud layers but also to make these layers flexible and sensitive enough to be able to react to runtime changes and thereby fully benefit from that synergy. this thesis proposes a self-adaptation approach that considers both application internals (architectural elasticity) and infrastructure (resource elasticity) to reduce the energy footprint in cloud infrastructures. each application and the infrastructure are equipped with their own autonomic manager, which allows them to autonomously optimize their execution. in order to get several autonomic managers working together, we propose an autonomic model for coordination and synchronization of multiple autonomic managers. the approach is experimentally validated through two studies: a qualitative (qos improvements and energy gains) and a quantitative one (scalability).
markovian bounds on functions of finite markov chains. in this paper, we obtain markovian bounds on a function of a homogeneous discrete time markov chain. for deriving such bounds, we use well known results on stochastic majorization of markov chains and the rogers-pitman's lumpability criterion. the proposed method of comparison between functions of markov chains is not equivalent to generalized coupling method of markov chains although we obtain same kind of majorization. we derive necessary and sufficient conditions for existence of our markovian bounds. we also discuss the choice of the geometric invariant related to the lumpability condition that we use.
eval-pdu: urban traffic and its environmental impacts modelling to assess urban mobility master plan : conception of a methodology based on the nantes case. the eval-pdu research, funded by french national research agency' program sustainable city, aims at testing methodologies for strategic environmental assessment of urban mobility master plans with an application to nantes metropolis. the general approach used for this interdisciplinary research consists in combining several modelling of flows: first vehicles flows in the city, then the flows of atmospheric pollutants and noise pollution resulting from the traffic. in front of phenomena that are quite impossible to be precisely measured considering their micro-local diversity, modelling seems the only solution to estimate the effect of transport policies on the urban environment (in comparison with a reference situation where they are not applied) and thus to enlighten collective action in order to minimize negative externalities.
nuclear waste disposal: i. laboratory simulation of repository properties. after more than 30 years of research and development, there is a broad technical consensus that geologic disposal will provide for the safety of humankind, now and far into the future. safety analyses have demonstrated that the risk, measured as the exposure to radiation, will be of little consequence. still, there is not yet an operating geologic repository for highly radioactive waste, and there remains substantial public concern about the long-term safety of geologic disposal. in the two linked papers we argue for a stronger connection between the scientific data (this paper i) and the safety analysis, particularly in the context of societal expectations (paper ii). in the present paper i, we use new experimental data on the properties of clay formations simulating geological disposal conditions to illustrate how one can understand the ability of clay to isolate radionuclides. the data include percolation tests on various intact clay–rock cores with different calcite contents. for the first time, hydrodynamic parameters (anion and cation accessible porosities, permeability, dispersion and diffusion coefficients), as well as retention parameters (sorption behavior of iodine, cesium) and materials interaction parameters (glass dissolution rates, etc.) have been obtained for a series of clay–rock samples of varying mineralogy. increased calcite content leads to lower permeability and porosity, but the difference between anion and cation accessible porosity diminished. the data confirm very slow radionuclide migration, and a direct extrapolation to repository geometry yields isolation times, for a 70 m clay–rock formation, of many hundreds of thousands of years, even for the most mobile radionuclides such as iodine-129 and chlorine 36 and complete retention for the more radiotoxic, less mobile radionuclides such as the actinides or cesium-137.in order to assess the meaning of the technical results and derived models for long-term safety, paper ii addresses model validity and credibility not only from a technical perspective, but in a much broader historical, epistemological and societal context. safety analysis is treated in its social and temporal dimensions. this approach provides new insights into the societal dimension of scenarios and risk, and it shows that there is certainly no direct link between increased scientific understanding and a public position for or against different strategies of nuclear waste disposal.
role of structural effects on the collective transverse flow and the energy of vanishing flow in nuclear collisions. we address the question of why so far most of the simulation approaches to find the energy of vanishing flow (evf) in light systems have failed to reproduce the experimental data. by investigating systematically the dependence of the evf on the initial setup of the nuclei in these approaches we find out that for light systems a small variation of this setup can create large differences in the evf whereas for large systems the dependence is weak. these studies have been done with the isospin-dependent quantum molecular dynamics model.
limits on spin-dependent wimp-nucleon cross sections from 225 live days of xenon100 data. we present new experimental constraints on the elastic, spin-dependent wimp-nucleon cross section using recent data from the xenon100 experiment, operated in the laboratori nazionali del gran sasso in italy. an analysis of 224.6 live days x 34 kg of exposure acquired during 2011 and 2012 revealed no excess signal due to axial-vector wimp interactions with 129-xe and 131-xe nuclei. this leads to the most stringent upper limits on wimp-neutron cross sections for wimp masses above 6 gev, with a minimum cross section of 3.5 x 10^{-40} cm^2 at a wimp mass of 45 gev, at 90% confidence level.
application of a bivariate deterioration model for a pavement management optimization. this work is a part of a research project for the development of new condition-based approaches for road maintenance optimization. the degradation pattern on interest is here the longitudinal cracking process due to cumulative fatigue (the traffic repetition leads to the occurrence of cracks in the road basement which grows up to the road surface). in this context, we have proposed a new theoretical deterioration model based on two dependent indicators -the observable deterioration measurement and the potential deterioration growth. even if the construction of the model is based on practical considerations, its application in an operational context remains difficult. the objective of this communication is to study the applicability of such a model and propose some improvements. after analyzing the original model to highlight its strengths and limitations, we propose to revisit the definitions of the decision parameters while specifying the construction of the associated functions. a statistical inference procedure is discussed. a numerical example based on the original maintenance model is presented to illustrate the benefits of this approach that we will present as some best practices for future road pavement management.
influence of an inhomogeneous and expanding medium on signals of the qcd phase transition. according to a fluid dynamic expansion of the fireball we investigate how the inhomogeneity of the system influences the chiral phase transition of qcd. we compare the averaged values of the order parameter in equilibrium with that of a homogeneous system. if the temperature is averaged over a certain region of the fireball the corresponding correlation length does not diverge in an expansion with a critical point.
occurrence of natural organic chlorine in soils for different land uses. consideration of natural formation of organochlorine compounds in soils is necessary in radioecology in order to understand chlorine radioisotope (36cl) cycling in various environments for safety assessment purposes, but also in ecotoxicology because certain chlorinated organics in soils are toxic compounds. on the other hand, occurrence of organic chlorine in soils is poorly documented, especially in non-forest ecosystems. we measured total and organic chlorine concentrations in 51 french surface soils sampled from grassland, arable and forest sites on a national scale (french soil quality monitoring network) in order to characterize the variability of organic chlorine concentrations for these different land uses. while previous studies reported that the chlorination of soil organic matter is responsible for chlorine retention in temperate forest ecosystems, this study shows that the non-extractable organohalogen pool accounts for the majority (&gt;80 % on an average) of the total measurable chlorine in grassland and agricultural soils. this suggests that natural chlorination is a widespread phenomenon in all kinds of soils. a multiple linear regression analysis performed on the dataset indicated that retention of organochlorine in soils is related to the organic carbon content, cl input and soil ph.
the pickup and delivery problem with shuttle routes. the main objective of this communication is to show how realistic-sized instances of the pickup and delivery problem with transfers (pdpt) can be solved to optimality with a branch-and-cut-and-price method, under realistic hypotheses. we introduce the pickup and delivery problem with shuttle routes (pdp-s) which is a special case of the pdpt relying on a structured network with two categories of routes. {\em pickup routes} visit a set of pickup points independently of their delivery points and end at one delivery point. {\em shuttle routes} are direct trips between two delivery points. we propose a path based formulation of the pdp-s and solve it with a branch-and-cut-and-price algorithm. we show that this approach is able to solve real-life instances with up to 87 transportation requests.
chemical and biological evaluation of scandium(iii)-polyaminopolycarboxylate complexes as potential pet agents and radiopharmaceuticals. null
energetic and exergetic assessment of solar and wind potentials in europe. this paper deals with a physics-based assessment of renewable energy potential in europe, particularly solar and wind energy sources, using two literature models. a sensibility analysis with the weather data is first done. actual temperature, pressure, relative humidity, global radiation and wind speed data are employed to develop energy and exergy maps for europe, based on iso-areas of land-use. these maps are compared with similar existing ones. a good agreement is obtained. a paradoxical result is found for wind exergy efficiency. the yearly average exergy efficiency where wind speed is less than 5 m/s is greater than that where wind speed is greater than 7 m/s. this can be explained by the 'dome' shape of wind exergy efficiency. a solar efficiency map for europe is also developed to serve as a useful guide for choosing a renewable energy form based on yearly energy production.
minimization of absorption contrast for accurate amorphous phase quantification: application to zro2 nanoparticles. monoclinic and tetragonal zirconia samples were characterized by x-ray diffraction, pycnometry, thermogravimetric analysis (tga), fourier transform (ft) ir and mass (ms) spectroscopies, and scanning and transmission electron (tem) microscopies. the results show, for the particular case of a tetragonal zirconia sample, an x-ray-undetected subproduct identified as an amorphous organic phase by ftir-atr (attenuated total reflection) and tga-ms. the observations by tem allowed this amorphous phase to be localized on the surface as a shell coating the nanoparticles. moreover, this amorphous phase was quantified by rietveld refinement via the addition of an internal silicon standard. because zirconia and silicon have different linear absorption coefficients, the microabsorption effect was minimized by using small particle sizes. the amorphous phase was calculated to constitute 11.4 (30)% of the initial mass before brindley correction and 10.6 (30)% of the initial mass after brindley correction. the closeness of these values shows that the contribution of the brindley correction can be neglected if precautions are taken on the microabsorption effect. this work has also highlighted the importance of thoroughly characterizing commercial products, which are not necessarily pure. indeed, the presence of impurities could become a non-negligible parameter for physical and chemical properties studies related to commercial materials.
multiplicity dependence of pion, kaon, proton and lambda production in p--pb collisions at sqrt(s_nn) = 5.02 tev. in this letter, comprehensive results on {\pi}+-, k+-, k^0_s, p(antiproton) and {\lambda} anti-{\lambda} production at mid-rapidity (0 &lt; y_cms &lt; 0.5) in p-pb collisions at sqrt(s_nn) = 5.02 tev, measured by the alice detector at the lhc, are reported. the transverse momentum distributions exhibit a hardening as a function of event multiplicity, which is stronger for heavier particles. this behavior is similar to what has been observed in pp and pb--pb collisions at the lhc. the measured pt distributions are compared to results at lower energy and with predictions based on qcd-inspired and hydrodynamic models.
simulation of the 3γ imaging using liquid xenon compton telescope. nuclear medical 3γ imaging is an innovative technique which is studied at the subatech laboratory. it isbased on the three-dimensional localization of a (β+, γ) radioisotope emitter, the 44sc, by using a liquid xenon compton telescope. the position of the disintegration of this radioisotope is obtained by the intersection of the line of response, built by the detection of two 511 kevphotons from the annihilation of a positron, and the cone determined by the third photon. a small prototype xemis1 (xenon medical imaging system) was developed to demonstrate experimentally the feasibility of 3γ imaging. the results of this prototype are quite encouraging in terms of energy resolution, purity of liquid xenon and electronic noise. the monte carlo simulation is an indispensable tool to support the r&amp;d and to evaluate the new proposed technique of imaging ; this thesis work is to develop the simulation of 3γ imaging system by using gate (geant4 application for tomographic emission). new functionalities have been added to gate to simulate a tpc (time projection chamber) detector. we performed a simulation of xemis1 prototype and obtained results in good agreement with our experimental data. the next step of the project is to build a full liquid xenon cylindrical camera for the small animal imaging. the results presented in this thesis of the simulations of this camera demonstrate the ability to locate every decayalong the line of response with very good accuracy and good detection sensitivity. the first direct images of simple phantoms, realized event by event, and after tomographic reconstruction are also presented.
multi-strange baryon production at mid-rapidity in pb-pb collisions at sqrt(s_nn) = 2.76 tev. the alice experiment at the lhc has measured the production of {\xi}- and {\omega}- baryons and their anti-particles in pb-pb collisions at sqrt(s_nn) = 2.76 tev. the transverse momentum spectra at mid-rapidity (|y| &lt; 0.5) for charged {\xi} and {\omega} hyperons have been studied in the range 0.6 &lt; pt &lt; 8.0 gev/c and 1.2 &lt; pt &lt; 7.0 gev/c, respectively, and in several centrality intervals (from the most central 0-10% to the most peripheral 60-80% collisions). these spectra have been compared with the predictions of recent hydrodynamic models. in particular, the krak{ó}w and epos models give a satisfactory description of the data, with the latter covering a wider pt range. mid-rapidity yields, integrated over pt, have been determined. the hyperon-to-pion ratios are similar to those at rhic: they rise smoothly with centrality up to ~ 150 and saturate thereafter. the enhancements (yields per participant nucleon relative to p-p collisions) increase both with the strangeness content of the baryon and with centrality, but are less pronounced than at lower energies.
k^0_s and {\lambda} production in pb-pb collisions at sqrt(snn) = 2.76 tev. the alice measurement of k^0_s and {\lambda} production at mid-rapidity in pb-pb collisions at sqrt(snn) = 2.76 tev is presented. the transverse momentum (pt) spectra are shown for several collision centrality intervals and in the pt range from 0.4 gev/c (0.6 gev/c for {\lambda}) to 12 gev/c. the pt dependence of the {\lambda}/k^0_s ratios exhibits maxima in the vicinity of 3 gev/c, and the positions of the maxima shift towards higher pt with increasing collision centrality. the magnitude of these maxima increases by almost a factor of three between most peripheral and most central pb-pb collisions. this baryon excess at intermediate pt is not observed in pp interactions at sqrt(s) = 0.9 tev and at sqrt(s) = 7 tev. qualitatively, the baryon enhancement in heavy-ion collisions is expected from radial flow. however, the measured pt spectra above 2 gev/c progressively decouple from hydrodynamical-model calculations. for higher values of pt, models that incorporate the influence of the medium on the fragmentation and hadronization processes describe qualitatively the pt dependence of the {\lambda}/k^0_s ratio.
gluon radiation by heavy quarks at intermediate energies. employing scalar qcd we study the gluon emission of heavy quarks created by the interaction with light quarks. we develop approximation formulas for the high energy limit and study when the full calculation reaches this high energy limit. for zero quark masses and in the high energy limit our model reproduces the gunion-bertsch results. we justify why scalar qcd represents a good approximation to the full qcd approach for the energy loss of heavy quarks. in the regime of accessible phenomenology we observe that the emission at small transverse momentum (dead cone effect) is less suppressed than originally suggested. we also investigate the influence of a finite gluon mass on the discussed results.
reference monitors for security and interoperability in oauth 2.0. the oauth 2.0 protocol is a recent ietf standard devoted to providing authorization to clients requiring access to specific resources over http. it was recently adopted by major internet players like google, facebook, and microsoft. it has been pointed out that this protocol is potentially subject to security issues, as well as difficulties concerning the interoperability between protocol participants and application evolution. as we show in this paper, there are indeed multiple reasons that make this protocol hard to implement and impede interoperability in the presence of different kinds of clients. our main contribution consists in a framework that harnesses a type based policy language and aspect-based support for protocol adaptation through flexible reference monitors in order to handle security, interoperability and evolution issues of oauth 2.0. we apply our framework in the context of three scenarios that make explicit variations in the protocol and show how to handle those issues.
medium modification of jet fragmentation in au+au collisions at sqrt(s_nn)=200 gev measured in direct photon-hadron correlations. the jet fragmentation function is measured with direct photon-hadron correlations in p+p and au+au collisions at sqrt(s_nn)=200 gev. the p_t of the photon is an excellent approximation to the initial p_t of the jet and the ratio z_t=p_t^h/p_t^\gamma is used as a proxy for the jet fragmentation function. a statistical subtraction is used to extract the direct photon-hadron yields in au+au collisions while a photon isolation cut is applied in p+p. i_ aa, the ratio of jet fragment yield in au+au to that in p+p, indicates modification of the jet fragmentation function. suppression, most likely due to energy loss in the medium, is seen at high z_t. the fragment yield at low z_t is enhanced at large angles. such a trend is expected from redistribution of the lost energy into increased production of low-momentum particles.
evidence for flow in ppb collisions at 5 tev from v2 mass splitting. we show that a fluid dynamical scenario describes quantitatively the observed mass splitting of the elliptical flow coefficients v2 for pions, kaons, and protons. this provides a strong argument in favor of the existence of a fluid dynamical expansion in ppb collisions at 5tev.
long-range angular correlations of pi, k and p in p--pb collisions at sqrt(s_nn) = 5.02 tev. angular correlations between unidentified charged trigger particles and various species of charged associated particles (unidentified particles, pions, kaons, protons and antiprotons) are measured by the alice detector in p-pb collisions at a nucleon--nucleon centre-of-mass energy of 5.02 tev in the transverse-momentum range 0.3 &lt; p_t &lt; 4 gev/c. the correlations expressed as associated yield per trigger particle are obtained in the pseudorapidity range |eta_lab|&lt;0.8. fourier coefficients are extracted from the long-range correlations projected onto the azimuthal angle difference and studied as a function of $\pt$ and in intervals of event multiplicity. in high-multiplicity events, the second-order coefficient for protons, v_2^p, is observed to be smaller than that for pions, v_2^pi, up to about p_t = 2 gev/c. to reduce correlations due to jets, the per-trigger yield measured in low-multiplicity events is subtracted from that in high-multiplicity events. a two-ridge structure is obtained for all particle species. the fourier decomposition of this structure shows that the second-order coefficients for pions and kaons are similar. the v_2^p is found to be smaller at low p_t and larger at higher p_t than v_2^pi, with a crossing occurring at about 2 gev. this is qualitatively similar to the elliptic-flow pattern observed in heavy-ion collisions. a mass ordering effect at low transverse momenta is consistent with expectations from hydrodynamic model calculations assuming a collectively expanding system.
insight into the mechanism of carbon steel corrosion under aerobic and anaerobic conditions. we particularly focused our study on identifying the corrosion products formed at 30 °c on carbon steel under aerobic and anaerobic conditions and on following their evolution with time due to enhanced microbial activity under environmental and geological conditions. the nature and structural properties of corrosion products were investigated by scanning electron microscopy/energy dispersive x-ray spectroscopy (sem/eds), x-ray diffraction (xrd) and confocal micro-raman spectroscopy. structural characterisation clearly showed the formation of iron oxides (magnetite and maghemite) under aerobic conditions. under anaerobic conditions, the first corrosion product formed on the steel surface was nanocrystalline mackinawite, which was then followed by a fast transformation process into the pyrrhotite phase, and the raman spectrum of monoclinic pyrrhotite was proposed for the first time. finally, this study also shows that in the context of geological disposal of radioactive waste, the corrosion of carbon steel containers in anoxic and sulphidogenic environments sustained by sulphate-reducing bacteria may not be a problem notably due to the formation of a passive layer on the steel surface.
multiplicity dependence of two-particle azimuthal correlations in pp collisions at the lhc. we present the measurements of particle pair yields per trigger particle obtained from di-hadron azimuthal correlations in pp collisions at sqrt(s) = 0.9, 2.76, and 7 tev recorded with the alice detector. the yields are studied as a function of the charged particle multiplicity. taken together with the single particle yields the pair yields provide information about parton fragmentation at low transverse momenta, as well as on the contribution of multiple parton interactions to particle production. data are compared to calculations using the pythia6, pythia8, and phojet event generators.
chapter 10. dynamics models for deformable manipulators. null
reduced locomotion dynamics with passive internal dofs: application to non-holonomic and soft robotics. this article proposes a general modelling approach for locomotion dynamics of mobile multibody systems (mms) containing passive internal degrees of freedom (dofs) concentrated into (ideal or not) joints and/or distributed along deformable bodies of the system. the approach embraces the case of non-holonomic mobile multibody systems with passive wheels, the pendular climbers and the locomotion systems bio-inspired by animals that exploit the advantages of soft appendages such as the fish swimming with their caudal fin or the moths using the softness of their flapping wings to improve flight performance. the article proposes a general structured modelling approach of mms with tree-like structures along with efficient computational algorithms of the resulting equations. the approach is illustrated through non-trivial examples such as the 3d bicycle and a compliant version of the snake-board.
early cosmic ray research in france. the french research on cosmic rays in the first half of the 20th century is summarized. the main experiments are described as the discovery of air cosmic ray showers by pierre auger. the results obtained at the french altitude laboratories like the "pic du midi de bigorre" are also briefly presented.
multiplicity dependence of the average transverse momentum in pp, p-pb, and pb-pb collisions at the lhc. the average transverse momentum versus the charged-particle multiplicity n_ch was measured in p-pb collisions at a collision energy per nucleon-nucleon pair sqrt(s_nn) = 5.02 tev and in pp collisions at collision energies of sqrt(s) = 0.9, 2.76, and 7 tev in the kinematic range 0.15 &lt; p_t &lt; 10.0 gev/c and |eta| &lt; 0.3 with the alice apparatus at the lhc. these data are compared to results in pb-pb collisions at sqrt(s_nn) = 2.76 tev at similar charged-particle multiplicities. in pp and p-pb collisions, a strong increase of with n_ch is observed, which is much stronger than that measured in pb-pb collisions. for pp collisions, this could be attributed, within a model of hadronizing strings, to multiple-parton interactions and to a final-state color reconnection mechanism. the data in p-pb and pb-pb collisions cannot be described by an incoherent superposition of nucleon-nucleon collisions and pose a challenge to most of the event generators.
energy dependence of the transverse momentum distributions of charged particles in pp collisions measured by alice. differential cross sections of charged particles in inelastic pp collisions as a function of p_t have been measured at sqrt(s) = 0.9, 2.76 and 7 tev at the lhc. the p_t spectra are compared to nlo-pqcd calculations. though the differential cross section for an individual sqrt(s) cannot be described by nlo-pqcd, the relative increase of cross section with sqrt(s) is in agreement with nlo-pqcd. based on these measurements and observations, procedures are discussed to construct pp reference spectra at sqrt(s) = 2.76 and 5.02 tev up to p_t = 50 gev/c as required for the calculation of the nuclear modification factor in nucleus-nucleus and proton-nucleus collisions.
effect of h2 produced through steam methane reforming on chp plant efficiency. in situ hydrogen production is carried out by a catalytic reformer kit set up into exhaust gases for a chp plant based on spark ignition engine running under lean conditions. an overall auto-thermal reforming process is achieved. hydrogen production is mainly dependent on o2 content in exhaust gases. experiments are conducted at constant speed at 2 air/fuel ratios and 4 additional natural gas flow rates. h2 content varies in the range 6-10% in vol. h2 content effect is analysed with respect to performance and emissions. comparing with egr shows an increasing of electrical efficiency of 1% whilst heat recovery decreases by 1%. no and hc decrease by 18% and 12%, but co increases by 14%, respectively. the results show that: (i) graphite joints were destroyed under effect of h2 and high temperature; (ii) a cold spot appeared in the rgr line, and condensation has as consequence a carbon deposit; and (iii) no back-fire or knock occurred. © 2011, hydrogen energy publications, llc. published by elsevier ltd. all rights reserved.
recycling flows in emergy evaluation: a mathematical paradox?. this paper is a contribution to the emergy evaluation of systems involving recycling or reuse of waste. if waste exergy (its residual usefulness) is not negligible, wastes could serve as input to another process or be recycled. in cases of continuous waste recycle or reuse, what then is the role of emergy? emergy is carried by matter and its value is shown to be the product of specific energy with mass flow rate and its transformity. this transformity (τ) given as the ratio of the total emergy input and the useful available energy in the product (exergy) is commonly calculated over a specific period of time (usually yearly) which makes transformity a time dependent factor. assuming a process in which a part of the non-renewable input is an output (waste) from a previous system, for the waste to be reused, an emergy investment is needed. the transformity of the reused or recycled material should be calculated based on the pathway of the reused material at a certain time (t) which results in a specific transformity value (τ). in case of a second recycle of the same material that had undergone the previous recycle, the material pathway has a new time (t+t 1) which results in a transformity value (τ 1). recycling flows as in the case of feedback is a dynamic process and as such the process introduces its own time period depending on its pathway which has to be considered in emergy evaluations. through the inspiration of previous emergy studies, authors have tried to develop formulae which could be used in such cases of continuous recycling of material in this paper. the developed approach is then applied to a case study to give the reader a better understanding of the concept. as a result, a 'factor' is introduced which could be included on emergy evaluation tables to account for subsequent transformity changes in multiple recycling. this factor can be used to solve the difficulties in evaluating aggregated systems, serve as a correction factor to up-level such models keeping the correct evaluation and also solve problems of memory loss in emergy evaluation. the discussion deals with the questions; is it a pure mathematical paradox in the rules of emergy? is it consistent with previous work? what were the previous solutions to avoid the cumulative problem in a reuse? what are the consequences?. © 2011 elsevier b.v.
entropy production and field synergy principle in turbulent vortical flows. the heat transfer in turbulent vortical flows is investigated by three different physical approaches. vortical structures are generated by inclined baffles in a turbulent pipe flow, of three different configurations. in the first, the vortex generators are aligned and inclined in the flow direction (called the reference geometry); in the second, a periodic 45° rotation is applied to the tab arrays (alternating geometry); the third is the reference geometry used in the direction opposite to the flow (reversed geometry). the effect of the flow structure on the temperature distribution in these different configurations is analyzed. the conventional approach based on heat-transfer analysis using the nusselt number and the enhancement factor is used to determine the efficiency of these geometries relative to other heat exchangers in the literature. the effect of vorticity on the nusselt number is clearly demonstrated, and so as to highlight the respective roles of the coherent structures and the turbulence, a new parameter is defined as the ratio of the vortex circulation to the turbulent viscosity. the relative contribution of the radial convection to heat transfer appears to increase with reynolds number. the effect of mixing performance on the temperature distribution is investigated by the field synergy method. a global parameter, namely the intersection angle between the velocity and temperature gradient, is defined in order to compare performances. finally, an analysis of energetic efficiency by entropy production, involving both heat transfer and pressure losses, is carried out to determine the overall performance of the heat exchangers. all these approaches lead to the same conclusion: that the reversed geometry presents the best heat transfer coefficient and the best energetic efficiency. the reference geometry shows the worst performance, and the alternating array has intermediate performance. © 2011 elsevier masson sas. all rights reserved.
exact computation of emergy based on a mathematical reinterpretation of the rules of emergy algebra. the emergy algebra is based on four rules, the use of which is sometimes confusing or reserved only to the experts of the domain. the emergy computation does not obey conservation logic (i.e. emergy computation does not obey kirchoff-like circuit law). in this paper the authors propose to reformulate the emergy rules into three axioms which provide (i) a rigourous mathematical framework for emergy computation and (ii) an exact recursive algorithm to compute emergy within a system of interconnected processes at steady state modeled by an oriented graph named the emergy graph.because emergy algebra follows a logic of memorization, the evaluation principles deal with paths in the emergy graph. the underlying algebraic structure is the set of non-negative real numbers operated on by three processes, the maximum (max), addition (+) and multiplication (*). the maximum is associated with the co-product problem. addition is linked with the split problem or with the independence of two emergy flows. and multiplication is related to the logic of memorization. the axioms describe how to use the different operators max, + and * to combine flows without any confusion or ambiguity. the method is tested on five benchmark emergy examples. © 2012 elsevier b.v.
effects on chp plant efficiency of h 2 production through partial oxydation of natural gas over two group viii metal catalysts. blending h 2 with natural gas in spark ignition engines can increase for electric efficiency. in-situ h 2 production for spark ignition engines fuelled by natural gas has therefore been investigated recently, and reformed exhaust gas recirculation (rgr) has been identified a potentially advantageous approach: rgr uses the steam and o 2 contained in exhaust gases under lean combustion, for reforming natural gas and producing h 2, co, and co 2. in this paper, an alternative approach is introduced: air gas reforming circulation (agrc). agrc uses directly the o 2 contained in air, rendering the chemical pathway comparable to partial oxidation. formulations based on palladium and platinum have been selected as potential catalysts. with agrc, the concentrations of the constituents of the reformed gas are approximately 25% hydrogen, 10% carbon monoxide, 8% unconverted hydrocarbons and 55% nitrogen. experimental results are presented for the electric efficiency and exhaust gas (co and hc) composition of the overall system (si engine equipped with agrc). it is demonstrated that the electric efficiency can increase for specific ratios of air to natural gas over the catalyst. although the electric efficiency gain with agrc is modest at around 0.2%, agrc can be cost effective because of its straightforward and inexpensive implementation. misfiring and knock were both not observed in the tests reported here. nevertheless, technical means of avoiding knock are described by adjusting the main flow of natural gas and the additional flow of agrc. copyright © 2012, hydrogen energy publications, llc. published by elsevier ltd. all rights reserved.
impact of building material recycle or reuse on selected emergy ratios. while the emergy evaluation method has been used successfully in recycling processes, this area of application still requires further development. one of such is developing emergy ratios or indices that reflect changes depending on the number of times a material is recycled. some of these materials may either have been recycled or reused continuously as inputs to a building, for example, and thus could have various impacts on the emergy evaluation of the building. the paper focuses on reuse building materials in the context of environmental protection and sustainable development. it presents the results of an emergy evaluation of a low-energy building (leb) in which a percentage of input materials are from recycled sources. the corresponding impacts on the emergy yield ratio (eyr b) and the environmental loading ratio (elr b) are studied. the eyr which is the total emergy used up per unit of emergy invested, is a measure of how much an investment enables a process to exploit local resources in order to further contribute to the economy. the elr however, is the total nonrenewable and imported emergy used up per unit of local renewable resource and indicates the stress a process exhibits on the environment. the evaluation provides values for the selected ratios based on different recycle times. results show that values of the emergy indices vary, even more, when greater amounts of material is recycled with higher amount of additional emergy required for recycling. this provides relevant information prioritizing the selection of materials for recycling or reuse in a building, and the optimum number of reuse or recycle times of a specific material. © 2012 elsevier b.v. all rights reserved.
carbon footprint and emergy combination for eco-environmental assessment of cleaner heat production. the aim of this paper is to study via environmental indicators to which extent, replacing fossil fuel with biomass for heating is an environmentally friendly solution. the environmental impact of using biomass depends mostly on the transportation process. authors define the notion of maximum supply distance, beyond which biomass transportation becomes too environmentally intensive compared to a fossil fuel fired heating system. in this work a carbon footprint analysis and an emergy evaluation, has been chosen to study the substitution of wood for natural gas. the comparative study seeks to examine, via the two approaches, two heating systems: one is fired with wood, transported by trucks and the other one is fired with natural gas transported by pipelines. the results are expressed in terms of maximum supply distance of wood. in the emergy evaluation it represents the maximum supply distance permitting wood to be more emergy saving than natural gas. in the carbon footprint analysis, it represents the maximum supply distance permitting wood to be a carbon saving alternative to natural gas. furthermore, the unification of carbon footprint and emergy evaluation permits to define, for both approaches, the minimum theoretical wood burner first law efficiency that allows, co 2 or emergy to be saved, when there is no wood transport. in order to identify the impacts of the main parameters of the study a sensitivity analysis has been carried out. the case study investigated in this paper shows that there is a large gap between the results. the maximum supply distances calculated via carbon footprint and emergy evaluation are about 5000 km and 1000 km, respectively, anthe minimum theoretical wood burner efficiencies are about 5% and 54%, respectively. © 2012 elsevier ltd. all rights reserved.
a rigourous mathematical framework for computing a sustainability ratio: the emergy. the computational problem of emergy within a general system of interconnected processes at steady state is a subject of interest in literature today. when there is no co-product the proposed method coincides with the track summing method of tennenbaum which was developed precisely for interconnected networks with feedbacks and splits of emergy. as the underlying algebraic structure of the tennenbaum's method is the linear algebra, it is not well-suited to account for the co-product problem which induces the idempotent operator max. thus, authors have chosen another underlying algebraic structure which is the idempotent semiring structure (i.e. a semiring equipped with an idempotent addition). this method is divided into two parts. the first part is the emergy flow enumeration, where paths from an emergy source to the input of a given process are enumerated avoiding double counting of emergy assignation. this part is a path-finding problem which is a slight modification of gerbier of null square approach to find elementary/simple paths in a graph. the second part evaluates the emergy flowing between two components of the system. it is a quantitative part in which the problem of avoiding double counting split and co-product flows are dealt with by introducing a way to mark splits and co-products flows. the method is partially parallelizable. however, the method enumerates paths on a graph thus, in worst cases, its complexity is not polynomial. this paper provides a rigorous framework based on an axiomatic basis to conduct the emergy evaluation of an emergy graph. © 2012 iseis all rights reserved.
how does the solvation unveil ato+ reactivity?. the ato+ molecular ion, a potential precursor for the synthesis of radiotherapeutic agents in nuclear medicine, readily reacts in aqueous solution with organic and inorganic compounds, but at first glance, these reactions must be hindered by spin restriction quantum rules. using relativistic quantum calculations, coupled to implicit solvation models, on the most stable ato+(h2o)6 clusters, we demonstrate that specific interactions with water molecules of the first solvation shell induce a spin change for the ato+ ground state, from a spin state of triplet character in the gas phase to a kramers-restricted closed-shell configuration in solution. this peculiarity allows rationalization of the ato+ reactivity with closed-shell species in aqueous solution and may explain the differences in astatine reactivity observed in 211at production protocols based on "wet" and "dry" processes.
refactoring composite to visitor and inverse transformation in java. we describe how to use refactoring tools to transform a java program conforming to the composite design pattern into a program conforming to the visitor design pattern with the same external behavior. we also describe the inverse transformation. we use the refactoring tool provided by intellij idea.
a framework for the coordination of multiple autonomic managers in cloud environments. null
asynchronous forward bounding revisited. null
estimating the power consumption of an idle virtual machine. power management has become one of the main challenges for data center infrastructures. currently, the cost of powering a server is approaching the cost of the server hardware itself, and, in a near future, the former will continue to increase, while the latter will go down. in this context, virtualization is used to decrease the number of servers, and increase the efficiency of the remaining ones. if virtualization can be used to positively impact on the data center energy consumption, this new abstraction layer disconnects user services (hosted on a virtual machine) from their operating cost. in this paper, we propose an approach and a model to estimate the total power consumption of a virtual machine, by taking into account its static (e.g. memory) and dynamic (e.g. cpu) consumption of resources. this model permits to reconnect each vm to its corresponding operating cost, and provides more information to virtual infrastructure providers and users to optimize their infrastructure/applications. it can be observed from results of experiments that the proposed method outperforms the methods found in the literature that only consider the dynamic consumption of resources.
femtoscopy application of the new epos model to the star experiment. the space-time structure at hadronization was studied within new epos model using femtoscopical methods. the results of the study was compared with the star hbt data for auau collision and first alice hbt data for pp collisions. the model predicted mt and centrality dependence of r out, r side and r long femtoscopy parameters were found to be in accordance with the star data.
formation of superheavy elements in the capture of very heavy ions at high excitation energies. the potential barriers governing the reactions 58fe+244pu, 238u+64ni, and 238u+72ge have been determined from a liquid-drop model taking into account the proximity energy, shell energies, rotational energy, and deformation of the incoming nuclei in the quasimolecular shape valley. double-humped potential barriers appear in these entrance channels. the external saddle-point corresponds to two touching ellipsoidal nuclei when the shell and pairing effects are taken into account, while the inner barrier is due to the shell effects at the vicinity of the spherical shape of the composite system. between them, a large potential pocket exists and persists at very high angular momenta allowing the capture of very heavy ions at high excitation energies.
the "ridge" in proton-proton scattering at 7 tev. one of the most important experimental results for proton-proton scattering at the lhc is the observation of a so-called "ridge" structure in the two particle correlation function versus the pseudorapidity difference $\delta\eta$ and the azimuthal angle difference $\delta\phi$. one finds a strong correlation around $\delta\phi=0$, extended over many units in $\delta\eta$. we show that a hydrodynamical expansion based on flux tube initial conditions leads in a natural way to the observed structure. to get this result, we have to perform an event-by-event calculation, because the effect is due to statistical fluctuations of the initial conditions, together with a subsequent collective expansion. this is a strong point in favour of a fluid-like behavior even in $pp$ scattering, where we have to deal with length scales of the order of 0.1 fm.
considerations concerning the fluctuation of the ratios of two observables. we discuss several possible caveats which arise with the interpretation of measurements of fluctuations in heavy ion collisions. we especially focus on the ratios of particle yields, which has been advocated as a possible signature of a critical point in the qcd phase diagram. we conclude that current experimental observables are not well-defined and are without a proper quantitative meaning.
the lateral distribution function of coherent radio emission from extensive air showers; determining the chemical composition of cosmic rays. the lateral distribution function (ldf) for coherent electromagnetic radiation from air showers initiated by ultra-high-energy cosmic rays is calculated using a macroscopic description. a new expression is derived to calculate the coherent radio pulse at small distances from the observer. it is shown that for small distances to the shower axis the shape of the electric pulse is determined by the 'pancake' function, describing the longitudinal distribution of charged particles within the shower front, while for large distances the pulse is determined by the shower profile. this reflects in a different scaling of the ldf at small and at large distances. as a first application we calculate the ldf for proton- and iron-induced showers and we show that this offers a very sensitive measure to discriminate between these two. we show that due to interference between the geo-magnetic and the charge-excess contributions the intensity pattern of the radiation is not circular symmetric.
longitudinal spin transfer to $\lambda$ and $\bar{\lambda}$ hyperons in polarized proton-proton collisions at $\sqrt{s}$ = 200 gev. the longitudinal spin transfer, $d_{ll}$, from high energy polarized protons to $\lambda$ and $\bar{\lambda}$ hyperons has been measured for the first time in proton-proton collisions at $\sqrt{s} = 200 \mathrm{gev}$ with the star detector at rhic. the measurements cover pseudorapidity, $\eta$, in the range $|\eta| &lt; 1.2$ and transverse momenta, $p_\mathrm{t}$, up to $4 \mathrm{gev}/c$. the longitudinal spin transfer is found to be $d_{ll}= -0.03\pm 0.13(\mathrm{stat}) \pm 0.04(\mathrm{syst})$ for inclusive $\lambda$ and $d_{ll} = -0.12 \pm 0.08(\mathrm{stat}) \pm 0.03(\mathrm{syst})$ for inclusive $\bar{\lambda}$ hyperons with $&lt;\eta&gt; = 0.5$ and $ = 3.7 \mathrm{gev}/c$. the dependence on $\eta$ and $p_\mathrm{t}$ is presented.
scaling properties at freeze-out in relativistic heavy ion collisions. identified charged pion, kaon, and proton spectra are used to explore the system size dependence of bulk freeze-out properties in cu+cu collisions at $\sqrt{s_{nn}}$=200 and 62.4 gev. the data are studied with hydrodynamically-motivated blast-wave and statistical model frameworks in order to characterize the freeze-out properties of the system. the dependence of freeze-out parameters on beam energy and collision centrality is discussed. using the existing results from au+au and $pp$ collisions, the dependence of freeze-out parameters on the system size is also explored. this multi-dimensional systematic study furthers our understanding of the qcd phase diagram revealing the importance of the initial geometrical overlap of the colliding ions. the analysis of cu+cu collisions, which expands the system size dependence studies from au+au data with detailed measurements in the smaller system, shows that the bulk freeze-out properties of charged particles studied here scale with the total charged particle multiplicity at mid-rapidity, suggesting the relevance of initial state effects.
comparison and aggregation of max-plus linear systems. we study linear systems in the max-plus algebra, where the basic operations are maximum and addition. we define a preorder to compare the state vectors of max-plus linear systems with the same dimension. we provide two algebraic methods to get bounds (with respect to this preorder) on the state vectors of a lumped max-plus linear system. the first method is based on the strong lumpability. the second method is based on the coherency property, which also allows one to provide bounds on the state vectors of the original linear system from those for the lumped system. we provide the algorithms to compute all the proposed bounds. we show that they can be used for models with a large state index set by means of a time and space complexity analysis.
mass dependence of balance energy for different n/z ratio. we present the study for the mass dependence of e$_{bal}$ for various n/z ratios covering pure symmetric systems to highly neutron-rich ones.
role of asymmetry of the reaction and momentum dependent interactions on the balance energy for neutron rich nuclei. null
effect of isospin degree of freedom on the counterbalancing of collective transverse in-plane flow. here we aim to understand the effect of isospin dependence of cross section and coulomb repulsion on the counterbalancing of collective flow.
effect of isospin dependence of cross section on symmetric and neutron rich systems. we aim to explore the effect of isospin dependence of cross section on symmetric and neutron rich system. we also aim to explore whether the analysis is affacted if one discusses in terms of "$e_{bal}$ as a function of n/z or n/a" of the system.
observable to explore high density behaviour of symmetry energy. we aim to see the sensitivity of collective transverse in-plane flow to symmetry energy at low as well as high densities and also to see the effect of different density dependencies of symmetry energy on the same.
a dynamical description of neutron star crusts. neutron stars are natural laboratories where fundamental properties of matter under extreme conditions can be explored. modern nuclear physics input as well as many-body theories are valuable tools which may allow us to improve our understanding of the physics of those compact objects. in this work the occurrence of exotic structures in the outermost layers of neutron stars is investigated within the framework of a microscopic model. in this approach the nucleonic dynamics is described by a time-dependent mean field approach at around zero temperature. starting from an initial crystalline lattice of nuclei at subnuclear densities the system evolves toward a manifold of self-organized structures with different shapes and similar energies. these structures are studied in terms of a phase diagram in density and the corresponding sensitivity to the isospin-dependent part of the equation of state and to the isotopic composition is investigated.
dynamical description of exotic structures at subnuclear densities. the dynamics of infinite nuclear matter in the conditions of density and temperature expected in the outermost layers of neutron stars is studied in the framework of a microscopic time-dependent mean-field approach around zero temperature. dynamical processes in inhomogeneous nuclear matter are studied using a large number of nucleons in numerical simulations without any assumptions on the morphology of nuclear matter. the occurrence of exotic structures when varying internal conditions as densities, nuclear species and elementary cell symmetries is investigated. the corresponding structures are studied in terms of a phase diagram in density space evidencing some sensitivity to the isospin-dependent part of the equation of state.
the physics of epos. null
stresscloud: an infrastructure stresser for virtual machine managers. null
generalized liquid drop model and fission, fusion, alpha and cluster radioactivity and superheavy nuclei. a particular version of the liquid drop model taking into account both the mass and charge asymmetries, the proximity energy, the rotational energy, the shell and pairing energies and the temperature has been developed to describe smoothly the transition between one and two-body shapes in entrance and exit channels of nuclear reactions. in the quasi-molecular shape valley where the proximity energy is optimized, the calculated l-dependent fusion and fission barriers, alpha and cluster radioactivity half-lives as well as actinide half-lives are in good agreement with the available experimental data. in this particular deformation path, double-humped potential barriers begin to appear even macroscopically for heavy nuclear systems due to the influence of the proximity forces and, consequently, quasi-molecular isomeric states can survive in the second minimum of the potential barriers in a large angular momentum range.
how to determine experimentally the k+ nucleus potential and the k+ n rescattering cross section in a hadronic environment?. comparing k+ spectra at low transverse momenta for different symmetric collision systems at beam energies around 1 agev allows for a direct determination of both the strength of the k+ nucleus potential as well as of the k+n rescattering cross section in a hadronic environment. other little known or unknown quantities which enter the k+ dynamics, like the production cross sections of k+ mesons or the hadronic equation of state, do not spoil this signal as they cancel by using ratios of spectra. this procedure is based on transport model calculations using the isospin quantum molecular dynamics (iqmd) model which describes the available data quantitatively.
alice potential for direct photon measurements in p-p and pb-pb collisions. the production of direct photons, not coming from hadron decays, at large transverse momentum pt &gt; 2 gev/c in proton-proton collisions at the lhc, is an interesting process to test the predictions of perturbative quantum chromodynamics at the highest energies ever and to put constraints on the gluon density in the proton. furthermore, they provide a baseline reference for quark-gluon-plasma studies in pb-pb collisions. we will present the experimental capabilities of the alice electromagnetic calorimeter emcal to reconstruct the direct and isolated photon spectra in p-p and pb-pb collisions.
study of the electromagnetic background in the xenon100 experiment. the xenon100 experiment, located at the laboratori nazionali del gran sasso, aims to directly detect dark matter in the form of weakly interacting massive particles via their elastic scattering off xenon nuclei. we present a comprehensive study of the predicted electronic recoil background coming from radioactive decays inside the detector and shield materials and intrinsic radioactivity in the liquid xenon. based on geant4 monte carlo simulations using a detailed geometry together with the measured radioactivity of all detector components, we predict an electronic recoil background in the energy region of interest and 30 kg fiducial mass of less than 10-2 events*kg-1*day-1*kev-1, consistent with the experiment's design goal. the predicted background spectrum is in very good agreement with the data taken during the commissioning of the detector in fall 2009.
coherent cherenkov radiation from cosmic-ray-induced air showers. very energetic cosmic rays entering the atmosphere of earth will create a plasma cloud moving with almost the speed of light. the magnetic field of earth induces an electric current in this cloud which is responsible for the emission of coherent electromagnetic radiation. we propose to search for a new effect: because of the index of refraction of air, this radiation is collimated in a cherenkov cone. to express the difference from usual cherenkov radiation, i.e., the emission from a fast-moving electric charge, we call this magnetically induced cherenkov radiation. we indicate its signature and possible experimental verification.
photon signals from quarkyonic matter. we calculate the bremsstrahlung photon spectrum emitted from dynamically evolving quarkyonic matter, and compare this spectrum with that of a high chemical potential quark-gluon plasma as well as to a hadron gas. we find that the transverse momentum distribution and the harmonic coefficient is markedly different in the three cases. the transverse momentum distribution of quarkyonic matter can be fit with an exponential, but is markedly steeper than the distribution expected for the quark-gluon plasma or a hadron gas, even at the lower temperatures expected in the critical point region. the quarkyonic elliptic flow coefficient fluctuates randomly from event to event, and within the same event at different transverse momenta. the latter effect, which can be explained by the shape of quark wavefunctions within quarkyonic matter, might be considered as a quarkyonic matter signature, provided initial temperature is low enough that the quarkyonic regime dominates over deconfinement effects, and the reaction-plane flow can be separated from the fluctuating component.
recent results on heavy quark quenching in ultrarelativistic heavy ion collisions: the impact of coherent gluon radiation. we present a model for radiative energy loss of heavy quarks in quark gluon plasma which incorporates coherence effects. we then study its consequences on the radiation spectra as well as on the nuclear modification factor of open heavy mesons produced in ultrarelativistic heavy ion collisions.
measurement of j/{psi} azimuthal anisotropy in au+au collisions at {sqrt{s_{nn}}} = 200 gev. the measurement of j/{psi} azimuthal anisotropy is presented as a function of transverse momentum for different centralities in au+au collisions at {sqrt{s_{nn}}} = 200 gev. the measured j/{psi} elliptic flow is consistent with zero within errors for transverse momentum between 2 and 10 gev/c. our measurement suggests that j/{psi} with relatively large transverse momentum are not dominantly produced by coalescence from thermalized charm quarks, when comparing to model calculations.
chiral fluid dynamics with explicit propagation of the polyakov loop. we present a fully dynamical model to study nonequilibrium effects in both the chiral and the deconfinement phase transition. the sigma field and the polyakov loop as the corresponding order parameters are propagated by langevin equations of motion. the locally thermalized background is provided by a fluid of quarks and antiquarks. allowing for an exchange of energy and momentum through dissipative and stochastic processes we ensure that the total energy of the coupled system remains conserved. we study its relaxational dynamics in different quench scenarios and are able to observe critical slowing down as well as the enhancement of long wavelength modes at the critical point. during the fluid dynamical expansion of a hot plasma fireball typical nonequilibrium effects like supercooling and domain formation occur when the system evolves through the first order phase transition.
fluctuations and correlations in polyakov loop extended chiral fluid dynamics. we study nonequilibrium effects at the qcd phase transition within the framework of polyakov loop extended chiral fluid dynamics. the quark degrees of freedom act as a locally equilibrated heat bath for the sigma field and a dynamical polyakov loop. their evolution is described by a langevin equation with dissipation and noise. at a critical point we observe the formation of long-range correlations after equilibration. during a hydrodynamical expansion nonequilibrium fluctuations are enhanced at the first order phase transition compared to the critical point.
the air shower maximum probed by cherenkov effects from radio emission. radio detection of cosmic-ray-induced air showers has come to a flight the last decade. along with the experimental efforts, several theoretical models were developed. the main radio-emission mechanisms are established to be the geomagnetic emission due to deflection of electrons and positrons in earth's magnetic field and the charge-excess emission due to a net electron excess in the air shower front. it was only recently shown that cherenkov effects play an important role in the radio emission from air showers. in this article we show the importance of these effects to extract quantitatively the position of the shower maximum from the radio signal, which is a sensitive measure for the mass of the initial cosmic ray. we also show that the relative magnitude of the charge-excess and geomagnetic emission changes considerably at small observer distances where cherenkov effects apply.
dynamic scalability of a consolidation service. in the coming years, cloud environments will increasingly face energy saving issues. while consolidating the virtual machines running in a cloud is a well-accepted solution to reduce the energy consumption, ensuring the scalability of the consolidation service remains a challenging issue. in this paper, we propose an elastic consolidation service that scales according to the dynamic needs of the cloud environment. our proposition is based on (i) virtualizing the consolidation manager, (ii) partitioning the consolidation work and (iii) regulating the consolidation scalability through an autonomic control loop. our proposition has been tested and validated through several experiments.
relativistic langevin dynamics in expanding media. we study the consequences of different realizations of diffusion processes in relativistic langevin simulations. we elaborate on the ito-stratonovich dilemma by showing how microscopically calculated transport coefficients as obtained from a boltzmann/fokker-planck equation can be implemented to lead to an unambiguous realization of the langevin process. pertinent examples within the pre-point (ito) and post-point (hänggi-klimontovich) langevin prescriptions are worked out explicitly. deviations from this implementation are shown to generate variants of the boltzmann distribution as the stationary (equilibrium) solutions. finally, we explicitly verify how the lorentz invariance of the langevin process is maintained in the presence of an expanding medium, including the case of an "elliptic flow" transmitted to a brownian test particle. this is particularly relevant for using heavy-flavor diffusion as a quantitative tool to diagnose transport properties of qcd matter as created in ultrarelativistic heavy-ion collisions.
azimuthal correlations of heavy quarks in pb + pb collisions at s√=2.76 tev at the cern large hadron collider. in this paper we study the azimuthal correlations of heavy quarks in pb+pb collisions with $\sqrt{s}=2.76$ tev at lhc. due to the interaction with the medium heavy quarks and antiquarks are deflected from their original direction and the initial correlation of the pair is broadened. we investigate this effect for different transverse momentum classes. low-momentum heavy-quark pairs lose their leading order back-to-back initial correlation, while a significant residual correlation survives at large momenta. due to the larger acquired average deflection from their original directions the azimuthal correlations of heavy-quark pairs are broadened more efficiently in a purely collisional energy loss mechanism compared to including radiative corrections. this discriminatory feature survives when next-to-leading order production processes are included.
hypernuclear spectroscopy of products from 6li projectiles on a carbon target at 2 agev. a novel experiment, aiming at demonstrating the feasibility of hypernuclear spectroscopy with heavy-ion beams, was conducted. using the invariant mass method, the spectroscopy of hypernuclear products of 6li projectiles on a carbon target at 2 agev was performed. signals of the \lambda-hyperon and 3\lambda h and 4\lambda h hypernuclei were observed for final states of p+\pi^-, 3he+\pi^- and 4he+\pi^-, respectively, with significance values of 6.7, 4.7 and 4.9\sigma. by analyzing the proper decay time from secondary vertex distribution with the unbinned maximum likelihood fitting method, their lifetime values were deduced to be $262 ^{+56}_{-43} \pm 45$ ps for \lambda, $183 ^{+42}_{-32} \pm 37$ ps for 3\lambda h, and $140 ^{+48}_{-33}\pm 35 $ ps for 4\lambda h.
influence of hadronic bound states above $t_c$ on heavy-quark observables in pb+pb collisions at lhc. we investigate how the possible existence of hadronic bound states above the deconfinement transition temperature $t_c$ affects heavy-quark observables like the nuclear modification factor, the elliptic flow and azimuthal correlations. lattice qcd calculations suggest that above $t_c$ the effective degrees of freedom might not be exclusively partonic but that a certain fraction of hadronic degrees of freedom might already form at higher temperatures. this is an interesting questions by itself but also has a strong influence on other probes of the strongly interacting matter produced in ultrarelativistic heavy-ion collisions. a substantial fraction of hadronic bound states above $t_c$ reduces on average the interaction of the heavy quarks with colored constituents of the medium. we find that all studied observables are highly sensitive to the active degrees of freedom in the quark-gluon plasma.
directed flow of charged particles at mid-rapidity relative to the spectator plane in pb-pb collisions at sqrt{s_nn}=2.76 tev. the directed flow of charged particles at mid-rapidity is measured in pb-pb collisions at sqrt{s_nn}=2.76 tev relative to the collision plane defined by the spectator nucleons. both, the rapidity odd (v_1^odd) and even (v_1^even) directed flow components are reported. the v_1^odd component has a negative slope as a function of pseudorapidity similar to that observed at the highest rhic energy, but with about a three times smaller magnitude. the v^even component is found to be non-zero and independent of pseudorapidity. both components show little dependence on the collision centrality and change sign at transverse momenta around 1.2-1.7 gev/c for midcentral collisions. the shape of v_1^even as a function of transverse momentum and a vanishing transverse momentum shift along the spectator deflection for v_1^even are consistent with dipole-like initial density fluctuations in the overlap zone of the nuclei.
beta decay of fission products for the non-proliferation and decay heat of nuclear reactors. today, nuclear energy represents a non-negligible part of the global energy market, most likely a rolling wheel to grow in the coming decades. reactors of the future must face the criteria including additional economic but also safety, non-proliferation, optimized fuel management and responsible management of nuclear waste. in the framework of this thesis, studies on non-proliferation of nuclear weapons are discussed in the context of research and development of a new potential tool for monitoring nuclear reactors, the detection of reactor antineutrinos, because the properties of these particles may be of interest for the international agency of atomic energy (iaea), in charge of the verification of the compliance by states with their safeguards obligations as well as on matters relating to international peace and security. the iaea encouraged its member states to carry on a feasibility study. a first study of non-proliferation is performed with a simulation, using a proliferating scenario with a candu reactor and the associated antineutrinos emission. we derive a prediction of the sensitivity of an antineutrino detector of modest size for the purpose of the diversion of a significant amount of plutonium. a second study was realized as part of the nucifer project, an antineutrino detector placed nearby the osiris research reactor. the nucifer antineutrino detector is dedicated to non-proliferation with an optimized efficiency, designed to be a demonstrator for the iaea. the simulation of the osiris reactor is developed here for calculating the emission of antineutrinos which will be compared with the data measured by the detector and also for characterizing the level of background noises emitted by the reactor detected in nucifer. in general, the reactor antineutrinos are emitted during radioactive decay of fission products. these radioactive decays are also the cause of the decay heat emitted after the shutdown of a nuclear reactor of which the estimation is an issue of nuclear safety. in this thesis, we present an experimental work which aims to measure the properties of beta decay of fission products important to the non-proliferation and reactor decay heat. first steps using the technique of total absorption gamma-ray spectroscopy (tags) were carried on at the radioactive beam facility of the university of jyvaskyla. we will present the technique used, the experimental setup and part of the analysis of this experiment.
transformations between composite and visitor implementations in java. basic automated refactoring operations can be chained to perform complex structure transformations. this is useful for recovering the initial architecture of a source code which has been degenerated with successive evolutions during its maintenance lifetime. this is also useful for changing the structure of a program so that a maintenance task at hand becomes modular when it would be initially crosscutting. we focus on programs structured according to composite and visitor design patterns, which have dual properties with respect to modularity. we consider a refactoring-based round-trip transformation between these two structures and we study how that transformation is impacted by four variations in the implementation of these patterns. we validate that study by computing the smallest preconditions for the resulting transformations. we also automate the transformation and apply it to jhotdraw, where the studied variations occur.
performance of the alice vzero system. alice is an lhc experiment devoted to the study of strongly interacting matter in proton--proton, proton--nucleus and nucleus--nucleus collisions at ultra-relativistic energies. the alice vzero system, made of two scintillator arrays at asymmetric positions, one on each side of the interaction point, plays a central role in alice. in addition to its core function as a trigger, the vzero system is used to monitor lhc beam conditions, to reject beam-induced backgrounds and to measure basic physics quantities such as luminosity, particle multiplicity, centrality and event plane direction in nucleus-nucleus collisions. after describing the vzero system, this publication presents its performance over more than four years of operation at the lhc.
automated verification of model. transformations in the automotive industry. many companies have adopted mdd for developing their software systems. several studies have reported on such industrial experiences by discussing the effects of mdd and the issues that still need to be addressed. however, only a few studies have discussed using automated verification of industrial model transformations. we previously demonstrated how transformations can be used to migrate gm legacy models to autosar models. in this study, we investigate using automated verification for such industrial transformations. we report on applying an automated verification approach to the gm-to-autosar transformation that is based on checking the satisfiability of a relational transformation representation, or a transformation model, with respect to well-formedness ocl constraints. an implementation of this approach is available as a prototype for the atl language. we present the verification results of this transformation and discuss the practicality of using such tools on industrial size problems.
optimal control with preview for lateral steering of a passenger car: design and test on a driving simulator. this chapter is dedicated to studying the characteristics of the optimal preview control for lateral steering of a passenger vehicle. such control is known to guarantee improved performance when the near future of the exogenous signal, here the road curvature, is known. the synthesis is performed in continuous time and leads to a two-degrees of freedom feedback and feedforward controller, whose feedforward part is a finite impulse response filter. the controller has been implemented on the scanertm driving simulator available at irccyn, whose steering column is electrically powered. a methodology for choosing the weighting matrices in the quadratic index and the preview time are finally proposed. the obtained experimental results are discussed as well.
decoupling without prediction of linear systems with delays: a structural approach. we exhibit, using a new structural approach specific for linear systems with delays, a necessary and sufficient condition for static row-by-row decoupling without prediction. moreover, the decoupling problem is analyzed through two different concepts: the total one (without any time assumption), and the partial one (in a temporal mean), which can be considered if the total one has no solution.
on exponential stabilizability for boundary control systems in hilbert spaces. the paper is concerned with infinite-dimensional boundary control systems in hilbert spaces. in order to handle controllability and stabilizability, the first part is devoted to transform equations of the initial boundary control system. some necessary and sufficient conditions for exact controllability are proposed. in this case, the extended controllability gramian is uniformly positive definite and bounded. this operator allows the authors to define a feedback control law which stabilizes exponentially the closed-loop system with an arbitrary decay rate. this feedback is defined from the inverse of the controllability gramian.
real-time optimization of reactive technician tours. null
real-time optimization of technician tours in dynamic environment. null
on exponential stabilizability with arbitrary decay rate for linear systems in hilbert spaces. in this paper, we deal with linear infinite dimensional systems in hilbert spaces. in the beginning, systems are continuous. in the second part, we use a sampling to transform our first system in a discrete-time system. for these systems, exact controllability implies that the extended controllability gramian is uniformly positive definite. this operator allows us to define a feedback control law which stabilizes exponentially the closed-loop system. in the first case, we want to push eigenvalues in the negative part of the complex plane and to have an arbitrary decay rate. in the discrete-time system, we want to push eigenvalues of the closed loop system in the unit ball that is to say to minimize the norm of the closed loop operator. results of continuous-time system are applied to a system described by a wave equation.
open source erp adoption: a technological innovation perspective. this research-in-progress aims to indentify the salient factors explaining adoption of open source software (oss), as a technological innovation. the theoretical background of the paper is based on the technological innovation literature. we choose to focus on the open erp case, as it is considered as a promising innovation for firms - especially medium firms - but open erp also faces numerous challenges. the paper provides a framework and a method for investigation that has to be implemented.
knowledge sharing in the age of the web 2.0: a social capital perspective. null
new complexity results for parallel identical machine scheduling problems with preemption, release dates and regular criteria. in this paper, we are interested in parallel identical machine scheduling problems with preemption and release dates in case of a regular criterion to be minimized. we show that solutions having a permutation flow shop structure are dominant if there exists an optimal solution with completion times scheduled in the same order as the release dates, or if there is no release date. we also prove that, for a subclass of these problems, the completion times of all jobs can be ordered in an optimal solution. using these two results, we provide new results on polynomially solvable problems and hence refine the boundary between p and np for these problems.
a database for quarkonium and open heavy-flavour production in hadronic collisions with hepdata. we report on the creation of a database for quarkonium and open heavy-flavour production in hadronic collisions. this database, made as a collaboration between hepdata and the retequarkonii network of the integrating activity i3hp2 of the 7th framework programme, provides an up-to-date review on quarkonia and open heavy-flavour existing data. we first present the physics motivation for this project, which is connected to the aim of the retequarkonii network, studies of open heavy-flavour hadrons and quarkonia in nucleus-nucleus collisions. then we give a general overview of the database and describe the hepdata database for particle physics, which is the framework of the quarkonia database. finally we describe the functionalities of the database with as example the comparison of the production cross section for the j/$\psi$ meson at different energies.
study of the prospective usefulness of the detection of antineutrinos to monitor nuclear power plants for non proliferation purpose. the field of applied neutrino physics has shown new developments in the last decade. indeed, the antineutrinos (ne) emitted by a nuclear power plant depend on the composition of the fuel : thus their detection could be exploited for determining the isotopic composition of the reactor fuel. the international atomic energy agency (iaea) has expressed its interest in the potentialities of this detection as a new safeguard tool and has created an ad hoc working group devoted to this study. our aim is to determine on the one hand, the current sensitivity reached with the ne detection and on the other hand, the sensitivity required to be useful to the iaea and then we deduce the required performances required for a cubic meter detector.we will first present the physics on which our feasibility study relies : the neutrinos, the b-decay of the fission products (fp) and their conversion into ne spectra. we will then present our simulation tools : we use a package called mcnp utility for reactor evolution (mure), initially developed by cnrs/in2p3 labs to study generation iv reactors. thanks to mure coupled with nuclear data bases, we build the ne spectra by summing the fp contributions. the method is the only one that allows the ne spectra calculations associated to future reactors : we will present the predicted spectra for various innovative fuels. we then calculate the emitted ne associated to various concepts of current and future nuclear reactors in order to determine the sensitivity of the ne probe to various diversion scenarios, taking neutronics into account. the reactor studied are canadian deuterium, pebble bed reactor and fast breeder reactor.
biogas as a renewable energy source : hydrogen sulfide and siloxanes separation. this work presents a study of the siloxanes and hydrogen sulfide separation process applied to the biogas treatment. a bibliography review shows the interest in the development of new technologies of low cost, to integrate them into an overall process of biogas up-grading. one part of this study is focused on the possible separation process by gas-liquid transfer into oils. this technology is compared with a more classical treatment process by adsorption into activated carbons. the results showed that both technologies are complementary, the absorption into oils used primarily to the abatement of high concentrations and the adsorption into the activated carbon as a finishing process. a second part of this study is focused on the hydrogen sulfide treatment. the requirements of abatement are very low, thus the approach is focused on a finishing process to complement the more classical methods. thus a system by physic adsorption into pre-humidified activated carbon cloth was studied. the filter was regenerated in situ by direct electric heating under vacuum pressure. the study of the operating conditions allowed establishing the regeneration parameters and the process sustainability. the interest here is focused on the soft conditions of temperature and vacuum used to achieve the regeneration.
rotating hyperdeformed quasi-molecular states formed in capture of light nuclei and in collision of very heavy ions. within a rotational liquid drop model including the nuclear proximity energy the l-dependent potential barriers governing the capture reactions of light nuclei and of very heavy ions have been determined. rotating quasi-molecular hyperdeformed states appear at high angular momenta. the energy range of these very deformed high spin states is given for light systems. the same approach explains the observation of ternary cluster decay from56ni and 60zn through hyperdeformed shapes at angular momenta around 45 . the apparently observed superheavy nuclear systems in the u+ni and u+ge reactions at high excitation energy might correspond to these rotating isomeric states formed at very high angular momenta even though the shell effects vanish.
harvesting models from web 2.0 databases. data rather than functionality are the sources of competitive advantage for web2.0 applications such as wikis, blogs and social networking websites. this valuable information might need to be capitalized by third-party applications or be subject to migration or data analysis. model-driven engineering (mde) can be used for these purposes. however, mde ﬁrst requires obtaining models from the wiki/blog/website database (a.k.a. model harvesting). this can be achieved through sql scripts embedded in a program. however, this approach leads to laborious code that exposes the iterations and table joins that serve to build the model. by contrast, a domain-speciﬁc language (dsl) can hide these "how" concerns, leaving the designer to focus on the "what", i.e. the mapping of database schemas to model classes. this paper introduces schemol, a dsl tailored for extracting models out of databases which considers web2.0 specifics. web2.0 applications are often built on top of general frameworks (a.k.a. engines) that set the database schema (e.g.,mediawiki, blojsom). hence, table names offer little help in automating the extraction process. in addition, web2.0 data tend to be annotated. user-provided data (e.g., wiki articles, blog entries) might contain semantic markups which provide helpful hints for model extraction. unfortunately, these data end up being stored as opaque strings. therefore, there exists a considerable conceptual gap between the source database and the target metamodel. schemol offers extractive functions and view-like mechanisms to confront these issues. examples using blojsom as the blog engine are available for download.
non-functional requirements in architectural decision making. null
latitudinal and seasonal variations of o2 and d/h on mars using herschel/hifi. as a non-condensible species, molecular oxygen on mars is expected to show spatial and temporal variations, but these measurements have not been performed yet. in addition, mapping the d/h ratio and recording its seasonal variations is a key diagnostic for understanding the past history of water on mars, as well as surface/atmosphere exchange in the water cycle (montmessin et al. jgr 110, e3, citeid e03006, 2005). we have been using hifi abord herschel to study the latitudinal variations of o2 and d/h on mars for two different seasons, ls = 47° (dec. 23, 2011) and ls = 108-115° (may 09-25, 2012). three sets of transitions have been recorded : h218o and hdo around 1630 ghz, o2 and hdo around 1815 ghz, and 13co and co18 around 1870 ghz. the diameter of mars was 8.3 arcsec on dec. 23, 2011, and 8-9 arcsec in may 2012. the herschel field of view is 11.3 arcsec at 1870 ghz and 9.8 arcsec at 1630 ghz. for each period, three observations were successively recorded, centered along the central meridian, at the south limb, the center and the north limb. the total observing time, over the two periods, was 26 hours. a preliminary reduction indicates a mean o2 abundance in agreement with previous measurements (1400 ppm, hartogh et al. aa 521, id.l49, 2010). no significant variation is observed in o2 and co between north and south for ls = 47°, as expected in the vicinity of equinox (forget et al. lpi-1494, 2009). an analysis of the two data sets will be presented.
d meson elliptic flow in non-central pb-pb collisions at sqrts(s_nn) = 2.76tev. azimuthally anisotropic distributions of d0, d+ and d*+ mesons were studied in the central rapidity region (|y|&lt;0.8) in pb-pb collisions at a centre-of-mass energy sqrts(s_nn) = 2.76tev per nucleon-nucleon collision, with the alice detector at the lhc. the second fourier coefficient v2 (commonly denoted elliptic flow) was measured in the centrality class 30-50% as a function of the d meson transverse momentum pt, in the range 2-16gev/c$. the measured v2 of d mesons is comparable in magnitude to that of light-flavour hadrons. it is positive in the range 2 &lt; pt &lt; 6 gev/c with 5.7 sigma significance, based on the combination of statistical and systematic uncertainties.
bounds on the density of sources of ultra-high energy cosmic rays from the pierre auger observatory. we derive lower bounds on the density of sources of ultra-high energy cosmic rays from the lack of significant clustering in the arrival directions of the highest energy events detected at the pierre auger observatory. the density of uniformly distributed sources of equal intrinsic intensity was found to be larger than $\sim (0.06 - 5) \times 10^{-4}$ mpc$^{-3}$ at 95% cl, depending on the magnitude of the magnetic deflections. similar bounds, in the range $(0.2 - 7) \times 10^{-4}$ mpc$^{-3}$, were obtained for sources following the local matter distribution.
practical use of static composition of refactoring operations. refactoring tools are commonly used for remodularization tasks. basic refactoring operations are combined to perform complex program transformations, but the resulting composed operations are rarely reused, even partially, because popular tools have few support for composition. in this paper, we recast two calculus for static composition of refactorings in a type system framework and we discuss their use for inferring useful properties. we illustrate the value of support for static composition in refactoring tools with a complex remodularization use case: a round-trip transformation between programs conforming to the composite and visitor patterns.
charmonium and e+e- pair photoproduction at mid-rapidity in ultra-peripheral pb-pb collisions at sqrt(snn) = 2.76 tev. the alice collaboration at the lhc has measured the j/psi and psi' photoproduction at mid-rapidity in ultra-peripheral pb-pb collisions at sqrt(snn) = 2.76 tev. the charmonium is identified via its leptonic decay for events where the hadronic activity is required to be minimal. the analysis is based on an event sample corresponding to an integrated luminosity of about 23 {\mu}b^{-1}. the cross section for coherent and incoherent j/psi production in the rapidity interval -0.9 &lt; y &lt; 0.9, are d{\sigma}_{j/{\psi}}^{coh}/dy = 2.38^{+0.34}_{-0.24}(sta+sys) mb and d{\sigma}_{j/{\psi}}^{inc}/dy = 0.98^{+0.19}_{-0.17}(sta+sys) mb, respectively. the results are compared to theoretical models for j/{\psi} production and the coherent cross section is found to be in good agreement with those models which include nuclear gluon shadowing consistent with eps09 parametrization. in addition the cross section for the process {\gamma}{\gamma} -&gt; e+e- has been measured and found to be in agreement with the starlight monte carlo predictions.
mid-rapidity anti-baryon to baryon ratios in pp collisions at sqrt(s) = 0.9, 2.76 and 7 tev measured by alice. the ratios of yields of anti-baryons to baryons probes the mechanisms of baryon-number transport. results for anti-p/p, anti-\lambda/\lambda, anti-\xi+/\xi- and anti-\omega+/\omega- in pp collisions at sqrt(s) = 0.9, 2.76 and 7tev, measured with the alice detector at the lhc, are reported. within the experimental uncertainties and ranges covered by our measurement, these ratios are independent of rapidity, transverse momentum and multiplicity for all measured energies. the results are compared to expectations from event generators, such as pythia and hijing/b, that are used to model the particle production in pp collisions. the energy dependence of anti-p/p, anti-\lambda/\lambda, anti-\xi+/\xi- and anti-\omega+/\omega-, reaching values compatible with unity for sqrt(s) = 7tev, complement the earlier anti-p/p measurement of alice. these dependencies can be described by exchanges with the regge-trajectory intercept of \alpha_j ~ 0.5, which are suppressed with increasing rapidity interval \delta y. any significant contribution of an exchange not suppressed at large \delta y (reached at lhc energies) is disfavoured.
correlated background and impact on the measurement of theta_13 with the double chooz detector. the double chooz experiment uses antineutrinos emitted from the chooz nuclear power plant (france) to measure the oscillation mixing parameter θ13. by using two detectors at different baselines, a precise measurement of antineutrinos disappearance is anticipated. the far detector has been taking physics data since april 2011, while the near detector is under construction. data from april 13th 2011 to march 30th 2012 taken with the far detector only have been analyzed and an indication for antineutrino disappearance, consistent with the current neutrino oscillation hypothesis, has been found. the best fit value for the neutrino mixing parameter sin2(2θ13) is 0.109 ± 0.030(stat.) ± 0.025(syst.). this thesis present an accurate description of the double chooz experiment, with particular emphasis on the far detector and its acquisition system. the main focus of the thesis is the accurate study of the correlated background affecting the double chooz antineutrinos sample and its impact on the measurement of the mixing parameter θ13. a general overview of the current experimental scenario which aim to the characterization of the neutrino oscillation is also provided, focusing on the recent results obtained in this field.
the impact of dissipation and noise on fluctuations in chiral fluid dynamics. we investigate the nonequilibrium evolution of the sigma field coupled to a fluid dynamic expansion of a hot fireball to model the chiral phase transition in heavy-ion collisions. the dissipative processes and fluctuations are allowed under the assumption that the total energy of the coupled system is conserved. we use the linear sigma model with constituent quarks to investigate the effects of the chiral phase transition on the equilibration and excitation of the sigma modes. the quark fluid acts as a heat bath in local thermal equilibrium and the sigma field evolves according to a semiclassical stochastic langevin equation of motion. the effects of supercooling and reheating of the fluid in a first order phase transition are observed via the delayed relaxation of the sigma field to a new equilibrium state. at the first order phase transition the nonequilibrium fluctuations are strongly enhanced.
experimental study of anguilliform swimming : application to a biomimetic robot. in order to improve the performance of the submarine robots, the robotics community has been considered a new approach known as the biomimetic. it consist on the study of a living systems such, fish, to design and construct a bio-inspired robot. in this context, recently was took place an european project called angels, in which the objective is to design and construct a fish-like robot inspired from the swimming of the eel. this thesis takes place in this project and is dedicate to the study of the swimming of the robot. experiments were carried out in a hydrodynamic test bed designed and entirely set up for this study. at first,the kinematic shapes (i.e. deformation of the body) adopted by living eel during its swimming against or slantwise a uniform flow, were characterized by mean ofan image processing analysis technique. this study has allowed the establishing of a mathematical correlative model, describing the deformation of the eel's body in these swimming conditions. secondly, we studied the effects of the body's deformation on the lateral flow produced during swimming. piv experiences were carried out on different elliptic cylinder shapes. these experiments have allowed the understanding and the validation of a theoretical approach, concerning the swimming dynamic of the fish, used to obtain the propulsion force produced in reply of the body deformation during swimming. finally, experiments were carried out during the anguilliform swimming in a non-uniform flow such as, avon-kàrmàn vortex street. the goal was to study the hydrodynamics interactions and in particular the mechanisms of the exploited vortices adopted by fish. these experiences were realized on the swimming of a living eel and an anguilliform robot. experiments led on the robot show that under certain conditions, the propulsive force of the robot swimming in a von-kàrmàn vortex street can be increased of about 30 comparison to its swimming in a uniform flow. experiments with eel have allowed the highlighting of a particular shape of its body deformation formed when it's swimming in a reverse von-kàrmàn vortex street. the qualitative analysis realized on this kinematic observation led us to propose a mechanism adopted by the eel to exploited energy from altered flow.
toward cooperative management of large-scale virtualized infrastructures : the case of scheduling. the increasing need in computing power has been satisfied by federating more and more computers (called nodes) to build the so-called distributed infrastructures. over the past few years, system virtualization has been introduced in these infrastructures (the software is decoupled from the hardware by packaging it in virtual machines), which has lead to the development of software managers in charge of operating these virtualized infrastructures. most of these managers are highly centralized (management tasks are performed by a restricted set of dedicated nodes). as established, this restricts the scalability of managers, in other words their ability to be reactive to manage large-scale infrastructures, that are more and more common. during this ph.d., we studied how to mitigate these concerns ; one solution is to decentralize the processing of management tasks, when appropriate. our work focused in particular on the dynamic scheduling of virtual machines, resulting in the dvms (distributed virtual machine scheduler) proposal. we implemented a prototype, that was validated by means of simulations (especially with the simgrid tool) and with experiments on the grid'5000 test bed. we observed that dvms was very reactive to schedule tens of thousands of virtual machines distributed over thousands of nodes. we then took an interest in the perspectives to improve and extend dvms. the final goal is to build a full decentralized manager. this goal should be reached by the discovery initiative,that will leverage this work.
weighted poisson mixed model for underdispersed longitudinal count data. null
on strong stability and stabilizability of systems of neutral type. for linear stationary systems, the infinite dimensional framework allows one to distinguish different notions of stability: weak, strong or exponential. the purpose of this chapler is to investigate the problem of strong stability, i.e. asymptotic non-exponential stability for linear systems of neutral type in order to use this characterization in the study of the stabilizability problem for this type of systems. an important tool in this investigation is the riesz basis property of generalized eigenspaces of the neutral system, because that the generalized eigenvectors do not form, in general, a riesz basis. this allows one to describe more precisely asymptotic non-exponential stability of neutral systems. for a particular case, conditions of strong stabilizability of neutral type systems are given with a feedback law without derivative of the delayed state.
strong stabilizability for a class of linear time delay systems of neutral type. we consider the strong stabilizability, i.e. asymptotic non-exponential stabilizability, problem for a class of neutral type systems. a constructive solution of the feedback law is given, without the derivative of the localized delayed state. our results are based on an abstract theorem on the strong stabilizability of contractive systems in hilbert space. the paper is an extended version of the article published in "applied mathematics letters", vol. 18, 4 (2005), pp. 463--469, cf. hal-00819335-v1.
on exact controllability and complete stabilizability for linear systems in hilbert spaces. a criterion of exact controllabilty using the resolvent of the state space operator is given for linear control system in hilbert space . only surjectivity of the semi-group operators is assumed. this condition is necessary for exact controllability, so the criterion is quite general. relations between exact controllability and complete stabilizability are specified.
on a class of strongly stabilizable systems of neutral type. we consider the strong stabilizability problem for delayed system of neutral type. for simplicity the case of one delay in state is studied. we separate a class of such systems and give a constructive solution of the problem in this case, without the derivative of the localized delayed state. our results are based on an abstract theorem on the strong stabilizability of contractive systems in hilbert space. an illustrating example is also given.
weak structure at infinity and row-by-row decoupling for linear delay systems. we consider the row-by-row decoupling problem for linear delay systems and show some close connections between the design of a decoupling controller and some particular structures of delay systems, namely the so-called weak structure at infinity. the realization by static state feedback of decoupling precompensators is studied, in particular, generalized state feedback laws which may incorporate derivatives of the delayed new reference.
on the structure at infinity of linear delay systems with application to the disturbance decoupling problem. the disturbance decoupling problem is studied for linear delay systems. the structural approach is used to design a decoupling precompensator. the realization of the given precompensator by static state feedback is studied. using various structural and geometric tools, a detailed description of the feedback is given, in particular, derivative of the delayed disturbance can be needed in the realization of the precompensator.
on exponential stabilizability of linear neutral systems. in this paper, we deal with linear neutral functional differential systems. using an extended state space and an extended control operator, we transform the initial neutral system in an infinite dimensional linear system. we give a sufficient condition for admissibility of the control operator $b$, conditions under which operator $b$ can be acceptable in order to work with controllability and stabilizability. necessary and sufficient conditions for exact controllability are provided; in terms of a gramian of controllability $n(\mu)$. assuming admissibility and exact controllability, a feedback control law is defined from the inverse of the operator $n(\mu)$ in order to stabilize exponentially the closed loop system. in this case, the semigroup generated by the closed loop system has an arbitrary decay rate.
anisotropic flow of charged hadrons, pions and (anti-)protons measured at high transverse momentum in pb-pb collisions at root s(nn)=2.76 tev. the elliptic, v2v2, triangular, v3v3, and quadrangular, v4v4, azimuthal anisotropic flow coefficients are measured for unidentified charged particles, pions, and (anti-)protons in pb-pb collisions at view the mathml sourcesnn=2.76 tev with the alice detector at the large hadron collider. results obtained with the event plane and four-particle cumulant methods are reported for the pseudo-rapidity range |η|&lt;0.8|η|&lt;0.8 at different collision centralities and as a function of transverse momentum, ptpt, out to pt=20 gev/cpt=20 gev/c. the observed non-zero elliptic and triangular flow depends only weakly on transverse momentum for pt&gt;8 gev/cpt&gt;8 gev/c. the small ptpt dependence of the difference between elliptic flow results obtained from the event plane and four-particle cumulant methods suggests a common origin of flow fluctuations up to pt=8 gev/cpt=8 gev/c. the magnitude of the (anti-)proton elliptic and triangular flow is larger than that of pions out to at least pt=8 gev/cpt=8 gev/c indicating that the particle type dependence persists out to high ptpt. the goal of ultra-relativistic nucleus-nucleus collisions is to study nuclear matter under extreme conditions. for non-central collisions, in the plane perpendicular to the beam direction, the geometrical overlap region, where the highly lorentz contracted nuclei intersect and where the initial interactions occur, is azimuthally anisotropic. this initial spatial asymmetry is converted via interactions into an anisotropy in momentum space, a phenomenon referred to as transverse anisotropic flow (for a review see [1]). anisotropic flow has become a key observable for the characterization of the properties and the evolution of the system created in a nucleus-nucleus collision. identified particle anisotropic flow provides valuable information on the particle production mechanism in different transverse momentum, ptpt, regions [1]. for pt&lt;2-3 gev/cpt&lt;2-3 gev/c, the flow pattern of different particle species is qualitatively described by hydrodynamic model calculations [2]. at intermediate ptpt, 3.
on decoupling of linear time delay systems by generalized output feedback. we consider the row-by-row decoupling problem for linear delay systems by output feedback. the characterization of the solvability of this problem is given in terms of some easily checkable structural conditions. the main contribution is, in particular, to use generalized output feedback laws which may incorporate derivatives of the delayed new reference.
location of concentration warehouses in a pooled distribution network in the retail sector. null
generalized riesz basis property in the analysis of neutral type systems. the functional differential equation of neutral type is studied. we consider the corresponding operator model in hilbert space m2 = cn × l2(−1, 0;cn) and prove that there exists a sequence of invariant finite-dimensional subspaces which constitute a riesz basis in m2. we also give an example emphasizing that the generalized eigenspaces do not form a riesz basis.
stability analysis of neutral type systems in hilbert space. the asymtoptic stability properties of neutral type systems are studied mainly in the critical case when the exponential stability is not possible. we consider an operator model of the system in hilbert space and use recent results on the existence of a riesz basis of invariant finite-dimensional subspaces in order to verify its dissipativity. the main results concern the conditions of asymptotic non exponential stability. we show that the property of asymptotic stability is not determinated only by the spectrum of the system but essentially depends on the geometric spectral characteristic of its main neutral term. moreover, we present an example of two systems of neutral type which have both the same spectrum in the open left-half plane and the main neutral term but one of them is asymptotically stable while the other is unstable.
analyzing flowgraphs with atl. this paper presents a solution to the flowgraphs case study for the transformation tool contest 2013 (ttc 2013). starting from java source code, we execute a chain of model transformations to derive a simplifi ed model of the program, its control flow graph and its data flow graph. finally we develop a model transformation that validates the program flow by comparing it with a set of flow specifi cations written in a domain speci c language. the proposed solution has been implemented using atl.
enabling the collaborative definition of dsmls. software development processes are collaborative in nature. neglecting the key role of end-users leads to software that does not satisfy their needs. this collaboration becomes specially important when creating domain-specific modeling languages (dsmls), which are (modeling) languages specifically designed to carry out the tasks of a particular domain. while end-users are actually the experts of the domain for which a dsml is developed, their participation in the dsml specification process is still rather limited nowadays. in this paper we propose a more community-aware language development process by enabling the active participation of all community members (both developers and end-users of the dsml) from the very beginning. our proposal is based on a dsml itself, called collaboro, which allows representing change proposals on the dsml design and discussing (and tracing back) possible solutions, comments and decisions arisen during the collaboration.
behind the scenes in sante: a combination of static and dynamic analyses. while the development of one software verification tool is often seen as a difficult task, the realization of a tool combining various verification techniques is even more complex. this paper presents an innovative tool for verification of c programs called sante (static analysis and testing). we show how several tools based on heterogeneous techniques such as abstract interpretation, dependency analysis, program slicing, constraint solving and test generation can be combined within one tool. we describe the integration of these tools and discuss particular aspects of each underlying tool that are beneficial for the whole combination.
advanced validation of the dvms approach to fully distributed vm scheduling. the holy grail for infrastructure as a service (iaas) providers is to maximize the utilization of their infrastructure while ensuring the quality of service (qos) for the virtual machines they host. although the frameworks in charge of managing virtual machines (vm) on pools of physical ones (pm) have been significantly improved, enabling to manage large- scale infrastructures composed of hundreds of pms, most of them do not efficiently handle the aforementioned objective. the main reason is that advanced scheduling policies are subject to important and hard scalability problems, that become even worse when vm image transfers have to be considered. in this article, we provide a new validation of the distributed vm scheduler approach (dvms) in a twofold manner. first, we provide a formal proof of the algorithm based on temporal logic. second, we discuss large-scale evaluations involving up to 4.7k vms distributed over 467 nodes of the grid'5000 testbed. as far as we know, these experiments constitute the largest in vivo validation that has been performed so far with decentralized vm schedulers. these results show that a cooperative approach such as ours permits to fix overload problems in a reactive and scalable way.
nogood-based asynchronous forward checking algorithms. we propose two new algorithms for solving distributed constraint satisfaction problems (discsps). the first algorithm, afc-ng, is a nogood-based version of asynchronous forward checking (afc). besides its use of nogoods as justification of value removals, afc-ng allows simultaneous backtracks going from different agents to different destinations. the second algorithm, asynchronous forward checking tree (afc- tree), is based on the afc-ng algorithm and is performed on a pseudo-tree ordering of the constraint graph. afc-tree runs simultaneous search processes in disjoint problem subtrees and exploits the parallelism inherent in the problem. we prove that afc-ng and afc-tree only need polynomial space. we compare the performance of these algorithms with other discsp algorithms on random discsps and instances from real benchmarks: sensor networks and distributed meeting scheduling. our experiments show that afc-ng improves on afc and that afc-tree outperforms all compared algorithms, particularly on sparse problems.
working modes with a declarative modeler. this paper presents the point of view of a designer about declarative modelers. first, we propose a model of this point of view. our approach is based on the work we have done to develop some of our declarative modelers. it takes into account the main functions used by a designer in these modelers. then, we explain also the scene-building process and we present working processes that can be used with those modelers. finally, we discuss the world-wide-web compliancy of declarative modelers.
a generic formulation and a memetic algorithm for the hub location-routing problem. in many logistic systems for less than truckload (ltl) shipments, transportation of goods from one origin to its destination is made through collection tours to a hub and delivery tours from the same or another hub, while the goods are shipped between two hubs using full truckload (ftl) shipments. the hub location-routing problem (hlrp), consists in determining the location of the hubs, the allocation of non-hub nodes, and the optimal collection and delivery routes within the network. the proposed mathematical model is a mip based on the following assumptions: each supplier and customer is allocated to one hub; each vehicle route begins and ends at the same hub; vehicles are capacitated; and collection/delivery routes are distinguished. based on different hypothesis for the hubs, we develop several variations of the model: fixed cost hlrp and p-hlrp with and without capacity. in these models, there are three main variables to determine: hub location variables, inter-hub flow variables and routing variables. the objective function minimizes the total fixed cost and transportation costs. we propose a memetic algorithm composed of a genetic algorithm (ga) combined with some local searches. computational experiments are presented based on some randomly generated data sets and australian post data sets.
a state of the art and a general formulation model of hub location-routing problems for ltl shipments. in many logistic systems for less than truckload (ltl) shipments, transportation of goods from one origin to its destination is made through collection tours to a hub and delivery tours from the same or another hub, while the goods are shipped between two hubs using full truckload (ftl) shipments. therefore, managers need to determine the location of the hubs, the allocation of non-hub nodes, and the optimal collection and delivery routes within the network. this problem, known as the hub location-routing problem (hlrp), is related to both the hub location problem (hlp) and the location-routing problem (lrp). the hlp involves the location of hub facilities concentrating flows in order to take advantage of economies of scale and through which flows are to be routed from origins to destinations. the objective of the hlrp is to minimize the total costs including hub costs, inter-hub transportation costs, and collection/distribution routing costs. based on the literatures review, the aims of this paper are to analyze the state of the art, propose some generic mathematical models for the hlrp and implement some tests using a mip solver.
tactical planning of procurement of a supply chain with environmental concern. in this paper, we study the economic and environmental aspects of the tactical procurement planning of an industrial plant within a supply chain, corresponding to a real case of a european company. a procurement plan is obtained by solving an integer linear programming model. two objectives are used: the first is to minimize the total cost including transportation, handling, and inventory holding cost, while the second is to minimize the co2 emissions from transport taking into account the distance and the loads of the vehicles. experimentations performed on the basis of real industrial data validate the approach and allow the analysis of tradeoffs between the two objectives.
an exact algorithm and a metaheuristic for the multi-vehicle covering tour problem with a constraint on the number of vertices. the multi-vehicle covering tour (m-ctp) involves finding a minimum-length set of vehicle routes passing through a subset of vertices, subject to capacity constraints on the length of each route and the number of vertices that it contains, such that each vertex not included in any route lies within a given distance of a route. this paper tackles a particular case of m-ctp where only the restriction on the number of vertices is considered, i.e., the constraint on the length is relaxed. the problem is solved by a branch-and-cut algorithm and a metaheuristic. to develop the branch-and-cut algorithm, we use a new integer programming formulation based on a two-commodity flow model. the metaheuristic is based on the evolutionary local search (els) method proposed in [20]. computational results are reported for a set of test problems derived from the tsplib.
exact and hybrid methods for the multi-period field service routing problem. this article deals with a particular class of routing problem, consisting of the planning and routing of technicians in the eld. this problem has been identi ed as a multiperiod, multidepot uncapacitated vehicle routing problem with speci c constraints that we call the multiperiod field service routing problem (mpfsrp). we propose a set covering formulation of the problem for the column generation technique and we develop an exact branch and price solution method for small-sized instances. we also propose several heuristic versions for larger instances. we present the results of experiments on realistic data adapted from an industrial application.
radio-détection of ultra-high energy cosmic rays. analysis, simulation and interpretation. despite the use of giant detectors suitable for low flux beyond 1018 ev, the origin of ultra energy cosmic rays, remains unclear. in the 60', the radiodetection of air shower is proposed as a complementary technique to the ground particle detection and to the fluorescence method. a revival of this technique took place in the 2000s in particular with codalema experiment. the first results show both a strong dependence of the signal to the geomagnetic field and a strong correlation between energy estimated by the radiodetectors and by particle detectors. the new generation of autonomous detectors created by the codalema collaboration indicates that it is now possible to detect air showers autonomously. due to the expected performances (a nearly 100% duty cycle, a signal generated by the complete shower, simplicity and low cost of a detector), it is possible to consider to deploy this technique for the future large arrays. in order to interpret experimental data, a simulation tool, selfas, is developed in this wok. this simulation code allowed us to highlight the existence of a second radioemission mechanism. a first interpretation of the longitudinal profile as an observable of a privileged instant of the shower development is also proposed, which could give an estimation of the nature of the primary.
initiating a benchmark for uml and ocl analysis tools. the object constraint language (ocl) is becoming more and more popular for model-based engineering, in particular for the development of models and model transformations. ocl is supported by a variety of analysis tools having different scopes, aims and technological corner stones. the spectrum ranges from treating issues concerning formal proof techniques to testing approaches, from validation to verification, and from logic programming and rewriting to sat-based technologies. this paper is a first step towards a well-founded benchmark for assessing validation and verification techniques on uml and ocl models. the paper puts forward a set of uml and ocl models together with particular questions for these models roughly characterized by the notions consistency, independence, consequences, and reachability. the paper sketches how these questions are handled by two ocl tools, use and emftocsp. the claim of the paper is not to present a complete benchmark right now. the paper is intended to initiate the development of further uml and ocl models and accompanying questions within the uml and ocl community. the ocl community is invited to check the presented uml and ocl models with their approaches and tools and to contribute further models and questions which emphasize the possibilities offered by their own tools.
checking model transformation refinement. refinement is a central notion in computer science, meaning that some artefact s can be safely replaced by a refinement r, which preserves s's properties. having available techniques and tools to check transformation refinement would enable (a) the reasoning on whether a transformation correctly implements some requirements, (b) whether a transformation implementation can be safely replaced by another one (e.g. when migrating from qvt-r to atl), and (c) bring techniques from stepwise refinement for the engineering of model transformations. in this paper, we propose an automated methodology and tool support to check transformation refinement. our procedure admits heterogeneous specification (e.g. pamomo, tracts, ocl) and implementation languages (e.g. atl, qvt), relying on their translation to ocl as a common representation formalism and on the use of model finding tools.
lightweight string reasoning in model finding. models play a key role in assuring software quality in the model-driven approach. precise models usually require the definition of well-formedness rules to specify constraints that cannot be expressed graphically. the object constraint language (ocl) is a de-facto standard to define such rules. techniques that check the satisfiability of such models and find corresponding instances of them are important in various activities, such as model-based testing and validation. several tools for these activities have been developed, but to our knowledge, none of them supports ocl string operations on scale that is sufficient for, e.g., model-based testing. as, in contrast, many industrial models do contain such operations, there is evidently a gap. we present a lightweight solver that is specifically tailored to generate large solutions for tractable string constraints in model finding, and that is suitable for directly express the main operations of the ocl datatype string. it is based on constraint logic programming (clp) and constraint handling rules (chr), and can be seamlessly combined with other constraint solvers in clp. we have integrated our solver into the emftocsp model finder, and we show that our implementation efficiently solves several common string constraints on a large instances.
ultra-high energy cosmic rays: analysis of extensive air showers and their associated electromagnetic signal in the mhz domain. null
model-driven standardization of public authority data interchange. in the past decade, several electronic data exchange processes between public authorities have been established by the german public administration. in the context of various legacy systems and numerous suppliers of software for public authorities, it is crucial that these interfaces are open and precisely and uniformly defined, in order to foster free competition and interoperability. a community of such projects and specifications for various public administration domains has arisen from an early adopter project in the domain of data interchange between the 5,400 german municipal citizen registers. a central coordination office provides a framework for these projects that is put into operation by a unified model-driven method, supported by tools and components, involving uml profiles, model validation, and model-to-text transformations into several technical domains. we report how this model-driven approach has already proven to be effective in a number of projects, and how it could contribute to the development of standardized e-government specifications in various ways.
on spectral assignment for neutral type systems. for a large class of linear neutral type systems the problem of eigenvalues and eigenvectors assignment is investigated, i.e. finding the system which has the given spectrum and almost all, in some sense, eigenvectors.
a constraint-based approach for the shift design personnel task scheduling problem with equity. null
approximations for the two-machine cross-docking flow shop problem. we consider in this article the two-machine cross-docking flow shop problem, which is a special case of scheduling with typed tasks, where we have two types of tasks and one machine per type. precedence constraints exist between tasks, but only from a task of the first type to a task of the second type. the precedence relation is thus a directed bipartite graph. minimizing the makespan is strongly np-hard even with unit processing times, but any greedy method yields a 2-approximation solution. in this paper, we are interested in establishing new approximability results for this problem. more specifically, we investigate three directions: list scheduling algorithms based on the relaxation of the resources, the decomposition of the problem according to the connected components of the precedence graph, and finally the search of the induced balanced subgraph with a bounded degree.
energy recovery of ccb-treated wood using thermo chemical processes (pyrolysis and hydroliquefaction) : application to ccb-treated wood. the amount of treated-wood waste was estimated to 27% of the deposit of hazardous waste in france. these wastes are incinerated in specials incineration plants "installations classées pour la protection de l'environnement". however, incineration produces harmful residues and contaminated gases released into the atmosphere inevitably. in this context, this work aims to develop and implement other ways of energy recovery from treated-wood waste using thermo-chemical processes. for this, the pyrolysis and hydroliquefaction processes were performed for energy recovery from ccb treated wood waste (copper-chromium-boron) representing 8750 t / year in france. natural wood were impregnated with salts of ccb in our laboratories according to industrial processing to control the balance of metals in pyrolysis and hydroliquefaction products. a preliminary study was carried out by thermogravimetric analysis in order to determine the temperature range for effective mass degradation of ccb treatedwood. in this temperature range, the experimental parameters of slow pyrolysis have been optimized to concentrate metals in charcoal. subsequently, a parametric study was conducted by the method of experimental design for the conversion of coal into bio-oil. in addition, the optimization of the conversion hydroliquefaction treated wood into bio-oil was performed. the results show that the metals initially present in the treated wood are divided between the bio-oil and coke whatever the processes energy recovery used (hydroliquefaction / pyrolysis + hydroliquefaction). however, the immediate characteristics of bio-oil and biodiesel are quite similar. the use of catalyst during charcoal conversion improves the quality of the bio-oil but also the energy balance of the process.
an iterated local search heuristic for multi-capacity bin packing and machine reassignment problems. this paper proposes an efficient multi-start iterated local search for packing problems (ms-ils-pps) metaheuristic for multi-capacity bin packing problems (mcbpp) and machine reassignment problems (mrp). the mcbpp is a generalization of the classical bin-packing problem in which the machine (bin) capacity and task (item) sizes are given by multiple (resource) dimensions. the mrp is a challenging and novel optimization problem, aimed at maximizing the usage of available machines by reallocating tasks/processes among those machines in a cost-efficient manner, while fulfilling several capacity, conflict, and dependency-related constraints. the proposed ms-ils-pp approach relies on simple neighborhoods as well as problem-tailored shaking procedures. we perform computational experiments on mrp benchmark instances containing between 100 and 50,000 processes. near-optimum multi-resource allocation and scheduling solutions are obtained while meeting specified processing-time requirements (on the order of minutes). in particular, for 9/28 instances with more than 1000 processes, the gap between the solution value and a lower bound measure is smaller than 0.1%. our optimization method is also applied to solve classical benchmark instances for the mcbpp, yielding the best known solutions and optimum ones in most cases. in addition, several upper bounds for non-solved problems were improved.
a non optimization-based method for reconstructing wind instruments bore shape. the presented work is concerned with a prospective method towards the reconstruction of the bore shape of wind instruments. one contemplated application is to the study of ancient wind instruments. the so-called 'horn equation' is well-known for modelling propagation in variable section bores of wind instruments. apart from strict modelling purposes, it has been used together with several optimization methods for the reconstruction of bore shapes of wind instruments. this work investigates properties of a quadratic invariant of second order linear differential equations of the sturm-liouville type. the question of reconstructing the potential appearing in such an equation thanks to this invariant is then applied to the horn wave equation, amounting to the bore shape reconstruction from external measurements that at this stage, are supposed to be feasible. contrary to most of the approaches known to the author, the method presented here does not rely on optimization. on another side, one pending question, thus critical to the method, is that of the measurements that should be made in order to make this approach effective in practice. this should motivate discussions within the conference audience. as this research is prospective, no case study will be presented.
ultrahigh energy neutrinos at the pierre auger observatory. the observation of ultrahigh energy neutrinos (uheνs) has become a priority in experimental astroparticle physics. uheνs can be detected with a variety of techniques. in particular, neutrinos can interact in the atmosphere (downward-going ν) or in the earth crust (earth-skimming ν), producing air showers that can be observed with arrays of detectors at the ground. with the surface detector array of the pierre auger observatory we can detect these types of cascades. the distinguishing signature for neutrino events is the presence of very inclined showers produced close to the ground (i.e., after having traversed a large amount of atmosphere). in this work we review the procedure and criteria established to search for uheνs in the data collected with the ground array of the pierre auger observatory. this includes earth-skimming as well as downward-going neutrinos. no neutrino candidates have been found, which allows us to place competitive limits to the diffuse flux of uheνs in the eev range and above.
optimization of nitrogen and phosphorus removal in constructed wetland. nutrient (phosphorus and nitrogen) discharges from wastewater lead to water quality degradation (74 % of the total french territory in 2006). according to the actual situation, the french government has adopted a water framework on december 30, 2006 to achieve a"good ecological status of water" in 2015. therefore,more stringent standards on nutrients removal for raw wastewater treatment plants are expected (down to 15mg tn.l-1 and 2 mg tp.l-1). however, standards actually remain under the responsibility of departmental authorities according to the water quality of the receiving environment. since 1990s, vertical flow constructed wetlands(vfcws) have been more and more popular in treating raw domestic wastewater for small collectivities of less than 2000 person equivalent (&gt; 2500 units in 2012). however, nitrogen and phosphorus removal are limited in vfcws (70-80 mg tn.l-1; &gt; 10 mg tp.l-1) according to the new legislation. the aim of this work was on the one hand to implement a recirculation of treated effluent on a vfcw for better nitrogen removal and on the other to use reactive materials to improve phosphorus removal. the experimental scientific approach consisted in monitoring both laboratory and field pilot scale systems under process conditions during 2 years. treatment and hydraulic performances were monitored over time. two vfcws (2,5 m²) filled with expanded schist (mayennite®) were fed with raw domestic wastewater. the effect of a saturated layer and of the recirculation of treated effluent were studied. results showed that 38 % of a saturated layer and 100 % of recirculation enabled to improve nitrogen treatment performances (&lt; 20 mgtkn.l-1; &lt; 45 mg tn.l-1) and to meet the french standard d4. electric arc furnace slags were selected as reactive materials to improve phosphorus treatment performance of the vfcws in laboratory and field pilot scale systems. five laboratory-scale column experiments (6l) were fed with a phosphorus synthetic effluent then with a secondary effluent. four horizontal subsurface flow active filters (0,3 m²; 34 l) were fed with a secondary effluent. the main results showed : (i)differences exist between laboratory and pilot scale regarding treatment performance (&lt; 2 mg p.l-1 during20 months in the laboratory ; seasonal variations at pilot scale) and removal mechanisms (adsorption/precipitation ca-p in laboratory ; ca-p + fe-p at pilot scale), (ii) an increase of temperature (&gt; 15°c) and/or hydraulic retention time (more than 2 days) improved the kinetics of phosphorus removal at pilot scale, (iii) the active filter implementing is limited by the discharge standard required (&gt; 3 mg ptot.l-1) and the distance to the steel factory (transport costs).
taming aspects with monads and membranes. when a software system is developed using several aspects, special care must be taken to ensure that the resulting behavior is correct. this is known as the aspect interference problem, and existing approaches essentially aim to detect whether a system exhibits problematic interferences of aspects. in this paper we describe how to control aspect interference by construction by relying on the type system. more precisely, we combine a monadic embedding of the pointcut/advice model in haskell with the notion of membranes for aspect-oriented programming. aspects must explicitly declare the side effectsa nd the context they can act upon. allowed patterns of control flow interference are declared at the membrane level and statically enforced. finally, computational interference between aspects is controlled by the membrane topology. to combine independent and reusable aspects and monadic components into a program specification we use monad views, a recent technique for conveniently handling the monadic stack.
advances in quark gluon plasma. in the last 20 years, heavy-ion collisions have been a unique way to study the hadronic matter in the laboratory. its phase diagram remains unknown, although many experimental and theoretical studies have been undertaken in the last decades. the relativistic heavy ion collider (rhic) at bnl was the first ever built heavy-ion collider. rhic delivered its first collisions in june 2000 boosting the heavy-ion community. impressive amount of experimental results has been obtained. in november 2010, the large hadron collider (lhc) at cern delivered lead-lead collisions at unprecedented center-of-mass energies, 14 times larger than that at rhic. needless to say that the heavy-ion programs at rhic and lhc promise fascinating and exciting results in the next decade. in the second part, a historical approach will be adopted, starting with the notion of limiting temperature of matter introduced by hagedorn in the 60's and the discovery of the qcd asymptotic freedom in the 70's. the phase diagram of hadronic matter, conceived as nowadays, will be shown together with the most important predictions of lattice qcd calculations at finite temperature. in the third part, the heavy-ion collisions at ultra-relativistic energies will be proposed as a unique experimental method to study qgp in the laboratory, as suggested by the bjorken model. in the last part of these lectures, i will present my biased review of the numerous experimental results obtained in the last decade at rhic which lead to the concept of strong interacting qgp, and the first results obtained at lhc with the 2010 and 2011 pbpb runs. finally, the last section is devoted to refer to other lectures about quark gluon plasma and heavy ion physics.
third harmonic flow of charged particles in au+au collisions at sqrtsnn = 200 gev. we report measurements of the third harmonic coefficient of the azimuthal anisotropy, v_3, known as triangular flow. the analysis is for charged particles in au+au collisions at $\sqrtsnn = 200$ gev, based on data from the star experiment at the relativistic heavy ion collider. two-particle correlations as a function of their pseudorapidity separation are fit with narrow and wide gaussians. measurements of triangular flow are extracted from the wide gaussian, from two-particle cumulants with a pseudorapidity gap, and also from event plane analysis methods with a large pseudorapidity gap between the particles and the event plane. these results are reported as a function of transverse momentum and centrality. a large dependence on the pseudorapidity gap is found. results are compared with other experiments and model calculations.
observation of an energy-dependent difference in elliptic flow between particles and anti-particles in relativistic heavy ion collisions. elliptic flow ($v_{2}$) values for identified particles at mid-rapidity in au+au collisions, measured by the star experiment in the beam energy scan at rhic at $\sqrt{s_{nn}}=$ 7.7--62.4 gev, are presented. a beam-energy dependent difference of the values of $v_{2}$ between particles and corresponding anti-particles was observed. the difference increases with decreasing beam energy and is larger for baryons compared to mesons. this implies that, at lower energies, particles and anti-particles are not consistent with the universal number-of-constituent-quark (ncq) scaling of $v_{2}$ that was observed at $\sqrt{s_{nn}}=$ 200 gev.
elliptic flow of identified hadrons in au+au collisions at $\sqrt{s_{nn}}=$ 7.7--62.4 gev. measurements of the elliptic flow, $v_{2}$, of identified hadrons ($\pi^{\pm}$, $k^{\pm}$, $k_{s}^{0}$, $p$, $\bar{p}$, $\phi$, $\lambda$, $\bar{\lambda}$, $\xi^{-}$, $\bar{\xi}^{+}$, $\omega^{-}$, $\bar{\omega}^{+}$) in au+au collisions at $\sqrt{s_{nn}}=$ 7.7, 11.5, 19.6, 27, 39 and 62.4 gev are presented. the measurements were done at mid-rapidity using the time projection chamber and the time-of-flight detectors of the star experiment during the beam energy scan program at rhic. a significant difference in the $v_{2}$ values for particles and the corresponding anti-particles was observed at all transverse momenta for the first time. the difference increases with decreasing center-of-mass energy, $\sqrt{s_{nn}}$ (or increasing baryon chemical potential, $\mu_{b}$) and is larger for the baryons as compared to the mesons. this implies that particles and anti-particles are no longer consistent with the universal number-of-constituent quark (ncq) scaling of $v_{2}$ that was observed at $\sqrt{s_{nn}}=$ 200 gev. however, for the group of particles ncq scaling at $(m_{t}-m_{0})/n_{q}&gt;$ 0.4 gev/$c^{2}$ is not violated within $\pm$10%. the $v_{2}$ values for $\phi$ mesons at 7.7 and 11.5 gev are approximately two standard deviations from the trend defined by the other hadrons at the highest measured $p_{t}$ values.
system-size dependence of transverse momentum correlations at √snn=62.4 and 200 gev at the bnl relativistic heavy ion collider. we present a study of the average transverse momentum ($p_t$) fluctuations and $p_t$ correlations for charged particles produced in cu+cu collisions at midrapidity for $\sqrt{s_{nn}} =$ 62.4 and 200 gev. these results are compared with those published for au+au collisions at same energies, to explore the system size dependence. in addition to the collision energy and system size dependence, the $p_t$ correlations results have been studied as functions of the collision centralities, the ranges in $p_t$, the pseudo-rapidity $\eta$, and the azimuthal angle $\phi$, for which the correlations are measured. the square root of the measured $p_t$ correlations when scaled by mean-$p_t$ are found to be independent of both colliding beam energy and system size studied. the transport based model calculations are found to have a better quantitative agreement with the measurements compared to models which incorporate only jet-like correlations.
freeze-out dynamics via charged kaon femtoscopy in sqrt(snn)=200 gev central au+au collisions. we present measurements of three-dimensional correlation functions of like-sign low transverse momentum kaon pairs from sqrt(snn)=200 gev au+au collisions. a cartesian surface-spherical harmonic decomposition technique was used to extract the kaon source function. the latter was found to have a three-dimensional gaussian shape and can be adequately reproduced by therminator event generator simulations with resonance contributions taken into account. compared to the pion one, the kaon source function is generally narrower and does not have the long tail along the pair transverse momentum direction. the kaon gaussian radii display a monotonic decrease with increasing transverse mass m_t over the interval of 0.55&lt;=m_t&lt;=1.15 gev/c^2. while the kaon radii are adequately described by the m_t-scaling in the outward and sideward directions, in the longitudinal direction the lowest m_t value exceeds the expectations from a pure hydrodynamical model prediction.
fluctuations of charge separation perpendicular to the event plane and local parity violation in sqrt(snn)=200 gev au+au collisions at rhic. recent experimental results from the star collaboration suggest event-by-event charge separation fluctuations perpendicular to the event plane in non-central heavy-ion collisions. here we present the correlator previously used split into its two component parts to reveal correlations parallel and perpendicular to the event plane. the results are from a high statistics 200 gev au+au collisions data set collected by the star experiment at rhic. we explicitly count units of charge separation from which we find clear evidence for more charge separation fluctuations perpendicular than parallel to the event plane. we also employ a modified correlator to study the possible p-even background in same and opposite charge correlations.
centrality dependence of the pseudorapidity density distribution for charged particles in pb-pb collisions at sqrt(snn) = 2.76 tev. we present the first wide-range measurement of the charged-particle pseudorapidity density distribution, for different centralities (the 0-5%, 5-10%, 10-20%, and 20-30% most central events) in pb-pb collisions at sqrt(snn) = 2.76 tev at the lhc. the measurement is performed using the full coverage of the alice detectors, -5.0 &lt; eta &lt; 5.5, and employing a special analysis technique based on collisions arising from lhc 'satellite' bunches. we present the pseudorapidity density as a function of the number of participating nucleons as well as an extrapolation to the total number of produced charged particles nch = 17165 +/- 772 for the 0-5% most central collisions). from the measured dnch/deta distribution we derive the rapidity density distribution, dnch/dy, under simple assumptions. the rapidity density distribution is found to be significantly wider than the predictions of the landau model, which reproduce data well at rhic energies. we assess the validity of longitudinal scaling by comparing to lower energy results from rhic. finally the mechanisms of the underlying particle production are discussed based on a comparison with various theoretical models.
radiative energy loss of relativistic charged particles in absorptive media. we determine the energy loss spectrum per time-interval of a relativistic charge traversing a dispersive medium. polarization and absorption effects in the medium are modelled via a complex index of refraction. we find that the spectrum amplitude becomes exponentially damped due to absorption mechanisms. taking explicitly the effect of multiple scatterings on the charge trajectory into account, we confirm results obtained in a previous work.
molecular dynamics description of an expanding $q$/$\bar{q}$ plasma with the nambu--jona-lasinio model and applications to heavy ion collisions at rhic and lhc energies. we present a relativistic molecular dynamics approach based on the nambu--jona-lasinio lagrangian. we derive the relativistic time evolution equations for an expanding plasma, discuss the hadronization cross section and how they act in such a scenario. we present in detail how one can transform the time evolution equation to a simulation program and apply this program to study the expansion of a plasma created in experiments at rhic and lhc. we present first results on the centrality dependence of $v_2$ and of the transverse momentum spectra of pions and kaons and discuss in detail the hadronisation mechanism.
system size and energy dependence of dilepton production in heavy-ion collisions at sis energies. we study the dilepton production in heavy-ion collisions at energies of 1-2 agev as well as in proton induced pp, pn, pd and p+a reactions from 1 gev up to 3.5 gev. for the analysis we employ three different transport models - the microscopic off-shell hadron-string-dynamics (hsd) transport approach, the isospin quantum molecular dynamics (iqmd) approach as well as the ultra-relativistic quantum molecular dynamics (urqmd) approach. we confirm the experimentally observed enhancement of the dilepton yield (normalized to the multiplicity of neutral pions $n_{\pi^0}$) in heavy-ion collisions with respect to that measured in $nn = (pp+pn)/2$ collisions. we identify two contributions to this enhancement: a) the $pn$ bremsstrahlung which scales with the number of collisions and not with the number of participants, i.e. pions; b) the dilepton emission from intermediate $\delta$'s which are part of the reaction cycles $\delta \to \pi n ; \pi n \to \delta$ and $nn\to n\delta; n\delta \to nn$. with increasing system size more generations of intermediate $\delta$'s are created. if such $\delta$ decays into a pion, the pion can be reabsorbed, however, if it decays into a dilepton, the dilepton escapes from the system. thus, experimentally one observes only one pion (from the last produced $\delta$) whereas the dilepton yield accumulates the contributions from all $\delta$'s of the cycle. we show as well that the fermi motion enhances the production of pions and dileptons in the same way. furthermore, employing the off-shell hsd approach, we explore the influence of in-medium effects like the modification of self-energies and spectral functions of the vector mesons due to their interactions with the hadronic environment.
spontaneous fission half-lives of heavy and superheavy nuclei within a generalized liquid drop model. we systematically calculate the spontaneous fission half-lives for heavy and superheavy nuclei between u and fl isotopes. the spontaneous fission process is studied within the semi-empirical wkb approximation. the potential barrier is obtained using a generalized liquid drop model, taking into account the nuclear proximity, the mass asymmetry, the phenomenological pairing correction, and the microscopic shell correction. macroscopic inertial-mass function has been employed for the calculation of the fission half-life. the results reproduce rather well the experimental data. relatively long half-lives are predicted for many unknown nuclei, sufficient to detect them if synthesized in a laboratory.
charm quark energy loss in proton-proton collisions at lhc energies. null
impact of gluon damping on heavy-quark quenching. in this conference contribution, we discuss the influence of gluon-bremsstrahlung damping in hot, absorptive qcd matter on the heavy-quark radiation spectra. within our monte-carlo implementation for the description of the heavy-quark in-medium propagation we demonstrate that as a consequence of gluon damping the quenching of heavy quarks becomes significantly affected at higher transverse momenta.
j/psi elliptic flow in pb-pb collisions at sqrt(snn) = 2.76 tev. we report on the first measurement of inclusive j/psi elliptic flow, v2, in heavy-ion collisions at the lhc. the measurement is performed with the alice detector in pb-pb collisions at sqrt(snn) = 2.76 tev in the rapidity range 2.5 &lt; y &lt; 4.0. the dependence of the j/psi v2 on the collision centrality and on the j/psi transverse momentum is studied in the range 0 &lt;= pt &lt; 10 gev/c. for semi-central pb-pb collisions at sqrt(snn) = 2.76 tev, an indication of non-zero v2 is observed with a maximum value of v2 = 0.116+/-0.046(stat.)+/-0.029(syst.) for j/psi in the transverse momentum range 2 &lt;= pt &lt; 4 gev/c. the elliptic flow measurement complements the previously reported alice results on the inclusive j/psi nuclear modification factor and favors the scenario of a significant fraction of j/psi production from charm quarks in a deconfined partonic phase.
techniques for measuring aerosol attenuation using the central laser facility at the pierre auger observatory. the pierre auger observatory in malargüe, argentina, is designed to study the properties of ultra-high energy cosmic rays with energies above 1018 ev. it is a hybrid facility that employs a fluorescence detector to perform nearly calorimetric measurements of extensive air shower energies. to obtain reliable calorimetric information from the fd, the atmospheric conditions at the observatory need to be continuously monitored during data acquisition. in particular, light attenuation due to aerosols is an important atmospheric correction. the aerosol concentration is highly variable, so that the aerosol attenuation needs to be evaluated hourly. we use light from the central laser facility, located near the center of the observatory site, having an optical signature comparable to that of the highest energy showers detected by the fd. this paper presents two procedures developed to retrieve the aerosol attenuation of fluorescence light from clf laser shots. cross checks between the two methods demonstrate that results from both analyses are compatible, and that the uncertainties are well understood. the measurements of the aerosol attenuation provided by the two procedures are currently used at the pierre auger observatory to reconstruct air shower data.
investigation of astatine(iii) hydrolyzed species: experiments and relativistic calculations. this work aims to resolve some controversies about astatine(iii) hydroxide species present in oxidant aqueous solution. ato+ is the dominant species existing under oxidizing and acidic ph conditions. this is consistent with highperformance ion-exchange chromatography data showing the existence of one species holding one positive charge. a change in speciation occurs as the ph changes from 1 to 4, while remaining under oxidizing conditions. dynamic experiments with ion-exchange resins evidence the existence of a neutral species witnessed by its elution in the void volume. batch-experiments using a competition method show the exchange of one proton indicating the formation of the ato(oh) species. the hydrolysis thermodynamic constant, extrapolated to zero ionic strength, was determined to be 10−1.9. this value is supported by two-component relativistic quantum calculations and therefore allows disclosing unambiguously the structure of the formed species.
on state feedback h_infinity control for discrete-time singular systems. this paper deals with the state feedback hinf control problem for linear time-invariant discrete-time singular systems. relied on the use of auxiliary matrices and a positive scalar, a novel necessary and sufficient condition for the bounded real lemma is derived for discrete-time singular systems. the characterization is reduced to a strict linear matrix inequality (lmi) when the scalar is fixed, and the resulting lmi is non- conservative as long as the scalar is chosen sufficiently large. moreover, the result is further expanded to hinf controller design, and a numerically efficient and reliable design procedure is given. since no particular restriction is imposed on the auxiliary matrices, the proposed result outperforms the existing methods in the literature. numerical examples are included to illustrate the effectiveness of the present result.
univalence for free. we present an internalization of the 2-groupoid interpretation of the calculus of construction that allows to realize the univalence axiom, proof irrelevance and reasoning modulo. as an example, we show that in our setting, the type of church integers is equal to the inductive type of natural numbers.
feasibility algorithms for two pickup and delivery problems with transfers. this presentation follows the phd thesis of renaud masson [1] on the pickup and delivery problem with transfers (pdpt). the motivating application is a dial-a-ride problem in which a passenger may be transferred from the vehicle that picked him/her up to another vehicle at some predetermined location, called transfer point. both the pdpt and the dial-a-ride problem with transfers (darpt) were investigated. an adaptive large neighborhood search has been developed to solve the pdpt [2] and also adapted to the darpt [3]. in both algorithms, multiple insertions of requests in routes are tested. e ciently evaluating their feasibility with respect to the temporal constraints of the problem is a key issue.
modeling energy demand of buildings at urban scale. urban scale is now considered as one of the most relevant scales to face energy and climate challenges. specific needs for knowledge, decision making tools and evaluation are identified at urban scale. modelling energy demand from residential buildings is one key aspect, priorto energy retrofitting of existing building asset or to valorisation of local energy sources. diversity of local contexts, stake holder goals and data availability lead to search flexible models, with ability to produce information for different applications, from alternative input data sets, combining different types of basic models (namely both physical and statistical ones), according to user needs. the present work is exploring the potential of bottom-up approaches, based on engineering models, developed originally for isolated buildings. these models are extrapolated for the complete set of buildings in a city or neighbourhood, based on building archetypes. two key questions tackled are the selection of suitable archetypes and the reconstitution of relevant input data, statistically representative for the area of interest sensitivity analysis techniques have been applied to a thermal simulation programme (esp-r), particularly the morris elementary effects method. a non-linear response of the model has been emphasized, caused by scattering of input parameters and interaction effects. the most influencing and interacting parameters have been identified. they concern the buildings themselves, their environment and the inhabitants. data collection or statistical reconstitution must be concentrated in priority to these main parameters. a model of the heat demand at a neighbourhood scale has been developed and tested on the sector st-félix in nantes. it is called medus (modelling energy demand at urban scale). application is based on three building archetypes. census data (insee) available at the sector scale are the main input data. results are analyzed both to check archetype relevancy and to study a possible application for evaluating actions at sector scale, such as energy retrofitting.
from object-oriented programming to service-oriented computing: how to improve interoperability by preserving subtyping. the object-oriented paradigm is increasingly used in the implementation and the use of web services. however, the mismatch between objects and document structures in the wire has a negative impact over interoperability, more particularly when subtyping is involved. in this paper, we discuss how to improve interoperability in this context by preserving the subsumption property associated to subtyping. first we show the weaknesses of existing web service frameworks used for serialization and deserialization. second we propose new foundations for serialization and deserialization, which leads to the specification of a new data binding between objects and document structures, compatible with subtyping.
an adaptive large neighborhood search for the pickup and delivery problem with transfers. the pickup and delivery problem (pdp) consists in defining a set of routes that satisfy transportation requests between a set of pickup points and a set of delivery points. this paper addresses a variant of the pdp where requests can change vehicle during their trip. the transshipment is made at specific locations called "transfer points". the corresponding problem is called the pickup and delivery with transfers (pdpt). solving the pdpt leads to new modeling and algorithmic difficulties. we propose new heuristics capable of efficiently inserting requests through transfer points. these heuristics are embedded into an adaptive large neighborhood search. we evaluate the method on generated instances and apply it to the transportation of people with disabilities. on these real-life instances we show that the introduction of transfer points generally brings non-negligible improvements (up to 9%). experiments on related instances from the literature show clearly that the method is competitive with existing method.
dissipative performance control with output regulation for continuous-time descriptor systems. this paper is concerned with the problem of dissipative performance control under output regulation constraints for continuous-time descriptor systems. in this problem, an output is to be regulated asymptotically with presence of an infinite-energy exo-system, while a specific dissipative performance from a finite external disturbance to a tracking error has also to be satisfied. based on a generalized sylvester equation, the asymptotical regulation objective is achieved and a specific structure of the resulting controller is deduced. using this structure, the solution to the defined multi-objective control problem is characterized in terms of a set of linear matrix inequalities (lmis).
the neutrons for science facility at spiral-2. the neutrons for science (nfs) facility is a component of spiral-2 laboratory under construction at caen (france). spiral-2 is dedicated to the production of high intensity radioactive ions beams (rib). it is based on a high-power linear accelerator (linag) to accelerate deuterons beams in order to produce neutrons by breakup reactions on a c converter. these neutrons will induce fission in 238u for production of radioactive isotopes. additionally to the rib production, the proton and deuteron beams delivered by the accelerator will be used in the nfs facility. nfs is composed of a pulsed neutron beam and irradiation stations for cross-section measurements and material studies. the beams delivered by the linag will allow producing intense neutron beams in the 100 kev-40 mev energy range with either a continuous or quasi-mono-energetic spectrum. at nfs available average fluxes will be up to 2 orders of magnitude higher than those of other existing time-of-flight facilities in the 1 mev - 40 mev range. nfs will be a very powerful tool for fundamental physics and application related research in support of the transmutation of nuclear waste, design of future fission and fusion reactors, nuclear medicine or test and development of new detectors. the facility and its characteristics are described, and several examples of the first potential experiments are presented.
measurement of charge multiplicity asymmetry correlations in high energy nucleus-nucleus collisions at 200 gev. a study is reported of the same- and opposite-sign charge-dependent azimuthal correlations with respect to the event plane in au+au collisions at 200 gev. the charge multiplicity asymmetries between the up/down and left/right hemispheres relative to the event plane are utilized. the contributions from statistical fluctuations and detector effects were subtracted from the (co-)variance of the observed charge multiplicity asymmetries. in the mid- to most-central collisions, the same- (opposite-) sign pairs are preferentially emitted in back-to-back (aligned on the same-side) directions. the charge separation across the event plane, measured by the difference, $\delta$, between the like- and unlike-sign up/down $-$ left/right correlations, is largest near the event plane. the difference is found to be proportional to the event-by-event final-state particle ellipticity (via the observed second-order harmonic $v_2^{\rm obs}$, where $\delta=(1.3\pm1.4({\rm stat})^{+4.0}_{-1.0}({\rm syst}))\times10^{-5}+(3.2\pm0.2({\rm stat})^{+0.4}_{-0.3}({\rm syst}))\times10^{-3}v_2^{\rm obs}$ for 20-40% au+au collisions. the implications for the proposed chiral magnetic effect (\cme) are discussed.
separating jets from bulk matter in heavy ion collisions at the lhc. null
a realistic treatment of geomagnetic cherenkov radiation from cosmic ray air showers. null
centrality dependence of pi, k, p production in pb-pb collisions at sqrt(snn) = 2.76 tev. in this paper measurements are presented of pi+, pi-, k+, k-, p and antiproton production at mid-rapidity &lt; 0.5, in pb-pb collisions at sqrt(snn) = 2.76 tev as a function of centrality. the measurement covers the transverse momentum pt range from 100, 200, 300 mev/c up to 3, 3, 4.6 gev/c, for pi, k, and p respectively. the measured pt distributions and yields are compared to expectations based on hydrodynamic, thermal and recombination models. the spectral shapes of central collisions show a stronger radial flow than measured at lower energies, which can be described in hydrodynamic models. in peripheral collisions, the pt distributions are not well reproduced by hydrodynamic models. ratios of integrated particle yields are found to be nearly independent of centrality. the yield of protons normalized to pions is a factor ~1.5 lower than the expectation from thermal models.
hydrodynamical evolution in heavy ion collisions and pp scatterings a the lhc ridges in aa and pp scattering. null
electron-d$^0$ correlations in p+p and au+au collisions at 200 gev with the star experiment at rhic. null
heavy flavour at rhic. null
environment reconstruction and navigation with electric sense based on kalman filter. electric fish sense the perturbations of a self generated electric field through their electro- receptive skin. this sense allows them to navigate and reconstruct their environment in conditions where vision and sonar cannot work. in this article, we use a sensor inspired by this sense to address the problem of locating and reconstructing small objects (electrolocation) and navigating in a tank. based on a kalman filter, any small object in the surroundings of the motion controlled sensor can be modeled as an equivalent sphere whose location is well estimated by the filter. as a first application to the problem of navigation, the filter is included into a closed feedback loop in order to achieve wall following in a tank. our experimental results demonstrate the feasibility of this approach.
cumulative scheduling with overloads of resource : global constraint and decompositions. constraint programming is an interesting approach to solve scheduling problems. in cumulative scheduling, activities are defined by their starting date, their duration and the amount of resource necessary for their execution. the total available resource at each point in time (the capacity) is fixed. in constraint programming, the cumulative global constraint models this problem. in several practical cases, the deadline of theproject is fixed and can not be delayed. in this case, it is not always possible to find a schedule that does not lead to an overload of the resource capacity. it can be tolerated to relax the capacity constraint, in a reasonable limit, to obtain a solution. we propose a new global constraint : the softcumulative constraint that extends the cumulative constraint to handle these overloads. we illustrate its modeling power on several practical problems, and we present various filtering algorithms. in particular, we adapt the sweep and edge-finding algorithms to the softcumulative constraint. we also show that some practical problems require to impose overloads to satisfy business rules, modelled by additional constraints. we present an original filtering procedure to deal with these imposed overloads. we complete our study by an approach by decomposition. at last, we test and validate our different resolution techniques through a series of experiments.
shared steering control between a driver and an automation: stability in the presence of driver behaviour uncertainty. this paper presents an advanced driver assistance system (adas) for lane keeping, together with an analysis of its performance and stability with respect to variations in driver behavior. the automotive adas proposed is designed so as to share control of the steering wheel with the driver in the best possible way. its development was derived from a h2-preview optimization control problem, which is based on the global driver-vehicle-road (dvr) system. the dvr model makes use of a cybernetic driver model so as to take into account any driver-vehicle interactions. such a formulation allows to: i) consider driver-assistance cooperation criteria in the control synthesis, ii) improve the performance of the assistance as a cooperative copilot, and iii) analyze the stability of the whole system in the presence of driver model uncertainty. the results have been validated experimentally with one participant using a fixed-base driving simulator. the developed assistance system improved lane-keeping performance and reduced the risk of a lane departure accident. good results were obtained using several criteria for human-machine cooperation. poor stability situations were successfully avoided thanks to the robustness of the whole system in spite of a large range of driver model uncertainty.
new methods for the multi-skills project scheduling problem. in this phd thesis we introduce several procedures to solve the multi-skill project scheduling problem (mspsp). the aim is to find a schedule that minimizes the completion time (makespan) of a project, composed of a set of activities. precedence relations and resource constraints are considered. in this problem, resources are staff members that master several skills. thus, a given number of workers must be assigned to perform each skill required by an activity. furthermore, we give a particula rimportance to exact methods for solving the multi-skill project scheduling problem (mspsp), since there are still several instances for which optimality is still to be proven. nevertheless, with the purpose of solving big sized instances we also developed and implemented a heuristic approach.
study of nuclear energy systems and double strata scenarios for minor actinides transmutation in ads. the french law of 28th june 2006 regarding advanced nuclear waste management requires a scientific assessment to define future industrial strategies. the present phd thesis was carried in this framework and concerns specifically the research axis of minoractinides transmutation. a high power accelerator driven system (ads) concept is developed at subatech for this purpose. a 1 gev proton beam feeds three liquid lead-bismuth spallation targets. the multiple spallation target (must) ads reaches the thermal powers up to 1 gw with a high specific power. a nuclear reactor dimensioning method has been developed and applied to different double strata scenarios. in these scenarios, sfr (sodium fastreactors) or pwr (pressurized water reactors) power reactors produce minor actinides that will be transmuted into ads. in each core (sfr and ads), the plutonium multi-reprocessing strategy is performed while ads subcritical core also multi-reprocesses minor actinides. to limit the core reactivity and improve the fuel thermal conductivity, the minor actinides fuel is mixed with mgo inert matrix. nuclear branches with lead and sodium coolants for the ads, have been studied for different irradiation times and two transmutation strategies have been assessed : whether whole minor actinides, whether americium only is tranmuted. the thesis presents precisely the must ads design methodology and the calculations to get a fuel composition at equilibrium. then a one cycle evolution is performed and analysed for the fuel and the multiplication factor. radiotoxicity and thermal power of the waste produced are then compared. finally, the study of double strata scenarios is performed to analyse the plutonium and minor actinides inventories in cycle and also the waste produced according to the transmutation strategies applied and the first stratum evolution.
efficient feasibility testing for request insertion in the pickup and delivery problem with transfers. the pickup and delivery problem with transfers (pdpt) consists of defining a set of minimum cost routes in order to satisfy a set of transportation requests, allowing them to change vehicles at specific locations. in this problem, routes are strongly interdependent due to request transfers. then it is critical to efficiently check if inserting a request into a partial solution is feasible or not. in this article, we present a method to perform this check in constant time.
beyond cross-functional teams: knowledge integration during organizational projects and the role of social capital. large organizational projects must integrate the specific and dispersed knowledge of many individuals and groups to succeed. thus, frequent exchanges between the project team and the organization's members are required. in this context, understanding of the knowledge integration process during cross-functional projects can be enhanced through the conceptual framework of social capital. a qualitative investigation of a french small firm conceptualizes knowledge integration as a three-phase model: collection, interpretation, and assimilation. the case shows that the integration process is cyclical with overlaps and inter-dependencies among the phases. this study leads to refinement of the social capital role in knowledge integration and reveals the dynamics of internal and external facets of social capital. that is, internal and external social capital play differentiated roles depending on the three phases of the knowledge integration process. finally, the study reveals the co-evolution of social capital and knowledge integration as a resulting long-term effect.
active lateral acceleration control of a narrow tilting vehicle. narrow tilting vehicles (ntv) are the convergence of a car and a motorcycle. one meter wide, these vehicles are designed for one or two people sitting the one in front the other. the idea behind the conception of ntv is the minimization of traffic congestion, energy consumption and pollutant emission. but because of their dimensions, these cars would have to lean into corners in order to compensate for the lateral acceleration and maintain their stability. the tilting should be automatic, and can be achieved by a tilting torque generated by a dedicated tilting actuator (dtc) or by modifying the steering angle (stc) or both (sdtc). in this thesis, we first propose a methodology for the design of an output feedback structured regulator, minimizing the h2 norm of a well-posed problem, built to optimize the lateral acceleration of the ntv, considering dtc and sdtc systems.the designed controllers, with the longitudinal velocity as a parameter, lead to the minimization of the tilting torque and of the lateral acceleration perceived by the driver, and have good performances as well as good robustness properties. furthermore, the tuning methodology allows the comparison of a pure dtc solution and a mixed sdtc alternative. compared to the literature, the originalities in this thesis are the direct control of the measured value of the lateral acceleration (instead of the tilting angle), and the anticipation of the tilt, thanks to the use of the steering angle and angular velocity. furthermore, the sdtc solution allows to drive both the stc and dtc systems in a coordinated manner. the design strategies are based on a preliminary study of vehicle models, and a design model with 5dof was developed. we demonstrated that the model has the nice property to be flat, and in the last section of the thesis, used this property to initiate the design of a non-linear robust controller, which can a priori lead to better performances in case of "large motions".
control of a lower mobility dual arm system. this paper studies the kinematic modeling and control of two cooperative manipulators. the system is composed of the two arms of the humanoid nao robot of aldebaran. the serial structure of each arm has five degrees of freedom, in the closed chain formulation when transporting a common object, it has 4-dof. the kinematic and dynamics representing the closed chain system is studied. a new control scheme demonstrates how the cooperative task space can be combined with a minimum representation of the task to control the 4 dof of object. furthermore by modeling the object grasp as a passive joint, we show that all 6-dof of the object can be controlled.
automata and constraint programming for personnel scheduling problems. as soon as a structure is organized, the ability to put the right people at the right time is critical to satisfy the need of a department, a school or a company. we define personnel scheduling problems as the process of building, in an optimized manner, the personnel schedules. the aims of this thesis are to propose a mean to express those problems in a simple and automatic way, avoiding the user to interact with the technical aspects of the resolution. for that matter, we propose to mix the modeling power of automata with the efficiency and modularity of constraint programming for complex problem solving. thus, we use the expressiveness of the finite multi-valued automata to model complex scheduling rules. then, to make use of those built automata, we introduce a new filtering algorithm for multi-valued finite automata based on lagrangian relaxation : multicost-regular. we also introduce a soft version of this constraint that has the ability to penalize violated rules defined by the automaton : soft-multicost-regular. the constraint model is automatically built. it is solved using the constraint library choco and the whole modeling-solving process has been tested on realistic instances from asap and nrp10 libraries. the solution search is finally improved using specialized regret-based heuristics using the structure of multicost-regular and soft-multicost-regular.
biological treatment of malodorous gaseous compounds stemming from the industrial sector of waste management : study of a biotrickling filter/biofilter combination. waste treatment industries generate gaseous emissions that may induce odor annoyance to the surrounding populations. these gaseous effluents contain a large variety of volatile compounds such as oxygenated (volatile fatty acids, ketones, aldehydes and alcohols), nitrogen and sulphur compounds (hydrogen sulphide (h2s), dimethylsulphide (dms), dimethyldisulfide (dmds) and methanethiol (mt). these gaseous emissions are controlled by using an adequate system such as biotechniques. nevertheless, because of their very low odor thresholds, complete elimination of sulphur compounds has to be assessed, as the residual concentration can induce an odorous impact on neighbourhood populations. the aim of this study is to improve these bioprocesses performances by carrying out an adequate system strategy. the originality of this work is to evaluate the removal efficiency of a mixture of sulphur compounds by implementing a combination of two bioprocesses and more precisely a biotrickling filter and biofilter.the first step of this phd. work consisted of evaluating the ph impact on the biodegradation activity of a mixture of sulphur compounds (h2s, dms and dmds) by using microcosms. the ph has an impact on the removal efficiency of dms and dmds. the total removal of these compounds is observed for a ph range between 5 and 7. the performances of the coupling have been compared with those reached by implementing control biofilters (duplicated). after an acclimatization period, stable performances are maintained under constant operating conditions. the efficiency of the coupling have been highlighted, the dms and dmds abatement levels are superior (around 20%) for the bioprocesses combination.the microbiological component has been investigated within all biofilters by estimating the densities of two populations involved in the biodegradation of sulphur compounds (hyphomicrobium and thiobacillus thioparus), by using qpcr. the obtained results highlighted the presence of both populations at high level (104 copies of dnar-16s gene/ng extracted dna for thiobacillus thioparus and 104-106 copies of dnar-16s gene/ng extracted dna for hyphomicrobium). the repartition of these two bacterial populations is similar in both cases (coupling system and reference biofilters). under transient shock load conditions, the robustness of the coupling has been revealed. the efficiency levels before the shock load are recovered 48 hours after the perturbation off. finally, the monitoring of an on- site pilot (rendering facility) has been carried out during three months. the laboratory results have been confirmed and the suitability of such a system has been showed under industrial gas variability.
spectroscopic measurement of plume mission from femtosecond laser ablation. null
microbial aerosol behavior in hvac system. microbial indoor air quality is an important issue in particular in the professional sector. this thesis aims to investigate the conditions leading to microbial development on to fibrous filters and to microbial release down stream of filters that could decrease air quality. the first part of the thesis was realized on laboratory and consisted in the filtration of a microbial consortium composed with staphylococcus epidermidis (bacterium specie) and penicillium oxalicum (fungi specie). the effects of three parameters on the microbial behavior were studied : the relative humidity (rh) of the air, the filter material, the airflow presence/absence. whatever conditions, s. epidermidis did not grow up. however, p. oxalicum has demonstrated its ability to develop itself when rh was close to 100% and some p. oxalicumspores were released downstream of filter after growth, when ventilation was restarted. the second part of the thesis consisted in working with a semi-urban outdoor air. two air handling unit (ahu) have operated during 5 months. the ventilation of one ahu was stopped each week-end and restarted each beginning of week. temperature and rh of the air, filters pressure drop and total concentration of pm in air before filtration were monitored. concentration of total cultivable microorganisms upstream and downstream of both filters was also measured each week, in particular at the restart of ventilation for one ahu. according to seasonal variations of microbial concentrations, results have revealed for instance that the filtration efficiency of cultivable bacteria was particularly weak, and sometimes negative, for the ahu operating continuously.
transforming bpmn process models to bpel process definitions with atl. null
towards an advanced model-driven engineering toolbox. null
presence in e-learning: theoretical model and perspectives for research. this article proposes a model of presence in e-learning that has some similarities with but also some important distinctions from the model of community of inquiry in e-learning (garrison &amp; anderson, 2003). it addresses the notion of presence from a different angle, characterizes and specifies it differently. the author outlines the epistemological referents of the model proposed. then, she describes the interaction processes at work in each of the three dimensions of the model: socio-cognitive presence (1), socio-affective presence (2) and pedagogical presence (3). she also provides a schematic representation of this model. then, she shows how its three dimensions can be related to each another and presents the main hypotheses that result from these relations. in conclusion, the author outlines that theoretical and empirical research is needed to confirm the relevance of the model proposed, to identify its strengths and to suggest axes for improvement.
an amma/atl solution for the grabats 2009 reverse engineering case study. this paper presents a solution to the reverse engineering case study of grabats 2009 implemented using the atlanmod transformation language (atl), and the atlanmod model management architecture (amma). two scalability approaches are presented: a classical one, as well as an optimization for multiple queries on the same model. the task showing the genericity of the transformation tool is also solved.
achieving rule interoperability using chains of model transformations. null
on the use of higher-order model transformations. the level of maturity that has been reached by model transformation technologies is proved by the growing literature on transformation libraries that address an increasingly wide spectrum of applications. with the success of the modeling and transformation paradigm, the need arises to address more complex applications that require a direct manipulation of model transformations. the uniformity and flexibility of the model-driven paradigm allows this class of applications to make use of the same transformation infrastructure. this is possible because transformations can be translated into transformation models and given as objects to a different class of model transformations, called higher-order transformations (hot). this paper provides an introduction to hots and a survey of the several application cases where their use is relevant. a number of possible future applications of hots is also proposed.
measuring discovered models. model driven engineering (mde) is based on the principle of uniﬁcation. most of the artifacts used or produced in a software project may be uniformly represented by models conforming to various metamodels. in this paper we show how two independent projects could be bridged to produce additional results. the ﬁrst one is about discovering models from legacy code and the second one is about measuring various kind of models. due to the interoperability properties of mde, the integration of these projects was straightforward and allowed to provide a new generic legacy measurement infrastructure.
integration by model-driven virtual tool. null
program comprehension case study for grabats 2009. null
experiments with a higher-level navigation language. writing navigation expressions is an important part of the task of developing a model transformation deﬁnition. when navigation is complex and the size of source models is signiﬁcant, performance issues cannot be neglected. model transformation languages often implement some variants of ocl as their navigation language. writing eﬃcient code in ocl is usually a diﬃcult task because of the nature of the language and the lack of optimizing ocl compilers. moreover, optimizations generally reduce readability. an approach to tackle this issue is to raise the level of abstraction of the navigation language. we propose to complement the regular navigation language of model transformation languages with a high-level navigation language, in order to improve both performance and readability. this paper reports on the initial results of our experiments creating the hln language: a declarative high-level navigation language. we will motivate the problem, and will we describe the language as well as the main design guidelines.
flexible and expressive aspect-based control over service compositions in the cloud. accountability properties (e.g., security and privacy proper- ties for trustworthy data stewardship) are becoming increas- ingly important for cloud applications. frequently, they have to be enforced on large-scale service-based legacy ap- plications. in this paper we argue that real-world service in- frastructures are best modeled in terms of three abstraction levels and that (partially invasive) adaptations involving all levels are needed to handle the corresponding evolution sce- narios. in this paper, we motivate these issues for the case of apache cxf, a popular service infrastructure, and secure logging as a basic accountability property. we propose an initial version of a dsl for flexible and expressive control over the execution of service compositions on three levels: service, interceptor and implementation. we also present a corresponding prototype tool and infrastructure we have implemented over cxf. finally, we show how our method can be applied to enable secure logging.
coupling static and dynamic models information. in model driven engineering (mde) a system may be represented by a model conforming to a given metamodel. the joint use of several models representing the same system is called multimodeling. in this work we show how different models representing the same legacy system may be used for program comprehension. more precisely, we show how to jointly exploit static (structural) and dynamic (behavioral) models of the same program to enhance understandability. the key advantage of mde is that all models are based on a uniform representation and thus the joint use of these models is greatly facilitated.
supporting tool reuse with model transformation. null
parsing sbvr-based controlled languages. null
modeling and solving the generalized routing problems. the routing problem is one of the most popular and challenging combinatorial optimization problems. it involves finding the optimal set of routes for fleet of vehicles in order to serve a given set of customers. in the classic transportation problems, each customer is normally served by only one node (or arc). therefore, there is always a given set of required nodes (or arcs) that have to be visited or traversed, and we just need to find the solution from this set of nodes (or arcs). but in many real applications where a customer can be served by from more than one node (or arc), the generalized resulting problems are more complex. the primary goal of this thesis is to study three generalized routing problems. the first one, the close-enough arc routing problem(cearp), has an interesting real-life application to routing for meter reading while the others two, the multi-vehicle covering tour problem (mctp) and the generalized vehicle routing problem(gvrp), can model problems concerned with the design of bilevel transportation networks. the problems are solved by exact methods as well as metaheuristics. to develop exact methods, we formulate each problem as a mathematical program, and then develop branch-and-cut algorithms. the metaheuristics are based on the evolutionary local search (els) method et on the greedy randomized adaptive search procedure (grasp) method. the extensive computational experiments show the performance of our methods.
possible benefits of bridging eclipse-emf &amp; microsoft "oslo". null
megamodeling software platforms: automated discovery of usable cartography from available metadata. model-driven reverse engineering focuses on automatically discovering models from different kinds of available information on existing software systems. although the source code of an application is often used as a basic input, this information may take various forms such as: design "models", bug reports, or any kind of documentation in general. all this metadata may have been either built manually or generated (semi)automatically during the whole software life cycle, from the specification and development phase to the effective running of the system. this paper proposes an automated and extensible mde approach to build a usable cartography of a given platform from available metadata by combining several mde techniques. as a running example, the approach has been applied to the topcased mde platform for embedded &amp; real-time systems.
interpretation of the depths of maximum of extensive air showers measured by the pierre auger observatory. to interpret the mean depth of cosmic ray air shower maximum and its dispersion, we parametrize those two observables as functions of the first two moments of the $\ln a$ distribution. we examine the goodness of this simple method through simulations of test mass distributions. the application of the parameterization to pierre auger observatory data allows one to study the energy dependence of the mean $\ln a$ and of its variance under the assumption of selected hadronic interaction models. we discuss possible implications of these dependences in term of interaction models and astrophysical cosmic ray sources.
towards qos-oriented sla guarantees for online cloud services. cloud computing provides a convenient means of remote on-demand and pay-per-use access to computing re- sources. however, its ad-hoc management of quality-of-service and sla poses significant challenges to the performance, dependability and costs of online cloud services. the paper precisely addresses this issue and makes a threefold contribu- tion. first, it introduces a new cloud model, the slaaas (sla aware service) model. slaaas enables a systematic integration of qos levels and sla into the cloud. it is orthogonal to other cloud models such as saas or paas, and may apply to any of them. second, the paper introduces csla, a novel language to describe qos-oriented sla associated with cloud services. third, the paper presents a control-theoretic approach to provide performance, dependability and cost guarantees for online cloud services, with time-varying workloads. the proposed approach is validated through case studies and extensive experiments with online services hosted in clouds such as amazon ec2. the case studies illustrate sla guarantees for various services such as a mapreduce service, a cluster-based multi-tier e-commerce service, and a low-level locking service.
fate of emerging pollutants during photochemical or photocatalytic treatment under solar irradiation. industrialisation, the use of numerous chemical products in domestic activities and the use of medicine drugs have led to the release in the environment of various substances named "emerging pollutants". the existing wastewater treatments are not designed to eliminate this kind of pollution and then these pollutants are released into the natural aquatic media. to limit the release of these compounds by waste water treatment plant effluent, a solution could be the use of additional treatment processes such as advanced oxidation processes. in this context, the european project clean water has started in 2009. clean water involves 7 entities including the gepea laboratory-ecole des mines de nantes. the aim of the clean water project is to develop sustainable and cost effective water treatment and detoxification processes using tio2 nanomaterials with uv-visible light response under solar light. these processes act to remove emerging contaminants such as endocrine disruptors and pharmaceuticals. in this program, thegepea laboratory is concerned with the evaluation of the efficiency of novel photocatalysts under uv and visible irradiations for the elimination of emerging pollutants. for this purpose, an experimental methodology was established to express the efficiency of the tested catalysts in terms of degradation kinetic constants, pollutants conversion and mineralisation and also in terms of the intermediate products formed. the efficiency of photocatalysts is also evaluated in terms of intermediates biodegradability, toxicity and endocrine disruption effects. first, the experimental methodology was tested on the degradation of tetracycline with a reference catalyst. then, it was applied to the degradation of bisphenol a and estradiol respectively with the reference catalyst and the catalysts developed within the clean water project. the results obtained on the tetracycline degradation have showed that: i) tetracycline intermediate products are less toxic than tetracycline ii) the intermediates structure is similar to that of tetracycline, this can explain the low biodegradability observed for these intermediates. for the degradation of bisphenol a and estradiol, the results showed that: i) the photocatalysts are efficient under simulated solar irradiation. however, the catalyst photocatalytic efficiency depends on the compound to be degraded ii) the nature of the bisphenol a reaction intermediates identified depends on the catalyst used iii)the estrogenic effect of the estradiol treated solution persists during the photocatalytic treatment.
study of in vivo generators pb-212/bi--212 and u-230/th-226 for alpha radioimmunotherapy. alpha-radioimmunotherapy is a promising cancer therapy that uses a-particles vectorized by monoclonal antibody to break down cancerous tumors. the notion of in vivo generator was introduced in 1989 by leonard mausner. the concept involves labeling of various molecular carriers (antibodies, peptides, etc) with intermediate half-life generator parents, which after accumulation in the desired tissue generate much shorter half-life daughter radionuclide. this thesis focuses on the study of two in vivo generators potentially interesting for alpha-radioimmunotherapy: pb-212 / bi-212 generator and u-230 / th-226 generator. the first part of this work presents the pb-212 / bi-212 generator, two approaches allowing the vectorisation. chelation approach on a protein and an approach by encapsulation in liposomes have been proposed. this last approach appears to be the most interesting. in vitro stability studies have been performed on these labeling. the second part of this work presents the u-230 / th-226 generator. studies have first been made to achieve a theoretical model to describe the speciation of th(iv) in human serum. the efficacy of dtpa as chelating agent for complexation of th(iv) in human serum could thus be estimated.
hydrogen production from anaerobic co-digestion of coffee mucilage and swine manure. this research investigates an alternative approach to the use of two wastes from agricultural and livestock activities developed in colombia. swinemanure and coffee mucilage were used to evaluatean anaerobic co-digestion process focused on hydrogen production. in addition, the aims covered a further stage in order to close the cycle of the both wastes. the thesis was conducted in three phases : 1. evaluation of hydrogen production from the co-digestion of coffee mucilage and swine manure during dark fermentation ; 2. trends over retention time through the monitoring of microorganisms by quantitative pcr and other parameters incluiding ph, oxidation reduction potential, and hydrogen partial pressure ; 3. treatment of the effluent from hydrogen production process by anaerobic digestion with methane production. the experimental results showed that mixtures of both wastes are able to produce hydrogen. a substrate ratio of 5:5, which was associated with a c/n ratio of 53, was suitable for hydrogen production. moreover, the stability and optimization of the process were evaluated by increasing the influent organic load rate. this wasthe best experimental condition in terms of average cumulative hydrogen volume, production rate and yield which were 2661 nml, 760 nmlh2/lwd and 43 nml h2/gcod, respectively. this performance was preserved over time, which was verified through the repetitive batch cultivation during 43 days. two trends were identified over retention time associated with similar cumulative hydrogen, but with differences in lag-phase time and hydrogen production rate. t.thermosaccharolyticum was the dominating genus during the short trend related to the shortest lag phase time and highest hydrogen production rate. the long trends were associated with a decrease of bacillus sp. concentration at the beginning of the experiments and with the possible competition for soluble substrates between t.thermosaccharolyticum and clostridium sp. the third phase showed that the use of a second stage to produce methane was useful enhancing the treatment of both wastes. finally, the overall energy produced for both biofuels (hydrogen andmethane) showed similar levels with other process. however, hydrogen was around the 10% of the overall energy produced in the process. in addition, both gases could be mixed to produce biohythane which improves the properties of biogas.
centrality determination of pb-pb collisions at sqrt(snn) = 2.76 tev with alice. this publication describes the methods used to measure the centrality of inelastic pb-pb collisions at a center-of-mass energy of 2.76 tev per colliding nucleon pair with alice. the centrality is a key parameter in the study of the properties of qcd matter at extreme temperature and energy density, because it is directly related to the initial overlap region of the colliding nuclei. geometrical properties of the collision, such as the number of participating nucleons and number of binary nucleon-nucleon collisions, are deduced from a glauber model with a sharp impact parameter selection, and shown to be consistent with those extracted from the data. the centrality determination provides a tool to compare alice measurements with those of other experiments and with theoretical calculations.
accountability for cloud and other future internet services. presentation of current accountability problems and tracks for future work.
charge correlations using the balance function in pb-pb collisions at sqrt{s_{nn}} = 2.76 tev. in high-energy heavy-ion collisions, the correlations between the emitted particles can be used as a probe to gain insight into the charge creation mechanisms. in this article, we report the first results of such studies using the electric charge balance function in the relative pseudorapidity \delta\eta and azimuthal angle \delta\phi in pb-pb collisions at sqrt{s_{nn}} = 2.76 tev with the alice detector at the large hadron collider. the width of the balance function decreases with growing centrality (i.e. for more central collisions) in both projections. this centrality dependence is not reproduced by hijing, while ampt, a model which incorporates strings and parton rescattering, exhibits qualitative agreement with the measured correlations in \delta\phi but fails to describe the correlations in \delta\eta. a thermal blast wave model incorporating local charge conservation and tuned to describe the p_t spectra and v_2 measurements reported by alice, is used to fit the centrality dependence of the width of the balance function and to extract the average separation of balancing charges at freeze-out. the comparison of our results with measurements at lower energies reveals an ordering with sqrt{s_{nn}}: the balance functions become narrower with increasing energy for all centralities. this is consistent with the effect of larger radial flow at the lhc energies but also with the late stage creation scenario of balancing charges. however, the relative decrease of the balance function widths in \delta\eta and \delta\phi with centrality from the highest sps to the lhc energy exhibits only small differences. this observation cannot be interpreted solely within the framework where the majority of the charge is produced at a later stage in the evolution of the heavy-ion collision.
measurement of the inclusive differential jet cross section in pp collisions at sqrt{s} = 2.76 tev. the alice collaboration at the cern large hadron collider reports the first measurement of the inclusive differential jet cross section at mid-rapidity in pp collisions at sqrt(s) = 2.76 tev, with integrated luminosity of 13.6 nb^-1. jets are measured over the transverse momentum range 20 to 125 gev/c and are corrected to the particle level. calculations based on next-to-leading order perturbative qcd are in good agreement with the measurements. the ratio of inclusive jet cross sections for jet radii r = 0.2 and r = 0.4 is reported, and is also well reproduced by a next-to-leading order perturbative qcd calculation when hadronization effects are included.
first measurement of theta_13 from delayed neutron capture on hydrogen in the double chooz experiment. the double chooz experiment has determined the value of the neutrino oscillation parameter $\theta_{13}$ from an analysis of inverse beta decay interactions with neutron capture on hydrogen. this analysis uses a three times larger fiducial volume than the standard double chooz assessment, which is restricted to a region doped with gadolinium (gd), yielding an exposure of 113.1 gw-ton-years. the data sample used in this analysis is distinct from that of the gd analysis, and the systematic uncertainties are also largely independent, with some exceptions, such as the reactor neutrino flux prediction. a combined rate- and energy-dependent fit finds $\sin^2 2\theta_{13}=0.097\pm 0.034(stat.) \pm 0.034 (syst.)$, excluding the no-oscillation hypothesis at 2.0 \sigma. this result is consistent with previous measurements of $\sin^2 2\theta_{13}$.
the radioelements at subatech. null
direct measurement of backgrounds using reactor-off data in double chooz. double chooz is unique among modern reactor-based neutrino experiments studying ν̅ e disappearance in that data can be collected with all reactors off. in this paper, we present data from 7.53 days of reactor-off running. applying the same selection criteria as used in the double chooz reactor-on oscillation analysis, a measured background rate of 1.0±0.4 events/day is obtained. the background model for accidentals, cosmogenic β-n-emitting isotopes, fast neutrons from cosmic muons, and stopped-μ decays used in the oscillation analysis is demonstrated to be correct within the uncertainties. kinematic distributions of the events, which are dominantly cosmic-ray-produced correlated-background events, are provided. the background rates are scaled to the shielding depths of two other reactor-based oscillation experiments, daya bay and reno.
steel slag filters to upgrade phosphorus removal in small wastewater treatment plants. this thesis aimed at developing the use of electric arc furnace steel slag (eaf-slag) and basic oxygen furnace steel slag (bof-slag) in filters designed to upgrade phosphorus (p) removal in small wastewater treatment plants. an integrated approach was followed, with investigation at different scales: (i) batch experiments were performed to establish an overview of the p removal capacities of steel slag produced in europe, and then to select the most suitable samples for p removal; (ii)continuous flow column experiments were performed to investigate the effect of various parameters including slag size and composition, and column design on treatment and hydraulic performances of lab-scale slag filters; (iii)finally, field experiments were performed to investigate hydraulic and treatment performances of demonstration scale slag filters designed to remove p from the effluent of a constructed wetland. the experimental results indicated that the major mechanism of p removal was related tocao-slag dissolution followed by precipitation of caphosphate and recrystallisation into hydroxyapatite (hap).over 100 weeks of continuous feeding of a synthetic psolution (mean inlet total p 10.2 mg p/l), columns filled with small-size slag (6-12 mm bof-slag; 5-16 mm eafslag)removed &gt;98% of inlet total p, whereas columnsfilled with big-size slag (20-50 mm bof-slag and 20-40mm eaf-slag) removed 56 and 86% of inlet total p,respectively. most probably, the smaller was the size ofslag, the greater was the specific surface for cao-slagdissolution and adsorption of ca phosphate precipitates.field experiments confirmed that eaf-slag and bof-slagare efficient substrate for p removal from the effluent of aconstructed wetland (mean inlet total p 8.3 mg p/l). overa period of 85 weeks of operation, eaf-slag removed 36%of inlet total p, whereas bof-slag removed 59% of inlettotal p. p removal efficiencies increased with increasing temperature and void hydraulic retention time (hrtv),most probably because the increase in temperature and hrtv affected the rate of cao dissolution and caphosphate precipitation. however, it was found that longhrtv (&gt;3 days) may produce high ph of the effluents(&gt;9), as the result of excessive cao-slag dissolution. however, the results of field experiments demonstrated that at shorter hrtv (1-2 days), slag filters produced ph that were elevated only during the first 5 weeks of operation, and then stabilized below a ph of 9. finally, a dimensioning equation based on the experimental results was proposed.
nuclear graphite waste's behaviour under disposal conditions : study of the release and repartition of organic and inorganic forms of carbon 14 and tritium in alkaline media. 23000 tons of graphite wastes will be generated during dismantling of the first generation of french reactors (9 gas cooled reactors). these wastes are classified as long lived low level wastes (llw-ll). as requested by the law, the french national radioactive waste management agency (andra) is studying concepts of low-depth disposals.in this work we focus on carbon 14, the main long-lived radionuclide in graphite waste (5730y), but also on tritium, which is the main contributor to the radioactivity in the short term. carbon 14 and tritium may be released from graphite waste in many forms in gaseous phase (14co2, ht...) or in solution (14co32-, hto...). their speciation will strongly affect their migration from the disposal site to the environment. leaching experiments, in alkaline solution (0.1 m naoh simulating repository conditions) have been performed on irradiated graphite, from saint-laurent a2 and g2 reactors, in order to quantify their release and characterize their speciation. the studies show that carbon 14 exists in both gaseous and aqueous phases. in the gaseous phase, release is weak (&lt;0.1%) and corresponds to oxidizable species. carbon 14 is mainly released into liquid phase, as both inorganic and organic species. 65% of released fraction is inorganic and 35% organic carbon. two tritiated species have been identified in gaseous phase: hto and ht/organically bond tritium. more than 90% of tritium in that phase corresponds to ht/obt. but release is weak (&lt;0.1%). hto is mainly in the liquid phase.
molecular models of natural organic matter and its colloidal aggregation in aqueous solutions: challenges and opportunities for computer simulations (keynote talk). null
structure and dynamics of co2, h2co3, hco3-, co32- in aqueous solutions: ab initio molecular dynamics simulations. null
structure and h-bonding of aqueous carbonate species from ab initio md simulation (invited talk). null
hybrid and nonlinear control of power converters. switched electronic systems are used in a huge number of everyday domestic and industrial utilities: liquid crystal displays, home appliances, lighting, personal computers, power plants, transportation vehicles and so on. efficient operations of all such applications depend on the essential "hidden work" done by switched electronic systems, whose behavior is determined by a suitable interconnection and control of analog and digital devices. as a motivation of this work, we consider the dc-dc power converters. this thesis contributes to provide hybrid and nonlinear control problem solutions to several types of power converters. in the first part we are interested in the problem of voltage regulation of power converters operating in discontinuous conducting mode. two power converters are considered: the boost converter and the buck-boost converter. the system does not admit a (continuous--time) average model approximation, hence is a hybrid system where the control objective is the generation of a periodic orbit and the actuator commands are switching times. our main contribution is a simple robust algorithm that gives explicit formulas for the switching times without approximations. simulation and experimental results that illustrate the robustness of the scheme to parameter uncertainty, as well as performance comparisons with current practice, are presented. in the second part a class of power converters that can be globally stabilized with an output-feedback pi controller has been identified. moreover, we will prove that the i&amp;i observer can be combined with the pi controller preserving the gas properties of the closed-loop. the class is characterized by a simple linear matrix inequality. the new controller is illustrated with the widely-popular, and difficult to control, single-ended primary inductor converter, for which simulation and experimental results are presented.
study and desgin of a very high spatial resolution beta imaging system. the b autoradiography is a widely used technique in pharmacology or biological fields. it is able to locate in two dimensions molecules labeled with beta emitters. the development of a gaseous detector incorporating micromesh called pim in the subatech laboratory leads to the construction of a very high spatial resolution apparatus dedicated to b imaging. this device is devoted to small analysis surface of a half microscope slide in particular of 3h or 14c and the measured spatial resolution is 20 μm fwhm. the recent development of a new reconstruction method allows enlarging the field of investigation to high energy beta emitters such as 131i, 18f or 46sc. a new device with a large active area of 18x18 cm2 has been built with a user friendly design. this allows to image simultaneously 10 microscope slides. thanks to a multi-modality solution, it retains the good characteristics of spatial resolution obtained previously on a small surface. moreover, different kinds of samples, like microscope slides or scotches can be analysed. the simulation and experimentation work achieved during this thesis led to an optimal disposition of the inner structure of the detector. these results and characterization show that the pim structure has to be considered for a next generation of b-imager.
charged kaon femtoscopic correlations in pp collisions at $\sqrt{s}=7$ tev. correlations of two charged identical kaons (kch kch) are measured in pp collisions at sqrt{s}=7 tev by the alice experiment at the large hadron collider (lhc). one-dimensional kch kch correlation functions are constructed in three multiplicity and four transverse momentum ranges. the kch kch femtoscopic source parameters r and lambda are extracted. the kch kch correlations show a slight increase of femtoscopic radii with increasing multiplicity and a slight decrease of radii with increasing transverse momentum. these trends are similar to the ones observed for pi pi and ks0 ks0 correlations in pp and heavy-ion collisions. however, the observed one dimensional correlation radii for charged kaons are larger at high multiplicities than those for pions in contrast to what was observed in heavy-ion collisions at rhic.
complexation with metal ions and colloidal aggregation of natural organic matter in aqueous solutions: a computational molecular modeling perspective (invited talk). null
fluid-fluid phase separation under metamorphic conditions: md simulations of a generalized composition h2o-co2-nacl (invited talk). null
hydrogen bonding and molecular ordering of water at mineral-solution interfaces (keynote talk). null
molecular dynamics simulations on the structure and dynamics of h2o, k+, nh4+ on hydrated muscovite surface. the investigation of water-mineral interfaces plays a pivotal role in the study of various environmental and geochemical problems. the prime reason for exploring the ion-water-mineral interactions lies in the fact that it determines several phenomena ranging from transport of elements, swelling, dispersion, and mineral alteration and so on. these interactions would be different for different surface cations and at different degrees of hydration. recent experimental studies support the notion that the k+ ions on the hydrated muscovite surface can be exchanged for the hydronium ions. similarly, surface k+ ions can also be exchanged for ammonium in the aqueous solution (e.g., in tobelite). we have investigated the structure and dynamics of these ionic species at the hydrated surface of muscovite by molecular dynamics (md) computer simulations using clayff force field. in addition to replacing the interlayer cations, a series of md simulations have been performed with varying amounts of water on the muscovite surface at ambient conditions for each of the exchanged cations. atomic density profiles as functions of the distance from the muscovite surface were calculated for all atoms types present in the simulation. the comparison of hydration states and the topological details of the h-bonding network around the three ions on the surface help to interpret the structure and dynamics of the ions in the confined geometries. the angular distributions of the h2o, h3o+, and nh4+ molecules with respect to the muscovite surface have been studied. the dynamics of water molecules were further examined by self-diffusion coefficients calculated from the mean square displacement of oxygen (or nitrogen) atoms and by exchange/reorientation time correlation functions of the surface molecules. results obtained from the simulations are compared with the available experimental data and other previous simulations and provide reliable molecular-scale view of the structure and dynamics of ions and water on the muscovite surface.
computational molecular modeling for nuclear waste management and other radiochemical applications (invited talk). safe and sustainable management of nuclear energy poses major scientific and engineering challenges, one of which is the necessity to make the environmental impacts of the long-term nuclear waste storage as small as possible. this requires significant improvements in our understanding of the behaviour of radionuclides and their retention mechanisms in geological formations of nuclear waste repositories over the ranges of time and distance spanning many orders of magnitude. detailed molecular scale knowledge of the complex chemical and physical processes controlling the interaction of radionuclides with clay and cementitious materials is crucial for building better predictive models of their adsorption and mobility in natural and engineered barriers of the nuclear waste repositories. the presence of natural organic matter (nom) in clayey formations and its complexation with metal ions in aqueous solutions has significant effect on the transport properties of the radionuclides. in this presentation, we will overview our current efforts to apply computational molecular modeling techniques to address these problems on the fundamental molecular level. we use classical molecular dynamics (md) simulations for detailed quantitative studies of the structural, energetic and dynamic aspects of interactions between radionuclides, organic matter and clay particles. structural and thermodynamic parameters are obtained by studying different processes such as hydration, adsorption, complexation, and intercalation. the complexation mechanisms of organic molecules with aqueous metal ions will then be presented using the free energy calculations. metal cations can strongly associate with negatively charged functional groups of organic molecules and with negatively charged clay surfaces. this allows us to predict that cationic bridging could be the most probable mechanism responsible for the controlling effects of organics on the behaviour of radionuclides is clays and other repository materials. our most recent results demonstrating how the nature of the adsorbed cations affects the structural and dynamic properties of the mineral-water interface and also the effect of disordered substitution in on the adsorption and swelling behaviour of clay minerals will then be discussed briefly.
molecular structure and dynamics of nano-confined aqueous solutions: computer simulations of clay, cement, and polymer membranes. molecular-scale knowledge of the thermodynamic, structural, and transport properties of water confined by interfaces and nano-pores of various materials is crucial for quantitative understanding and prediction of many natural and technological processes, including carbon sequestration, water desalination, nuclear waste storage, cement chemistry, fuel cell technology, etc. experimental nanoscale studies of such systems are not always feasible, and their results often require considerable interpretation in the efforts to extract surface- and confinement-specific quantitative information from the measurements. computational molecular modeling significantly complements such efforts and provides invaluable molecular-scale background for better understanding of the specific effects of the substrate structure and composition on the structure, dynamics and reactivity of interfacial and nano-confined aqueous solutions. based on the successful development and implementation of the clayff force field (cygan et al., 2004), we have recently performed a series of molecular dynamics simulations of aqueous interfaces with several representative inorganic and organic nanoporous materials (ahn et al., 2008; kalinichev et al., 2007, 2010; wang et al., 2006, 2009) in order to better understand and quantify the effects of the substrate composition and structure on the properties of interfacial and nano-confined aqueous solutions. individual h2o molecules and hydrated ions at interfaces simultaneously participate in several dynamic processes, which can be characterized by different, but equally important time- and length- scales. the first molecular layer of interfacial water at all substrates is often highly ordered, indicating reduced translational and orientational mobility of the h2o molecules. however, this ordering can not be simply described as "ice-like", but rather resembles the behavior of supercooled water or amorphous ice, although with significant substrate-specific variations. these results help to interpret experimental nmr, ir, x-ray, and neutron scattering measurements performed for the same systems. ahn, w.-y., kalinichev, a.g., clark, m.m. (2008) effects of background cations on the fouling of polyethersulfone membranes by natural organic matter: experimental and molecular modeling study. j.membr.sci., 309,128-140. cygan r.t., liang j.-j., kalinichev a.g. (2004) molecular models of hydroxide, oxyhydroxide, and clay phases and the development of a general force field. journal of physical chemistry b, 108, 1255-1266. kalinichev a.g., wang, j., kirkpatrick r. j. (2007) molecular dynamics modeling of the structure, dynamics and energetics of mineral-water interfaces: application to cement materials. cement and concrete res., 37, 337-347. kalinichev, a.g., kumar, p., kirkpatrick, r.j. (2010) effects of hydrogen bonding on the properties of layered double hydroxides intercalated with organic acids: mdf computer simulations. philos. mag., 90, 2475-2488. wang j., kalinichev a.g., kirkpatrick r.j. (2006) effects of substrate structure and composition on the structure, dynamics and energetics of water on mineral surfaces: md modeling study. geochim. cosmochim. acta, 70, 562-582. wang j., kalinichev a.g., kirkpatrick r.j. (2009) asymmetric hydrogen bonding and orientational ordering of water at hydrophobic and hydrophilic surfaces: a comparison of water/vapor, water/talc, and water/mica interfaces. j.phys.chem.c, 113, 11077-11085.
structure and energetics of smectite interlayer hydration: molecular dynamics investigations of na- and ca hectorite. molecular-scale interactions present at mineral-water interfaces and in clay interlayer galleries control numerous environmental processes, including chemical interactions in soils and transport of nutrients and pollutants through them.[1-4] understanding these processes requires accurate knowledge of the structure, energetics, and dynamics of the interaction among the mineral substrate, ions, and water molecules.[5, 6] challenges to this objective include experimental difficulties in probing these interfaces and interlayers at the molecular scale; fully characterizing the mineral substrate; and identifying how the mineral surface, ions, and water molecules each contribute to the overall structure, energetics, and dynamics of these systems.[6] linked computational molecular dynamics (md) simulations and experimental nuclear magnetic resonance (nmr) studies are particularly effective in addressing these issues.[7-9] here we focus on md studies of na- and ca-smectite (hectorite) interlayer galleries to provide a molecular-scale picture of the structure and dynamics of their hydration[9, 10] and to complement our earlier nmr investigations of these systems.[7-9] classical md simulations were undertaken in the npt and nvt ensembles to determine the structural and energetic changes with increasing hydration with focus on the single- and double-layer hydrates. the results show substantial changes in the hydration of the interlayer cations, the orientations of the water molecules, the hydrogen bond network involving the water molecules and basal oxygen atoms, and the resulting potential energies as the interlayer gallery expands. [1] scheidegger et al. (1996) soil science 161 813-831. [2] stumm (1997) colloids and surfaces a-physicochemical and engineering aspects 120 143-166. [3] o'day (1999) reviews of geophysics 37 249-274. [4] koretsky (2000) journal of hydrology 230 127-171. [5] wang et al. (2001) chemistry of materials 13 145-150. [6] wang et al. (2006) geochimica et cosmochimica acta 70 562-582. [7] bowers et al. (2008) journal of physical chemistry c 112 6430-6438. [8] bowers et al. (2011) journal of physical chemistry c 115 23395-23407. [9] bowers et al. (2012), unpublished. [10] morrow et al. (2012) journal of physical chemistry c, submitted.
metal cation complexation with natural organic matter in aqueous solutions: molecular dynamics simulations and potentials of mean force. null
molecular models of natural organic matter and its colloidal aggregation in aqueous solutions: challenges and opportunities for computer simulations. natural organic matter (nom) is ubiquitous in soil and groundwater and its aqueous complexation with various inorganic and organic species can strongly affect the speciation, solubility and toxicity of many elements in the environment. despite significant geochemical, environmental and industrial interest, the molecular-scale mechanisms of the physical and chemical processes involving nom are not yet fully understood. recent molecular dynamics (md) simulations using relatively simple models of nom fragments are used here to illustrate the challenges and opportunities for the application of computational molecular modeling techniques to the structural, dynamic, and energetic characterization of metal-nom complexation and colloidal aggregation in aqueous solutions. the predictions from large-scale md simulations are in good qualitative agreement with available experimental observations, but also point out to the need for simulations at much larger time and length scales with more complex nom models in order to fully capture the diversity of molecular processes involving nom.
computational molecular modeling of mineral-water interfaces for geochemical and environmental applications (invited lecture). null
slow diffusional dynamics of water in cement nanopores: multiscale challenges for atomistic modeling (topic leader). molecular modeling of the properties of aqueous solutions confined in the nanopores and at the interfaces of cementitious materials is complicated by the significant structural and compositional heterogeneity of these phases and also by the fact that many of the important processes span several orders of magnitude both in time and in length. here we present an attempt to quantify the diffusional dynamics of 0.25 m kcl aqueous solution in contact with a model c-s-h binding phase (tobermorite) on the basis of molecular dynamics computer simulations. at the (001) surface of tobermorite, two types of h2o molecules can be effectively distinguished: the ones that spend most of their time within channels between the drierketten chains of silica on the tobermorite surface, and the more mobile adsorbed molecules that reside right above the interface. within the channels, h2o molecules donate h-bonds to both the bridging and non-bridging oxygens of the si-tetrahedra as well as to other h2o. some of these molecules form particularly strong h-bonds persisting well over 100 ps, but many others undergo frequent librational motions and occasional diffusional jumps from one surface site to another. the average diffusion coefficients of the surface-associated h2o molecules that spend most of their time in the channels and those that lie above the nominal interface differ by about one order of magnitude (dh2o[internal]=5.0×10−11 m2/s and dh2o[external]=6.0×10−10 m2/s, respectively). the average diffusion coefficient for all surface-associated h2o molecules is about 1.0×10−10 m2/s. all of these values are significantly less than the value of 2.3×10−9 m2/s, characteristic of h2o self-diffusion in bulk liquid water. the md simulations provided an opportunity to further quantify these relatively slow diffusional motions of h2o at the tobermorite interface on the longer time- and length- scale in terms of the van hove self-correlation function (vhscf). the emerging picture is in surprisingly good agreement with available experimental data on the dynamics of surface-associated water in similar cement materials obtained by 1h nmr [1,2]. 1. korb j.p., monteilhet l., mcdonald p.j., mitchell j., microstructure and texture of hydrated cement-based materials: a proton field cycling relaxometry approach. cement and concrete research, 37, 2007, 295-302. 2. korb j.p., nmr and nuclear spin relaxation of cement and concrete materialscurrent opinion in colloid &amp; interface science, 14, 2009, 192-202.
mineral-fluid interactions on the molecular scale: computational atomistic modeling of clay-related materials for geochemical and environmental applications. null
molecular dynamics simuation of cs+ on the hydrated muscovite surface: local structural environment and dynamics. adsorption of metal cations on mineral surfaces often controls their distribution in both natural and technological environments. the callovo-oxfordian (cox) formation, consisting largely of clay minerals like illite and smectite, is the location investigated in france as a site for geological nuclear waste disposal and storage. the uptake of radionuclides by layered clay minerals is the principal retention process for their diffusion. hence, detailed molecular-scale understanding of the adsorption mechanisms of radionuclides on silicate minerals is essential, because it can significantly influence their mobility under the conditions of nuclear waste repositories. cs+ ion is one of the important components of nuclear waste that is highly soluble in water and migrates easily in surface and sub-surface environments. the atomically smooth surface of muscovite mica, kal2(si3al)o10(oh)2 is often used as an accurate model of illite clay. experimental studies suggest that cs+ ion adsorbs directly at the muscovite surface as an "inner sphere complex" [1]. we have investigated the structure and dynamics of cs+ (exchanged for k+) and h2o molecules at the surface of muscovite at two different hydration levels by molecular dynamics (md) computer simulations using fully flexible clayff force field [2]. at the muscovite (001) surface, water molecules can donate 2 hydrogen bonds (to other h2o and/or to the surface o atoms) and accept 2 h-bonds (from other h2o). water molecules can also partially replace surface cations, because their hydrogens bear some positive charge. such surface-adsorbed h2o molecules have their negatively charged oxygen atoms exposed to the fluid phase and accessible for either h-bond acceptance from other h2os or for their coordination of surface cations in the inner-sphere or outer-sphere configuration. atomic density profiles of the surface species evidently support the presence of cs+ as inner sphere complexes at the muscovite interface. angular distributions of h2o molecular orientations with respect to the muscovite surface have also been studied, as well as the dynamical behaviour of surface species in terms of their self-diffusion coefficients, h-bonding time correlation functions, and residence times. the comparison of the surface behaviour at two different hydration states and the topological details of the interfacial h-bonding network provide new insight into the structure and dynamics of hydrated cs+ at confined geometries. the md simulation results are compared with available experimental data and the results of previous molecular simulations [3] to provide reliable molecular view of the hydrated cs+ ions at the surface of illite. references [1] kim, y., kirkpatrick, j.r., cygan, r.t. geochim. cosmochim acta, 60, 4059-4074 (1996). [2] cygan, r.t., liang, j.j., kalinichev, a.g. j. phys. chem. b, 108, 1255-1266 (2004). [3] wang, j.w., kalinichev, a.g., kirkpatrick, r.j., cygan, r.t. j. phys. chem. b, 109, 15893-15905 (2005).
molecular modeling of the swelling properties and interlayer structure of cs, and k-montmorillonites: effects of charge distribution in the clay layers. reliable prediction of the behaviour of radionuclides and their transport and retention in clayey formations at nuclear waste repositories requires detailed molecular scale understanding of these complex multicomponent systems. as the first step in our study of the effects of organic molecules on the adsorption and transport of radionuclides in hydrated clay systems we have investigated the effects of the ordering in charge distributions on the swelling behavior of montmorillonite (a smectite clay). montmorillonite layered structure consists of aluminum-oxygen octahedral sheet sandwiched between two opposing silicon-oxygen tetrahedral sheets giving rise to a 2:1 clay mineral. isomorphic substitutions in the tetrahedral and octahedral sheets are responsible of the negative layer charge of montmorillonite clay minerals having the chemical composition (si8-xxx)(al4-yyy)o20(oh)4 where x = al3+, y = mg2+, fe2+...[1]. the montmorillonite models for our study are based on a pyrophillite unit cell structure (5.16å×8.966å×9.347å) obtained from the crystallographic data of lee et al. [2]. the 4×4×2 simulation supercells were built and substitutions were made in the pyrophillite structure in order to approximate as close as possible the chemical composition of wyoming montmorillonite [1] m24(si248al8)(al112mg16)o640(oh)128, where m is either cs+, or k+. we have explored three different models of the substitution distributions. in the first model, the substitutions were uniformly and orderly distributed within the tetrahedral and octahedral sheets. in the second model, the substituted positions were kept ordered in the octahedral sheets but made disordered in the tetrahedral one. in the third model, the substituted positions of the octahedral sites were additionally made disordered. in order to study the swelling behavior of these montmorillonites, npt-ensemble molecular dynamics (md) simulations were run at t = 298 k and p = 1 bar for each of the three different substitution models and with 23 different hydration states ranging from 0 to 700 mgwater/gclay (from 0 to 42 h2o molecules per one monovalent cation). all md runs were performed for a total of 2 ns using the clayff force field [3]. after the system reached equilibrium, the last 1ns of each md trajectory was used to compute the clay basal spacing and the swelling thermodynamic properties: hydration energy, immersion energy, isosteric heat of adsorption. these calculations indicate that in addition to the commonly observed 1-layer and 2-layer hydrates, stable hydration states corresponding to 3-layer and 4-layer hydrates can also be distinguished. these stable states (minima of hydration and immersion energies) were then selected to run further 500 ps nvt-ensemble md simulations at the same temperature. the equilibrium parts of these nvt-simulated trajectories were then used to calculate radial distribution functions and atomic density profiles of the interlayer species in hydrated montmorillonites. references [1] tsipursky, s.i., drits, v.a. clay minerals, 19, 177-193 (1984). [2] lee, j.h. and guggenheim, s. american mineralogist, 66, 350-357 (1981). [3] cygan, r.t., liang, j.j., kalinichev, a.g. journal of physical chemistry b, 108, 1255-1266 (2004).
effects of surface cations on the structure and dynamics of the hydrogen-bonding network at the illite-water interface: a molecular dynamics simulation study. safe and sustainable management of nuclear energy poses major scientific and engineering challenges, one of which is the necessity to make the environmental impacts of the long-term nuclear waste storage as small as possible. this requires detailed understanding and prediction of the behaviour of radionuclides and their migration and retention properties in the geological formations of nuclear waste repositories. the callovo-oxfordien rock formation of the french nuclear repository site is mainly composed of clay minerals (illite, smectite and interstratified illite/smectite), quartz, calcite, with some non-negligible amount of organic matter. the adsorption of water can change the properties of mineral surfaces, including protonation state, surface charge, structure, and reactivity [1]. similarly, the properties of interfacial water are strongly affected by the mineral substrate structure and composition. recent advances in experimental techniques such as ftir [2], ellipsometry [3], synchrotron x-ray scattering [4], sum-frequency vibrational spectroscopy [5] are capable of probing the properties of mineral-water interfaces at different levels of hydration. however, the surface-specific results of these experiments are often difficult to quantitatively interpret without having a reliable molecular scale picture of the underlying physical and chemical processes. molecular computer simulations have become one of the most important tools in the study of such interfacial systems and phenomena by providing invaluable atomistic information on the underlying chemical and physical processes. the present study is aimed at investigating the structural and dynamics effects of three different cations (k+, nh4+, and h3o+) exchanged at the hydrated surface of muscovite mica, which is taken here as a model illite. molecular dynamics computer simulations were performed using the clayff force field [6] to investigate the important differences of the h-bonding configurations formed by the sorbed species, including h2o, h3o+, and nh4+, in contrast to the behavior of spherical metal ions, such as k+. at the muscovite (001) surface, h2o can donate 2 h-bonds (to other h2o and/or to the surface o atoms) and accept 2 h-bonds (from other h2o), but it can also partially replace surface k+, because the hydrogens of h2o bear some positive charge. this behavior was observed in previous md simulations of the mica surface [7]. such surface-adsorbed h2o molecules have their negatively charged oxygen atoms exposed to the fluid phase and accessible for either h-bond acceptance from other h2os or hydration of metal cations in outer-sphere coordination. for surface h3o+, the charges on the hydrogens are slightly higher than those of h2o, but the oxygen atom of hydronium is now almost hydrophobic, and cannot participate in a h-bond network (e.g., [8]). in contrast, nh4+ can equally well donate h-bonds to the surface o atoms and to the neighboring h2o molecules, but it cannot participate in the hydration shell of a displaced metal cation. thus, three similar species (h2o - two hb donors and 2 hb acceptors; h3o+ - 3 hb donors and no acceptors; nh4+ - 4 hb donors, no acceptors) can provide for three greatly different structural, energetic, and dynamical situations at the muscovite-water interface. since the hydrogen-bonding network in any aqueous media provides a natural mechanism of forming low-barrier reaction paths for proton transfer in such systems, it is also an important phenomenon controlling the surface reactivity under various ph conditions. in addition, a detailed study of the structural characteristics of surface-adsorbed nh4+ provides a way for better understanding of the mechanisms of adsorption for organic molecules having amino-groups in their structure, which is quite common for natural organic matter (e.g., [9]). each of the three systems was simulated at 7 different hydration states providing information on the structure and dynamics of the adsorbed water film in a wide range of relative humidity conditions. the atomic density profiles of water show significant layering at all hydration levels and the layering strongly depends upon the nature of the ionic species present on the surface. our studies support the fact that the h3o+ ion is less strongly bound when compared to k+ on the muscovite surface as observed in earlier studies [7]. at muscovite surfaces, both nh4+ and h3o+ cations establish strong hydrogen bonds with the surface bridging oxygen atoms and also with the neighbouring h2o molecules at all hydration levels. however, we observed that the interactions are different for both species at low hydration levels (&lt;&lt; molecular monolayer). at low hydration levels, h3o+ prefers to strongly bind as 3-cordinated species to the surface than with the neighbouring waters molecules. however, as the hydration levels increase, h3o+ binds as 2-cordinated species with the surface as is indicated by hydrogen bonding analysis (figure 1). in contrast, irrespective of the hydration levels, nh4+ ion strongly interacts with the surface as 3-cordinated species because of its tetrahedral geometry. at the same time, we observe from hydrogen bond analysis that the hydrogen bonding network of water has been strongly influenced by the nature of the surface cations present at the mineral-water interface. the dynamics of water molecules were examined by self-diffusion coefficients from the mean square displacement of water oxygen. the diffusion mechanism is similar for k+ and nh4+ but was different for h3o+, in particular at the low hydration states. furthermore, the spatial and orientation distributions of h2o and ions at the muscovite-water surface are analyzed in quantitative detail. all the simulation results are compared with available experimental data and the results of previous molecular simulations to provide reliable molecular view of the ions and water at the muscovite surface. references [1] henderson, m. a. surf. sci. rep. 2002, 46, 5-308. [2] cantrell and g. e. ewing. j. phys. chem. b, 2001, 105, 5434-5439. [3] beaglehole, d and christenson, h. k. j. phys. chem, 1992, 96, 3395-3403. [4] fenter, p. and sturchio, n. c. progress in surface science, 2004, 77, 171-258. [5] shen, y. r. and ostroverkhov, v. chem. rev., 2006, 106, 1140-1154. [6] cygan, r.t., liang, j.j., and kalinichev, a.g. j. phys. chem. b, 2004, 108, 1255-1266. [7] wang, j., kalinichev, a., kirkpatrick, r., cygan, r. j. phys. chem b., 2005, 109, 15893-15905. [8] petersen, p.b. and saykally, r.j. j. phys. chem. b, 2005, 109, 7976-7980. [9] leenheer, j.a. annals of environmental science, 2009, 3, 1-130.
molecular modeling of the swelling properties and interlayer structure of cs, na, k-montmorillonite: effects of charge distribution in the clay layers. safe and sustainable management of nuclear waste poses major scientific challenges to make the environmental footprint of nuclear energy as small as possible for very long periods of time. as many other countries, france is considering the deep geological disposal (in the callovo-oxfordian (cox) argillite formations of the paris basin) as a reliable way of storing high-level radioactive waste in order to provide adequate protection for humans and the environment. in addition to being proven geologically stable for million years, the natural and engineered clay barriers can benefit from many favorable properties, such as low permeability, high sorption capacity, etc. the mineralogical composition of the callovo-oxfordian argillite shows about 41% of clay minerals (23% of interstratified illite/smectite, 14% of illite-type minerals, 2% kaolinite and 2% chlorite) [1,2]. a non-negligible amount of organic matter is also present (~1%) [3], and it is known that the interaction of natural organic matter (nom) with radionuclides and clays can affect the solubility and toxicity of trace elements in natural aqueous environments [4,5]. reliable prediction of the behaviour of radionuclides and their transport and retention in clayey formations at nuclear waste repositories requires detailed molecular scale understanding of these complex multicomponent systems. computational molecular modelling has already become an important tool in the study of thermodynamic, structural and transport properties of hydrated clays (e.g., [6-8]). as the first step in our study of the effects of organic molecules on the adsorption and transport of radionuclides in hydrated clay systems we have investigated the effects of the ordering in charge distributions on the swelling behavior of simulated clays. montmorillonite was chosen as a model of smectite clay. montmorillonite structure consists of aluminum-oxygen octahedral sheet sandwiched between two opposing silicon-oxygen tetrahedral sheets giving rise to a 2:1 clay mineral. isomorphic substitutions in the tetrahedral and octahedral sheets are responsible of the negative layer charge of montmorillonite clay minerals having the chemical composition (si8-xxx)(al4-yyy)o20(oh)4 where x = al, y = mg, fe...[9]. the montmorillonite models for our study are based on a pyrophillite unit cell structure (5.16å×8.966å×9.347å) obtained from the crystallographic data of lee et al. [10]. the 4×4×2 simulation supercells were built and substitutions were made in the pyrophillite structure in order to approximate as close as possible the chemical composition of wyoming montmorillonite m24(si248al8)(al112mg16)o640(oh)128, where m is either cs+, na+, or k+ [9]. we explored three different models of substitution distributions. in the first model, the substitutions were uniformly and orderly distributed within the tetrahedral and octahedral sheets. in the second model, the substituted positions were kept ordered in the octahedral sheets but made disordered in the tetrahedral one. in the third model, the substituted positions of the octahedral sites were additionally made disordered. in order to study the swelling behavior of these montmorillonites, molecular dynamics (md) simulations were run in the npzt statistical ensemble (t = 298 k, pz = 1 bar) for each of the three different substitution models and with 22 different hydration states ranging from 0 to 700 mgwater/gclay (from 0 to 42 h2o molecules per one monovalent cation). all md runs were performed for a total of 2 ns using the clayff force field [11]. at the beginning of the simulations, the cations were placed at the midplane of the clay interlayer space and water molecules were added randomly. after the system reached equilibrium, the last 1ns of each md trajectory was used to compute the clay basal spacing and the swelling thermodynamic properties: hydration energy, immersion energy, isosteric heat of adsorption. the md simulation results indicate that in addition to the commonly observed 1-layer and 2-layer hydrates, stable hydration states corresponding to 3-layer and 4-layer hydrates can also be distinguished. the stable states corresponding to the minima of hydration energy were then selected to run further 500 ps nvt-ensemble md simulations at the same temperature and with the volume fixed at the average value resulting from the corresponding previous npzt simulation. the equilibrium parts of these nvt-simulated trajectories were then used to calculate the structural (radial distribution functions, atomic density profiles) and dynamical (diffusion coefficient) properties of the hydrated montmorillonite. references [1] erm (1997) echantillons d'argiles du forage est104 : etude minéralogique approfondie. rapport andra n° d.rp.0erm.97.008 [2] erm (1996b) caractérisation d'échantillons d'argiles du forage est103. rapport andra n° b.rp.0erm.96.003 [3] andra (2005) dossier 2005 argile, référentiel du site de meuse haute marne. c.r.p.ads.04.0022 andra : paris [4] buffle, j. (1988) complexation reactions in aquatic systems: an analytical approach; ellis horwood ltd.:chichester, p 692. [5] tipping, e. (2002) cation binding by humic substances, cambridge university press: cambridge, p 434. [6] smith, d.e., langmuir, 14, 5959-5967 (1998). [7] rotenberg, b., marry, v., vuilleumier, r., malikova, n., simon, c., turq, p., geochim. cosmochim. acta, 71, 5089-5101 (2007). [8] liu, x.d., lu, x.c., wang, r.c., zhou, h.q. geochim. cosmochim. acta, 72, 1837-1847 (2008). [9] tsipursky, s.i., drits, v.a. clay minerals, 19, 177-193 (1984). [10] lee, j.h. and guggenheim, s. american mineralogist, 66, 350-357 (1981). [11] cygan, r.t., liang, j.j., kalinichev, a.g. journal of physical chemistry b, 108, 1255-1266 (2004).
on the hydrogen bonding structure at the aqueous interface of ammonium-substituted mica: a molecular dynamics simulation. molecular dynamics (md) computer simulations were performed for an aqueous film of 3nm thickness adsorbed at the (001) surface of ammonium-substituted muscovite mica. the results provide a detailed picture of the near-surface structure and topological characteristics of the interfacial hydrogen bonding network. the effects of d/h isotopic substitution in n(h/d)4+ on the dynamics and consequently on the convergence of the structural properties have also been explored. unlike many earlier simulations, a much larger surface area representing 72 crystallographic unit cells was used, which allowed for a more realistic representation of the substrate surface with a more disordered distribution of al/si isomorphic substitutions in muscovite. the results clearly demonstrate that under ambient conditions both interfacial ammonium ions and the very first layer of water molecules are h-bonded only to the basal surface of muscovite, but do not form h-bonds with each other. as the distance from the surface increases, the h-bonds donated to the surface by both n(h/d)4+ and h2o are gradually replaced by the h-bonds to the neighboring water molecules, with the ammonia ions experiencing one reorientational transition region, while the h2o molecules experiencing three such distinct consecutive transitions. the hydrated n(h/d)4+ ions adsorb almost exclusively as inner-sphere surface complexes with the preferential coordination to the basal bridging oxygen atoms surrounding the al/si substitutions.
consistency of the french white certificates evaluation system with the framework proposed for the european energy services. according to the directive on energy end-use efficiency and energy services (esd), the european member states shall adopt a national indicative energy savings target of 9% (or beyond) in 2016. the issue of the energy savings evaluation is crucial for its implementation. the french white certificates (fwc) scheme is one of the important measures for france to fulfill its esd target. however, the accountings of energy savings in the fwc scheme and in the esd are different. therefore, an analysis of the consistency of the two systems is needed. a concrete example of actions on residential buildings is used to illustrate the challenges for policy marker and stakeholders to set harmonized evaluation rules. the fwc and esd calculations appear to be consistent from a physics point of view, as long as calculations are well-documented. but due to differences in the policy objectives, calculation routines may be necessary to convert national energy savings unit (e.g., kwh cumac) into supranational energy savings unit (e.g., esd kwh). finally, the work done to establish a transparent evaluation system brings additional benefits (e.g., increased visibility and quality of the actions), which will improve the results of the energy efficiency policies on long term.
forcing in coq. null
constraints on the origin of cosmic rays above 10^18 ev from large-scale anisotropy searches in data of the pierre auger observatory. a thorough search for large-scale anisotropies in the distribution of arrival directions of cosmic rays detected above 10^18 ev at the pierre auger observatory is reported. for the first time, these large-scale anisotropy searches are performed as a function of both the right ascension and the declination and expressed in terms of dipole and quadrupole moments. within the systematic uncertainties, no significant deviation from isotropy is revealed. upper limits on dipole and quadrupole amplitudes are derived under the hypothesis that any cosmic ray anisotropy is dominated by such moments in this energy range. these upper limits provide constraints on the production of cosmic rays above 1018 ev, since they allow us to challenge an origin from stationary galactic sources densely distributed in the galactic disk and emitting predominantly light particles in all directions.
anti-unification with type classes. the anti-unification problem is that of finding the most specific pattern of two terms. while dual to the unification problem, anti-unification has rarely been considered at the level of types. in this paper, we present an algorithm to compute the least general type of two types in haskell, using the logic programming power of type classes. that is, we define a type class for which the type class instances resolution performs anti-unification. we then use this type class to define a type-safe embedding of aspects in haskell.
a new approach to characterize the nanostructure of activated carbons from mathematical morphology applied to high resolution transmission electron microscopy images. a new characterization method of the nanoporous structure of activated carbons (acs) is proposed, based on mathematical morphology analysis of high resolution transmission electron microscopy (tem) images. it produces refined statistics describing the shape, size and orientation of the defective graphene sheets seen edge on as individual fringes on tem images. it also provides some quantitative information regarding their spatial arrangement. especially, assemblages composed of 2-4 nearly parallel fringe fragments could be detected, which were relevant of some partial stacking of the defective graphene sheets. such assemblages were possibly locally oriented along a common direction to form large continuous domains. to prove the ability of the image analysis tool to reveal distinctive features and degrees of disorder of the ac structures, a set of various commercial carbon adsorbents was characterized. the measured effective spaces separating the individual fringes, the stacks and the continuous domains were examined and compared with the porosity data derived from 77 k-n2 adsorption isotherms. consistency between the two sets of data was assessed and interpreted by considering the n2 diffusional limitations resulting from the micropore network connectivity.
adapting transformations to metamodel changes via external transformation composition. null
a typed monadic embedding of aspects. we describe a novel approach to embed pointcut/advice aspects in a typed functional programming language like haskell. aspects are first-class, can be deployed dynamically, and the pointcut language is extensible. type soundness is guaranteed by exploiting the un- derlying type system, in particular phantom types and a new anti- unification type class. the use of monads brings type-based rea- soning about effects for the first time in the pointcut/advice setting, thereby practically combining open modules and effectiveadvice, and enables modular extensions of the aspect language.
long-range angular correlations on the near and away side in p-pb collisions at sqrt(snn) = 5.02 tev. angular correlations between charged trigger and associated particles are measured by the alice detector in p-pb collisions at a nucleon-nucleon centre-of-mass energy of 5.02 tev for transverse momentum ranges within 0.5 &lt; pt,assoc &lt; pt,trig &lt; 4 gev/c. the correlations are measured over two units of pseudorapidity and full azimuthal angle in different intervals of event multiplicity, and expressed as associated yield per trigger particle. two long-range ridge-like structures, one on the near side and one on the away side, are observed when the per-trigger yield obtained in low-multiplicity events is subtracted from the one in high-multiplicity events. the excess on the near-side is qualitatively similar to that recently reported by the cms collaboration, while the excess on the away-side is reported for the first time. the two-ridge structure projected onto azimuthal angle is quantified with the second and third fourier coefficients as well as by near-side and away-side yields and widths. the yields on the near side and on the away side are equal within the uncertainties for all studied event multiplicity and pt bins, and the widths show no significant evolution with event multiplicity or pt. these findings suggest that the near-side ridge is accompanied by an essentially identical away-side ridge.
large-scale distribution of arrival directions of cosmic rays detected above 10^18 ev at the pierre auger observatory. a thorough search for large-scale anisotropies in the distribution of arrival directions of cosmic rays detected above 10^18 ev at the pierre auger observatory is presented. this search is performed as a function of both declination and right ascension in several energy ranges above 10^18 ev, and reported in terms of dipolar and quadrupolar coefficients. within the systematic uncertainties, no significant deviation from isotropy is revealed. assuming that any cosmic-ray anisotropy is dominated by dipole and quadrupole moments in this energy range, upper limits on their amplitudes are derived. these upper limits allow us to test the origin of cosmic rays above 10^18 ev from stationary galactic sources densely distributed in the galactic disk and predominantly emitting light particles in all directions.
the megapie-test project: supporting research and lessons learned in first-of-a-kind spallation target technology. the megawatt pilot experiment (megapie) has been launched by six european institutions (psi, fzk, cea, sck-cen, enea and cnrs), jaea (japan), doe (us) and kaeri (korea) with the aim to carry out an experiment, in the sinq target location at psi (switzerland), to demonstrate the safe operation of a liquid metal (lead–bismuth eutectic, lbe) spallation target hit by a not, vert, similar1 mw proton beam. the european commission has joined the megapie project through the 5-year (2001–2006) project named megapie-test. this project has been formally concluded with an international workshop, where the results and the lessons learned during the project have been summarised. this work presents a review of the outcome of that workshop.
action-perception trade-offs for anguilliform swimming robotic platforms with an electric sense. the work presented addresses the combination of anguilliform swimming-based propulsion with the use of an electric sensing modality for a class of unmanned underwater vehicles, and in particular investigates the relative influence of adjustments to the swimming gait on the platform's displacement speed and on sensing performance. this influence is quantified, for a relevant range of swimming gaits, using experimental data recordings of displacement speeds, and a boundary element method-based numerical simulation tool allowing to reconstruct electric measures. results show that swimming gaits providing greater movement speeds tend to degrade sensing performance. conversely, gaits yielding accurate sensing tend to prove slower. to reconcile opposing tendencies, a simple action-perception cost function is designed, with the purpose of adjusting an anguilliform swimmer's gait shape, in accordance with respective importance afforded to action (i.e. movement speed) and perception.
a hybrid dynamic model of an insect-like mav with soft wings. this paper presents a hybrid dynamic model of a 3-d aerial insect-like robot. the soft-bodied insect wings modeling is based on a continuous version of the newton-euler dynamics where the leading edge is treated as a continuous cosserat beam. these wings are connected to an insect's rigid thorax using a discrete recursive algorithm based on the newton-euler equations. here we detail the inverse dynamic model algorithm. this version of the dynamic model solves the following two problems involved in any locomotion task: 1◦) it enables the net motion of a reference body to be computed from the known data of internal motions (strain fields); 2◦) it gives the internal torques required to impose these internal (strain fields) motions. the essential fluid effects have been taken into account using a simplified analytical hovering flight aerodynamic model. to facilitate the analysis of numerical results, a visualization tool is developed.
estimation of relative position and coordination of mobile underwater robotic platforms through electric sensing. in the context of underwater robotics, positioning and coordination of mobile agents can prove a challenging problem. to address this issue, we propose the use of electric sensing, with a technique inspired by weakly electric fishes. in particular, the approach relies on one or several of the agents applying an electric field to their environment. using electric measures, others agents are able to reconstruct their relative position with respect to the emitter, over a range that is function of the geometry of the emitting agent and of the power applied to the environment. efficacy of the technique is illustrated using a number of numerical examples. the approach is shown to allow coordination of unmanned underwater vehicles, including that of bio-inspired swimming robotic platforms.
macro-continuous dynamics for hyper-redundant robots: application to locomotion bio-inspired by elongated animals. this article presents a unified dynamic modeling approach of continuum robots. the robot is modeled as a geometrically exact beam continuously actuated through an active strain law. once included into the geometric mechanics of locomotion, the approach applies to any hyper-redundant or continuous robot devoted to manipulation and/or locomotion. furthermore, exploiting the nature of the resulting models as being a continuous version of the newton-euler models of discrete robots, an algorithm is proposed which is capable of computing the internal control torques (and/or forces) as well as the rigid overall motions of the locomotor robot. the efficiency of the approach is finally illustrated through many examples directly related to the terrestrial locomotion of elongated animals as snakes, worms or caterpillars and their associated bio-mimetic artifacts.
azimuthal anisotropy of π0 production in au+au collisions at √snn=200 gev: path-length dependence of jet quenching and the role of initial geometry. we have measured the azimuthal anisotropy of π0 production for 1.
transverse momentum dependence of j/ψ polarization at midrapidity in p+p collisions at √s=200 gev. we report the measurement of the transverse momentum dependence of inclusive j/ψ polarization in p+p collisions at √s=200 gev performed by the phenix experiment at the relativistic heavy ion collider. the j/ψ polarization is studied in the helicity, gottfried-jackson, and collins-soper frames for pt&lt;5 gev/c and |y|&lt;0.35. the polarization in the helicity and gottfried-jackson frames is consistent with zero for all transverse momenta, with a slight (1.8 sigma) trend towards longitudinal polarization for transverse momenta above 2 gev/c. no conclusion is allowed due to the limited acceptance in the collins-soper frame and the uncertainties of the current data. the results are compared to observations for other collision systems and center of mass energies and to different quarkonia production models.
high pt direct photon and π0 triggered azimuthal jet correlations and measurement of kt for isolated direct photons in p+p collisions at √s=200 gev. correlations of charged hadrons of 1.
evaluation of the anthropogenic influx of metal and metalloid contaminants into the moulay bousselham lagoon, morocco, using chemometric methods coupled to geographical information systems. superficial and cored sediment samples from the moulay bousselham lagoon and sub-watershed were analyzed for al, fe, cu, zn, pb, mn, ni, cr, as, hg and cd. the temporal and spatial distributions of the main contamination sources of heavy metals were identified and described using chemometric and gis methods. sediments from coastal lagoons near urban and agricultural areas are commonly contaminated with heavy metals and the concentrations found in surface sediments are significantly higher than those from 50-100 years ago. the concentrations of these elements decrease sharply with depth in the sediment column and the elements are preferentially enriched in the &lt;2 µm-size fraction of the sediment. the zones of enhanced risk of heavy metals were detected by means of gis-based geostatistical modeling. according to sediment pollution indices and statistical analysis, heavy metals (pb, cu, ni, zn, cr and hg) that pose a risk have become largely enriched in the lagoon sediments during the recent period of agricultural intensification.
the conjunction of interval among constraints. an among constraint holds if the number of variables that belong to a given value domain is between given bounds. this paper focuses on the case where the variable and value domains are intervals. we investigate the conjunction of among constraints of this type. we prove that checking for satisfiability -- and thus, enforcing bound consistency -- can be done in polynomial time. the proof is based on a specific decomposition that can be used as such to filter inconsistent bounds from the variable domains. we show that this decomposition is incomparable with the natural conjunction of \textsc{among} constraints, and that both decompositions do not ensure bound consistency. still, experiments on randomly generated instances reveal the benefits of this new decomposition in practice. this paper also introduces a generalization of this problem to several dimensions and shows that satisfiability is np-complete in the multi-dimensional case.
let effects on the hydrogen production induced by the radiolysis of pure water. radiation chemical primary yields g(h2) have been determined for irradiations performed with 60co γ-rays source of lcp (orsay, france) and with helium ion beams (eα=5.0 mev-64.7 mev) using protective agent bromide anions in solution. the α (4he2+) irradiation experiments were performed either at cemhti or at the new arronax cyclotron facility (2010). both sources (γ and cyclotrons) allow working with a large let value range between 0.23 and 151.5 kev/μm. on one hand, the obtained results have been compared with those available in the literature and plotted as a function of the let parameter in order to discuss the effects of track structure on the production of molecular hydrogen. on the other hand, the primary radiation chemistry yield g(h2) values are compared with global radiation chemical yields g(h2) obtained during irradiations of pure water irradiated under air or argon without scavenging. for each system, it appears that radiation chemical yields increase with the let value. our results suggest that using bromide anions, at low concentration, as a protective agent becomes ineffective when the let value used is higher than 120±20 kev/μm.
optimality criteria for measurement poses selection in calibration of robot stiffness parameters. the paper focuses on the accuracy improvement of industrial robots by means of elasto-static parameters calibration. it proposes a new optimality criterion for measurement poses selection in calibration of robot stiffness parameters. this criterion is based on the concept of the manipulator test pose that is defined by the user via the joint angles and the external force. the proposed approach essentially differs from the traditional ones and ensures the best compliance error compensation for the test configuration. the advantages of this approach and its suitability for practical applications are illustrated by numerical examples, which deal with calibration of elasto-static parameters of planar manipulator with rigid links and compliant actuated joints.
stability of manipulator configuration under external loading. the paper is devoted to the analysis of robotic manipulator behavior under internal and external loadings. the main contributions are in the area of stability analysis of manipulator configurations corresponding to the loaded static equilibrium. in contrast to other works, in addition to usually studied the end-platform behavior with respect to the disturbance forces, the problem of configuration stability for each kinematic chain is considered. the proposed approach extends the classical notion of the stability for the static equilibrium configuration that is completely defined the properties of the cartesian stiffness matrix only. the advantages and practical significance of the proposed approach are illustrated by several examples that deal with serial kinematic chains and parallel manipulators. it is shown that under the loading the manipulator workspace may include some specific points that are referred to as elastostatic singularities where the chain configurations become unstable.
design of experiments for calibration of planar anthropomorphic manipulators. the paper presents a novel technique for the design of optimal calibration experiments for a planar anthropomorphic manipulator with n degrees of freedom. proposed approach for selection of manipulator configurations allows essentially improving calibration accuracy and reducing parameter identification errors. the results are illustrated by application examples that deal with typical anthropomorphic manipulators.
about the nature of the bonding in ato+. null
characterization of at- and ato+ species in simple media by high performance ion exchange chromatography coupled to gamma detector. application for astatine speciation in human serum. null
speciation of rn in blood serum: a key issue in alpha therapy. null
introducing the elf topological analysis in the field of quasirelativistic quantum calculations. we present an original formulation of the electron localization function (elf) in the field of relativistic two-component dft calculations. using i2 and at2 species as a test set, we show that the elf analysis is suitable to evaluate the spin-orbit effects on the electronic structure. beyond these examples, this approach opens up new opportunities for the bonding analysis of large molecular systems involving heavy and super-heavy elements.
on matrices, automata, and double counting in constraint programming. matrix models are ubiquitous for constraint problems. many such problems have a matrix of variablesm, with the same constraint c defined by a finitestate automaton a on each row ofmand a global cardinality constraint gcc on each column of m. we give two methods for deriving, by double counting, necessary conditions on the cardinality variables of the gcc constraints from the automaton a. the first method yields linear necessary conditions and simple arithmetic constraints. the second method introduces the cardinality automaton, which abstracts the overall behaviour of all the row automata and can be encoded by a set of linear constraints. we also provide a domain consistency filtering algorithm for the conjunction of lexicographic ordering constraints between adjacent rows ofmand (possibly different) automaton constraints on the rows. we evaluate the impact of our methods in terms of runtime and search effort on a large set of nurse rostering problem instances.
review of chemical and radiotoxicological properties of polonium. the discovery of polonium (po) was first published in july, 1898 by p. curie and m. curie. it was the first element to be discovered by the radiochemical method. polonium can be considered as a famous but neglected element: only a few studies of polonium chemistry have been published, mostly between 1950 and 1990. the recent (2006) event in which 210po evidently was used as a poison to kill a. litvinenko has raised new interest in polonium. 2011 being the 100th anniversary of the marie curie nobel prize in chemistry, the aim of this review is to look at the several aspects of polonium linked to its chemical properties and its radiotoxicity, including (i) its radiochemistry and interaction with matter; (ii) its main sources and uses; (iii) its physicochemical properties; (iv) its main analytical methods; (v) its background exposure risk in water, food, and other environmental media; (vi) its biokinetics and distribution following inhalation, ingestion, and wound contamination; (vii) its dosimetry; and (viii) treatments available (decorporation) in case of internal contamination.
effects of experience and workplace culture in human-robot team interaction in robotic surgery: a case study. robots are being used in the operating room to aid in surgery, prompting changes to workflow and adaptive behavior by the users. this case study presents a methodology for examining human-robot team interaction in a complex environment, along with the results of its application in a study of the effects of experience and workplace culture, for human-robot team interaction in the operating room. the analysis of verbal and non-verbal events in robotic surgery in two different surgical teams (one in the us and one in france) revealed differences in workflow, timeline, roles, and communication patterns as a function of experience and workplace culture. longer preparation times and more verbal exchanges related to uncertainty in use of the robotic equipment were found for the french team, who also happened to be less experienced. this study offers an effective method for studying human-robot team interaction and has implications for the future design and training of teamwork with robotic systems in other complex work environments.
haptic communication to support biopsy procedures learning in virtual environments. in interventional radiology, physicians require high haptic sensitivity and fine motor skills development because of the limited real-time visual feedback of the surgical site. the transfer of this type of surgical skill to novices is a challenging issue. this paper presents a study on the design of a biopsy procedure learning system. our methodology, based on a task-centered design approach, aims to bring out new design rules for virtual learning environments. a new collaborative haptic training paradigm is introduced to support human-haptic interaction in a virtual environment. the interaction paradigm supports haptic communication between two distant users to teach a surgical skill. in order to evaluate this paradigm, a user experiment was conducted. sixty volunteer medical students participated in the study to assess the influence of the teaching method on their performance in a biopsy procedure task. the results show that to transfer the skills, the combination of haptic communication with verbal and visual communications improves the novices' performance compared to conventional teaching methods. furthermore, the results show that, depending on the teaching method, participants developed different needle insertion profiles. we conclude that our interaction paradigm facilitates expert-novice haptic communication and improves skills transfer; and new skills acquisition depends on the availability of different communication channels between experts and novices. our findings indicate that the traditional fellowship methods in surgery should evolve to an off-patient collaborative environment that will continue to support visual and verbal communication, but also haptic communication, in order to achieve a better and more complete skills training.
influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments. collaborative virtual environments (cves) are 3d spaces in which users share virtual objects, communicate, and work together. to collaborate efficiently, users must develop a common representation of their shared virtual space. in this work, we investigated spatial communication in virtual environments. in order to perform an object co-manipulation task, the users must be able to communicate and exchange spatial information, such as object position, in a virtual environment. we conducted an experiment in which we manipulated the contents of the shared virtual space to understand how users verbally construct a common spatial representation of their environment. forty-four students participated in the experiment to assess the influence of contextual objects on spatial communication and sharing of viewpoints. the participants were asked to perform in dyads an object co-manipulation task. the results show that the presence of a contextual object such as fixed and lateralized visual landmarks in the virtual environment positively influences the way male operators collaborate to perform this task. these results allow us to provide some design recommendations for cves for object manipulation tasks.
stiffness modeling of non-perfect parallel manipulators. the paper focuses on the stiffness modeling of parallel manipulators composed of non-perfect serial chains, whose geometrical parameters differ from the nominal ones. in these manipulators, there usually exist essential internal forces/torques that considerably affect the stiffness properties and also change the end-effector location. these internal load-ings are caused by elastic deformations of the manipulator ele-ments during assembling, while the geometrical errors in the chains are compensated for by applying appropriate forces. for this type of manipulators, a non-linear stiffness modeling tech-nique is proposed that allows us to take into account inaccuracy in the chains and to aggregate their stiffness models for the case of both small and large deflections. advantages of the developed technique and its ability to compute and compensate for the compliance errors caused by different factors are illustrated by an example that deals with parallel manipulators of the or-thoglide family.
compliance error compensation technique for parallel robots composed of non-perfect serial chains. the paper presents the compliance errors compensation technique for over-constrained parallel manipulators under external and internal loadings. this technique is based on the non-linear stiffness modeling which is able to take into account the influence of non-perfect geometry of serial chains caused by manufacturing errors. within the developed technique, the deviation compensation reduces to an adjustment of a target trajectory that is modified in the off-line mode. the advantages and practical significance of the proposed technique are illustrated by an example that deals with groove milling by the orthoglide manipulator that considers different locations of the workpiece. it is also demonstrated that the impact of the compliance errors and the errors caused by inaccuracy in serial chains cannot be taken into account using the superposition principle.
optimization of measurement configurations for geometrical calibration of industrial robot. the paper is devoted to the geometrical calibration of industrial robots employed in precise manufacturing. to identify geometric parameters, an advanced calibration technique is proposed that is based on the non-linear experiment design theory, which is adopted for this particular application. in contrast to previous works, the calibration experiment quality is evaluated using a concept of the user-defined test-pose. in the frame of this concept, the related optimization problem is formulated and numerical routines are developed, which allow user to generate optimal set of manipulator configurations for a given number of calibration experiments. the efficiency of the developed technique is illustrated by several examples.
industry-oriented performance measures for design of robot calibration experiment. the paper focuses on the accuracy improvement of geometric and elasto-static calibration of industrial robots. it proposes industry-oriented performance measures for the calibration experiment design. they are based on the concept of manipulator test-pose and referred to the end-effector location accuracy after application of the error compensation algorithm, which implements the identified parameters. this approach allows the users to define optimal measurement configurations for robot calibration for given work piece location and machining forces/torques. these performance measures are suitable for comparing the calibration plans for both simple and complex trajectories to be performed. the advantages of the developed techniques are illustrated by an example that deals with machining using robotic manipulator.
optimal selection of measurement configurations for stiffness model calibration of anthropomorphic manipulators. the paper focuses on the calibration of elastostatic parameters of spatial anthropomorphic robots. it proposes a new strategy for optimal selection of the measurement configurations that essentially increases the efficiency of robot calibration. this strategy is based on the concept of the robot test-pose and ensures the best compliance error compensation for the test configuration. the advantages of the proposed approach and its suitability for practical applications are illustrated by numerical examples, which deal with calibration of elastostatic parameters of a 3 degrees of freedom anthropomorphic manipulator with rigid links and compliant actuated joints.
design of calibration experiments for identification of manipulator elastostatic parameters. the paper is devoted to the elastostatic calibration of industrial robots, which is used for precise machining of large-dimensional parts made of composite materials. in this technological process, the interaction between the robot and the workpiece causes essential elastic deflections of the manipulator components that should be compensated by the robot controller using relevant elastostatic model of this mechanism. to estimate parameters of this model, an advanced calibration technique is applied that is based on the non-linear experiment design theory, which is adopted for this particular application. in contrast to previous works, it is proposed a concept of the user-defined test-pose, which is used to evaluate the calibration experiments quality. in the frame of this concept, the related optimization problem is defined and numerical routines are developed, which allow generating optimal set of manipulator configurations and corresponding forces/torques for a given number of the calibration experiments. some specific kinematic constraints are also taken into account, which insure feasibility of calibration experiments for the obtained configurations and allow avoiding collision between the robotic manipulator and the measurement equipment. the efficiency of the developed technique is illustrated by an application example that deals with elastostatic calibration of the serial manipulator used for robot-based machining.
logic programming environments: dynamic program analysis and debugging. null
tracing prolog programs by source instrumentation is efficient enough. null
upsilon (1s+2s+3s) production in d+au and p+p collisions at sqrt(s_nn)=200 gev and cold-nuclear matter effects. the three upsilon states, upsilon(1s+2s+3s), are measured in d+au and p+p collisions at sqrt(s_nn)=200 gev and rapidities 1.2&lt;|y|&lt;2.2 by the phenix experiment at the relativistic heavy-ion collider. cross sections for the inclusive upsilon(1s+2s+3s) production are obtained. the inclusive yields per binary collision for d+au collisions relative to those in p+p collisions (r_dau) are found to be 0.62 +/- 0.26 (stat) +/- 0.13 (syst) in the gold-going direction and 0.91 +/- 0.33 (stat) +/- 0.16 (syst) in the deuteron-going direction. the measured results are compared to a nuclear-shadowing model, eps09 [jhep 04, 065 (2009)], combined with a final-state breakup cross section, sigma_br, and compared to lower energy p+a results. we also compare the results to the phenix j/psi results [phys. rev. lett. 107, 142301 (2011)]. the rapidity dependence of the observed upsilon suppression is consistent with lower energy p+a measurements.
modular and flexible causality control on the web. ajax has allowed javascript programmers to create interactive, collaborative, and user-centered web applications, known as web 2.0 applications. these web applications behave as distributed systems because processors are user machines that are used to send and receive messages between one another. unsurprisingly, these applications have to address the same causality issues present in distributed systems like the need a) to control the causality between messages sent and responses received and b) to react to distributed causal relations. javascript programmers overcome these issues using rudimentary and alternative techniques that largely ignore the distributed computing theory. in addition, these techniques are not very flexible and need to intrusively modify these web applications. in this paper, we study how causality issues affect these applications and present weca, a practical library that allows for modular and flexible control over these causality issues in web applications. in contrast to current proposals, weca is based on (stateful) aspects, message ordering strategies, and vector clocks. we illustrate weca in action with several practical examples from the realm of web applications. for instance, we analyze the flow of information in web applications like twitter using weca.
a search for point sources of eev neutrons. a thorough search of the sky exposed at the pierre auger cosmic ray observatory reveals no statistically significant excess of events in any small solid angle that would be indicative of a flux of neutral particles from a discrete source. the search covers from -90° to +15° in declination using four different energy ranges above 1 eev (1018 ev). the method used in this search is more sensitive to neutrons than to photons. the upper limit on a neutron flux is derived for a dense grid of directions for each of the four energy ranges. these results constrain scenarios for the production of ultrahigh energy cosmic rays in the galaxy.
how do software architects consider non-functional requirements: an exploratory study. dealing with non-functional requirements (nfrs) has posed a challenge onto software engineers for many years. over the years, many methods and techniques have been proposed to improve their elicitation, documentation, and validation. knowing more about the state of the practice on these topics may benefit both practitioners' and researchers' daily work. a few empirical studies have been conducted in the past, but none under the perspective of software architects, in spite of the great influence that nfrs have on daily architects' practices. this paper presents some of the findings of an empirical study based on 13 interviews with software architects. it addresses questions such as: who decides the nfrs, what types of nfrs matter to architects, how are nfrs documented, and how are nfrs validated. the results are contextualized with existing previous work.
a model driven reverse engineering framework for extracting business rules out of a java application. in order to react to the ever-changing market, every organization needs to periodically reevaluate and evolve its company policies. these policies must be enforced by its information system (is) by means of a set of business rules that drive the system behavior and data. clearly, policies and rules must be aligned at all times but unfortunately this is a challenging task. in most iss implementation of business rules is scattered among the code so appropriate techniques must be provided for the discovery and evolution of evolving business rules. in this paper we describe a model driven reverse engineering framework aiming at extracting business rules out of java source code. the use of modeling techniques facilitate the representation of the rules at a higher-abstraction level which enables stakeholders to understand and manipulate them.
model-driven software engineering in practice. this book is an agile and flexible tool that introduces you to the model-driven engineering world. it presents its basic principles and techniques, and puts them at work on freely available eclipse-based tools. this lets you choose the ideal set of mde instruments so to get benefit from mde right away.
a model seeker: extracting global constraint models from positive examples. we describe a system which generates finite domain constraint models from positive example solutions, for highly structured problems. the system is based on the global constraint catalog, providing the library of constraints that can be used in modeling, and the constraint seeker tool, which finds a ranked list of matching constraints given one or more sample call patterns. we have tested the modeler with 230 examples, ranging from 4 to 6,500 variables, using between 1 and 7,000 samples. these examples come from a variety of domains, including puzzles, sports-scheduling, packing &amp; placement, and design theory. when comparing against manually specified "canonical" models for the examples, we achieve a hit rate of 50\%, processing the complete benchmark set in less than one hour on a laptop. surprisingly, in many cases the system finds usable candidate lists even when working with a single, positive example.
some research challenges and remarks on cp. rather than providing general challenges, we first focus on a small set of concrete questions that we think are worth investigating. this means by no way that other issues, such as integrating continuous and discrete constraints, are not important, but we preferred to state focussed challenges. we then make some points regarding the development of sustainable cp solvers and the way cp interacts with other computer science areas.
building global constraint models from positive examples. we present a system which generates global constraint models from few positive examples of problem solutions. in contrast to previous constraint acquisition work, we present a novel approach based on the global constraint catalog and the constraint seeker tool which generates models for problems which can be expressed as regular conjunctions of similar constraints. our system first generates regular groupings of variables in the given samples. the constraint seeker is then used to find ranked, typical constraints which match all given positive examples. a dominance check, which removes implied constraints based on meta-data in the constraint catalog, leads to a final ranked set of candidate constraints for each problem. the system is implemented in sicstus prolog, and heavily relies on the constraint description and evaluators in the global constraint catalog. we show results for more than 200 example problems, ranging from puzzles to sports scheduling, placement and layout problems. the problems range from 4 to over 6000 variables, and use between one and 7000 samples, utilizing over 50 global constraints of the catalog. we achieve an overall hit-rate of about 50%.
on the reification of global constraints. we introduce a simple idea for deriving reified global constraints in a systematic way. it is based on the observation that most global constraints can be reformulated as a conjunction of pure functional dependency constraints together with a constraint that can be easily reified. we first show how the core constraints of the global constraint catalogue can be reified and we then identify several reification categories that apply to at least 82% of the constraints in the global constraint catalogue.
an o(n log n) bound consistency algorithm for the conjunction of an alldifferent and an inequality between a sum of variables and a constant, and its generalization. this paper gives an o(nlog n) bound-consistency filtering algorithm for the conjunction alldifferent(v0,v1,...,vn−1)∧ f(v0)⌖f(v1)⌖...⌖f(vn−1)≤ cst, (v0,v1,...,vn−1,cst∊ +), where (,⌖) is a commutative group, f is a unary function, and both ⌖ and f are monotone increasing. this complexity is equal to the complexity of the bound-consistency algorithm of the alldifferent constraint.
a scalable sweep algorithm for the cumulative constraint. this paper presents a sweep based algorithm for the cumulative constraint, which can operate in filtering mode as well as in greedy assignment mode. given n tasks, this algorithm has a worst-case time complexity of o(n^2). in practice, we use a variant with better average-case complexity but worst-case complexity of o(n 2 logn), which goes down to o(n log n) when all tasks have unit duration, i.e. in the bin-packing case. despite its worst-case time complexity, this algorithm scales well in practice, even when a significant number of tasks can be scheduled in parallel. it handles up to 1 million tasks in one single cumulative constraint in both choco and sicstus.
9th international conference on integration of ai and or techniques in constraint programming for combinatorial optimization problems (cpaior'12). the 9th international conference on integration of artificial intelligence and operations research techniques in constraint programming was held in nantes, france, may 28-june 1, 2012. the aim of the cpaior conference series is to bring together interested researchers from constraint programming (cp), artificial intelligence (ai), and operations research (or) to present new techniques or new applications in com- binatorial optimization and to provide an opportunity for researchers in one area to learn about techniques in the others. a main objective of this conference se- ries is also to give these researchers the occasion to show how the integration of techniques from different fields can lead to interesting results on large and complex problems. therefore papers that actively combine, integrate, or contrast approaches from more than one of the areas were especially solicited. high-quality papers from a single area were also welcome. finally, application papers showcasing cp/ai/or techniques on innovative and challenging applications or experience reports on such applications were strongly encouraged. submissions for this year were 64 papers. each paper received at least three independent peer reviews which formed the basis for the acceptance of 26 papers. these papers are published in full in these proceedings. the program committee made a good job of providing thorough reviews and discussions.
global constraint catalog, 2nd edition (revision a). this report presents a catalogue of global constraints where each constraint is explicitly described in terms of graph properties and/or automata and/or first order logical formulae with arithmetic. when available, it also presents some typical usage as well as some pointers to existing filtering algorithms.
a generalized arc-consistency algorithm for a class of counting constraints:revised edition that incorporates one correction. this paper introduces the seq bin meta-constraint with a polytime algorithm achieving generalized arc-consistency according to some properties. seq bin can be used for encoding counting constraints such as change, smooth or increasing nvalue. for some of these constraints and some of their variants gac can be enforced with a time and space complexity linear in the sum of domain sizes, which improves or equals the best known results of the literature.
learning structured constraint models: a first attempt. in this paper we give an overview of a novel tool which learns structured constraint models from flat, positive examples of solutions. it is based on previous work on a constraint seeker, which finds constraints in the global constraint catalog satisfying positive and negative examples. in the current tool we extend this system to find structured conjunctions of constraints on regular subsets of variables in the given solutions. two main elements of the approach are a bi-criteria optimization problem which finds conjunctions of constraints which are both regular and relevant, and a syntactic dominance check between conjunctions, which removes implied constraints without requiring a full theorem prover, using meta-data in the constraint catalog. some initial experiments on a proof-of-concept implementation show promising results.
using the global constraint seeker for learning structured constraint models: a first attempt. considering problems that have a strong internal structure, this paper shows how to generate constraint models from a set of positive, flat samples (i.e., solutions) without knowing a priori neither the constraint candidates, nor the way variables are shared within constraints. we describe two key contributions to building such a model generator: (1) first, learning is modeled as a bicriteria optimization problem over ranked constraint candidates returned by the constraint seeker, where we optimize both the compactness of the model, and the rank (or appropriateness) of the selected constraints. (2) second, filtering out irrelevant candidate models is achieved by using meta data of the global constraint catalog that describe links between constraints. some initial experiments on a proof-of-concept implementation show promising results.
a generalized arc-consistency algorithm for a class of counting constraints. this paper introduces the seq_bin meta-constraint with a polytime algorithm achieving generalized arc-consistency. seq_bin can be used for encoding counting constraints such as change, smooth, or increasing nvalue. for all of them the time and space complexity is linear in the sum of domain sizes, which improves or equals the best known results of the literature.
a constraint seeker: finding and ranking global constraints from examples. in this paper we describe a constraint seeker application which provides a web interface to search for global constraints in the global constraint catalog, given positive and negative, fully instantiated (ground) examples. based on the given instances the tool returns a ranked list of matching constraints, the rank indicating whether the constraint is likely to be the intended constraint of the user. we give some examples of use cases and generated output, describe the different elements of the search and ranking process, discuss the role of constraint programming in the different tools used, and provide evaluation results over the complete global constraint catalog. the constraint seeker is an example for the use of generic meta-data provided in the catalog to solve a specific problem.
the ordered distribute constraint. in this paper we introduce a new cardinality constraint: ordereddistribute. given a set of variables, this constraint limits for each value v the number of times v or any value greater than v is taken. it extends the global cardinality constraint, that constrains only the number of times a value v is taken by a set of variables and does not consider at the same time the occurrences of all the values greater than v. we design an algorithm for achieving generalized arc-consistency on ordereddistribute, with a time complexity linear in the sum of the number of variables and the number of values in the union of their domains. in addition, we give some experiments showing the advantage of this new constraint for problems where values represent levels whose overrunning has to be under control.
focus: a constraint for concentrating high costs. many constraint programming models use integer cost variables aggregated in an objective criterion. in this context, some constraints involving exclusively cost variables are often imposed. such constraints are complementary to the objective function. they characterize the solutions which are acceptable in practice. this paper deals with the case where the set of costs is a sequence, in which high values should be concentrated in a few number of areas. representing such a property through a search heuristic may be complex and overall not precise enough. to solve this issue, we introduce a new constraint, focus(x,yc,len, k), where x is a sequence of n integer variables, yc an integer variable, and len and k are two integers. to satisfy focus, the minimum number of distinct sub-sequences of consecutive variables in x, of length at most len and that involve exclusively values strictly greater than k, should be less than or equal to yc . we present two examples of problems involving focus. we propose a complete filtering algorithm in o(n) time complexity.
a theta(n) bound-consistency algorithm for the increasing sum constraint. given a sequence of variables x = 〈x 0, x 1, ..., x n − 1 〉, we consider the increasingsum constraint, which imposes ∀ i ∈ [0, n − 2] x i ≤ x i + 1, and ∑xi∈xxi=s . we propose an θ(n) bound-consistency algorithm for increasingsum.
filtering algorithms for discrete cumulative problems with overloads of resource. many cumulative problems are such that the horizon is fixed and cannot be delayed. in this situation, it often occurs that all the activities cannot be scheduled without exceeding the capacity at some points in time. moreover, this capacity is not necessarily always the same during the scheduling period. this article introduces a new constraint for solving this class of problems. we adapt two filtering algorithms to our context: sweep and p. vilím's edge-finding algorithm. we emphasize that in some problems violations are imposed. we design a new filtering procedure specific to this kind of events. we introduce a search heuristic specific to our constraint. we successfully experiment our constraint.
a note on the paper "minimizing total tardiness on parallel machines with preemptions" by kravchenko and werner (2012). in this note, we point out two major errors in the paper "minimizing total tardiness on parallel machines with preemptions" by kravchenko and werner (2012). more precisely, they claimed to have proved that both problems p|pmtn|∑t j and p|r j ,p j =p,pmtn|∑t j are np -hard. we give a counter-example to their proofs, letting the complexity of these two problems open.
extracting models from source code in software modernization. model-driven software modernization is a discipline in which model-driven development (mdd) techniques are used in the modernization of legacy systems. when existing software artifacts are evolved, they must be transformed into models to apply mdd techniques such as model transformations. since most modernization scenarios (e.g., application migration) involve dealing with code in general-purpose programming languages (gpl), the extraction of models from gpl code is an essential task in a model-based modernization process. this activity could be performed by tools to bridge grammarware and mdd technical spaces, which is normally carried out by dedicated parsers. grammar-to-model transformation language (gra2mol) is a domain-specific language (dsl) tailored to the extraction of models from gpl code. this dsl is actually a text-to-model transformation language which can be applied to any code conforming to a grammar. gra2mol aims to reduce the effort needed to implement grammarware-mdd bridges, since building dedicated parsers is a complex and time-consuming task. like atl and rubytl languages, gra2mol incorporates the binding concept needed to write mappings between grammar elements and metamodel elements in a simple declarative style. the language also provides a powerful query language which eases the retrieval of scattered information in syntax trees. moreover, it incorporates extensibility and grammar reuse mechanisms. this paper describes gra2mol in detail and includes a case study based on the application of the language in the extraction of models from delphi code.
comparison between internal and external dsls via rubytl and gra2mol. domain specific languages (dsl) are becoming increasingly more important with the emergence of model-driven paradigms. most literature on dsls is focused on describing particular languages, and there is still a lack of works that compare different approaches or carry out empirical studies regarding the construction or usage of dsls. several design choices must be made when building a dsl, but one important question is whether the dsl will be external or internal, since this affects the other aspects of the language. this chapter aims to provide developers confronting the internal-external dichotomy with guidance, through a comparison of the rubytl and gra2mol model transformations languages, which have been built as an internal dsl and an external dsl, respectively. both languages will first be introduced, and certain implementation issues will be discussed. the two languages will then be compared, and the advantages and disadvantages of each approach will be shown. finally, some of the lessons learned will be presented.
static analysis of model transformations for effective test generation. model transformations are an integral part of several computing systems that manipulate interconnected graphs of objects called models in an input domain specified by a metamodel and a set of invariants. test models are used to look for faults in a transformation. a test model contains a specific set of objects, their interconnections and values for their attributes. can we automatically generate an effective set of test models using knowledge from the transformation? we present a white-box testing approach that uses static analysis to guide the automatic generation of test inputs for transformations. our static analysis uncovers knowledge about how the input model elements are accessed by transformation operations. this information is called the input metamodel footprint due to the transformation. we transform footprint, input metamodel, its invariants, and transformation pre-conditions to a constraint satisfaction problem in alloy. we solve the problem to generate sets of test models containing traces of the footprint. are these test models effective? with the help of a case study transformation we evaluate the effectiveness of these test inputs. we use mutation analysis to show that the test models generated from footprints are more effective (97.62% avg. mutation score) in detecting faults than previously developed approaches based on input domain coverage criteria (89.9% avg.) and unguided generation (70.1% avg.).
heavy quark quenching from rhic to lhc and the consequences of gluon damping. in this contribution to the quark matter 2012 conference, we study whether energy loss models established for rhic energies to describe the quenching of heavy quarks can be applied at lhc with the same success. we also benefit from the larger $p_t$-range accessible at this accelerator to test the impact of gluon damping on observables such as the nuclear modification factor.
cut generation for obtaining strong makespan lower bounds for the multi-skill project scheduling problem. null
branch and price approach for the multi-skill project scheduling problem. null
new models for the multi-skill project scheduling problem. null
cut generation for the multi-skill project scheduling problem. null
recovering beam search approach for the multi-skill project scheduling problem. null
study of pionless two-nucleon k$-$ absorptions at rest with finuda. null
integrated column generation and lagrangian relaxation approach for the multi-skill project scheduling problem. null
non-proliferation studies with double chooz detectors. null
search for the neutron-rich hypernucleus 9{\lambda}he. search for the neutron-rich hypernucleus 9lhe is reported by the finuda experiment at dafne, infn-lnf, studying (pi+, pi-) pairs in coincidence from the k-stop + 9be --&gt; 9lhe + pi+ production reaction followed by 9lhe --&gt; 9li + pi- weak decay. an upper limit of the production rate of 9lhe undergoing this two-body pi- decay is determined to be (2.3 +/- 1.9) 10-6/k-stop at 90% confidence level.
results of a self-triggered prototype system for radio-detection of extensive air showers at the pierre auger observatory. we describe the experimental setup and the results of rauger, a small radio-antenna array, consisting of three fully autonomous and self-triggered radio-detection stations, installed close to the center of the surface detector (sd) of the pierre auger observatory in argentina. the setup has been designed for the detection of the electric field strength of air showers initiated by ultra-high energy cosmic rays, without using an auxiliary trigger from another detection system. installed in december 2006, rauger was terminated in may 2010 after 65 registered coincidences with the sd. the sky map in local angular coordinates (i.e., zenith and azimuth angles) of these events reveals a strong azimuthal asymmetry which is in agreement with a mechanism dominated by a geomagnetic emission process. the correlation between the electric field and the energy of the primary cosmic ray is presented for the first time, in an energy range covering two orders of magnitude between 0.1 eev and 10 eev. it is demonstrated that this setup is relatively more sensitive to inclined showers, with respect to the sd. in addition to these results, which underline the potential of the radio-detection technique, important information about the general behavior of self-triggering radio-detection systems has been obtained. in particular, we will discuss radio self-triggering under varying local electric-field conditions.
alpha localized radiolysis and corrosion mechanisms at the iron/water interface: role of molecular species. this paper is devoted to the iron corrosion phenomena induced by the α (4he2+) water radiolysis species studied in conjunction with the production/consumption of h2 at the solid/solution interface. on one hand, the solid surface is characterized during the 4he2+ ions irradiation by in situ raman spectroscopy; on another hand, the h2 gas produced by the water radiolysis is monitored by ex situ gas measurements. the 4he2+ ions irradiation experiments are provided either by the cemhti (e = 5.0 mev) either by the arronax (e = 64.7 mev) cyclotron facilities. the iron corrosion occurs only under irradiation and can be slowed down by h2 reductive atmosphere. pure iron and carbon steel solids are studied in order to show two distinct behaviors of these surfaces vs. the 4he2+ ions water irradiation: the corrosion products identified are the magnetite phase (fe(ii)fe(iii)2o4) correlated to an h2 consumption for pure iron and the lepidocrocite phase (γ-fe(iii)ooh) correlated to an h2 production for carbon steel sample. this paper underlined the correlation between the iron corrosion products formation onto the solid surface and the h2 production/consumption mechanisms. h2o2 species is considered as the single water radiolytic species involved into the corrosion reaction at the solid surface with an essential role in the oxidation reaction of the iron surface. we propose to bring some light to these mechanisms, in particular the h2 and h2o2 roles, by the in situ raman spectroscopy during and after the 4he2+ ions beam irradiation. this in situ experiment avoids the evolution of the solid surface, in particular phases which are reactive to the oxidation processing.
maintaining arc consistency asynchronously in synchronous distributed search. we recently proposed nogood-based asynchronous forward checking (afc-ng), an efficient and robust algorithm for solving distributed constraint satisfaction problems (discsps). afc-ng performs an asynchronous forward checking phase during synchronous search. in this paper, we propose two new algorithms based on the same mechanism as afc-ng. however, instead of using forward checking as a filtering property, we pro- pose to maintain arc consistency asynchronously (maca). the first algorithm we propose, maca-del, enforces arc consistency thanks to an additional type of messages, deletion messages. the second algorithm, maca-not, achieves arc consistency without any new type of message. we provide a theoretical analysis and an experimental evaluation of the proposed approach. our experiments show the good performance of maca algorithms, particularly those of maca-not.
development of a readout electronic for the measurement of ionization in liquid xenon compton telescope containing micro-patterns. null
bacterial syntenies: an exact approach with gene quorum. background: the automatic identification of syntenies across multiple species is a key step in comparative genomics that helps biologists shed light both on evolutionary and functional problems. results: in this paper, we present a versatile tool to extract all syntenies from multiple bacterial species based on a clear-cut and very flexible definition of the synteny blocks that allows for gene quorum, partial gene correspondence, gaps, and a partial or total conservation of the gene order. conclusions: we apply this tool to two different kinds of studies. the first one is a search for functional gene associations. in this context, we compare our tool to a widely used heuristic--i-adhore--and show that at least up to ten genomes, the problem remains tractable with our exact definition and algorithm. the second application is linked to evolutionary studies: we verify in a multiple alignment setting that pairs of orthologs in synteny are more conserved than pairs outside, thus extending a previous pairwise study. we then show that this observation is in fact a function of the size of the synteny: the larger the block of synteny is, the more conserved the genes are.
community-driven dsl development with collaboro. developing a dsl, textual or graphical, is rarely a single-person task. involving several users and developers at the same time is often required to ensure that the finally produced solution actually fits the expected needs. thus, when using the emf and related tools to define and implement a dsl, having more collaboration features could really bring practical additional value to that process. collaboro, hosted in eclipse labs, recently proposes an approach to make language development processes more participative, meaning that both dsl developers and users can collaborate more together. in this lightning talk, we quickly present the current main features of the prototype as well as some envisioned evolutions using other modeling projects.
erratum: measurement of transverse single-spin asymmetries for j/psi production in polarized p+p collisions at sqrt(s)=200 gev [phys. rev. d 82, 112008 (2010)]. we previously reported [phys. rev. d 82, 112008 (2010)] measurements of transverse single-spin asymmetries, a_n, in j/psi production from transversely polarized p+p collisions at sqrt(s)=200 gev with data taken by the phenix experiment at the relativistic heavy ion collider in 2006 and 2008. subsequently, we have found errors in the analysis procedures for the 2008 data, which resulted in an erroneous value for the extracted a_n. the errors affected the sorting of events into the correct left/right and forward/backward bins. this produced an incorrect value for the 2008 result, but the 2006 result is unaffected. we have conducted two independent reanalyses with these errors corrected, and we present here the corrected values for the 2008 data and the combined results for 2006 and 2008. the new combined spin asymmetry in the forward region is a_n = -0.026+/-0.026(stat)+/-0.003(sys). since this asymmetry is consistent with zero, we no longer claim that our results suggest a possible non-zero trigluon correlation function in transversely polarized protons.
empirical research in software product line engineering. null
hybrid multi micropattern gaseous photomultiplier for detection of liquid-xenon scintillation. null
a liquid xenon tpc for a medical imaging compton telescope. null
measurement cross sections for radioisotopes production. new radioactive isotopes for nuclear medicine can be produced using particle accelerators. this is one goal of arronax, a high energy - 70 mev - high intensity - 2*350 µa - cyclotron set up in nantes. a priority list was established containing beta- - 47sc, 67cu - beta+ - 44sc, 64cu, 82sr/82rb, 68ge/68ga - and alpha emitters - 211at. among these radioisotopes, the scandium 47 and the copper 67 have a strong interest in targeted therapy. the optimization of their productions required a good knowledge of their cross-sections but also of all the contaminants created during irradiation. we launched on arronax a program to measure these production cross-sections using the stacked-foils' technique. it consists in irradiating several groups of foils - target, monitor and degrader foils - and in measuring the produced isotopes by gamma-spectrometry. the monitor - cu or ni - is used to correct beam loss whereas degrader foils are used to lower beam energy. we chose to study the natti(p,x)47sc and 68zn(p,2p)67cu reactions. targets are respectively natural titanium foil - bought from goodfellow - and enriched zinc 68 deposited on silver. in the latter case, zn targets were prepared in-house - electroplating of 68zn - and a chemical separation between copper and gallium isotopes has to be made before gamma counting. cross-section values for more than 40 different reactions cross-sections have been obtained from 18 mev to 68 mev. a comparison with the talys code is systematically done. several parameters of theoretical models have been studied and we found that is not possible to reproduce faithfully all the cross-sections with a given set of parameters.
fission barriers and half-lives of actinides in the quasimolecular shape valley. null
pseudorapidity density of charged particles in p-pb collisions at sqrt(snn) = 5.02 tev. the charged-particle pseudorapidity density measured over 4 units of pseudorapidity in non-single-diffractive (nsd) p-pb collisions at a centre-of-mass energy per nucleon pair sqrt(snn) = 5.02 tev is presented. the average value at midrapidity is measured to be 16.81 \pm 0.71 (syst.), which corresponds to 2.14 \pm 0.17 (syst.) per participating nucleon. this is 16% lower than in nsd pp collisions interpolated to the same collision energy, and 84% higher than in d-au collisions at sqrt(snn) = 0.2 tev. the measured pseudorapidity density in p-pb collisions is compared to model predictions, and provides new constraints on the description of particle production in high-energy nuclear collisions.
dynamic vehicle routing : solution methods and computational tools. within the wide scope of logistics management,transportation plays a central role and is a crucialactivity in both production and service industry.among others, it allows for the timely distributionof goods and services between suppliers, productionunits, warehouses, retailers, and final customers.more specifically, vehicle routing problems(vrps) deal with the design of a set of minimal costroutes that serve the demand for goods orservices of a set of geographically spread customers,satisfying a group of operational constraints.while it was traditionally a static problem, recenttechnological advances provide organizations withthe right tools to manage their vehicle fleet in realtime. nonetheless, these new technologies alsointroduce more complexity in fleet managementtasks, unveiling the need for decision support systemsdedicated to dynamic vehicle routing. in thiscontext, the contributions of this ph.d. thesis arethreefold : (i) it presents a comprehensive reviewof the literature on dynamic vehicle routing ; (ii)it introduces flexible optimization frameworks thatcan cope with a wide variety of dynamic vehiclerouting problems ; (iii) it defines a new vehicle routingproblem with numerous applications.
transverse momentum distribution and nuclear modification factor of charged particles in p-pb collisions at sqrt{s_nn} = 5.02 tev. the transverse momentum (p_t) distribution of primary charged particles is measured in non single-diffractive p-pb collisions at sqrt{s_nn} = 5.02 tev with the alice detector at the lhc. the p_t spectra measured near central rapidity in the range 0.5 &lt; p_t &lt; 20 gev/c exhibit a weak pseudorapidity dependence. the nuclear modification factor r_ppb is consistent with unity for p_t above 2 gev/c. this measurement indicates that the strong suppression of hadron production at high p_t observed in pb-pb collisions at the lhc is not due to an initial-state effect. the measurement is compared to theoretical calculations.
an exact algorithm for the close enough traveling salesman problem with arc covering constraints. null
a fast re-optimization approach for dynamic vehicle routing. the present work deals with dynamic vehicle routing problems in which new customers appear during the design or execution of the routing. we propose a parallel adaptive large neighborhood search (palns) that produces high quality routes in a limited computational time. then, we introduce the notion of driver inconvenience and define a bi-objective optimization problem that minimizes the cost of routing while maintaining its consistency throughout the day. we consider a problem setting in which vehicles have an initial routing plan at the beginning of the day, that is periodically updated by a decision maker. we introduce a measure of the driver inconvenience resulting from each update and propose a bi-objective approach based on palns that is able to produce a set of non-dominated solutions in reasonable computational time. these solutions offer different tradeoffs between cost efficiency and consistency, and can be used by the decision maker to update the routing of the vehicles introducing a controlled number of changes.
on the dynamic technician routing and scheduling problem. the technician routing and scheduling problem (trsp) deals with a limited crew of technicians that serves a set of requests. in the trsp, each technician has a set of skills, tools, and spare parts, while requests require a subset of each. the problem is then to design a set of tours of minimal total duration such that each request is visited exactly once, within its time window, by a technician with the required skills, tools, and spare parts. the trsp naturally arises in a wide range of applications, including telecoms, public utilities, and maintenance operations. to the best of our knowledge, no work considers neither tools, nor spare parts, nor the arrival of new requests, three important components of real-world applications. the present work addresses this aspect and proposes an optimization approach for the dynamic version of the problem, noted d-trsp, where new requests arrive during the execution of the routes.
a review of dynamic vehicle routing problems. a number of technological advances have led to a renewed interest on dynamic vehicle routing problems. this survey classifies routing problems from the perspective of information quality and evolution. after presenting a general description of dynamic routing, we introduce the notion of degree of dynamism, and present a comprehensive review of applications and solution methods for dynamic vehicle routing problems.
a parallel matheuristic for the technician routing and scheduling problem. the technician routing and scheduling problem (trsp) consists in routing staff to serve requests for service, taking into account time windows, skills, tools, and spare parts. typical applications include maintenance operations and staff routing in telecoms, public utilities, and in the health care industry. in this paper, we present a formal definition of the trsp, discuss its relation with the vehicle routing problem with time windows (vrptw), and review related research. from a methodological perspective, we describe a matheuristic composed of a constructive heuristic, a parallel adaptive large neighborhood search (palns), and a mathematical programming based post-optimization procedure that successfully tackles the trsp. we validate the matheuristic on the solomon vrptw instances, where we achieve an average gap of 0.23%, and matched 44 out of 55 optimal solutions. finally, we illustrate how the matheuristic successfully solves a set of trsp instances extended from the solomon benchmark.
control through operators for quantum chemistry. we consider the problem of operator identification in quantum control. the free hamiltonian and the dipole moment are searched such that a given target state is reached at a given time. a local existence result is obtained. as a by-product, our works reveals necessary conditions on the laser field to make the identification feasible. in the last part of this work, some newton algorithms are proposed together with a continuation method to compute effectively these operators.
half-lives of cluster radioactivity with a generalized liquid-drop model. null
measurement of heavy-flavour decay muon production at forward rapidity in pp and pb-pb collisions at sqrt(s_nn)=2.76 tev with the alice experiment. the alice experiment measured the heavy-flavour production in the semi-muonic decay channel at forward rapidities ($2.5.
greywater treatment for reuse by slow sand filtration : study of pathogenic microorganisms and phage survival. in recent decades, most countries of the world have experienced a shortage of water and increase its rate of consumption. today, every country in the world are interested in this problem by trying to find alternatives to address this shortage. one solution is reuse greywater (gw) for irrigation after treatment. gw is all water generated from household except toilet water. the risks associated with the reuse of these waters are the presence of pathogens that can infect humans, animals and plants. in this thesis focused on studying treatment by slow sand filtration and the survival of representatives of pathogens, such as e. coli, p. aeruginosa , e. faecalis and bacteriophage ms2 which could be found in the greywater. the study factors was a physico-chemicals factors such as; temperature (6±2,23±2,42±2°c), salinity (1.75 and 3.5% nacl), oxygen (aerobic and anaerobic condition), nutrient ( rich media , 50%: 50% salt and poor media ), light with photocatalysis ( uv and visible lights) and slow sand filter (egyptian desert sand and swimming pool sand). a combination of high temperature, sunlight and photocatlysis are mainly responsible for the rapid decline of bacteria and ms2 coliphage. slow sand filter have clearly less influence on the survival of bacteria in the greywater, but it effective to decline turbidity and cod for short times.
modeling of electrolocation for bio-robotics. the goal of the european project angels is to build an eel-like robot capable to navigate by the electric sense and to divide itself in several mono-agents for exploration purposes. in the context of this project my work consisted in creating a perception model inspired from the electric fish with the virtue of being fast and so, to be usable in-line. two models of perception were built and were based on a simple but realistic geometry of sensor. the two models named after the « poly-spherical model » and the « reflexions model » which come respectively from a physical intuition and an appropriate mathematical model are calibrated once for all with an electrical simulator in order to have analytical forms. coupled with models of the electrical response of objects, the two models of perception permit the robot to achieve some basic tasks like detection and obstacles’ avoidance, which is a novelty in the history. in addition we have built a generic formalism of theelectrical response that extents the application of the models of perception. finally we have begun to estimate the influence of a complex geometry of sensor, that exhibs large insulating surfaces, on the measurement in order to open the way for the rapid modelling of a sensor of arbitrary shape.
architecture quality revisited. there is a common belief in the software community that nonfunctional quality is fundamentally important for architecture sustainability and project success. a recent study, however, suggests that nonfunctional quality is of little relevance for users and customers, but instead mainly a concern for architects. nontechnical constraints, such as licenses and technology providers, appear to be driving design as prominently as quality requirements. quality requirements, such as performance, are mainly defined by architects on the basis of their experiences, and are often poorly documented and validated. this column explores whether the software community actually overestimates the relevance of nonfunctional qualities or whether the study's observations indicate a valid position on nonfunctional quality for certain types of application domains, development approaches, and organizational setups.
alf-veriﬁer: an eclipse plugin for verifying alf/uml executable models. in this demonstration we present an eclipse plugin that implements a lightweight method for verifying ﬁne-grained operations at design time. this tool suﬃces to check that the execution of the operations (speciﬁed in alf action language) is consistent with the integrity constraints deﬁned in the class diagram (speciﬁed in uml) and returns a meaningful feedback that helps correcting them otherwise.
sla-driven capacity planning for cloud applications. cloud computing paradigm has become the solution to provide good service quality and exploit economies of scale. however, the management of such elastic resources, with different quality-of-service (qos) combined with on-demand self-service, is a complex issue. this paper proposes an approach driven by service level agreement (sla) for optimizing the capacity planning for cloud applications. the main challenge for a service provider is to determine the best trade-off between profit and customer satisfaction. in order to address this issue, we follow a queueing network proposal and present an analytical performance model to predict cloud service performance. based on a utility function and a capacity planning method, our solution calculates the optimal configuration. we rely on autonomic computing to adjust continuously the configuration. simulation experiments indicate that our model i) faithfully captures the performance of cloud applications for a number of workloads and configurations and ii) successfully keeps the best trade-off.
plasma damping effects on the radiative energy loss of relativistic particles. the energy loss of a relativistic charge undergoing multiple scatterings while traversing an infinite, polarizable and absorptive plasma is investigated. polarization and absorption mechanisms in the medium are phenomenologically modeled by a complex index of refraction. apart from the known ter-mikaelian effect related to the dielectric polarization of matter, we find an additional, substantial reduction of the energy loss due to the damping of radiation. the observed effect is more prominent for larger damping and/or larger energy of the charge. a conceivable analog of this phenomenon in qcd could influence the study of jet quenching phenomena in ultrarelativistic heavy-ion collisions at rhic and lhc.
a model-driven approach for the extraction of network access-control policies. network security constitutes a critical concern when developing and maintaining nowadays corporate information systems. firewalls are a key element of network security by filtering the traffic of the network in compliance with a number of access control rules that enforce a given security policy. unfortunately, once implemented, and due to the complexity of firewall configuration languages and the underlying network topology, knowing which security policy is actually being enforced by the network system is a complex and time consuming task that requires low-level and, often, vendor- specific expertise. in an always-evolving context, where security policies are often updated to respond to new security requirements, this discovery phase becomes critical since it could hamper the proper evolution of the system and compromise its security. to tackle this problem, our approach generates an abstract model of the firewall configurations in a network that facilitates the understanding and evolution of network security policies.
a catalogue of refactorings for model-to-model transformations. in object-oriented programming, continuous refactorings are used as the main mechanism to increase the maintainability of the code base. unfortunately, in the field of model transformations, such refactoring support is so far missing. this paper tackles this limitation by adapting the notion of refactorings to model-to-model (m2m) transformations. in particular, we present a dedicated catalogue of refactorings for improving the quality of m2m transformations. the refactorings have been explored by analyzing existing transformation examples defined in atl. however, the refactorings are not specifically tailored to atl, but applicable also to other m2m transformation languages.
model-driven and software product line engineering. many approaches to creating software product lines have emerged that are based on model-driven engineering. this book introduces both software product lines and model-driven engineering, which have separate success stories in industry, and focuses on the practical combination of them. it describes the challenges and benefits of merging these two software development trends and provides the reader with a novel approach and practical mechanisms to improve software development productivity. the book is aimed at engineers and students who wish to understand and apply software product lines and model-driven engineering in their activities today. the concepts and methods are illustrated with two product line examples: the classic smart-home systems and a collection manager information system.
inclusive cross section and single-transverse-spin asymmetry for very forward neutron production in polarized p+p collisions at sqrt(s)=200 gev. the energy dependence of the single-transverse-spin asymmetry, a_n, and the cross section for neutron production at very forward angles were measured in the phenix experiment at rhic for polarized p+p collisions at sqrt(s)=200 gev. the neutrons were observed in forward detectors covering an angular range of up to 2.2 mrad. we report results for neutrons with momentum fraction of x_f=0.45 to 1.0. the energy dependence of the measured cross sections were consistent with x_f scaling, compared to measurements by an isr experiment which measured neutron production in unpolarized p+p collisions at sqrt(s)=30.6--62.7 gev. the cross sections for large x_f neutron production for p+p collisions, as well as those in e+p collisions measured at hera, are described by a pion exchange mechanism. the observed forward neutron asymmetries were large, reaching a_n=-0.08+/-0.02 for x_f=0.8; the measured backward asymmetries, for negative x_f, were consistent with zero. the observed asymmetry for forward neutron production is discussed within the pion exchange framework, with interference between the spin-flip amplitude due to the pion exchange and nonflip amplitudes from all reggeon exchanges. within the pion exchange description, the measured neutron asymmetry is sensitive to the contribution of other reggeon exchanges even for small amplitudes.
new antineutrino energy spectra predictions from the summation of beta decay branches of the fission products. in this paper, we study the impact of the inclusion of the recently measured beta decay properties of the $^{102;104;105;106;107}$tc, $^{105}$mo, and $^{101}$nb nuclei in an updated calculation of the antineutrino energy spectra of the four fissible isotopes $^{235, 238}$u, and $^{239,241}$pu. these actinides are the main contributors to the fission processes in pressurized water reactors. the beta feeding probabilities of the above-mentioned tc, mo and nb isotopes have been found to play a major role in the $\gamma$ component of the decay heat of $^{239}$pu, solving a large part of the $\gamma$ discrepancy in the 4 to 3000\,s range. they have been measured using the total absorption technique (tas), avoiding the pandemonium effect. the calculations are performed using the information available nowadays in the nuclear databases, summing all the contributions of the beta decay branches of the fission products. our results provide a new prediction of the antineutrino energy spectra of $^{235}$u, $^{239,241}$pu and in particular of $^{238}$u for which no measurement has been published yet. we conclude that new tas measurements are mandatory to improve the reliability of the predicted spectra.
antennas for the detection of radio emission pulses from cosmic-ray induced air showers at the pierre auger observatory. the pierre auger observatory is exploring the potential of the radio detection technique to study extensive air showers induced by ultra-high energy cosmic rays. the auger engineering radio array (aera) addresses both technological and scientific aspects of the radio technique. a first phase of aera has been operating since september 2010 with detector stations observing radio signals at frequencies between 30 and 80 mhz. in this paper we present comparative studies to identify and optimize the antenna design for the final configuration of aera consisting of 160 individual radio detector stations. the transient nature of the air shower signal requires a detailed description of the antenna sensor. as the ultra-wideband reception of pulses is not widely discussed in antenna literature, we review the relevant antenna characteristics and enhance theoretical considerations towards the impulse response of antennas including polarization effects and multiple signal reflections. on the basis of the vector effective length we study the transient response characteristics of three candidate antennas in the time domain. observing the variation of the continuous galactic background intensity we rank the antennas with respect to the noise level added to the galactic signal.
transport properties of hot gluonic matter. we discuss the temperature dependence of the scaled jet quenching parameter of hot gluonic matter within a quasiparticle approach. a pronounced maximum in the vicinity of the transition temperature is observed, where the ratio of the scaled jet quenching parameter and the inverse specific shear viscosity increases above typical values for weakly coupled systems.
an automatic reversible transformation from composite to visitor in java. we build reversible transformations between composite and visitor design patterns in java programs. such transformations represent an automatic reversible switching between different program architectures with a guarantee of semantic preservation. in this paper, we detail the algorithms of the transformations implemented by composing elementary refactoring operations. the transformations were automated with the refactoring tool of a popular ide: intellij idea.
coherent j/psi photoproduction in ultra-peripheral pb-pb collisions at \sqrt{s_nn} = 2.76 tev. the alice collaboration has made the first measurement at the lhc of j/psi photoproduction in ultra-peripheral pb-pb-collisions at \sqrt{s_nn} = 2.76 tev. the j/psi is identified via its dimuon decay in the forward rapidity region with the muon spectrometer for events where the hadronic activity is required to be minimal. the analysis is based on an event sample corresponding to an integrated luminosity of about 55 mub-1. the cross section for coherent j/psi production in the rapidity interval -3.6 &lt; y &lt; -2.6 is measured to be dsigma/dy = 1.00 +/- 0.18 (stat) +0.24/-0.26 (syst) mb. the result is compared to theoretical models for coherent j/psi production and found to be in good agreement with models which include nuclear gluon shadowing.
design and sizing of an hybrid sailboat, based on a power modeling approach. nowadays, the ecological impacts are took into account by collective consciousness in all fields; in the manufacturing with the concern of the products end-of-life, in construction, or in transport. for this last context, oil is no more considered as the only energy source, and electric or even multi-sources propulsions, named "hybrid" propulsions, can be found. if the hybrid propulsion solution is now well known in the field of automotive, it is not so well integrated in the field of boating. however, with the potential availability of three energy sources, oil, electricity and wind, and a quite free environment of evolution, pleasure boats could be a promising field of application for the hybrid technology. that is the point analyzed in this thesis. to carry through this study, a sizing process had to be set up. several methodologies have already been proposed in the literature since the 90's in the automotive engineering, but most of them operate with a limited set of the constitutive components of the propulsion architecture to be designed. our work background is different. we propose an iterative sizing methodology based on power exchanges modelling. our objective is to represent by a single parameterized model each set of components, to reduce the complexity of the optimization problem linked to the sizing process. the proposed methodology is applied on two sizing problems; for a sailing boat and a hybrid car.
experimental study of aerosol behavior and their deposits in a bucket elevator : impact on carry-over of micro-ingredients in animal feed industry. carry-over of additives and/or medicated products is a common issue in feed industry and, by extension in most of powder handling industries. currently carry-over rate of a production line can be accurately defined but the causes are not identified yet. it can be broken into 2 phases : firstly, particle deposit during one batch processing and then, their collecting during the following batches. experimental studies, carried out on industrial sites or on test benches, charged the bucket elevator situated just after the mixer to be responsible for a significant increase of cross contamination rate of industrial feed production lines. therefore this work focuses on this handling device. it transfers mixing of several raw materials in powdery forms, which may contain micro-ingredients, especially additives or medicated products. the aim of this study is to understand how process operations affect cross contamination rates during bucket elevator handling. a test bench of this handling device, a reference product and laboratory methods have been setup. moreover, an experimental fractional factorial design highlights the effects of several process parameters : on one hand the discharge phase on elevator head (linked to belt velocity) and the discharge spout angle act on microingredients deposit mass. on the other hand, spacing between buckets and the leg's inner surface influences micro-ingredients collected mass. furthermore, ideal position of process parameters has been defined. by this way, cross contamination rate on the test bench has been decreased from 9 to 7 percent. finally, velocity fields observations during the discharge phase leads to better understanding of how these process parameters influence cross contamination rate.
constancy of energy partition in central heavy-ion reactions at intermediate energies. semiclassical transport simulation of nucleus-nucleus collisions for the range of incident energy from about the fermi energy up to a few hundred mev per nucleon evidences that the maximal excitation energy put into a nuclear system during the early compact stage of heavy-ion reaction is a constant fraction of the center-of-mass available energy of the system. analysis of experimental data without presuming reaction mechanism dominating the collision process on the best corroborate the found constancy of energy partition in central heavy-ion reactions.
j/psi elliptic flow measurement in pb-pb collisions at \sqrt{s_{nn}} = 2.76 tev at forward rapidity with the alice experiment. j/psi suppression induced by color screening of its constituent quarks was proposed 26 years ago as a signature of the formation of a quark gluon plasma in heavy-ion collisions. recent results from alice in pb-pb collisions exhibit a smaller suppression with respect to previous measurements at the sps and rhic. the study of azimuthal anisotropy in particle production gives information on the collective hydrodynamic expansion at the early stage of the fireball, where the matter created in high-energy nuclear collisions is expected to be in a deconfined state. in particular, j/psi elliptic flow v2 is important to test the degree of thermalization of heavy quarks. together with the production yields, the elliptic flow is a powerful observable to address the question of suppression and regeneration of j/psi in qgp. we present the first inclusive j/psi elliptic flow measurement performed with the muon spectrometer of alice, in pb-pb collisions, at forward rapidity. integrated and pt-differential v2 results are presented and a comparison with recent star results and with a parton transport model is also performed.
comment on "on the subtleties of searching for dark matter with liquid xenon detectors". in a recent manuscript (arxiv:1208.5046) peter sorensen claims that xenon100's upper limits on spin-independent wimp-nucleon cross sections for wimp masses below 10 gev "may be understated by one order of magnitude or more". having performed a similar, though more detailed analysis prior to the submission of our new result (arxiv:1207.5988), we do not confirm these findings. we point out the rationale for not considering the described effect in our final analysis and list several potential problems with his study.
radiative energy loss in the absorptive qgp: taming the long formation lengths in coherent emission. in an absorptive plasma, damping of radiation mechanisms can influence the bremsstrahlung formation in case of large radiation formation lengths. we study qualitatively the influence of this effect on the gluon bremsstrahlung spectrum off heavy quarks in the quark-gluon plasma. independent of the heavy-quark mass, the spectrum is found to be strongly suppressed in an intermediate gluon energy region which grows with increasing gluon damping rate and increasing energy of the heavy quark. thus, just as polarization effects in the plasma render the bremsstrahlung spectra independent of the quark mass in the soft gluon regime, damping effects tend to have a similar impact for larger gluon energies.
fusion excitation function revisited. we report on a comprehensive systematics of fusion-evaporation and/or fusion-fission cross sections for a very large variety of systems over an energy range 4-155 a.mev. scaled by the reaction cross sections, fusion cross sections do not show a universal behavior valid for all systems although a high degree of correlation is present within subsets of appropriately selected data: regularities show up when data are ordered by the system mass asymmetry. for the rather light and close to mass-symmetric systems the main characteristics of the complete and incomplete fusion excitation functions can be precisely determined. despite an evident lack of data above 15a.mev for all heavy systems the available data suggests that geometrical effects could explain the persistence of incomplete fusion at incident energies as high as 155a.mev.
a grasp for real life inventory routing problem: application to bulk gas distribution. null
a modelling framework for procurement of a retail distribution system with economic and environmental goals. null
adding high-level concurrency to escala. on the one hand, languages like eventjava combine event- based programming with concurrency. on the other hand, extending aspect-oriented programming with concurrency has been studied as well. seamlessly combining both styles with concurrency in a single language is possible with the right building blocks. we claim that the join is such a build- ing block.
information acquisition of new technology performance for maintenance/investment decisions. the possibility of new technology occurrence has an important impact on the maintenance/ replacement decision. therefore a challenge in maintenance decision marking is to determine maintenance policy and replacement investment plan under an uncertainty of technology improvement. in each period, we must decide whether to gather additional information on the potential improvement of a new technology, then chose the appropriate action for the asset (do nothing, maintenance or replacement). to formulate this scenario, we use a non stationary markov decision process (mdp) model and provide some properties of the optimal policy based on a given set of numerical examples.
impact of maintenance on replacement investment under technological improvement. an unexplored important area in the equipment investment problem under technological improvement is the impact of maintenance policy. in fact, maintenance not only helps to maximize the profitability of the asset, but also prolong its economic life while waiting the apparition of better technology in the near future. therefore, we propose a model that allows us to consider how replacement investment in a new or improved asset will be influenced by maintenance. the investment decisions are based on information about the profitability of the current asset and the technological environment. for the maintenance process, we also consider the dependency of its cost and efficiency on the deterioration state of asset that is represented by a profit parameter. we use a non-stationary markov decision process to solve for the optimal investment/maintenance policy and illustrate the potential benefits of integrating maintenance policies in the investment strategy through different numerical analysis.
production of k*(892)^0 and phi(1020) in pp collisions at sqrt(s)=7 tev. the production of k*(892)^0 and phi(1020) in pp collisions at sqrt(s)=7 tev was measured by the alice experiment at the lhc.the yields and the transverse momentum spectra d^2n/dydpt at midrapidity |y|&lt;0.5 in the range 0.
proceedings of the seventh workshop on domain-specific aspect languages (dsal 2012). it is our great pleasure to host the seventh edition of the domain-specific aspect languages workshop (dsal12), as part of aosd 2012: perspectives on modularity, the 11th international conference on aspect-oriented software development. the tendency to raise the abstraction level in programming languages towards a particular domain is also a major driving force in the research domain of aspect-oriented programming languages. as a matter of fact, pioneering work in this field was conducted by devising small domain-specific aspect languages (dsals) such as cool for concurrency management and ridl for serialization, rg, aml, and others. after a dominating focus on general-purpose languages, research in the aosd community is again taking this path in search of innovative approaches, insights and a deeper understanding of fundamentals behind aop. based on the successful dsal'06-'11 workshops, and the special issue of iet software journal on domain- specific aspect languages, this workshop series continues to support a growing trend in aosd research. the workshop aims to bring the research communities of domain-specific language engineering and domain-specific aspect design together. in the previous successful editions held at gpce06/oopsla06 and aosd07 we approached domain-specific aspect languages both from a design and a language implementation point of view. at aosd08-10 we also invited contributions of work on adding domain-specific extensions (dsxs) to general-purpose aspect languages (gpals). last year and this year our focus is on the use of multiple dsals, or multidomain aop, and how dsals may ease composition issues. if an application uses multiple dsals, one for each domain, how can interactions be treated and what advantages do dsals bring to this setting? this year we accepted 6 papers for presentation and publication, 4 technical papers and 2 short papers. in addition to this, we have two invited talks: "language-oriented modularity through awesome dsals", by david lorenz, and "disl: an extensible language for efficient and comprehensive dynamic program analysis", by lukas marek, danilo ansaloni, and walter binder. both talks serve as a complement to the presentations of the respective authors in the research track of the aosd12 conference.
proceedings of the sixth annual workshop on domain-specific aspect languages (dsal 2011). null
high-precision measurement of total fission cross sections in spallation reactions of 208pb and 238u. total cross sections for proton- and deuteron-induced-fission of 208pb and 238u have been determined in the energy range between 500 mev and 1 gev. the experiment has been performed in inverse kinematics at gsi darmstadt, facilitating the counting of the projectiles and the identification of the reaction products. high precision between 5 and 7 percent has been achieved by individually counting the beam particles and by registering both fission fragments in coincidence with high efficiency and full z resolution. fission was clearly distinguished from other reaction channels. the results were found to deviate by up to 30 percent from prokofiev's systematics on total fission cross sections. there is good agreement with an elaborate experiment performed in direct kinematics.
a common aspect languages interpreter. the value of using different (possibly domain-specific) aspect languages to deal with a variety of crosscutting concerns in the development of complex software systems is well recognized. one should be able to use several of these languages together in a single program. however, on the one hand, developing a new domain-specific aspect language (dsal) in order to capture all common programming patterns of the domain takes a lot of time, and on the other hand, the designer of a new language should manage the interactions with the other languages when they are used together. &lt;br/&gt; in this thesis, we introduce support for rapid prototyping and composing aspect languages based on interpreters. we start from a base interpreter of a subset of java and we analyze and present a solution for its modular extension to support aop based on a common semantics aspect base defined once and for all. the extension, called the aspect interpreter, implements a common aspect mechanism and leaves holes to be defined when developing concrete languages. the power of this approach is that the aspect languages are directly implemented from their operational semantics. this is illustrated by implementing a lightweight version of aspectj. to apply the same approach and the same architecture to full java without changing its interpreter (jvm), we reuse aspectj to perform a first step of static weaving, which we complement by a second step of dynamic weaving, implemented through a thin interpretation layer. this can be seen as an interesting example of reconciling interpreters and compilers. we validate our approach by describing prototypes for aspectj, eaop, cool and a couple of other dsals and demonstrating the openness of our aspectj implementation with two extensions, one dealing with dynamic scheduling of aspects and another with alternative pointcut semantics. different aspect languages implemented with our framework can be easily composed. moreover, we provide support for customizing this composition.
measurement of inelastic, single- and double-diffraction cross sections in proton--proton collisions at the lhc with alice. measurements of cross sections of inelastic and diffractive processes in proton--proton collisions at lhc energies were carried out with the alice detector. the fractions of diffractive processes in inelastic collisions were determined from a study of gaps in charged particle pseudorapidity distributions: for single diffraction (diffractive mass $m_x &lt; 200$ gev/$c^2$) $\sigma_{\rm sd}/\sigma_{\rm inel} = 0.21 \pm 0.03, 0.20^{+0.07}_{-0.08}$, and $0.20^{+0.04}_{-0.07}$, respectively at centre-of-mass energies $\sqrt{s} = 0.9, 2.76$, and 7 tev; for double diffraction (for a pseudorapidity gap $\delta\eta &gt; 3$) $\sigma_{\rm dd}/\sigma_{\rm inel} = 0.11 \pm 0.03, 0.12 \pm 0.05$, and $0.12^{+0.05}_{-0.04}$, respectively at $\sqrt{s} = 0.9, 2.76$, and 7 tev. to measure the inelastic cross section, beam properties were determined with van der meer scans, and, using a simulation of diffraction adjusted to data, the following values were obtained: $\sigma_{\rm inel} = 62.8^{+2.4}_{-4.0} (model) \pm 1.2 (lumi)$ mb at $\sqrt{s} =$ 2.76 tev and $73.2^{+2.0}_{-4.6} (model) \pm 2.6 (lumi)$ mb at $\sqrt{s}$ = 7 tev. the single- and double-diffractive cross sections were calculated combining relative rates of diffraction with inelastic cross sections. the results are compared to previous measurements at proton--antiproton and proton--proton colliders at lower energies, to measurements by other experiments at the lhc, and to theoretical models.
simple temporal problems in route scheduling for the dial-a-ride problem with transfers. the dial-a-ride problem (darp) consists in defining a set of routes that satisfy transportation requests between a set of pickup points and a set of delivery points. this paper addresses a variant of the darp where requests can change of vehicle during their trip. this transshipment is made on specific locations called "transfer points". the corresponding problem is called the dial-a-ride problem with transfers (darpt). solving the darpt yields modeling and algorithmic di culties. in this paper, we focus on e ciently checking the feasibility of routes with regards to the problem temporal constraints in a large neighborhood search. this feasibility problem is a simple temporal problem, well studied in particular in artificial intelligence. we propose necessary and su cient conditions to fasten the detection of unfeasible or feasible routes.
an alns and route scheduling algorithms for the dial-a-ride problem with transfers. in the dial-a-ride problem with transfers (darpt), a passenger may be transfered from the vehicle that picked him up to another vehicle at some predetermined location, called transfer point. transfers may generate non negligible savings in terms of routing cost. we propose an adaptive large neighborhood search (alns) to solve the pickup and delivery problem with transfers (pdpt). we present heuristics introduced to integrate transfers in routes and how the alns has been adapted to the darpt. checking whether a solution of the darpt remains feasible after inserting a request in one route, or in two routes using a transfer point, becomes a non-trivial problem. indeed, routes should be synchronized at transfer points and therefore, cannot be scheduled independently anymore. for a set of routes, the feasibility problem given by the darpt precedence and time constraints can be stated as a simple temporal problem (stp). since the stp has to be solved after all request insertions, solving it e ciently is a crucial issue. we propose some necessary and su cient feasibility conditions that reduce the time needed to establish feasibility or unfeasibility.
testing inexecutable conditions on input pointers in c programs with sante. combinations of static and dynamic analysis techniques make it possible to detect the risk of out-of-bounds memory access in c programs and to confirm it on concrete test data. however, this is not directly possible for input arrays/pointers in c functions. this paper presents a specific technique allowing the interpretation and execution of assertions involving the size of an input array (pointer) of a c function. we show how this technique was successfully exploited in the sante tool where it allowed potential out-of-bounds access errors to be detected and classified in several real-life programs. keywords: run-time errors, c pointers, static analysis, test generation.
deviation from quark number scaling of the anisotropy parameter v2 of pions, kaons, and protons in au+au collisions at √snn=200 gev. measurements of the anisotropy parameter v2 of identified hadrons (pions, kaons, and protons) as a function of centrality, transverse momentum pt, and transverse kinetic energy ket at midrapidity (|η|&lt;0.35) in au + au collisions at √snn=200 gev are presented. pions and protons are identified up to pt= 6 gev/c, and kaons up to pt= 4 gev/c, by combining information from time-of-flight and aerogel čerenkov detectors in the phenix experiment. the scaling of v2 with the number of valence quarks (nq) has been studied in different centrality bins as a function of transverse momentum and transverse kinetic energy. a deviation from previously observed quark-number scaling is observed at large values of ket/nq in noncentral au + au collisions (20-60%), but this scaling remains valid in central collisions (0-10%).
j/ψ suppression at forward rapidity in pb-pb collisions at √snn=2.76 tev. null
j/$\psi$ production at high transverse momenta in p+p and au+au collisions at sqrt(s_{nn}) = 200 gev. we report $j/\psi$ spectra for transverse momenta $p_t$&gt; 5 gev/$c$ at mid-rapidity in p+p and au+au collisions at sqrt(s_{nn}) = 200 gev.the inclusive $j/\psi$ spectrum and the extracted $b$-hadron feed-down are compared to models incorporating different production mechanisms. we observe significant suppression of the $j/\psi$ yields for $p_t$&gt; 5 gev/$c$ in 0-30% au+au collisions relative to the p+p yield scaled by the number of binary nucleon-nucleon collisions in au+au collisions. in 30-60% collisions, no such suppression is observed.the level of suppression is consistently less than that of high-$p_t$ $\pi^{\pm}$ and low-$p_t$ $j/\psi$.
verification of atl transformations using transformation models and model finders. in model-driven engineering, models constitute pivotal elements of the software to be built. if models are specified well, transformations can be employed for different purposes, e.g., to produce final code. however, it is important that models produced by a transformation from valid input models are valid, too, where validity refers to the metamodel constraints, often written in ocl. transformation models are a way to describe this hoare-style notion of partial correctness of model transformations using only metamodels and constraints. in this paper, we provide an automatic translation of declarative, rule-based atl transformations into such transformation models, providing an intuitive and versatile encoding of atl into ocl that can be used for the analysis of various properties of transformations. we furthermore show how existing model verifiers (satisfiability checkers) for ocl-annotated metamodels can be applied for the verification of the translated atl transformations, providing evidence for the effectiveness of our approach in practice.
centrality dependence of charged particle production at large transverse momentum in pb--pb collisions at $\sqrt{s_{\rm{nn}}} = 2.76$ tev. the inclusive transverse momentum ($p_{\rm t}$) distributions of primary charged particles are measured in the pseudo-rapidity range $|\eta|&lt;0.8$ as a function of event centrality in pb--pb collisions at $\sqrt{s_{\rm{nn}}}=2.76$ tev with alice at the lhc. the data are presented in the $p_{\rm t}$ range $0.1530$ gev/$c$. in peripheral collisions (70--80%), the suppression is weaker with $r_{\rm{aa}} \approx 0.7$ almost independently of $p_{\rm t}$. the measured nuclear modification factors are compared to other measurements and model calculations.
dtpa complexation of bismuth in human blood serum. the in vivo212pb/212bi generator is promising for application in targeted alpha therapy (tat) of cancer. one main limitation of its therapeutic application is due to potential release of 212bi from the radioconjugate upon radioactive decay of the mother nuclide 212pb, potentially leading to irradiation of healthy tissue. the objective of the present work is to assess whether the chelate chx-a′′-dtpa (n-(2-aminoethyl)-trans-1,2-diaminocyclohexane-n,n′,n′′-pentaacetic acid) bound to a biological carrier molecule may be able to re-complex released 212bi under in vivo conditions to limit its translocation from the target site. chx-a′′-dtpa was bound to bovine gamma globulin (bgg) to mimic a model conjugate and the stability of the bi-chx-a′′-dtpa-bgg conjugate was studied in blood serum by ultrafiltration. trlfs experiments using cm(iii) as a fluorescent probe demonstrated that linking chx-a′′-dtpa to bgg does not affect the coordination properties of the ligand. furthermore, comparable stability constants were observed between bi(iii) and free chx-a′′-dtpa, bgg-bound chx-a′′-dtpa and dtpa. the complexation constants determined between bi(iii) and the chelate molecules are sufficiently high to allow ultra trace amounts of the ligand to efficiently compete with serum transferrin controlling bi(iii) speciation in blood plasma conditions. nevertheless, chx-a′′-dtpa is not able to complex bi(iii) generated in blood serum because of the strong competition between bi(iii) and fe(ii) for the ligand. in other words, chx-a′′-dtpa is not "selective" enough to limit bi(iii) release in the body when applying the 212pb/212bi in vivo generator.
the strong interaction at the collider and cosmic-rays frontiers. first data on inclusive particle production measured in proton-proton collisions at the large hadron collider (lhc) are compared to predictions of various hadron-interaction monte carlos (qgsjet, epos and sibyll) used commonly in high-energy cosmic-ray physics. while reasonable overall agreement is found for some of the models, none of them reproduces consistently the sqrt(s) evolution of all the measured observables. we discuss the implications of the new lhc data for the modeling of the non-perturbative and semihard parton dynamics in hadron-hadron and cosmic-rays interactions at the highest energies studied today.
d_s meson production at central rapidity in proton--proton collisions at sqrt(s) = 7 tev. the pt-differential inclusive production cross section of the prompt charm-strange meson d_s in the rapidity range |y|&lt;0.5 was measured in proton--proton collisions at sqrt(s)=7 tev at the lhc using the alice detector. the analysis was performed on a data sample of 2.98 10^8 events collected with a minimum-bias trigger. the corresponding integrated luminosity is l_int=4.8 nb^-1. reconstructing the decay d_s -.&gt; phi pi, with phi -&gt; kk, and its charge conjugate, about 480 d_s mesons were counted, after selection cuts, in the transverse momentum range 2.
pion, kaon, and proton production in central pb--pb collisions at $\sqrt{s_{nn}} = 2.76$ tev. in this letter we report the first results on $\pi^\pm$, k$^\pm$, p and pbar production at mid-rapidity (|y|&lt;0.5) in central pb-pb collisions at sqrt{s_{nn}} = 2.76 tev, measured by the alice experiment at the lhc. the pt distributions and yields are compared to previous results at sqrt{s_{nn}} = 200 gev and expectations from hydrodynamic and thermal models. the spectral shapes indicate a strong increase of the radial flow velocity with sqrt{s_{nn}}, which in hydrodynamic models is expected as a consequence of the increasing particle density. while the k/pi ratio is in line with predictions from the thermal model, the p/pi ratio is found to be lower by a factor of about 1.5. this deviation from thermal model expectations is still to be understood.
measurement of electrons from beauty hadron decays in pp collisions at sqrt{s} = 7 tev. the production cross section of electrons from semileptonic decays of beauty hadrons was measured at mid-rapidity (|y| &lt; 0.8) in the transverse momentum range 1 &lt; pt &lt; 8 gev/c with the alice experiment at the cern lhc in pp collisions at a center of mass energy sqrt{s} = 7 tev using an integrated luminosity of 2.2 nb^{-1}. electrons from beauty hadron decays were selected based on the displacement of the decay vertex from the collision vertex. a perturbative qcd calculation agrees with the measurement within uncertainties. the data were extrapolated to the full phase space to determine the total cross section for the production of beauty quark-antiquark pairs.
dilepton production in proton-proton and pb+pb collisions at sqrt(s_nn)=2.76 tev. we study e^+e^- pair production in proton-proton and central pb+pb collisions at sqrt(s_nn)=2.76 tev within two models: an extended statistical hadronization model (shm) and the parton-hadron-string dynamics (phsd) transport approach. we find that the phsd calculations roughly agree with the dilepton spectrum from hadronic sources with the 'cocktail' estimates from the statistical hadronization model matched to available data at lhc energies. the dynamical simulations within the phsd show a moderate increase of the low mass dilepton yield essentially due to the in-medium modification of the rho-meson. furthermore, pronounced traces of the partonic degrees of freedom are found in the phsd results in the intermediate mass regime. the dilepton production from the strongly interacting quark-gluon plasma (sqgp) exceeds that from the semi-leptonic decays of open charm and bottom mesons. additionally, we observe that a transverse momentum cut of 1 gev/c further suppresses the relative contribution of the heavy meson decays to the dilepton yield, such that the sqgp radiation strongly dominates the spectrum for masses from 1 to 3 gev, allowing a closer look at the electromagnetic emissivity of the partonic plasma in the early phase of pb+pb collisions.
the rapid atmospheric monitoring system of the pierre auger observatory. the pierre auger observatory is a facility built to detect air showers produced by cosmic rays above 10^17 ev. during clear nights with a low illuminated moon fraction, the uv fluorescence light produced by air showers is recorded by optical telescopes at the observatory. to correct the observations for variations in atmospheric conditions, atmospheric monitoring is performed at regular intervals ranging from several minutes (for cloud identification) to several hours (for aerosol conditions) to several days (for vertical profiles of temperature, pressure, and humidity). in 2009, the monitoring program was upgraded to allow for additional targeted measurements of atmospheric conditions shortly after the detection of air showers of special interest, e.g., showers produced by very high-energy cosmic rays or showers with atypical longitudinal profiles. the former events are of particular importance for the determination of the energy scale of the observatory, and the latter are characteristic of unusual air shower physics or exotic primary particle types. the purpose of targeted (or "rapid") monitoring is to improve the resolution of the atmospheric measurements for such events. in this paper, we report on the implementation of the rapid monitoring program and its current status. the rapid monitoring data have been analyzed and applied to the reconstruction of air showers of high interest, and indicate that the air fluorescence measurements affected by clouds and aerosols are effectively corrected using measurements from the regular atmospheric monitoring program. we find that the rapid monitoring program has potential for supporting dedicated physics analyses beyond the standard event reconstruction.
measurement of the proton-air cross-section at $\sqrt{s}=57$ tev with the pierre auger observatory. we report a measurement of the proton-air cross-section for particle production at the center-of-mass energy per nucleon of 57 tev. this is derived from the distribution of the depths of shower maxima observed with the pierre auger observatory: systematic uncertainties are studied in detail. analysing the tail of the distribution of the shower maxima, a proton-air cross-section of $[505\pm22(stat)^{+28}_{-36}(sys)]$ mb is found.
reactor electron antineutrino disappearance in the double chooz experiment. the double chooz experiment has observed 8,249 candidate electron antineutrino events in 227.93 live days with 33.71 gw-ton-years (reactor power x detector mass x livetime) exposure using a 10.3 cubic meter fiducial volume detector located at 1050 m from the reactor cores of the chooz nuclear power plant in france. the expectation in case of theta13 = 0 is 8,937 events. the deficit is interpreted as evidence of electron antineutrino disappearance. from a rate plus spectral shape analysis we find sin^2 2{\theta}13 = 0.109 \pm 0.030(stat) \pm 0.025(syst). the data exclude the no-oscillation hypothesis at 99.9% cl (3.1{\sigma}).
interactions between nuclear fuel and water at the fukushima daiichi reactors. used nuclear fuel is a redox-sensitive semiconductor consisting of uranium dioxide containing a few percent of fission products and up to about one percent transuranium elements, mainly plutonium. the rapid increase in temperature in the cores of the fukushima reactors was caused by the loss of coolant in the aftermath of the damage from the tsunami. temperatures probably well above 2000 °c caused melting of not only the uo2 in the fuel but also the zircaloy cladding and steel, forming a quenched melt, termed corium. substantial amounts of volatile fission products, such as cs and i, were released during melting, but the less volatile fission products and the actinides (probably &gt;99.9%) were incorporated into the corium as the melt cooled and was quenched. the corium still contains these radionuclides, which leads to a very large long-term radiotoxicity of the molten reactor core. the challenge for environmental scientists is to assess the long-term interactions between water and the mixture of corium and potentially still-existing unmelted fuel, particularly if the molten reactor core is left in place and covered with a sarcophagus for hundreds of years. part of the answer to this question can be found in the knowledge that has been gained from research into the disposal of spent nuclear fuel in a geologic repository.
dark matter results from 225 live days of xenon100 data. we report on a search for particle dark matter with the xenon100 experiment, operated at the laboratori nazionali del gran sasso (lngs) for 13 months during 2011 and 2012. xenon100 features an ultra-low electromagnetic background of (5.3\pm0.6)\times10^-3 events (kg day kevee)^-1 in the energy region of interest. a blind analysis of 224.6 live days \times 34 kg exposure has yielded no evidence for dark matter interactions. the two candidate events observed in the pre-defined nuclear recoil energy range of 6.6-30.5 kevnr are consistent with the background expectation of (1.0 \pm 0.2) events. a profile likelihood analysis using a 6.6-43.3 kevnr energy range sets the most stringent limit on the spin-independent elastic wimp-nucleon scattering cross section for wimp masses above 8 gev/c^2, with a minimum of 2 \times 10^-45 cm^2 at 55 gev/c^2 and 90% confidence level.
net-charge fluctuations in pb-pb collisions at \sqrt(s)_nn = 2.76 tev. we report the first measurement of the net-charge fluctuations in pb-pb collisions at \surd s_nn 2.76 tev, measured with the alice detector at the cern large hadron collider. the dynamical fluctuations per unit entropy are observed to decrease when going from peripheral to central collisions. an additional reduction in the amount of fluctuations is seen in comparison to the results from lower energies. we examine the dependence of fluctuations on the pseudo-rapidity interval, which may account for the dilution of fluctuations during the evolution of the system. we find that the alice data points are between the theoretically predicted values for a hadron gas and a quark-gluon plasma.
from implicit to explicit pavings. a combinatorial search can either be performed by using an implicit search tree, where an initial state is recursively transformed until some goal state is reached, or by using an explicit search tree, where an initial tree structure containing the root state is iteratively expanded until the leaves match the set of goal states. this paper proposes an exploratory study aimed at showing that explicit search trees can play a distinguished role in the field of numerical constraints. the first advantage of an explicit search is expressiveness: we can write new algorithms or reformulate existing ones in a simple and unified way. the second advantage is efficiency, since an implicit search may also lead to a blowup of redundant computations. this is illustrated through various examples.
search for point-like sources of ultra-high energy neutrinos at the pierre auger observatory and improved limit on the diffuse flux of tau neutrinos. the surface detector array of the pierre auger observatory can detect neutrinos with energy e ν between 1017 ev and 1020 ev from point-like sources across the sky south of +55° and north of -65° declinations. a search has been performed for highly inclined extensive air showers produced by the interaction of neutrinos of all flavors in the atmosphere (downward-going neutrinos), and by the decay of tau leptons originating from tau neutrino interactions in earth's crust (earth-skimming neutrinos). no candidate neutrinos have been found in data up to 2010 may 31. this corresponds to an equivalent exposure of ~3.5 years of a full surface detector array for the earth-skimming channel and ~2 years for the downward-going channel. an improved upper limit on the diffuse flux of tau neutrinos has been derived. upper limits on the neutrino flux from point-like sources have been derived as a function of the source declination. assuming a differential neutrino flux k ps * e -2 ν from a point-like source, 90% confidence level upper limits for k ps at the level of ≈5 × 10-7 and 2.5 × 10-6 gev cm-2 s-1 have been obtained over a broad range of declinations from the searches for earth-skimming and downward-going neutrinos, respectively.
recent results on heavy quark quenching in ultrarelativistic heavy ion collisions. in this contribution, we present some predictions for the production of d and b mesons in ultrarelativistic heavy ion collisions at rhic and lhc energies and confront them with experimental results obtained so far by the star, phenix, alice and cms collaborations. we next discuss some preliminary results obtained with an improved description of the medium based on epos initial conditions, and its possible implications on the nuclear modification factor and on the elliptic flow of heavy quarks.
tonneau: a multidetector array for charged-particle and light-fragment 4π detection. null
durability of an as2s3 chalcogenide glass: optical properties and dissolution kinetics. the durability of a as2s3 chalcogenide glass composition was studied in de-ionized water at different temperatures (60-90 °c) for different periods of time, up to 120 days. the evolutions of the chemical composition and the ph of the solutions as well as the optical transmission of bulk samples, in the 2-10 μm region, were measured as a function of corrosion time. atomic force microscopy and optical microscopy were used to investigate the roughness of corroded surfaces and the evolution of surface defects. the water corrosion of as2s3 glass was found to follow a congruent dissolution mechanism, a possible glass-water reaction mechanism was proposed. the optical transmission of the glass was found to be affected by the corrosion. the optical loss increased from 4 to 21% with corrosion time, this variation was attributed to the texturation of the surface by the reaction of corrosion. moreover, the experimental results show that high temperature value enhances the corrosion reaction: an activation energy of 103 ± 2 kj/mol was computed from experimental measurements.
exclusive study of nucleus-nucleus reactions at intermediate energies : impact parameter dependence of preequilibrium emission, collective flow and hot nuclei formation. null
energy and system-size dependence of two- and four-particle $v_2$ measurements in heavy-ion collisions at rhic and their implications on flow fluctuations and nonflow. we present star measurements of azimuthal anisotropy by means of the two- and four-particle cumulants $v_2$ ($v_2\{2\}$ and $v_2\{4\}$) for au+au and cu+cu collisions at center of mass energies $\sqrt{s_{_{\mathrm{nn}}}} = 62.4$ and 200 gev. the difference between $v_2\{2\}^2$ and $v_2\{4\}^2$ is related to $v_{2}$ fluctuations ($\sigma_{v_2}$) and nonflow $(\delta_{2})$. we present an upper limit to $\sigma_{v_2}/v_{2}$. following the assumption that eccentricity fluctuations $\sigma_{\epsilon}$ dominate $v_2$ fluctuations $\frac{\sigma_{v_2}}{v_2} \approx \frac{\sigma_{\epsilon}}{\epsilon}$ we deduce the nonflow implied for several models of eccentricity fluctuations that would be required for consistency with $v_2\{2\}$ and $v_2\{4\}$. we also present results on the ratio of $v_2$ to eccentricity.
an event-driven optimization framework for dynamic vehicle routing. the real-time operation of a fleet of vehicles introduces challenging optimization problems researches in a wide range of applications, thus, it is appealing to both academia and practitioners in industry. in this work we focus on dynamic vehicle routing problems and present an event-driven framework that can anticipate unknown changes in the problem information. the proposed framework is intrinsically parallelized to take advantage of modern multi-core and multi-threaded computing architectures. it is also designed to be easily embeddable in decision support systems that cope with a wide range of contexts and side constraints. we illustrate the flexibility of the framework by showing how it can be adapted to tackle the dynamic vehicle routing problem with stochastic demands. computational results show that while our approach is competitive against state-of-the art algorithms, it still ensures greater reactivity and requires less assumptions (e.g., demand distributions).
lightweight string reasoning for ocl. models play a key role in assuring software quality in the modeldriven approach. precise models usually require the definition of ocl expressions to specify model constraints that cannot be expressed graphically. techniques that check the satisfiability of such models and find corresponding instances of them are important in various activities, such as model-based testing and validation. several tools to check model satisfiability have been developed but to our knowledge, none of them yet supports the analysis of ocl expressions including operations on strings in general terms. as, in contrast, many industrial models do contain such operations, there is evidently a gap. there has been much research on formal reasoning on strings in general, but so far the results could not be included into model finding approaches. for model finding, string reasoning only contributes a sub-problem, therefore, a string reasoning approach for model finding should not add up front too much computational complexity to the global model finding problem. we present such a lightweight approach based on constraint satisfaction problems and constraint rewriting. our approach efficiently solves several common kinds of string constraints and it is integrated into the emftocsp model finder.
on verifying atl transformations using 'off-the-shelf' smt solvers. mde is a software development process where models constitute pivotal elements of the software to be built. if models are well-specified, transformations can be employed for various purposes, e.g., to produce final code. however, transformations are only meaningful when they are 'correct': they must produce valid models from valid input models. a valid model has conformance to its meta-model and fulfils its constraints, usually written in ocl. in this paper, we propose a novel methodology to perform automatic, unbounded verification of atl transformations. its main component is a novel first-order semantics for atl transformations, based on the interpretation of the corresponding rules and their execution semantics as first-order predicates. although, our semantics is not complete, it does cover a significant subset of the atl language. using this semantics, transformation correctness can be automatically verified with respect to non-trivial ocl pre- and postconditions by using smt solvers, e.g. z3 and yices.
charge separation relative to the reaction plane in pb-pb collisions at $\sqrt{s_{nn}}= 2.76$ tev. measurements of charge dependent azimuthal correlations with the alice detector at the lhc are reported for pb-pb collisions at $\sqrt{s_{nn}} = 2.76$ tev. two- and three-particle charge-dependent azimuthal correlations in the pseudo-rapidity range $|\eta | &lt; 0.8$ are presented as a function of the collision centrality, particle separation in pseudo-rapidity, and transverse momentum. a clear signal compatible with the expectation of a charge-dependent separation relative to the reaction plane is observed, which shows little or no collision energy dependence when compared to measurements at rhic energies. models incorporating effects of local parity violation in strong interactions fail to describe the observed collision energy dependence.
well-typed services cannot go wrong. service-oriented applications are frequently used in highly dynamic contexts: ser- vice compositions may change dynamically, in particular, because new services are discovered at runtime. moreover, subtyping has recently been identified as a strong requirement for service dis- covery. correctness guarantees over service compositions, provided in particular by type systems, are highly desirable in this context. however, while service oriented applications can be built using various technologies and protocols, none of them provides decent support ensuring that well-typed services cannot go wrong. an emitted message, for instance, may be dangling and remain as a ghost message in the network if there is no agent to receive it. in this article, we introduce a formal model for service compositions and define a type system with subtyping that ensures type soundness by combining static and dynamic checks. we also demonstrate how to preserve type soundness in presence of malicious agents and insecure communication channels.
enhanced stiffness modeling of serial and parallel manipulators for robotic-based processing of high performance materials. the thesis focuses on the enhancement of stiffness modeling of serial and parallel manipulators with passive joints under an essential loading. the developed technique takes into account different types of loadings: external force/torque applied to the end-point, internal preloading in the joints and auxiliary forces/torques applied to intermediate points. in contrast to previous works, the proposed technique includes computing an equilibrium configuration, which exactly corresponds to the loading. this allows to obtain the full-scale force-deflection relation for any given workspace point and to linearise it taking into account variation of the manipulator jacobian because of the external force/torque. the proposed approach also enables designer to evaluate critical forces that may provoke non-linear behaviours of the manipulators, such as sudden failure due to elastic instability (buckling), which has not been previously studied in robotics. in the frame of this work, it is assumed that the manipulator elasticity is described by a multidimensional lumped-parameter model, which corresponds to a set of rigid bodies connected by 6-dof virtual springs. each of these springs characterize flexibility of the corresponding link or actuated joint and takes into account both their compliance and joint particularities. to increase the model accuracy, the stiffness parameters of the spring are evaluated using fea-based virtual experiments and dedicated identification technique developed in the thesis. this gives almost the same accuracy as fea but essentially reduces the computational effort, eliminating repetitive re-meshing through the workspace.
inclusive charged hadron elliptic flow in au + au collisions at $\sqrt{s_{nn}}$ = 7.7 - 39 gev. a systematic study is presented for centrality, transverse momentum ($p_t$) and pseudorapidity ($\eta$) dependence of the inclusive charged hadron elliptic flow ($v_2$) at midrapidity ($|\eta| &lt; 1.0$) in au+au collisions at $\sqrt{s_{nn}}$ = 7.7, 11.5, 19.6, 27 and 39 gev. the results obtained with different methods, including correlations with the event plane reconstructed in a region separated by a large pseudorapidity gap, and 4-particle cumulants ($v_2\{4\}$), are presented in order to investigate non-flow correlations and $v_2$ fluctuations. we observe that the difference between $v_2\{2\}$ and $v_2\{4\}$ is smaller at the lower collision energies. values of $v_2$, scaled by the initial coordinate space eccentricity, $v_{2}/\varepsilon$, as a function of $p_t$ are larger in more central collisions, suggesting stronger collective flow develops in more central collisions, similar to the results at higher collision energies. these results are compared to measurements at higher energies at rhic ($\sqrt{s_{nn}}$ = 62.4 and 200 gev) and at lhc (pb + pb collisions at $\sqrt{s_{nn}}$ = 2.76 tev). the $v_2(p_t)$ values for fixed $p_t$ rise with increasing collision energy within the $p_t$ range studied ($&lt; 2 {\rm gev}/c$). we compare the $v_2$ results to urqmd and ampt transport model calculations, and physics implications on the dominance of partonic versus hadronic phases in the system created at bes energies are discussed.
atltest: a white-box test generation approach for atl transformations. mde is being applied to the development of increasingly complex systems that require larger model transformations. given that the specification of such transformations is an error-prone task, techniques to guarantee their quality must be provided. testing is a well-known technique for finding errors in programs. in this sense, adoption of testing techniques in the model transformation domain would be helpful to improve their quality. so far, testing of model transformations has focused on black-box testing techniques. instead, in this paper we provide a white-box test model generation approach for atl model transformations.
report on the detection and acquisition systems at nfs. null
transforming very large models in the cloud: a research roadmap. model transformations are widely used by model-driven engineering (mde) platforms to apply different kinds of operations over models, such as model translation, evolution or composition. however, existing solutions are not designed to handle very large models (vlms), thus facing scalability issues. coupling mde with cloud-based platforms may help solving these issues. since cloud-based platforms are relatively new, researchers still need to investigate if/how/when mde solutions can benefit from them. in this paper, we investigate the problem of transforming vlms in the cloud by addressing the two phases of 1) model storage and 2) model transformation execution in the cloud. for both aspects we identify a set of research questions, possible solutions and probable challenges researchers may face.
monte carlo simulation of ion-exchange response of copper sulfide ion selective electrode for metal pollutant detection. the present work aims at studying the potentiometric response of an ion-selective electrode. the target material is a copper sulfide (cus) thin film designed for the detection of cu+2 ions in solutions. the electrode is prepared by mean of an electrochemical deposition of copper sulfide on a silicon substrate. the cu+2 response is then studied and a near nernstian behavior of this electrode is observed in the range of pcu 6-1. in order to quantitatively explain the exchange process behind the cu+2 response, a stochastic computational method is established to explain the potentiometric response of the copper sulfide sensors that can be used for water pollutant detection. the numerical scheme is based on monte carlo simulation of ion exchange between the solution and the cus membrane surface. the probability of this cu+2 -ion exchange is implemented as the main factor, which governs the variation of the electrode potential vs. the cu+2 concentration in the solution. three characteristics of the detection response are studied, namely the detection threshold, the slope and the saturation concentration. the model validation is achieved by predicting the nernstian behavior for the studied cus sensor and by comparing the obtained results with data from the literature dealing with cu+2 detection.
self-management of cloud applications and infrastructure for energy optimization. null
hsv-1 cgal+ infection promotes quaking rna binding protein production and induces nuclear-cytoplasmic shuttling of quaking i-5 isoform in human hepatoma cells. herpesvirus type 1 (hsv-1) based oncolytic vectors arise as a promising therapeutic alternative for neoplastic diseases including hepatocellular carcinoma. however, the mechanisms mediating the host cell response to such treatments are not completely known. it is well established that hsv-1 infection induces functional and structural alterations in the nucleus of the host cell. in the present work, we have used gel-based and shotgun proteomic strategies to elucidate the signaling pathways impaired in the nucleus of human hepatoma cells (huh7) upon hsv-1 cgal(+) infection. both approaches allowed the identification of differential proteins suggesting impairment of cell functions involved in many aspects of host-virus interaction such as transcription regulation, mrna processing, and mrna splicing. based on our proteomic data and additional functional studies, cellular protein quaking content (qki) increases 4 hours postinfection (hpi), when viral immediate-early genes such as icp4 and icp27 could be also detected. depletion of qki expression by small interfering rna results in reduction of viral immediate-early protein levels, subsequent decrease in early and late viral protein content, and a reduction in the viral yield indicating that qki directly interferes with viral replication. in particular, hsv-1 cgal(+) induces a transient increase in quaking i-5 isoform (qki-5) levels, in parallel with an enhancement of p27(kip1) protein content. moreover, immunofluorescence microscopy showed an early nuclear redistribution of qki-5, shuttling from the nucleus to the cytosol and colocalizing with nectin-1 in cell to cell contact regions at 16-24 hpi. this evidence sheds new light on mechanisms mediating hepatoma cell response to hsv-1 vectors highlighting qki as a central molecular mediator.
photocatalytic oxidation of volatile organic compounds and monitor of their reaction intermediates : investigation of static and dynamic reactors at typical concentrations of indoor air. heterogeneous photocatalysis is a technique of oxidation used for the removal of volatile organic compounds (vocs). aim is to study the degradation of initial vocs and the production of reaction intermediates during this process in conditions close to the indoor air (voc concentration in mixture). three model vocs (toluene, decane, trichloroethylene) are studied separately and then in mixture in a static reactor and in a dynamic multi-pass reactor. the obtained results show that (i) the degradation efficiency depends on the nature and the number of vocs, on the photocatalyst characteristics and on process conditions, (ii) the major and the most persistent intermediates are light aldehydes, (iii) the elimination of aldehydes is inhibited when the initial vocs are in mixture, (iv) increasing the residence time on the photocatalyst provides a higher removal rate of initial vocs and of byproducts.
local search for a personnel scheduling problem with fixed jobs and an equity objective. null
a tour scheduling problem with fixed jobs: use of constraint programming. null
improving the asymmetric tsp by considering graph structure. recent works on cost based relaxations have improved constraint programming (cp) models for the traveling salesman problem (tsp). we provide a short survey over solving asymmetric tsp with cp. then, we suggest new implied propagators based on general graph properties. we experimentally show that such implied propagators bring robustness to pathological instances and highlight the fact that graph structure can significantly improve search heuristics behavior. finally, we show that our approach outperforms current state of the art results.
k0s-k0s correlations in pp collisions at sqrt{s}=7 tev from the lhc alice experiment. identical neutral kaon pair correlations are measured in sqrt{s}=7 tev pp collisions in the alice experiment. one-dimensional k0s-k0s correlation functions in terms of the invariant momentum difference of kaon pairs are formed in two multiplicity and two transverse momentum ranges. the femtoscopic parameters for the radius and correlation strength of the kaon source are extracted. the fit includes quantum statistics and final-state interactions of the a0/f0 resonance. k0s-k0s correlations show an increase in radius for increasing multiplicity and a slight decrease in radius for increasing transverse mass, mt, as seen in pion-pion correlations in the pp system and in heavy-ion collisions. transverse mass scaling is observed between the k0s-k0s and pion-pion radii. also, the first observation is made of the decay of the f2'(1525) meson into the k0s-k0s channel in pp collisions.
single spin asymmetry $a_n$ in polarized proton-proton elastic scattering at $\sqrt{s}=200$ gev. we report a high precision measurement of the transverse single spin asymmetry $a_n$ at $\sqrt{s}=200$ gev in elastic proton-proton scattering by the star experiment at rhic. the $a_n$ was measured in the four-momentum transfer $t$ range $0.003 \leqslant |t| \leqslant 0.035$ $\gevcsq$, the region of a significant interference between the electromagnetic and hadronic scattering amplitudes. the measured values of $a_n$ and its $t$-dependence are consistent with the absence of a hadronic spin-flip amplitude, thus providing strong constraints on the ratio of the single spin-flip to the non-flip amplitudes. since the hadronic amplitude is dominated by the pomeron amplitude at this $\sqrt{s}$, we conclude that this measurement addresses the question about the presence of a hadronic spin flip due to the pomeron exchange in polarized proton-proton elastic scattering.
interval-based robustness of linear parametrized filters. this article deals with the resilient implementation of parametrized linear filters (or controllers), i.e. realizations that are robust with respect to their implementation with fixed-point arithmetic. the implementation of a linear filter/controller in an embedded device is a difficult task because the numerical version of such algorithms suff ers from a deterioration in performances and characteristics. this degradation has two separate origins, corresponding to the quantization of the embedded coefficients and the round-off occurring during the computations. the optimal realization problem is to find, for a given filter, the most resilient realization. we here consider linear filters that depends on a set of parameters that are not exactly known during the design. they are used for example in automotive control, where a very late re-tuning is required. the paper presents results on fwl resiliency analyzis using interval optimization methods [2], and compare them to those obtained with the sensitivity approach.
structural changes upon lithium insertion in ni0.5 tiopo4. null
measurement of charm production at central rapidity in proton-proton collisions at sqrt(s) = 2.76 tev. the pt-differential production cross sections of the prompt (b feed-down subtracted) charmed mesons d0, d+, and d*+ in the rapidity range |y|&lt;0.5, and for transverse momentum 1&lt; pt &lt;12 gev/c, were measured in proton-proton collisions at sqrt(s) = 2.76 tev with the alice detector at the large hadron collider. the analysis exploited the hadronic decays d0 -&gt; k pi, d+ -&gt; k pi pi, d*+ -&gt; d0 pi, and their charge conjugates, and was performed on a l_{int} = 1.35 nb^{-1} event sample collected in 2011 with a minimum-bias trigger. the total charm production cross section at sqrt(s) = 2.76 tev and at 7 tev was evaluated by extrapolating to the full phase space the pt-differential production cross sections at sqrt(s) = 2.76 tev and our previous measurements at sqrt(s) = 7 tev. the results were compared to existing measurements and to perturbative-qcd calculations. the fraction of cubar d mesons produced in a vector state was also determined.
modelling and monitoring of correlated faults in redundant computer control systems. null
forecasting risk analysis for supply chains with intermittent demand. the paper focuses on the forecasting risk analysis in the supply chains with the intermittent demand, which is typical for the inventory management of the "slow moving items" such as service parts or high-priced capital goods. the adopted demand model is based on the generalised beta-binomial distribution, which is capable to incorporate the additive distortions in the demand historical records as parameters. for this settings, there are proposed explicit expressions for the forecasting risk and the prediction function, which minimises the error impact on the risk. the efficiency of the proposed approach is confirmed by computer simulation and is illustrated by an application example for forecasting of the intermittent demand values for car spare parts.
kinematic aspects of a robot-positioner system in an arc welding application. this paper focuses on the kinematic control of a redundant robotic system taking into account particularities of the arc welding technology. the considered system consists of a 6-axis industrial robot (welding tool manipulator) and a 2-axis welding positioner (workpiece manipulator) that is intended to optimise a weld joint orientation during the technological process. the particular contribution of the paper lies in the area of the positioner inverse kinematics, which is a key issue of such system off-line programming and control. it has been proposed a novel formulation and a closed-form solution of the inverse kinematic problem that deals with the explicit definition of the weld joint orientation relative to the gravity. similar results have also been obtained for the known problem statement that is based on a unit vector transformation. for both the cases, a detailed investigation of the singularities and uniqueness-existence topics have been carried out. the presented results are implemented in a commercial software package and verified for real-life applications in the automotive industry.
multiobjective optimization of robot motion for laser cutting applications. this paper focuses on the enhancement of automatic robot programming techniques for laser cutting applications. such technology has already gained essential industrial acceptance, but its application for small lot production is limited by the tedious and time-consuming process of robot programming. currently, even sophisticated graphical simulation systems do not allow optimization of robot motion using multiple criteria, nor does it take into account redundancy caused by the tool axial symmetry. the particular contribution of this paper lies in the area of multiobjective optimization of robot motions via graph representation of the search space and dynamic programming procedures. it presents algorithms that allow generation of smooth manipulator trajectories within acceptable time, simultaneously considering kinematics, collision and singularities constraints of the robotic system, as well as the limitations of the robot control units. the efficiency of the algorithms has been carefully investigated via computer simulation. the presented results are implemented in a commercial software package and verified for real-life applications in the automotive industry.
underwater robot navigation around a sphere using electrolocation sense and kalman filter. the aim of this paper is to perform the navigation of an underwater robot equipped with a sensor using the electric sense. the robot navigates in an unbounded environment in presence of spheres. this sensor is inspired of some species of electric fish. a model of this sensor composed of n spherical electrodes is established. the variations of the current due to the presence of the sphere is related to the model of rasnow [3]. unscented kalman filter is used to localize the robot with respect to the sphere and to estimate the size of the sphere. we show that bio-inspired motions improve the detection of the spheres. we illustrate the efficiency of the method in two cases: a two electrodes sensor and a four electrodes sensor.
towards an electric-sense-based bioinspired embodied robotic perception system: the modelling aspect. in the context of the new paradigm of embodied intelligence invoked by both the roboticians and the cognitive scientists, a sensor bio-inspired from the electric fish was built. a certain geometry was pointed out for an accurate analytical prediction of the electrical measurements on the body in the presence of exterior objects. such perception model can establish potentially a direct relation between the location of the object and the body positions measurements as well as the shape of the object and the body direction. in addition to a potential novel tomography technique and a new potential electrolocation system it o ers new insights in understanding the role of the body in perceiving the world.
sensor model for the navigation of underwater vehicles by the electric sense. we present an analytical model of a sensor for the navigation of underwater vehicles by the electric sense. this model is inspired from the electroreception structure of the electric fish. in our model, that we call the poly-spherical model (psm), the sensor is composed of n spherical electrodes. some electrodes play the role of current-emitters whereas others play the role of current-receivers. by imposing values of the electrical potential on each electrode we create an electric field in the vicinity of the sensor. the region where the electric field is created is considered as the bubble of perception of the sensor. each object that enters this bubble is electrically polarized and creates in return a perturbation. this perturbation induces a variation of the measured current by the sensor. the model is tested on objects for which the expression of the polarizability is known. a unique off-line calibration of the poly-spherical model permits to predict the measured current of a real immersed sensor in an aquarium. comparisons in a basic scene between the predicted current given by the poly-spherical model and the measured current given by our test bed show a very good agreement, which confirms the interest of using such fast analytical models for the purpose of navigation.
direct-photon production in p+p collisions at sqrt(s)=200 gev at midrapidity. the differential cross section for the production of direct photons in p+p collisions at sqrt(s)=200 gev at midrapidity was measured in the phenix detector at the relativistic heavy ion collider. inclusive-direct photons were measured in the transverse-momentum range from 5.5--25 gev/c, extending the range beyond previous measurements. event structure was studied with an isolation criterion. next-to-leading-order perturbative-quantum-chromodynamics calculations give a good description of the spectrum. when the cross section is expressed versus x_t, the phenix data are seen to be in agreement with measurements from other experiments at different center-of-mass energies.
anisotropic flow of charged hadrons, pions and (anti-)protons measured at high transverse momentum in pb-pb collisions at $\snn=2.76$ tev. the elliptic, $v_2$, triangular, $v_3$, and quadrangular, $v_4$, flow coefficients are measured for unidentified charged particles, pions and (anti-)protons in pb-pb collisions at $\snn = 2.76$ tev with the alice detector at the large hadron collider. results obtained with the event plane and four-particle correlation methods are reported for the pseudo-rapidity range $|\eta|&lt;0.8$ at different collision centralities and as a function of transverse momentum, $\pt$, out to $\pt=20$ gev/$c$. the observed non-zero elliptic and triangular flow depends only weakly on transverse momentum for $\pt&gt;8$ gev/$c$. the small $\pt$ dependence of the difference between elliptic flow results obtained from two- and four-particle cumulant methods suggests a common origin of flow fluctuations up to $\pt=8$ gev/$c$. the magnitude of the (anti-)proton elliptic and triangular flow is larger than that of pions out to at least $\pt=8$ gev/$c$ indicating that the particle type dependence persists out to high $\pt$.
study of the production yield of j/psi and single muons in collisions protn-proton with the muon spectrometer of the alice experiment at lhc. the quark-gluon plasma is a state of nuclear matter appearing at very high temperature. in the laboratory, it is possible to reach such conditions using heavy-ion collisions at ultra-relativistic energies. the alice experiment at lhc is dedicated to the study of the quark-gluon plasma with pb-pb collisions at 2.76 tev. the first part of this study, presented as an annex, will consist in the very first results of the alice muon spectrometer, obtained using cosmic rays. the second part will present the evolution of the reconstruction efficiency of the muon spectrometer during its first two years of running. this study will show that the total reconstruction efficiency of the tracking chamber is more than 90% in proton-proton collisions, and 85% in lead-lead collisions. a track selection method based on the value of the product momentum - distance of closest approach will also be presented. this selection will allow to remove the tracks coming from muons produced in collisions between the beam and the residual gas in the beam line, and fake tracks in the most central pb-pb collisions. finally, this thesis will present a first analysis of the production yield of j/psi and single muons as a function of the collision's charged-particle multiplicity in proton-proton collisions.
transverse single-spin asymmetry and cross-section for pi0 and eta mesons at large feynman-x in polarized p+p collisions at sqrt(s)=200 gev. measurements of the differential cross-section and the transverse single-spin asymmetry, a_n, vs. x_f for pi0 and eta mesons are reported for 0.4 &lt; x_f &lt; 0.75 at an average pseudorapidity of 3.68. a data sample of approximately 6.3 pb^{-1} was analyzed, which was recorded during p+p collisions at sqrt{s} = 200 gev by the star experiment at rhic. the average transverse beam polarization was 56%. the cross-section for pi0 is consistent with a perturbative qcd prediction, and the eta/pi0 cross-section ratio agrees with previous mid-rapidity measurements. for 0.55 &lt; x_f &lt; 0.75, a_n for eta (0.210 +- 0.056) is 2.2 standard deviations larger than a_n for pi0 (0.081 +- 0.016).
noninteger flux - why it does not work. we consider the dirac operator on a 2-sphere without one point in the case of non-integer magnetic flux. we show that the spectral problem for the hamiltonian (the square of dirac operator) can always be well defined, if including in the hilbert space only nonsingular on 2-sphere wave functions. however, this hilbert space is not invariant under the action of the dirac operator; the action of the latter on some nonsingular states produces singular functions. this breaks explicitly the supersymmetry of the spectrum. in the integer flux case, the supersymmetry can be restored if extending the hilbert space to include locally regular sections of the corresponding fiber bundle. for non-integer fluxes, such an extention is not possible.
once more on the witten index of 3d supersymmetric ym-cs theory. the problem of counting the vacuum states in the supersymmetric 3d yang-mills-chern-simons theory is reconsidered. we resolve the controversy between its original calculation by witten at large volumes and the calculation based on the evaluation of the effective lagrangian in the small volume limit. we show that the latter calculation suffers from uncertainties associated with the singularities in the moduli space of classical vacua where the born-oppenheimer approximation breaks down. we also show that these singularities can be accurately treated in the hamiltonian born-oppenheimer method, where one has to match carefully the effective wave functions on the abelian valley and the wave functions of reduced non-abelian qm theory near the singularities. this gives the same result as original witten's calculation.
real and complex supersymmetric d = 1 sigma models with torsions. we derive and discuss, at both the classical and the quantum levels, generalized n = 2 supersymmetric quantum mechanical sigma models describing the motion over an arbitrary real or an arbitrary complex manifold with extra torsions. we analyze the relevant vacuum states to make explicit the fact that their number is not affected by adding the torsion terms.
v2 scaling in pbpb collisions at 2.76 tev. we investigate scaling properties of the elliptical flow parameter v_{2} in pbpb collisions at 2.76 tev within a recently introduced new theoretical scheme which accounts for hydrodynamically expanding bulk matter, jets, and the interaction between the two.
production of muons from heavy flavour decays at forward rapidity in pp and pb-pb collisions at $\sqrt {s_{nn}}$ = 2.76 tev. the alice collaboration has measured the inclusive production of muons from heavy flavour decays at forward rapidity, 2.5 &lt; y &lt; 4, in pp and pb-pb collisions at $\sqrt {s_{nn}}$ = 2.76 tev. the pt-differential inclusive cross section of muons from heavy flavour decays in pp collisions is compared to perturbative qcd calculations. the nuclear modification factor is studied as a function of pt and collision centrality. a weak suppression is measured in peripheral collisions. in the most central collisions, a suppression of a factor of about 3-4 is observed in 6 &lt; pt &lt; 10 gev/c. the suppression shows no significant pt dependence.
measurement of prompt and non-prompt j/psi production cross sections at mid-rapidity in pp collisions at sqrt(s) = 7 tev. the alice experiment at the lhc has studied j/psi production at mid-rapidity in pp collisions at {\surd}s = 7 tev through its electron pair decay on a data sample corresponding to an integrated luminosity l_int = 5.6nb-1. the fraction of j/psi from the decay of long-lived beauty hadrons was determined for j/psi candidates with transverse momentum pt &gt; 1.3 gev/c and rapidity |y| &lt; 0.9. the cross section for prompt j/psi mesons, i.e. directly produced j/psi and prompt decays of heavier charmonium states such as the psi(2s) and csi_c resonances, is sigma_prompt-j/psi(pt &gt; 1.3 gev/c, |y| &lt; 0.9) = 7.2 +- 0.7(stat.) +- 1.0(syst.)+1.3-1.2 (syst.pol.) mb. the pt-differential cross section for prompt j/psi has also been measured. the cross section for the production of b-hadrons decaying to j/psi with transverse momentum greater than 1.3 gev/c in the rapidity range |y| &lt; 0.9 is sigma_{j/psi&lt;-h_b} = 1.26 +- 0.33 (stat.) +0.23-0.28 (syst.) mb. the results are compared to qcd model predictions. the shape of the pt and y distributions of b-quarks predicted by perturbative qcd model calculations are used to extrapolate the measured cross section to derive the bb pair total cross section and dsigma/dy at midrapidity.
heavy flavour measurements in pb-pb collisions at $\sqrt{s_{nn}}$ = 2.76 tev whith the alice experiment. null
measurement of electrons from semileptonic heavy-flavour hadron decays in pp collisions at \sqrt{s} = 7 tev. the differential production cross section of electrons from semileptonic heavy-flavour hadron decays has been measured at mid-rapidity ($|y| &lt; 0.5$) in proton-proton collisions at $\sqrt{s} = 7$ tev with alice at the lhc. data were collected in the transverse momentum range 0.5 $&lt;$ p_t $&lt;$ 8 gev/$c$. predictions from a fixed order perturbative qcd calculation with next-to-leading-log resummation agree with the data within the theoretical and experimental uncertainties.
the synthesis problem for trusted service-based collaborations. the growth of internet has extended the scope of software applications, leading to network-based ar- chitectures. the main characteristic of these architectures is that they restrict the communication between remote components to message passing. service-oriented computing is a solution to organise the exchange of messages in a network-based architecture, by using services as primitive components. since a service- oriented application typically spans a number of different organisations, its execution is subject to stringent requirements at two levels : business and security. at each level, the requirements could be implemen- ted by a centralised control maintaining the correct interactions. but this solution is not realistic. indeed, whereas each partner develops its own services, the whole application results from the collaboration of all the services in a truly concurrent way, without any centralised control. thus, the partners involved gene- rally define a contract at the global level in order to enforce interaction policies, dealing with business and security functionalities. by the contract, they increase their mutual trust. from the contract, each partner deduces by projection a specification of the functionalities that it must locally implement. of course, all these projections must ensure that the local functionalities, once gathered, effectively collaborate to realise the global contract. for each partner, it remains to realise the projection, by a local implementation.
towards a unified formal model for service orchestration and choreography. the growth of internet has extended the scope of software applications, leading to network-based architectures. the main characteristic of these architectures is that they restrict the communication between remote components to message passing. service-oriented computing is a solution to organise the exchange of messages in a network-based architecture, by using services as primitive components. thus, each component can be a client, a server or both. since a service-oriented application typically spans a number of different organizations, its executions is subject to stringent security requirements. that is the reason why the partners involved generally define a contract at the global level in order to enforce some security policy. from the contract, each partner deduces by projection a specification of the security functionalities that it must locally implement. of course, in order to be useful, all these projections must ensure that the local functionalities effectively collaborate to realize the global contract.
compensation of tool deflection in robotic-based milling. null
di-electron spectrum at mid-rapidity in $p+p$ collisions at $\sqrt{s} = 200$ gev. we report on mid-rapidity mass spectrum of di-electrons and cross sections of pseudoscalar and vector mesons via $e^{+}e^{-}$ decays, from $\sqrt{s} = 200$ gev $p+p$ collisions, measured by the large acceptance experiment star at rhic. the ratio of the di-electron continuum to the combinatorial background is larger than 10% over the entire mass range. simulations of di-electrons from light-meson decays and heavy-flavor decays (charmonium and open charm correlation) are found to describe the data. the extracted $\omega\rightarrow e^{+}e^{-}$ invariant yields are consistent with previous measurements. the mid-rapidity yields ($dn/dy$) of $\phi$ and $j/\psi$ are extracted through their di-electron decay channels and are consistent with the previous measurements of $\phi\rightarrow k^{+}k^{-}$ and $j/\psi\rightarrow e^{+}e^{-}$. our results suggest a new upper limit of the branching ratio of the $\eta \rightarrow e^{+}e^{-}$ of $1.7\times10^{-5}$ at 90% confidence level.
transverse sphericity of primary charged particles in minimum bias proton-proton collisions at sqrt(s)=0.9, 2.76 and 7 tev. measurements of the sphericity of primary charged particles in minimum bias proton--proton collisions at sqrt(s)=0.9, 2.76 and 7 tev with the alice detector at the lhc are presented. the observable is linearized to be collinear safe and is measured in the plane perpendicular to the beam direction using primary charged tracks with $p_{\rm t}\geq0.5$ gev/c in $|\eta|\leq0.8$. the mean sphericity as a function of the charged particle multiplicity at mid-rapidity ($n_{\rm ch}$) is reported for events with different $p_{\rm t}$ scales ("soft" and "hard") defined by the transverse momentum of the leading particle. in addition, the mean charged particle transverse momentum versus multiplicity is presented for the different event classes, and the sphericity distributions in bins of multiplicity are presented. the data are compared with calculations of standard monte carlo event generators. the transverse sphericity is found to grow with multiplicity at all collision energies, with a steeper rise at low $n_{\rm ch}$, whereas the event generators show the opposite tendency. the combined study of the sphericity and the mean $p_{\rm t}$ with multiplicity indicates that most of the tested event generators produce events with higher multiplicity by generating more back-to-back jets resulting in decreased sphericity (and isotropy). the pythia6 generator with tune perugia-2011 exhibits a noticeable improvement in describing the data, compared to the other tested generators.
longitudinal and transverse spin asymmetries for inclusive jet production at mid-rapidity in polarized p+p collisions at sqrt{s}=200 gev. we report star measurements of the longitudinal double-spin asymmetry a_ll, the transverse single-spin asymmetry a_n, and the transverse double-spin asymmetries a_sigma and a_tt for inclusive jet production at mid-rapidity in polarized p+p collisions at a center-of-mass energy of sqrt{s} = 200 gev. the data represent integrated luminosities of 7.6 /pb with longitudinal polarization and 1.8 /pb with transverse polarization, with 50-55% beam polarization, and were recorded in 2005 and 2006. no evidence is found for the existence of statistically significant jet a_n, a_sigma, or a_tt at mid-rapidity. recent model calculations indicate the a_n results may provide new limits on the gluon sivers distribution in the proton. the asymmetry a_ll significantly improves the knowledge of gluon polarization in the nucleon.
environmental information adaptive condition-based maintenance policies. this paper deals with the construction and optimisation of accurate condition based maintenance policies for cumulative deteriorating systems. in this context, the system condition behaviour can be influenced by different environmental factors which contribute to an increase or decrease the degradation rate. the observed condition can deviate from the expected condition if the degradation model does not embrace these environmental factors. moreover, if more information is available on the environment variations, the maintenance decision framework should take advantage of this new information and update the decision. the question is how shall we model the decision framework for this? we propose to model the effect of the random operating environment on the system behaviour with a randomization of the gamma process-degradation parameters. a new decision rule is introduced to update the maintenance decision in case of a significant deviation from the expected condition. the performance of the introduction of this new decision rule is discussed according different contexts: the level of knowledge on the real stress data and the restriction of a potential updating in the policy. a mathematical framework and optimization procedures are presented and numerical experiments are conducted to highlight the benefits of the different models.
antimatter production in proton-proton and heavy-ion collisions at ultrarelativistic energies. one of the striking features of particle production at high beam energies is the near equal abundance of matter and antimatter in the central rapidity region. in this paper we study how this symmetry is reached as the beam energy is increased. in particular, we quantify explicitly the energy dependence of the approach to matter/antimatter symmetry in proton-proton and in heavy-ion collisions. expectations are presented also for the production of more complex forms of antimatter like antihypernuclei.
evolution of the differential transverse momentum correlation function with centrality in au+au collisions at $\sqrt{s_{nn}} = 200$ gev. we present first measurements of the evolution of the differential transverse momentum correlation function, {\it c}, with collision centrality in au+au interactions at $\sqrt{s_{nn}} = 200$ gev. {\it c} exhibits a strong dependence on collision centrality that is qualitatively similar to that of number correlations previously reported. we use the observed longitudinal broadening of the near-side peak of {\it c} with increasing centrality to estimate the ratio of the shear viscosity to entropy density, $\eta/s$, of the matter formed in central au+au interactions. we obtain an upper limit estimate of $\eta/s$ that suggests that the produced medium has a small viscosity per unit entropy.
material screening and selection for xenon100. results of the extensive radioactivity screening campaign to identify materials for the construction of xenon100 are reported. this dark matter search experiment is operated underground at laboratori nazionali del gran sasso (lngs), italy. several ultra sensitive high purity germanium detectors (hpge) have been used for gamma ray spectrometry. mass spectrometry has been applied for a few low mass plastic samples. detailed tables with the radioactive contaminations of all screened samples are presented, together with the implications for xenon100.
soft hadron production at the lhc. the phenomenon of color coherence in quantum chromodynamics (qcd) is presented, and the basic observable to test it experimentally is introduced. first results for the cms experiment at the lhc are discussed.
radiodetection and characterization of the cosmic rays air showers radio emission for energies higher than 10^16 ev with the codalema experiment. null
first results on a sensor bio-inspired by electric fish. this article presents the first results of a work which aims at designing an active sensor inspired by the electric fish. its interest is its potential for robotics underwater navigation and exploration tasks in conditions where vision and sonar would meet difficulty. it could also be used as a complementary omnidirectional, short range sense to vision and sonar. combined with a well defined engine geometry, this sensor can be modeled analytically. in this article, we focus on a particular measurement mode where one electrode of the sensor acts as a current emitter and the others as current receivers. in spite of the high sensitivity required by electric sense, the first results show that we can obtain a detection range of the order of the sensor length, which suggests that this sensor principle could be used in future for robotics obstacle avoidance.
antenna design and distribution of the lofar super station. the nançay radio astronomy observatory and associated laboratories are developing the concept of a "super station" for extending the lofar station now installed and operational in nançay. the lofar super station (lss) will increase the number of high sensitivity long baselines, provide short baselines, act as an alternate core, and be a large standalone instrument. it will operate in the low frequency band of lofar (15-80 mhz) and extend this range to lower frequencies. three key developments for the lss are described here: (i) the design of a specific antenna, and the distribution of such antennas; (ii) at small-scale (analog-phased mini-array); and (iii) at large-scale (the whole lss).
a search for anisotropy in the arrival directions of ultra high energy cosmic rays recorded at the pierre auger observatory. observations of cosmic ray arrival directions made with the pierre auger observatory have previously provided evidence of anisotropy at the 99% cl using the correlation of ultra high energy cosmic rays (uhecrs) with objects drawn from the véron-cetty véron catalog. in this paper we report on the use of three catalog independent methods to search for anisotropy. the 2pt-l, 2pt+ and 3pt methods, each giving a different measure of self-clustering in arrival directions, were tested on mock cosmic ray data sets to study the impacts of sample size and magnetic smearing on their results, accounting for both angular and energy resolutions. if the sources of uhecrs follow the same large scale structure as ordinary galaxies in the local universe and if uhecrs are deflected no more than a few degrees, a study of mock maps suggests that these three methods can efficiently respond to the resulting anisotropy with a p-value = 1.0% or smaller with data sets as few as 100 events. using data taken from january 1, 2004 to july 31, 2010 we examined the 20,30,...,110 highest energy events with a corresponding minimum energy threshold of about 49.3 eev. the minimum p-values found were 13.5% using the 2pt-l method, 1.0% using the 2pt+ method and 1.1% using the 3pt method for the highest 100 energy events. in view of the multiple (correlated) scans performed on the data set, these catalog-independent methods do not yield strong evidence of anisotropy in the highest energy cosmic rays.
stiffness modeling of robotic-manipulators under auxiliary loadings. the paper focuses on the extension of the virtual-joint-based stiffness modeling technique for the case of different types of loadings applied both to the robot end-effector and to manipulator intermediate points (auxiliary loading). it is assumed that the manipulator can be presented as a set of compliant links separated by passive or active joints. it proposes a computationally efficient procedure that is able to obtain a non-linear force-deflection relation taking into account the internal and external loadings. it also produces the cartesian stiffness matrix. this allows to extend the classical stiffness mapping equation for the case of manipulators with auxiliary loading. the results are illustrated by numerical examples.
investigation on heat transfer evaluation for a more efficient two-zone combustion model in the case of natural gas si engines. two-zone model is one of the most interesting engine simulation tools, especially for si engines. however, the pertinence of the simulation depends on the accuracy of the heat transfer model. in fact, an important part of the fuel energy is transformed to heat loss from the chamber walls. also, knock appearance is closely related to heat exchange. however, in the previous studies using two-zone models, many choices are made for heat transfer evaluation and no choice influence study has been carried out, in the literature. the current study aims to investigate the effect of the choice of both the heat transfer correlation and burned zone heat transfer area calculation method and provide an optimized choice for a more efficient two-zone thermodynamic model, in the case of natural gas si engines. for this purpose, a computer simulation is developed. experimental measurements are carried out for comparison and validation. the effect of correlation choice has been first studied. the most known correlations have been tested and compared. our experimental pressure results, supported for more general and reliable conclusions, by a literature survey of many other studies, based on measured heat transfer rates for several si engines, are used for correlation selection. it is found that hohenberg's correlation is the best choice. however, the influence of the burned zone heat transfer area calculation method is negligible.
using models of partial knowledge to test model transformations. testers often use partial knowledge to build test models. this knowledge comes from sources such as requirements, known faults, existing inputs, and execution traces. in model-driven engineering, test inputs are models executed by model transformations. modelers build them using partial knowledge while meticulously satisfying several well-formedness rules imposed by the modelling language. this manual process is tedious and language constraints can force users to create complex models even for representing simple knowledge. in this paper, we want to simplify the development of test models by presenting an integrated methodology and semi-automated tool that allow users to build only small partial test models directly representing their testing intent. we argue that partial models are more readable and maintainable and can be automatically completed to full input models while considering language constraints. we validate this approach by evaluating the size and fault-detecting effectiveness of partial models compared to traditionally-built test models. we show that they can detect the same bugs/faults with a greatly reduced development effort.
emf profiles: a lightweight extension approach for emf models. null
lower bounds for a fixed job scheduling problem with an equity objective function. null
ridges in heavy ion collisions and pp scatterings at the lhc. null
light sterile neutrinos: a white paper. this white paper addresses the hypothesis of light sterile neutrinos based on recent anomalies observed in neutrino experiments and the latest astrophysical data.
uniform description of bulk observables in hydrokinetic model of a+a collisions at rhic and lhc. a successful simultaneous description of the hadronic yields, pion, kaon and proton spectra, elliptic flows and femtoscopy scales in hydrokinetic model of a+a collisions is presented at different centralities for the top rhic and lhc energies. the only changed parameter at different collision energies and centralities, except for peripheric events, is normalization to number of all charged particles. the hydrokinetic model is used in its hybrid version that allows one to switch correctly to the urqmd cascade at the isochronic hypersurface which separates the cascade stage and decaying hydrodynamic one. the results are compared with the standard hybrid model where hydrodynamics and hadronic cascade are matching just at the non-space-like hypersurface of chemical freeze-out. the initial conditions are based on both glauber- and kln- monte-carlo simulations, the results are compared. it seems that the observables, especially the femtoscopy data, prefer the glauber initial conditions. the modification of the particle number ratios caused, in particular, by the particle annihilations at the afterburn stage is analyzed.
production of innovative radionuclides at arronax and 211at rit. null
charm and beauty production at rhic. null
collective effects in pp scattering. null
on validation of atl transformation rules by transformation models. model-to-model transformations constitute an important ingredient in model-driven engineering. as real world transformations are complex, systematic approaches are required to ensure their correctness. the atlas transformation language (atl) is a mature transformation language which has been successfully applied in several areas. however, the executable nature of atl is a barrier for the validation of transformations. in contrast, transformation models provide an integrated structural description of the source and target metamodels and the transformation between them. while not being executable, transformation models are well-suited for analysis and verification of transformation properties. in this paper, we discuss (a) how atl transformations can be translated into equivalent transformation models and (b) illustrate how these surrogates can be employed to validate properties of the original transformation.
monte carlo simulation to reveal the copper dissolution kinetics of an ion selective electrode based on copper sulfide. the present work aims at studying copper dissolution of a cu2+ ion-selective electrode based on a cus thin film. the electrode is prepared using electrochemical deposition of cus on a silicon substrate. the obtained film exhibits an apparent cohesive granular structure with an average grain size of about 33 μm, a small porosity content (&lt;4%) and a thickness of about 7.48 μm. the cu2+ electrochemical response shows a nearly nernstian behavior in the range of pcu 6-1. the copper dissolution is experimentally studied in a wide ph range. in order to quantitatively predict copper mass dissolution, an original numerical model is developed based on monte carlo simulation. our main hypothesis is based on dissolution probability that triggers the whole dissolution process through solution/electrode surface exchanges. several probability forms are suggested accounting for the real observed electrochemical kinetics. the experimental results show that, under a low ph, the dissolution process severely leads to the consumption of large material. moreover, our predictions suggest a dissolution profile as a two-stage process irrespective of ph. our numerical model is able to fit correctly the observed kinetics considering an exponential probability form under all ph conditions.
a practical monadic aspect weaver. we present monascheme, an extensible aspect-oriented programming language based on monadic aspect weaving. extensions to the aspect language are deﬁned as monads, enabling easy, simple and modular prototyping. the language is implemented as an embedded language in racket. we illustrate the approach with an execution level monad and a level-aware exception transformer. semantic variations can be obtained through monad combinations. this work is also a ﬁrst step towards a framework for controlling aspects with monads in the pointcut and advice model of aop.
taming aspects with membranes. in most aspect-oriented languages, aspects have an unrestricted global view of computation. several approaches for aspect scoping and more strongly encapsulated modules have been formulated to restrict this controversial power of aspects. this paper leverages the concept of programmable membranes of boudol, schmitt and stefani, as a means to tame aspects by customizing the semantics of aspect weaving locally. membranes have the potential to subsume previous proposals in a uniform framework. because membranes give structure to computation, they enable ﬂexible scoping of aspects; because they are programmable, they enable visibility and safety constraints, both for the advised program and for the aspects. the power and simplicity of membranes open interesting perspectives to unify multiple approaches that tackle the unrestricted power of aspects.
j/psi suppression in p-a collisions from parton energy loss in cold qcd matter. the effects of energy loss in cold nuclear matter on j/psi suppression in p-a collisions are studied. a simple model based on first principles and depending on a single free parameter is able to reproduce j/psi suppression data at large xf and at various center-of-mass energies. these results strongly support energy loss as a dominant effect in quarkonium suppression. they also give some hint on its hadroproduction mechanism suggesting color neutralization to happen on long time-scales. predictions for j/psi and upsilon suppression in p-pb collisions at the lhc are made.
hypernuclei production in heavy ion collisions within a thermal model approach. null
selected highlights from the star experiment at rhic. null
nuclear matter properties from subthreshold kaon production. null
measurements of $d^{0}$ and $d^{*}$ production in $p$ + $p$ collisions at $\sqrt{s}$ = 200 gev. we report measurements of charmed-hadron ($d^{0}$, $d^{*}$) production cross sections at mid-rapidity in $p$ + $p$ collisions at a center-of-mass energy of 200 gev by the star experiment. charmed hadrons were reconstructed via the hadronic decays $d^{0}\rightarrow k^{-}\pi^{+}$, $d^{*+}\rightarrow d^{0}\pi^{+}\rightarrow k^{-}\pi^{+}\pi^{+}$ and their charge conjugates, covering the $p_t$ range of 0.6$-$2.0 gev/$c$ and 2.0$-$6.0 gev/$c$ for $d^{0}$ and $d^{*+}$, respectively. from this analysis, the charm-pair production cross section at mid-rapidity is $d\sigma/dy|_{y=0}^{c\bar{c}}$ = 170 $\pm$ 45 (stat.) $^{+38}_{-59}$ (sys.) $\mu$b. the extracted charm-pair cross section is compared to perturbative qcd calculations. the transverse momentum differential cross section is found to be consistent with the upper bound of a fixed-order next-to-leading logarithm calculation.
charm and beauty searches using electron-d0 azimuthal correlations and microvertexing techniques. null
correlation results from an event-by-event simulation of the three-dimensional hydrodynamic evolution in ultrarelativistic heavy ion collisions. null
femtoscopy within a hydrodynamic approach based on flux tube initial conditions. null
energy loss from heavy quarks in a dense medium. null
bulk and shear viscosities of the gluon plasma. null
is there a way to validate the different models for pp collisions?. null
on the possible experimental determination of the k+n potential on cross section in matter. null
witten index in supersymmetric 3d theories revisited. we have performed a direct calculation of witten index in n = 1,2,3 supersymmetric yang-mills chern-simons 3d theories. we do it in the framework of born-oppenheimer (bo) approach by putting the system into a small spatial box and studying the effective hamiltonian depending on the zero field harmonics. at the tree level, our results coincide with the results of witten, but there is a difference in the way the loop effects are implemented. in witten's approach, one has only take into account the fermion loops, which bring about a negative shift of the (chosen positive at the tree level) chern-simons coupling k. as a result, witten index vanishes and supersymmetry is broken at small k. in the effective bo hamiltonian framework, fermion, gluon and ghost loops contribute on an equal footing. fermion loop contribution to the effective hamiltonian can be evaluated exactly, and their effect amounts to the negative shift k -&gt; k - h/2 for n =1 and k -&gt; k - h for n = 2,3 in the tree-level formulae for the index. in our approach, with rather natural assumptions on the structure of bosonic corrections, the shift k -&gt; k + h brought about by the gluon loops also affects the index. since the total shift of k is positive or zero, witten index appears to be nonzero at nonzero k, and supersymmetry is not broken. we discuss possible reasons for such disagreement.
studies of isolated photon production in simulated proton-proton collisions with alice-emcal. null
radiative energy loss reduction in an absorptive plasma. null
quarkonia propagation in qgp: study of elastic and inelastic scattering processes. null
collective effects in proton proton and heavy ion scattering, and the "ridge" at rhic. null
a two degrees of freedom gain scheduled controller design methodology for a multi-motors web transport systems. in web transport systems, the main problem is to control the web velocity and tensions independently, to prevent web breaks, folding, or damage. interesting results have been obtained using multi-variable control strategies. unfortunately, most of the existing methodologies are either not systematic or deal with the tracking and disturbance rejection problems as a whole, and not separately. this paper presents a complete methodology in order to design two-degrees-of-freedom (2dof) controller. the feedforward part is based on a reference model allowing operators to obtain the desired tracking performances (in particular, web tensions and velocity decoupling). the feedback part ensures robustness and disturbance rejection and is designed using two high-level tuning parameters only, thanks to the standard state control (ssc) methodology. however, the system dynamics change greatly during the winding/unwinding process due to the winder/unwinder radius and inertia variations. therefore, a gain-scheduling controller is derived from the interpolation of consistent realizations of the h2 controllers obtained at different points of the operating domain. the resulting controller is tested on a realistic simulator first, and after discretization, on a 3-motor web-handling experimental platform.
you need to extend your models? emf facet vs. emf profiles. when using the eclipse modeling framework (emf), directly or as part of eclipse-based solutions, one often faces the problem of having to extend emf models in an efficient and structured way. however, either due to technical or business reasons, the respective original emf models/metamodels cannot be modified or "polluted" with the intended additional information. to this end, an advanced and lightweight model extension mechanism is worth a mint! emf facet and emf profiles, respectively hosted in eclipse-emft and eclipse labs, are two projects implementing such an extension mechanism. the former offers a way to dynamically extend models at runtime, while the latter provides a uml-like profile mechanism adapted to be used for any emf model. in this lightning talk, we introduce both tools very briefly and directly demonstrate on a simple example how they can be used in a complementary way.
strangeness production close to the threshold in proton-nucleus and heavy-ion collisions. we discuss strangeness production close to the threshold in p+a and a+a collision. comparing the body of available k+, k0, k-, and λ data with the iqmd transport code and for some key observables as well with the hsd transport code, we find good agreement for the large majority of the observables. the investigation of the reaction with the help of these codes reveals the complicated interaction of the strange particles with hadronic matter which makes strangeness production in heavy-ion collisions very different from that in elementary interactions. we show how different strange particle observables can be used to study the different facets of this interaction (production, rescattering and potential interaction) which finally merge into a comprehensive understanding of these interactions. we identify those observables which allow for studying (almost) exclusively one of these processes to show how the future high precision experiments can improve our quantitative understanding. finally, we discuss how the k+ multiplicity can be used to study the hadronic equation of state.
lambda over kaon enhancement in heavy ion collisions at several tev. we introduced recently a new theoretical scheme which accounts for hydrodynamically expanding bulk matter, jets, and the interaction between the two. important for the particle production at intermediate values of transverse momentum (p_t) are jet-hadrons produced inside the fluid. they pick up quarks and antiquarks (or diquarks) from the thermal matter rather than creating them via the schwinger mechanism -- the usual mechanism of hadron production from string fragmentation. these hadrons carry plasma properties (flavor, flow), but also the large momentum of the transversely moving string segment connecting quark and antiquark (or diquark). they therefore show up at quite large values of p_t, not polluted by soft particle production. we will show that this mechanism leads to a pronounced peak in the lambda / kaon ratio at intermediate p_t. the effect increases substantially with centrality, which reflects the increasing transverse size with centrality.
on the formation of bremsstrahlung in an absorptive qed/qcd medium. the radiative energy loss of a relativistic charge in a dense, absorptive medium can be affected significantly by damping phenomena. the effect is more pronounced for large energies of the charge and/or large damping of the radiation. this can be understood in terms of a competition between the formation time of bremsstrahlung and a damping time scale. discussing this competition in detail for the absorptive qed and qcd medium, we identify the regions in energy and parameter space, in which either coherence or damping effects are of major importance for the radiation spectrum. we show that damping mechanisms lead to a stronger suppression of the spectrum than coherence effects. this might be visible experimentally in correlations between hadrons at large momenta.
emftocsp: a tool for the lightweight verification of emf models. the increasing popularity of mde results in the creation of larger models and model transformations, hence converting the specification of mde artefacts in an error-prone task. therefore, mechanisms to ensure quality and absence of errors in models are needed to assure the reliability of the mde-based development process. formal methods have proven their worth in the verification of software and hardware systems. however, the adoption of formal methods as a valid alternative to ensure model correctness is compromised for the inner complexity of the problem. to circumvent this complexity, it is common to impose limitations such as reducing the type of constructs that can appear in the model, or turning the verification process from automatic into user assisted. since we consider these limitations to be counterproductive for the adoption of formal methods, in this paper we present emftocsp, a new tool for the fully automatic, decidable and expressive verification of emf models that uses constraint logic programming as the underlying formalism.
megapie spallation target: irradiation of the first protypical spallation target for future ads. null
achievements and lessons learned within the domain 1 "design" of the ip_eurotrans integrated project. null
strangeness enhancement - more than a core corona effect. null
beta decay studies of neutron rich nuclei using total absorption gamma-ray spectroscopy and delayed neutron measurements. null
xt-ads windowless spallation target design and corresponding r&amp;d topics. null
reactor neutrino detection for non proliferation with the nucifer experiment. null
from single heavy-quark to quarkonia formation. null
transport coefficients of the gluon plasma. null
hydrodynamical evolution (ebe) from flux-tube initial conditions in auau@rhic. null
simulation of the expansion and the phase transition of a quark plasma with the nambu-jona-lasinio model. null
quarkonia in urhic can the teach us something about qgp (properties)?. null
centrality dependence of observables more than a core - corona effect ?. null
importance of the conditioning of the chitosan support in a catalyst-containing ionic liquid phase immobilised on chitosan: the palladium-catalysed allylation reaction case. null
compensation of compliance errors in parallel manipulators composed of non-perfect kinematic chains. the paper is devoted to the compliance errors compensation for parallel manipulators under external loading. proposed approach is based on the non-linear stiffness modeling and reduces to a proper adjusting of a target trajectory. in contrast to previous works, in addition to compliance errors caused by machining forces, the problem of assembling errors caused by inaccuracy in the kinematic chains is considered. the advantages and practical significance of the proposed approach are illustrated by examples that deal with groove milling with orthoglide manipulator.
considerations concerning the fluctuations of the ratios of two observables. we discuss several possible caveats which arise with the interpretation of measurements of fluctuations in heavy-ion collisions. we especially focus on the ratios of particle yields, which have been advocated as a possible signature of a critical point in the qcd phase diagram. we conclude that current experimental observables are not well defined and are without a proper quantitative meaning.
elastostatic modeling and shape optimization of a 6-dof haptic interface device. this paper deals with the shape optimization of a six degree-of-freedom haptic interface device. this six-dof epicyclic-parallel manipulator has all actuators located on the ground. a regular dexterous workspace is introduced to represent the mobility of user's hand. throughout this workspace, the deviation of the mobile platform is bounded to provide a better feeling to the user and the masses in motion are minimized to increase the transparency of the haptic device. the stiffness model is written using a virtual joint method and compared with the results obtained with the finite element analysis to be validated. finally, the shape of the links are optimized in order to minimize the masses in motion while guaranteeing a given stiffness throughout the regular workspace of the mechanism.
nuclear-modification factor for open-heavy-flavor production at forward rapidity in cu+cu collisions at sqrt(s_nn)=200 gev. background: heavy-flavor production in p+p collisions tests perturbative-quantum-chromodynamics (pqcd) calculations. modification of heavy-flavor production in heavy-ion collisions relative to binary-collision scaling from p+p results, quantified with the nuclear-modification factor (r_aa), provides information on both cold- and hot-nuclear-matter effects. purpose: determine transverse-momentum, pt, spectra and the corresponding r_aa for muons from heavy-flavor mesons decay in p+p and cu+cu collisions at sqrt(s_nn)=200 gev and y=1.65. method: results are obtained using the semi-leptonic decay of heavy-flavor mesons into negative muons. the phenix muon-arm spectrometers measure the p_t spectra of inclusive muon candidates. backgrounds, primarily due to light hadrons, are determined with a monte-carlo calculation using a set of input hadron distributions tuned to match measured-hadron distributions in the same detector and statistically subtracted. results: the charm-production cross section in p+p collisions at sqrt{s}=200 gev, integrated over pt and in the rapidity range 1.4.
first dark matter results from the xenon100 experiment. null
the interaction of $\rho$ and k+ mesons with matter. null
parton energy loss in heavy-ion collisions via direct-photon and charged-particle azimuthal correlations. null
the lateral distribution function of coherent radio emission from extensive air showers: determining the chemical composition of cosmic rays. null
a lesson on structural testing with pathcrawler-online.com. abstract. pathcrawler is a test generation tool developed at cea list for structural testing of c programs. the new version of pathcrawler is developed in an entirely novel form: that of a test-case generation web service which is freely accessible at pathcrawler-online.com. this service allows many test-case generation sessions to be run in parallel in a completely robust and secure way. this tool demo and teaching experience paper presents pathcrawler-online.com in the form of a lesson on structural software testing, showing its beneﬁts, limitations and illustrating the usage of the tool on simple examples.
model ingredients and peak mass production in heavy-ion collisions. null
an optimal constraint programming approach to the open-shop problem. this paper presents an optimal constraint programming approach for the open-shop scheduling problem, which integrates recent constraint propagation and branching techniques with new upper bound heuristics. randomized restart policies combined with nogood recording allow us to search diversification and learning from restarts. this approach is compared with the best-known metaheuristics and exact algorithms, and it shows better results on a wide range of benchmark instances.
extending type theory with forcing. this paper presents an intuitionistic forcing translation for the calculus of constructions (coc), a translation that corresponds to an internalization of the presheaf construction in coc. depending on the chosen set of forcing conditions, the resulting type system can be extended with extra logical principles. the translation is proven correct-in the sense that it preserves type checking-and has been implemented in coq. as a case study, we show how the forcing translation on integers (which corresponds to the internalization of the topos of trees) allows us to define general inductive types in coq, without the strict positivity condition. using such general inductive types, we can construct a shallow embedding of the pure \lambda-calculus in coq, without defining an axiom on the existence of an universal domain. we also build another forcing layer where we prove the negation of the continuum hypothesis.
bin repacking scheduling in virtualized datacenters. a datacenter can be viewed as a dynamic bin packing system where servers host applications with varying resource requirements and varying relative placement constraints. when those needs are no longer satisfied, the system has to be reconfigured. virtualization allows to distribute applications into virtual machines (vms) to ease their manipulation. in particular, a vm can be freely migrated without disrupting its service, temporarily consuming resources both on its origin and destination. we introduce the bin repacking scheduling problem in this context. this problem is to find a final packing and to schedule the transitions from a given initial packing, accordingly to new resource and placement requirements, while minimizing the average transition completion time. our cp-based approach is implemented into entropy, an autonomous vm manager which detects reconfiguration needs, generates and solves the cp model, then applies the computed decision. cp provides the awaited flexibility to handle heterogeneous placement constraints and the ability to manage large datacenters with up to 2,000 servers and 10,000 vms.
multi-strange baryon production in pp collisions at (s)^1/2 = 7 tev with alice. a measurement of the multi-strange xi- and omega- baryons and their antiparticles by the alice experiment at the cern large hadron collider (lhc) is presented for proton-proton collisions at centre of mass energy of 7 tev. the transverse momentum (pt) distributions were studied at mid-rapidity (|y| &lt; 0.5) in the range of 0.6 &lt; pt &lt; 8.5 gev/c for xi- and xi+ baryons, and in the range of 0.8 &lt; pt &lt; 5 gev/c for omega- and omega+. baryons and anti-baryons were measured as separate particles and we find that the baryon to antibaryon ratio of both particle species is consistent with unity over the entire range of the measurement. the statistical precision of the current lhc data has allowed us to measure a difference between the mean pt of xi- (xi+) and omega- (omega+). particle yields, mean pt, and the spectra in the intermediate pt range are not well described by the pythia perugia 2011 tune monte carlo event generator, which has been tuned to reproduce the early lhc data. the discrepancy is largest for omega- (omega+). this pythia tune approaches the pt spectra of xi- and xi+ baryons below pt &lt; 0.85 gev/c and describes the xi- and xi+ spectra above pt &gt; 6.0 gev/c. we also illustrate the difference between the experimental data and model by comparing the corresponding ratios of (omega-+omega+)/(xi-+xi+) as a function of transverse mass.
physical mechanisms involved in grooved flat heat pipes: experimental and numerical analyses. an experimental database, obtained with flat plate heat pipes (fphp) with longitudinal grooves is presented. the capillary pressure measured by confocal microscopy and the temperature field in the wall are presented in various experimental conditions (vapour space thickness, filing ratio, heat transfer rate, tilt angle, fluid). coupled hydrodynamic and thermal models are developed. experimental results are compared to results of numerical models. physical mechanisms involved in grooved heat pipes are discussed, including the boiling limit and the effect of the interfacial shear stress. finally, recommendations for future experimental and theoretical research to increase the knowledge on fphp are discussed.
phenomenological interpolation of the inclusive j/psi cross section to proton-proton collisions at 2.76 tev and 5.5 tev. we present a study of the inclusive j/psi cross section at 2.76 tev and 5.5 tev. the energy dependence of the cross section, rapidity and transverse momentum distributions are evaluated phenomenologically. their knowledge is crucial as a reference for the interpretation of a-a and p-a j/psi results at the lhc. our approach is the following: first, we estimate the energy evolution of the pt-integrated j/psi cross section at mid-rapidity; then, we evaluate the rapidity dependence; finally, we study the transverse momentum distribution trend. whenever possible, both theory driven (based on pqcd predictions) and functional form (data driven fits) calculations are discussed. our predictions are compared with the recently obtained results by the alice collaboration in pp collisions at 2.76 tev.
jets, bulk matter, and their interaction in heavy ion collisions at several tev. we discuss a theoretical scheme which accounts for bulk matter, jets, and the interaction between the two. the aim is a complete description of particle production at all transverse momentum ($p_{t}$) scales. in this picture, the hard initial scatterings result in mainly longitudinal flux tubes, with transversely moving pieces carrying the $p_{t}$ of the partons from hard scatterings. these flux tubes constitute eventually both bulk matter (which thermalizes and flows) and jets. we introduce a criterion based on parton energy loss to decide whether a given string segment contributes to the bulk or leaves the matter to end up as a jet of hadrons. essentially low $p_{t}$ segments from inside the volume will constitute the bulk, high $p_{t}$ segments (or segments very close to the surface) contribute to the jets. the latter ones appear after the usual flux tube breaking via q-qbar production (schwinger mechanism). interesting is the transition region: intermediate $p_{t}$ segments produced inside the matter close to the surface but having enough energy to escape, are supposed to pick up q-qbar pairs from the thermal matter rather than creating them via the schwinger mechanism. this represents a communication between jets and the flowing bulk matter (fluid-jet interaction). also very important is the interaction between jet hadrons and the soft hadrons from the fluid freeze out. we employ the new picture to investigate pbpb collisions at 2.76 tev. we discuss the centrality and $p_{t}$ dependence of particle production and long range dihadron correlations at small and large $p_{t}$.
synchronization of multiple autonomic control loops: application to cloud computing. over the past years, autonomic computing has become very popular, especially in scenarios of cloud computing, where there might be several autonomic loops aiming at turning each layer of the cloud stack more autonomous, adaptable and aware of the runtime environment. nevertheless, due to conflicting objectives, non-synchronized autonomic loops may lead to global inconsistent states. for instance, in order to maintain its quality of service, an application provider might request more and more resources while the the infrastructure provider, due power shortage may be forced to reduce the resource provisioning. in this paper, we propose a generic model to deal with the synchronization and coordination of autonomic loops and how it can be applied in the context of cloud computing. we present some simulation results to show the scalability and feasibility of our proposal.
coherent radiation from extensive air showers. the generic properties of the emission of coherent radiation from a moving charge distribution are discussed. the general structure of the charge and current distributions in an extensive air shower are derived. these are subsequently used to develop a very intuitive picture for the properties of the emitted radio pulse. using this picture can be seen that the structure of the pulse is a direct reflection of the shower profile. at higher frequencies the emission is suppressed because the wavelength is shorter than the important length scale in the shower. it is shown that radio emission can be used to distinguish proton- and iron-induced air showers.
radio detection of cosmic ray air showers by the rauger experiment, a fully autonomous and self-triggered system installed at the pierre auger observatory. rauger is a radio experiment constituting three fully autonomous and self-triggered radio stations installed at the center of the pierre auger observatory's surface detector (sd). it aims at the radio detection of the electric field emitted by the secondary charged particles of the atmospherical shower initiated by ultra-high energy cosmic rays. installed in november 2006, we recorded the first atmospherical showers in coincidence with the sd in july 2007. up to now, 65 such coincidences have been obtained. the skymap in local coordinates (zenith angle, azimuth) of these events presents a strong azimuthal asymmetry in agreement with what was observed in the northern hemisphere by the codalema experiment (the asymmetry is simply switched by 180° in azimuth). we also recorded a threefold coincidence making possible a complete reconstruction: both the radio reconstructed shower axis and the shower energy are in perfect agreement with the sd estimations.
first results of the tianshan radio experiment for neutrino detection. we present the first results of a set-up called tianshan radio experiment for neutrino detection (trend) being presently deployed on the site of the 21 cm array (21cma) radio telescope, in xinjiang, china. we describe here its detection performances as well as the analysis method we applied to the data recorded with a small scale prototype. we demonstrate the ability of the trend set-up for an autonomous radio-detection of extended air showers induced by cosmic rays. the full set-up will consist of 80 antennas deployed over a 4 km2 area, and could result in a very attractive and unequalled radio-detection facility for the characterization of showers induced by ultra-high energy neutrinos with energies around 1017 ev.
coherent radio emission from cosmic ray air showers computed by monte-carlo simulation with selfas. an active dipole antenna is in operation since five years at the nançay radio observatory (france) in the codalema experiment. a new version of this active antenna has been developed, whose shape gave its name of "butterfly" antenna. compared to the previous version, this new antenna has been designed to be more efficient at low frequencies, which could permit the detection of atmospheric showers at large distances. despite a size of only 2 m×1 m in each polarization, its sensitivity is excellent in the 30-80 mhz bandwidth. three antennas in dual polarization were installed on the codalema experiment, and four other have been recently installed on the auger area in the scope of the aera project. the main characteristics of the butterfly antenna are detailed with an emphasis on its key features which make it a good candidate for the low frequency radioastronomy and the radio detection of transients induced by high energy cosmic rays.
non-thermal $p/\pi$ ratio at lhc as a consequence of hadronic final state interactions. recent lhc data on pb+pb reactions at sqrt s_{nn}=2.7 tev suggests that the p/pi is incompatible with thermal models. we explore several hadron ratios (k/pi, p/pi, lambda/pi, xi/pi) within a hydrodynamic model with hadronic after burner, namely urqmd 3.3, and show that the deviations can be understood as a final state effect. the measured values of the hadron ratios do then allow to gauge the transition energy density from hydrodynamics to the boltzmann description. we find that the data can be explained with transition energy densities of 840 +- 150 mev/fm^3.
characterization of at- species in simple and biological media by high performance anion exchange chromatography coupled to gamma detector. astatine is a rare radioelement belonging to the halogen group. considering the trace amounts of astatine produced in cyclotrons, its chemistry cannot be evaluated by spectroscopic tools. analytical tools, provided that they are coupled with a radioactive detection system, may be an alternative way to study its chemistry. in this research work, high performance anion exchange chromatography (hpaec) coupled to a gamma detector (γ) was used to evaluate astatine species under reducing conditions. also, to strengthen the reliability of the experiments, a quantitative analysis using a reactive transport model has been done. the results confirm the existence of one species bearing one negative charge in the ph range 27.5. with respect to the other halogens, its behavior indicates the existence of negative ion, astatide at-. the methodology was successfully applied to the speciation of the astatine in human serum. under fixed experimental conditions (ph 7.47.5 and redox potential of 250 mv) astatine exists mainly as astatide at- and does not interact with the major serum components. also, the method might be useful for the in vitro stability assessment of 211at-labelled molecules potentially applicable in nuclear medicine.
inclusive j/psi production in pp collisions at sqrt(s) = 2.76 tev. the alice collaboration has measured inclusive j/psi production in pp collisions at a center of mass energy sqrt(s)=2.76 tev at the lhc. the results presented in this letter refer to the rapidity ranges |y|&lt;0.9 and 2.5.
enhanced optimisation techniques for off-line programming of laser cutting robots. recent advances in laser technology, and especially essential increase of the cutting speed, motivate amendment of the existing robot path methods, which do not allow complete utilisation of the actuator capabilities and also neglect some particularities in the mechanical design of the manipulator arm wrist. this research addresses the optimisation of the 6-axes robot motions for the continuous contour tracking taking into account the redundancy caused by the tool axial symmetry. the particular contribution of the paper is in the area of multi-objective path planning using the graph-based search space representation. the developed path planning algorithm is based on the dynamic programming, which incorporates the constraint checking for each segment of a candidate solution and the penalty assignment for the constraint violation. to generate the smooth motion, each joint trajectory is evaluated by a set of performance indices such as the coordinate deviation, maximum increment, and total displacement of the axes. during the optimisation, the vector objective is converted into the scalar one using the weighted sum or minimax criterion, while the distance between the successive tool locations is evaluated using three types of the distance metrics. in contrast to the previous works, the developed optimisation technique explicitly incorporates verification of the velocity/acceleration constrains, allowing the designer interactively define their importance with respect to the path-smoothness objectives. in addition, it takes into account the capacity of some manipulator wrist axes for unlimited rotation in order to produce more economical motions. the efficiency of the developed algorithms has been carefully investigated via computer simulation. the presented results are implemented in a commercial software package and verified for real-life applications in automotive industry.
event-by-event simulation of the three-dimensional hydrodynamic evolution from flux tube initial conditions in ultrarelativistic heavy ion collisions. we present a sophisticated treatment of the hydrodynamic evolution of ultrarelativistic heavy ion collisions, based on the following features: initial conditions obtained from a flux tube approach, compatible with the string model and the color glass condensate picture; an event-by-event procedure, taking into the account the highly irregular space structure of single events, being experimentally visible via so-called ridge structures in two-particle correlations; the use of an efficient code for solving the hydrodynamic equations in 3+1 dimensions, including the conservation of baryon number, strangeness, and electric charge; the employment of a realistic equation of state, compatible with lattice gauge results; the use of a complete hadron resonance table, making our calculations compatible with the results from statistical models; and a hadronic cascade procedure after hadronization from the thermal matter at an early time.
non-disturbing characterization of natural organic matter (nom) contained in clay rock pore water by mass spectrometry using electrospray and atmospheric pressure chemical ionization modes. we have investigated the composition of the mobile natural organic matter (nom) present in callovo-oxfodian pore water using electrospray ionization mass spectrometry (esi-ms), atmospheric pressure chemical ionization mass spectrometry (apci-ms) and emission-excitation matrix (eem) spectroscopy. the generation of knowledge of the composition, structure and size of mobile nom is necessary if one wants to understand the interactions of these compounds with heavy metals/radionuclides, in the context of environmental studies, and particularly how the mobility of these trace elements is affected by mobile nom. the proposed methodology is very sensitive in unambiguously identifying the in situ composition of dissolved nom in water even at very low nom concentration, due to innovative non-disturbing water sampling and ionization (esi/apci-ms) techniques. it was possible to analyze a quite exhaustive inventory of the small organic compounds of clay pore water without proceeding to any chemical treatment at naturally occurring concentration levels. the structural features observed were mainly acidic compounds and fatty acids as well as aldehydes and amino acids.
use of fluorescence spectroscopy and voltammetry for the analysis of metal-organic matter interactions in the new caledonia lagoon. fluorescence, polarographic and potentiometric analysis of sea water from the new caledonia lagoon (located south of noumea) allowed the determination of the specific properties of the dissolved and particulate phases of organic matter (om)-metal complexes according to various regions of the lagoon. in particular, om complexes with ni, zn, pb, cu, cd were chosen in this study due to the sensitivity of these complexes to affect biocenosis of the nearby enclosed coral reef as well as their availability to enter the coast from erosion (terrigenous om) or human activities from nickel extraction or pollution from waste sites (anthopogenic om) that exist throughout new caledonia. combined with geochemical modelling, the om-metal complexes analysis allowed the determination of their conditional stability constants which in turn helped in predicting the fate of the metal pollution in the lagoon. for the first time, fluorescence, polarographic and potentiometric techniques combined with geochemical models that employed discrete pka distribution on om enabled the determination of the origin of the om, as either natural or anthopogenic.
lymphocyte subset reconstitution after unrelated cord blood or bone marrow transplantation in children. we report the post-transplant lymphocyte subset recovery of 226 children treated with unrelated cord blood transplant (ucbt) (n = 112) or unrelated bone marrow transplant (ubmt) (n = 114) for malignant or non-malignant diseases. absolute numbers of natural killer (nk), b and t cells were monitored by flow cytometry up to 5 years post-transplant. immunological endpoints were: time to achieve a cd3(+) cell count &gt; 0*5 and 1*5 × 10⁹/l, cd4(+) &gt; 0*2 and 0*5 × 10⁹/l, cd8(+) &gt; 0*25 ×10⁹/l, cd19(+) &gt; 0*2 × 10⁹/l, nk &gt; 0*1 × 10⁹/l. these endpoints were analysed through the use of cumulative incidence curves in the context of competing risks. cd8(+) t cell recovery was delayed after ucbt with a median time to reach cd8(+) t cells &gt; 0*25 × 10⁹/l of 7*7 months whereas it was 2*8 months in ubmt (p &lt; 0*001). b cell recovery was better in ucbt, with a median time to reach cd19(+) cells &gt; 0*2 × 10⁹/l of 3*2 months in ucbt and 6*4 months in ubmt (p = 0*03). median time for cd4(+) t cell and nk cell recovery was similar in ucbt and ubmt. cd4(+) t cells recovery was negatively correlated to age (better reconstitution in younger patients, p = 0*002). cd8(+) t cells recovery was shorter in recipients with a positive cytomegalovirus serology (p =0*001).
experimental development of a liquid xenon compton telescope for functional medical imaging. 3γ imaging is a new nuclear medical imaging technique which has been suggested by subatech laboratory. this technique involves locating three-dimensional position of the decay of an innovative radioisotope (β+, γ) emitter the 44sc. the principle consist in the detection of two photons of 511 kev gamma rays from the decay of the positron, provided by a pet ring detector, associated to the detection of the third photon by a liquid xenon compton telescope. the energy deposited in the interaction between the photon and xenon and its position are identified by measuring the ionization signal with a micromegas chamber (micromesh gaseous structure), while the trigger and time measurement of the interaction are provided by the detection of thescintillation signal. the principle of the tpc is thus usedto compton imaging.in order to demonstrate experimentally the feasibility of imaging 3γ, a small prototype, xemis (xenon medicalimaging system) was developed. this thesis is an important step towards the proof of feasibility. in this work are exposed the characterization of the detector response for a beam of 511 kev gamma rays and the analysis of data derived from it. the measurement of energy and time resolutions will be presented, as well as the purity of the liquid xenon.
extended h2 - h∞ controller synthesis for linear time invariant descriptor systems. the descriptor systems have been attracting the attention of many researchers over recent decades due to their capacity to preserve the structure of physical systems and to describe static constraints and impulsive behaviors. within the descriptor framework, the contributions of this dissertation are threefold: i) review of existing results for state-space systems, ii) generalization of classical results to descriptor systems, iii) exact and analytical solutions to non standard control problems. a realization independent kalman-yakubovich-popov (kyp) lemma and dilated lmi characterizations are deduced for descriptor systems. the solvability and corresponding numerical algorithms of generalized sylvester equations and generalized algebraic riccati equations (gare) associated with descriptor systems are provided. in addition, the simultaneous h∞ control problem is considered through extending recently reported results. a sufficient condition is proposed through a combination of a generalized algebraic riccati equation and a set of lmis. moreover, the nonstandard h2 and h∞ control problems with unstable and/or nonproper weighting functions or subject to regulation constraints are addressed. these contributions allow, without approximation or transformation, dealing with many practical problems defined within h2 or h∞ control methodologies, where the control signals are penalized at high frequency or unstable internal models specified by external signals is involved.
measurement of the cross section for electromagnetic dissociation with neutron emission in pb-pb collisions at {\surd}snn = 2.76 tev. the first measurement of neutron emission in electromagnetic dissociation of 208pb nuclei at the lhc is presented. the measurement is performed using the neutron zero degree calorimeters of the alice experiment, which detect neutral particles close to beam rapidity. the measured cross sections of single and mutual electromagnetic dissociation of pb nuclei at {\surd}snn = 2.76 tev with neutron emission are {\sigma}_single emd = 187.2{\pm}0.2 (stat.) +13.8-12.0 (syst.) b and {\sigma}_mutual emd = 6.2 {\pm} 0.1 (stat.) {\pm}0.4 (syst.) b respectively. the experimental results are compared to the predictions from a relativistic electromagnetic dissociation model.
suppression of high transverse momentum d mesons in central pb--pb collisions at $\sqrt{s_{nn}}=2.76$ tev. the production of the prompt charm mesons $d^0$, $d^+$, $d^{*+}$, and their antiparticles, was measured with the alice detector in pb-pb collisions at the lhc, at a centre-of-mass energy $\sqrt{s_{nn}}=2.76$ tev per nucleon--nucleon collision. the $\pt$-differential production yields in the range $2.
computational molecular modeling of the multi-scale dynamics of water and ions at cement interfaces. structural and dynamic behavior of h2o molecules and aqueous at in-terfaces and in nanopores of model c-s-h binding phase (tobermorite) is quanti-fied on the basis of molecular dynamics computer simulations. at the (001) sur-face of tobermorite in contact with 0.25 m kcl aqueous solution, we can effectively distinguish water molecules that spend most of their time within chan-nels between the drierketten chains of silica on the tobermorite surface from the adsorbed molecules residing slightly above the interface. within the channels, h2o molecules donate h-bonds to both the bridging and non-bridging oxygens of the si-tetrahedra as well as to other h2o. some of these molecules form very strong h-bonds persisting over 100 ps and longer, but many others undergo fre-quent librations and occasional diffusional jumps from one surface site to another. the average diffusion coefficients of the surface-associated h2o molecules that spend most of their time in the channels and those that lie above the nominal inter-face differ by about an order of magnitude (dh2o[internal]=5.0×10-11 m2/s and dh2o[external]=6.0×10-10 m2/s, respectively). the average diffusion coefficient for all surface-associated h2o molecules is about 1.0×10-10 m2/s. all of these values are significantly less than the value of 2.3×10-9 m2/s, characteristic of h2o self-diffusion in bulk liquid water, but they are in very good quantitative agreement with experimental data on the dynamics surface-associated water in similar ce-ment materials obtained be 1h nmr [1,2].
effects of organics on the adsorption and mobility of metal cations in clay systems: computational molecular modeling approach. understanding and prediction of many natural and anthropogenic environmental processes ultimately depend on a fundamental understanding of the chemistry occurring at the mineral-fluid interfaces. clay-related minerals and natural organic matter (nom) are ubiquitous in the environment, and metal-nom complexation induces strong correlations between the nom concentration in water and the capacity of clay particles to bind metals, thus affecting their speciation, solubility and toxicity in the environment. despite significant geochemical, environmental and technological interest, the molecular-level mechanisms and dynamics of the physical and chemical processes involving nom are not yet well understood. in this presentation we compare three different molecular dynamics (md) computer simulations of metal-nom complexation in aqueous solutions. the simulation results indicate that despite some obvious quantitative variations in the computed values depending on the size of the simulated system and on the parameters of the force field models used, all three simulations are quite robust and consistent. in particular, approximately 35-50% of ca2+ ions in all simulations are associated with the carboxylic groups of nom at near-neutral ph. the stability of bidentate-coordinated contact ion pair complexes is also always strongly preferred. easy association of metal cations with negatively charged nom functional groups and negatively charged clay surfaces allows us to predict that cationic bridging could be the most probable mechanism of nom association with clays in natural environments. new md simulations are currently in progress to quantitatively assess these predictions on a molecular scale for nuclear waste disposal applications. new larger-scale clay models incorporate a more realistic representation of the structural and compositional disorder of natural illites and smectites and employ clayff - a fully flexible general force field suitable for the molecular simulations of hydrated mineral systems in the presence of organics.
probing nuclear compressibility via fragmentation in au+au reactions at 35 amev. the molecular dynamics study of fragmentation in peripheral $^{197}$au +$^{197}$au collisions at 35 mev/nucleon is presented to probe the nuclear matter compressibility in low density regime. the yields of different fragment species, rapidity spectra, and multiplicities of charged particles with charge $3\leq z \leq 80$ are analyzed at different peripheral geometries employing a soft and a hard equations of state. fragment productions is found to be quite insensitive towards the choice of nucleon-nucleon cross sections allowing us to constrain nuclear matter compressibility. comparison of calculated charged particle multiplicities with the experimental data indicates preference for the \emph{soft} nature of nuclear matter.
isospin effects on the system size dependence of balance energy in heavy-ion collisions. we study the effect of isospin degree of freedom on balance energy throughout the mass range between 50 and 350 for two sets of isobaric systems with n/a = 0.5 and 0.58. our fndings indicate that different values of balance energy for two isobaric systems may be mainly due to the coulomb repulsion. we also demonstrate clearly the dominance of coulomb repulsion over symmetry energy.
is dtpa a good competing chelating agent for th(iv) in human serum and suitable in targeted alpha therapy?. the interaction between thorium and human serum components was studied using difference ultraviolet spectroscopy (dus), ultrafiltration and high-pressure-anion exchange chromatography (hpaec) with external inductively conducted plasma mass spectrometry (icp-ms) analysis. experimental data are compared with modelling results based on the law of mass action. human serum transferrin (hstf) interacts strongly with th(iv), forming a ternary complex including two synergistic carbonate anions. this complex governs th(iv) speciation under blood serum conditions. considering the generally used langmuir-type model, values of 1033.5 and 1032.5 were obtained for strong and weak sites, respectively. we showed that trace amounts of diethylene triamine pentaacetic acid (dtpa) cannot complex th(iv) in the blood serum at equilibrium. unexpectedly this effect is not related to the competition with hstf but is due to the strong competition with major divalent metal ions for dtpa. however, th-dtpa complex was shown to be stable for a few hours when it is formed before addition in the biological medium; this is related to the high kinetic stability of the complex. this makes dtpa a potential chelating agent for synthesis of 226th-labeled biomolecules for application in targeted alpha therapy.
highlights from the star experiment at rhic. experiments using heavy ion collisions at ultrarelativistic energies aim to explore the qcd phase transition and map out the qcd phase diagram. a wealth of remarkable results in this field have been reported recently, for example the upsilon suppression discovered recently. we discuss recent results from the star experiment focusing on strangeness, charm and beauty production.
role of isospin degree of freedom on the mass dependence of balance energy. the effect of isospin degree of freedom on balance energy and its mass dependence has been studied for the mass range between 50 and 350. our results shows the dominance of coulomb potential in isospin effects.
impact parameter dependence of isospin effects on the mass dependence of balance energy. we study the effect of isospin degree of freedom on the balance energy as well as its mass dependence throughout the mass range 48-270 for two sets of isobaric systems with n/z = 1 and 1.4 using isospin-dependent quantum molecular dynamics (iqmd) model. our fndings reveal the dominance of coulomb repulsion in isospin effects on balance energy as well as its mass dependence throughout the range of the colliding geometry.
dependence of balance energy on isospin degrees of freedom. using the isospin-dependent quantum molecular dynamics model we study the isospin effects on the disappearance of flow for the reactions of 58ni+58ni and 58fe+58fe as a function of impact parameter. we found good agreement between our calculations and experimentally measured energy of vanishing flow at all colliding geometries. our calculations reproduce the experimental data within 5%(10%) at central (peripheral) colliding geometries.
effect of nuclear compressibility on the fragmentation in peripheral au+au collisions at 35 amev. we studied the fragmentation in au(35 amev)+au collisions at reduced impact parameters in the range b/b_max=0.55 and 0.95 using soft and hard equations of state. the comparison of of qmd simulations at 100 fm/c as a function of reduced impact parameter $b/b_{max}$ with multics miniball data showed that soft eos accurately reproduces the experimental trend of declining fragment multiplicity with impact parameter. the hard eos on the contrary, seems too explosive to explain the data.
stability of the fragments and thermalization at peak center-of-mass energy. we simulate the central reactions of nearly symmetric, and asymmetric systems, for the energies at which the maximum production of imfs occurs (e$_{c.m.}^{peak}$).this study is carried out by using hard eos along with cugnon cross section and employing mstb method for clusterization. we study the various properties of fragments. the stability of fragments is checked through persistence coefficient and gain term. the information about the thermalization and stopping in heavy-ion collisions is obtained via relative momentum, anisotropy ratio, and rapidity distribution. we find that for a complete stopping of incoming nuclei very heavy systems are required. the mass dependence of various quantities (such as average and maximum central density, collision dynamics as well as the time zone for hot and dense nuclear matter) is also presented. in all cases (i.e., average and maximum central density, collision dynamics as well as the time zone for hot and dense nuclear matter) a power law dependence is obtained.
erratum to "the lateral trigger probability function for the ultra-high energy cosmic ray showers detected by the pierre auger observatory" [astroparticle physics 35 (2011) 266-276]. null
criojo: a pivot language for service-oriented computing - the introspective chemical abstract machine. interoperability remains a significant challenge in service-oriented computing. after proposing a pivot architecture to solve three interoperability problems, namely adaptation, integration and coordination problems between clients and servers, we explore the theoretical foundations for this architecture. a pivot architecture requires a universal language for orchestrating services and a universal language for interfacing resources. since there is no evidence today that web services technologies can provide this basis, we propose a new language called criojo and essentially show that it can be considered as a pivot language. we formalize the language criojo and its operational semantics, by resorting to a chemical abstract machine, and give an account of formal translations into criojo: in a distributed context, we deal with idiomatic languages for four major programming paradigms: imperative programming, logic programming, functional programming and concurrent programming.
csi-thgem gaseous photomultipliers for rich and noble-liquid detectors. the properties of uv-photon imaging detectors consisting of csi-coated thgem electron multipliers are summarized. new results related to detection of cherenkov light (rich) and scintillation photons in noble liquid are presented.
n=4, 3d supersymmetric quantum mechanics in non-abelian monopole background. using the harmonic superspace approach, we construct the three-dimensional n=4 supersymmetric quantum mechanics of the supermultiplet (3,4,1) coupled to an external su(2) gauge field. the off-shell n=4 supersymmetry requires the gauge field to be a static form of the 't hooft ansatz for the 4d self-dual su(2) gauge fields, that is a particular solution of bogomolny equations for bps monopoles. we present the explicit form of the corresponding superfield and component actions, as well as of the quantum hamiltonian and n=4 supercharges. the latter can be used to describe a more general n=4 mechanics system, with an arbitrary bps monopole background and on-shell n=4 supersymmetry. the essential feature of our construction is the use of semi-dynamical spin (4,4,0) multiplet with the wess-zumino type action.
four-point vector correlators and ads/qcd correspondence. we derive the four-point vector correlators in qcd from ads/qcd correspondence. it is shown that meson poles are correctly reproduced. the final expression also suggests a nonzero amplitude in the limit of zero virtuality of two longitudinal gluons. this fact does not mean that one can produce, absorb or scatter real longitudinal gluons.
constraint singularity-free design of the irsbot-2. null
center of mass energy and system-size dependence of photon production at forward rapidity at rhic. we present the multiplicity and pseudorapidity distributions of photons produced in au+au and cu+cu collisions at \sqrt{s_nn} = 62.4 and 200 gev. the photons are measured in the region -3.7 &lt; \eta &lt; -2.3 using the photon multiplicity detector in the star experiment at rhic. the number of photons produced per average number of participating nucleon pairs increases with the beam energy and is independent of the collision centrality. for collisions with similar average numbers of participating nucleons the photon multiplicities are observed to be similar for au+au and cu+cu collisions at a given beam energy. the ratios of the number of charged particles to photons in the measured pseudorapidity range are found to be 1.4 +/- 0.1 and 1.2 +/- 0.1 for \sqrt{s_nn} = 62.4 gev and 200 gev, respectively. the energy dependence of this ratio could reflect varying contributions from baryons to charged particles, while mesons are the dominant contributors to photon production in the given kinematic region. the photon pseudorapidity distributions normalized by average number of participating nucleon pairs, when plotted as a function of \eta - ybeam, are found to follow a longitudinal scaling independent of centrality and colliding ion species at both beam energies.
essential aop: the a calculus. null
manipulator motion planning for high-speed robotic laser cutting. recent advances in laser technology, especially the increase of the cutting speed, has motivated the amendment of the existing robot path methods, which do not allow the complete utilisation of the actuator capabilities and neglect certain particularities in the mechanical design of the wrist of the manipulator arm. this research addresses the optimisation of the six-axis robot motion for continuous contour tracking while considering the redundancy caused by the tool axial symmetry. the particular contribution of the paper is in the area of multi-objective path planning using the graph-based search space representation. in contrast to previous work, the developed optimisation technique is based on dynamic programming and explicitly incorporates verification of the velocity/acceleration constraints. this allows the designer to define interactively their importance with respect to the path-smoothness objectives. in addition, this optimisation technique takes into account the capacity of certain manipulator wrist axes for unlimited rotation in order to produce more economical motion. the efficiency of the developed algorithms has been carefully investigated via computer simulation. the presented results are implemented in a commercial software package and verified for real-life applications in the automotive industry.
study of fragmentation using clusterization algorithm with realistic binding energies. we here study fragmentation using a simulated annealing clusterization algorithm (saca) with binding energy at a microscopic level. in an earlier version, a constant binding energy (4 mev/nucleon) was used. we improve this binding energy criterion by calculating the binding energy of different clusters using a modified bethe-weizsäcker mass (bwm) formula. we also compare our calculations with experimental data of the aladin group. nearly no effect of this modification is visible.
cooperative and reactive scheduling in large-scale virtualized platforms with dvms. null
tabu search and lower bound for an industrial complex shop scheduling problem. this paper deals with an industrial shop scheduling problem that arises in a metal goods production group. the scheduling problem can be seen as a multi-mode job shop with assembly. jobs have additional constraints such as release date, due date and sequence-dependent setup times. the aim of the decision-makers is to minimize the maximum lateness. this article introduces a tabu search procedure to solve the whole problem and a valid lower bound used to evaluate the tabu search procedure.
csla : a language for improving cloud sla management. cloud computing is a paradigm for enabling remote, on-demand access to a set of configurable computing resources as a service. the pay-per-use model enables service providers to offer their services to customers in different quality-of-service (qos) levels. service level agreement (sla) is a negotiated agreement between a service provider and a customer where qos parameters specify the quality level of service that the service provider have to guarantee. however, due to the dynamic nature of the cloud and its instability, some sla violations can occurred and the service providers can be charged for penalties. in this paper, we aim at addressing the cloud instability to better control sla management (in particular sla violations) and indirectly the cloud elasticity. we propose csla, a new sla language directly integrating some features dealing with qos uncertainty and cloud fluctuation. in our evaluation, we present a novel profit model for service provider and new algorithms (for admission control and scheduling) to meet sla requirements (e.g. prevent sla violations) while tackling scalability and dynamic issues.
the northern site of the pierre auger observatory. the pierre auger observatory is an international facility dedicated to the full-sky study of the highest-energy cosmic rays. the southern site of the auger observatory was completed in june 2008. data collected since january 2004 have yielded important information on the energy spectrum, the primary particle composition, the fluxes of photons and neutrinos and on the anisotropic distribution of the arrival directions of the most energetic particles. on this basis, the scientific motivation for the northern auger observatory site in colorado, usa, is discussed. the overall layout, the key components and the expected performance of this 20 000 km2 hybrid observatory comprised of an array of 4400 surface detectors and 39 fluorescence telescopes are described.
a multiple plan approach for the dynamic technician routing and scheduling problem. the dynamic technician routing and scheduling problem (dtrsp) deals with a crew of technicians that serves dynamically appearing requests. in the dtrsp, each technician has a set of skills, tools, and spare parts, while requests require a subset of each. the problem is then to design a set of tours of minimal total duration such that each request is visited exactly once, within its time window, by a compatible technician, and to dynamically insert new requests into existing tours. we propose a multiple plan approach to solve the dtrsp and illustrate its performance on benchmark instances.
route consistency vehicle routing: a bi-objective approach. dynamic vehicle routing problems (d-vrps) are an extension of classical vrps in which the information available to the decision maker changes or is updated dynamically. most studies on d-vrps consider that routes can be designed online, which means that vehicle drivers do not know their next destination until they finish serving their current customer. although this assumption is theoretically appealing, it may not be desirable in a practical context in which drivers are used to know their routes from the beginning of the day. in this work we propose an optimization algorithm able to optimize the minimization of a cost function (the total traveled distance), and the minimization of the changes made in the vehicles routes in a dynamic routing context, and we study the tradeoff between both objectives.
hinf control with unstable and nonproper weights for descriptor systems. this paper is concerned with a nonstandard hinf output feedback control problem for continuous-time descriptor systems, where unstable and nonproper weighting functions are used. based on two generalized sylvester equations and two generalized algebraic riccati equations (gares) together with a spectral radius condition, an explicit parametrization of all desirable controllers is deduced. a numerical example is included to illustrate the validity of the proposed result.
probing the qgp phase boundary with thermal properties of $\phi$ mesons. a novel attempt has been made to probe the qcd phase boundary by using the experimental data for transverse momenta of {\phi} mesons produced in nuclear collisions at ags, sps and rhic energies. the data are confronted with simple thermodynamic expectations and lattice qcd results. the experimental data indicate a first-order phase transition, with a mixed phase stretching the energy density between \sim1 and 3.2 gev/fm3 corresponding to sps energies.
pion femtoscopy in p+p collisions at sqrt(s)=200 gev. the star collaboration at rhic has measured two-pion correlation functions from p+p collisions at sqrt(s)=200 gev. spatial scales are extracted via a femtoscopic analysis of the correlations, though this analysis is complicated by the presence of strong non-femtoscopic effects. our results are put into the context of the world dataset of femtoscopy in hadron-hadron collisions. we present the first direct comparison of femtoscopy in p+p and heavy ion collisions, under identical analysis and detector conditions.
reactor simulation for antineutrino experiments using dragon and mure. rising interest in nuclear reactors as a source of antineutrinos for experiments motivates validated, fast, and accessible simulations to predict reactor fission rates. here we present results from the dragon and mure simulation codes and compare them to other industry standards for reactor core modeling. we use published data from the takahama-3 reactor to evaluate the quality of these simulations against the independently measured fuel isotopic composition. the propagation of the uncertainty in the reactor operating parameters to the resulting antineutrino flux predictions is also discussed.
cross sections and double-helicity asymmetries of midrapidity inclusive charged hadrons in p+p collisions at sqrt(s)=62.4 gev. unpolarized cross sections and double-helicity asymmetries of single-inclusive positive and negative charged hadrons at midrapidity from p+p collisions at sqrt(s)=62.4 gev are presented. the phenix measurements for 1.0 &lt; p_t &lt; 4.5 gev/c are consistent with perturbative qcd calculations at next-to-leading order in the strong coupling constant, alpha_s. resummed pqcd calculations including terms with next-to-leading-log accuracy, yielding reduced theoretical uncertainties, also agree with the data. the double-helicity asymmetry, sensitive at leading order to the gluon polarization in a momentum-fraction range of 0.05 ~&lt; x_gluon ~&lt; 0.2, is consistent with recent global parameterizations disfavoring large gluon polarization.
poincare-chetayev equations and flexible multi-body systems. this article is devoted to the dynamics of flexible multi-body systems and to their links with a fundamental set of equations discovered by h. poincaré one hundred years ago [1]. these equations, called "poincaré-chetayev equations", are today known to be the foundation of the lagrangian reduction theory. starting with the extension of these equations to a cosserat medium, we show that the two basic sets of equations used in flexible multi-body dynamics. the generalized newton-euler model of flexible multi-body systems in the floating frame approach and the partial differential equations of the nonlinear geometrically exact theory in the galilean approach, are poincaré-chetayev equations.
optimization of a shared passengers and goods urban transportation network. optimizing passengers and goods flows reduces the costs and ecological footprint of transportation systems, as well as congestion in cities. since passengers and goods do not have the same nature (a person is active and a good is passive), modeling each flow separately seems natural. but local authorities need to consider passenger and freight transport together as a single logistic system. in this paper, we assess the efficiency of an existing transportation system where the spare capacity of public transport is used to distribute goods toward the city core. we propose two optimization approaches. the first one considers the problem as a multidepot vrptw followed by an assignment problem. the second one solves the whole model as a pickup and delivery problem with transfers (pdpt). these approaches are evaluated on data sets generated following a field study in the medium-sized city of la rochelle in france.
a state of the art of supply chain design models and methods integrating the principles of sustainable development. the aim of this study is to survey the optimization models and methods for supply chain design problems considering the concepts of sustainable development. supply chain design is one of the most significant issues in supply chain management. the mathematical models in the field of facility location, logistics, supply chain design and strategic planning have been incorporating more and more features of the real-life applications, such as such as collaborative supply chain, supplier selection, dynamic aspect, uncertainty and risk management. although sustainable development is one of the most challenging features to be taken into account, it is yet poorly considered in the or models and almost not mentioned in the existing reviews. we will develop the following issues: (i) the constituent parts (economic, environmental and social factors) and ingredients of sustainable development that are included in the mathematical models, (ii) the consequences of considering sustainable development factors on mathematical modeling and optimization methods, (iii) applications in various economic sectors.
spectroscopic study of 26si for application to nova gamma-ray emission. 26al was the first cosmic radioactivity ever detected in the galaxy. its nucleosynthesis in novae outbursts is still uncertain mainly due to the lack of nuclear information concerning the 25al(p,g )26si reaction. we report here on a neutron-gamma coincidence measurement of the 24mg(3he,ng )26si reaction performed at the orsay tandem facility aiming at the spectroscopic study of astrophysically important 26si states. a new level in the gamow peak is observed at ex = 5:888 mev and the gamma-ray decay scheme of all levels below the proton threshold is confirmed.
neutron and light charged particle production in neutron or proton‐induced reaction on iron, lead and uranium at intermediate energy (20 to 200 mev). the process of particle emission in the pre‐equilibrium stage has a very important contribution in this energy region and several approaches have been proposed to explain it. their prediction power must be tested using comparison with the data for a variety of configurations. calculations have been done using the exciton model and two main approaches proposed to improve its predictive power for complex particle emission. data reported in this work allow the extension to higher energies of databases that are now limited to energies around 60 mev. together with other experimental results available in the literature they allow a more global view on the capabilities of each approach. © 2005 american institute of physics.
program transformation based views for modular maintenance. modular programming is a practical solution for separation of concerns but the support for modularity provided by programming languages does not resolve the classic expression problem and more generally the tyranny of the dominant decomposition: evolutions are modular only on the principal axis of decomposition. to solve this problem, a practical solution would be to be able to choose the architecture of an application each time one has to make it evolve. we provide a prototype tool for the haskell language to support that. our tool allows to build transformations to switch haskell programs from one structure to another. we do this by driving a refactoring tool for haskell (hare): transformations are built by chaining elementary operations of refactoring. since each elementary refactoring operation preserve the semantics, the whole transformations also do.
stiffness matrix of manipulators with passive joints: computational aspects. the paper focuses on stiffness matrix computation for manipulators with passive joints, compliant actuators and flexible links. it proposes both explicit analytical expressions and an efficient recursive procedure that are applicable in the general case and allow obtaining the desired matrix either in analytical or numerical form. advantages of the developed technique and its ability to produce both singular and non-singular stiffness matrices are illustrated by application examples that deal with stiffness modeling of two stewart-gough platforms.
self-management of applications and systems to optimize energy in data centers. as a direct consequence of the increasing popularity of cloud computing solutions, data centers are amazingly growing and hence have to urgently face with the energy consumption issue. available solutions are focused basically on the system layer, by leveraging virtualization technologies to improve energy efficiency. another body of works relies on cloud computing models and virtualization techniques to scale up/down application based on their performance metrics. although those proposals can reduce the energy footprint of applications and by transitivity of cloud infrastructures, they do not consider the internal characteristics of applications to finely define a trade-­‐off between applications quality of service and energy footprint. in this paper, we propose a self-­‐adaptation approach that considers both application internals and system to reduce the energy footprint in cloud infrastructure. each application and the infrastructure are equipped with control loops, which allows them to autonomously optimize their executions. we implemented the control loops and simulated them in order to show their feasibility. in addition, we show how our solution fits in federated clouds through a motivating scenario. finally, we provide some discussion about open issues on models and implementation of our proposal.
j/psi production as a function of charged particle multiplicity in pp collisions at sqrt{s} = 7 tev. the alice collaboration reports the measurement of the inclusive j/psi yield as a function of charged particle pseudorapidity density dn_{ch}/deta in pp collisions at sqrt{s} = 7 tev at the lhc. j/psi particles are detected for p_t &gt; 0, in the rapidity interval |y| &lt; 0.9 via decay into e+e-, and in the interval 2.5 &lt; y &lt; 4.0 via decay into mu+mu- pairs. an approximately linear increase of the j/psi yields normalized to their event average (dn_{j/psi}/dy)/ with (dn_{ch}/deta)/ is observed in both rapidity ranges, where dn_{ch}/deta is measured within |eta| &lt; 1 and p_t &gt; 0. in the highest multiplicity interval with = 24.1, corresponding to four times the minimum bias multiplicity density, an enhancement relative to the minimum bias j/psi yield by a factor of about 5 at 2.5 &lt; y &lt; 4 (8 at |y| &lt; 0.9) is observed.
newton-euler approach for bio-robotics locomotion dynamics : from discrete to continuous systems. this thesis proposes a general and unified methodological framework suitable for studying the locomotion of a wide range of robots, especially bio-inspired. the objective of this thesis is twofold. first, it contributes to the classification of locomotion robots by adopting the mathematical tools developed by the american school of geometric mechanics.secondly, by taking advantage of the recursive nature of the newton-euler formulation, it proposes numerous efficient tools in the form of computational algorithms capable of solving the external direct dynamics and the internal inverse dynamics of any locomotion robot considered as a mobile multi-body system. these generic tools can help the engineers or researchers in the design, control and motion planning of manipulators as well as locomotion robots with a large number of internal degrees of freedom. the efficient algorithms are proposed for discrete and continuous robots. these methodological tools are applied to numerous illustrative examples taken from the bio-inspired robotics such as snake-like robots, caterpillars, and others like snake-board, etc.
preferred solutions computed with a label setting algorithm based on choquet integral for multi-objective shortest paths. the problem investigated in this paper concerns the integration of a decision maker preference model within a labeling algorithm for the multi-objective shortest path problem. the aim is to use a preference model built a priori for computing efficiently exact preferred solutions. the approach is based on the choquet integral, which can model not only relative importances but also interactions between criteria. the paper introduces choquet dominance rules, which replaces the pareto dominance. the rules are integrated within the label setting algorithm originally proposed in 1984 by martins. numerical experiments report significant performance improvements, and conclude on the efficiency of the rules for reducing the search space.
a lower bound of the choquet integral integrated within martins' algorithm. the problem investigated in this work concerns the integration of a decision-maker preference model within an exact algorithm in multiobjective combinatorial optimization. rather than computing the complete set of efficient solutions and choosing a solution afterwards, our aim is to efficiently compute one solution satisfying the decision maker preferences elicited a priori. the preference model is based on the choquet integral. the reference optimization problem is the multiobjective shortest path problem, where martins' algorithm is used. a lower bound of the choquet integral is proposed that aims to prune useless partial paths at the labeling stage of the algorithm. various procedures exploiting the proposed bound are presented and evaluated on a collection of benchmarks. numerical experiments show significant improvements compared to the exhaustive enumeration of solutions.
a unified formal model for service oriented architecture to enforce security contracts. in this paper we introduce a model as a foundation for heterogeneous services, therefore unifying web services technologies in soa (service oriented architecture), specifically, soap/ws* and restful models. this model abstracts away from service implementations, in order to verify and to enforce some important security properties.
a message-passing model for service oriented computing. service-based applications can be built according to multiple technologies. although there is a clear need for a model integrating them in multiple real-world contexts, no integrated model does (yet) exist. in this paper we introduce a model as a foundation for heterogeneous services, with particularly studying soap/ws* and restful models. the model completely abstracts away from service implementations, composes them in a truly concurrent manner, and supports asynchronous message passing as well as mobility of typed channels. we consider the application of this model to the problem of type checking messages and communications in presence of channel mobility and malicious agents.
charged and strange hadron elliptic flow in cu+cu collisions at $\sqrt{s_{nn}}$ = 62.4 and 200 gev. we present the results of an elliptic flow analysis of cu+cu collisions recorded with the star detector at 62.4 and 200gev. elliptic flow as a function of transverse momentum is reported for different collision centralities for charged hadrons and strangeness containing hadrons $k_{s}^{0}$, $\lambda$, $\xi$, $\phi$ in the midrapidity region $|eta|&lt;1.0$. significant reduction in systematic uncertainty of the measurement due to non-flow effects has been achieved by correlating particles at midrapidity, $|\eta|&lt;1.0$, with those at forward rapidity, $2.5&lt;|\eta|&lt;4.0$. we also present azimuthal correlations in p+p collisions at 200 gev to help estimating non-flow effects. to study the system-size dependence of elliptic flow, we present a detailed comparison with previously published results from au+au collisions at 200 gev. we observe that $v_{2}$($p_{t}$) of strange hadrons has similar scaling properties as were first observed in au+au collisions, i.e.: (i) at low transverse momenta, $p_t&lt;2gev/c$, $v_{2}$ scales with transverse kinetic energy, $m_{t}-m$, and (ii) at intermediate $p_t$, $2.
performances and stability of a 2.4 ton gd organic liquid scintillator target for antineutrino detection. in this work we report the performances and the chemical and physical properties of a (2 x 1.2) ton organic liquid scintillator target doped with gd up to ~0.1%, and the results of a 2 year long stability survey. in particular we have monitored the amount of both gd and primary fluor actually in solution, the optical and fluorescent properties of the gd-doped liquid scintillator (gdls) and its performances as a neutron detector, namely neutron capture efficiency and average capture time. the experimental survey is ongoing, the target being continuously monitored. after two years from the doping time the performances of the gd-doped liquid scintillator do not show any hint of degradation and instability; this conclusion comes both from the laboratory measurements and from the "in-tank" measurements. this is the largest stable gd-doped organic liquid scintillator target ever produced and continuously operated for a long period.
charge collection in the silicon drift detectors of the alice experiment. a detailed study of charge collection efficiency has been performed on the silicon drift detectors (sdd) of the alice experiment. three different methods to study the collected charge as a function of the drift time have been implemented. the first approach consists in measuring the charge at different injection distances moving an infrared laser by means of micrometric step motors. the second method is based on the measurement of the charge injected by the laser at fixed drift distance and varying the drift field, thus changing the drift time. in the last method, the measurement of the charge deposited by atmospheric muons is used to study the charge collection efficiency as a function of the drift time. the three methods gave consistent results and indicated that no charge loss during the drift is observed for the sensor types used in 99% of the sdd modules mounted on the alice inner tracking system. the atmospheric muons have also been used to test the effect of the zero-suppression applied to reduce the data size by erasing the counts in cells not passing the thresholds for noise removal. as expected, the zero suppression introduces a dependence of the reconstructed charge as a function of drift time because it cuts the signal in the tails of the electron clouds enlarged by diffusion effects. these measurements allowed also to validate the correction for this effect extracted from detailed monte carlo simulations of the detector response and applied in the offline data reconstruction.
design of a two dof gain scheduled frequency shaped lq controller for narrow tilting vehicles. narrow tilting vehicles (ntvs) are the convergence of a car and a motorcycle. they are expected to be the new generation of city cars considering their practical dimensions and lower energy consumption. but considering their height to breadth ratio, in order to maintain lateral stability, ntvs should tilt when cornering. unlike the motorcycle's case, where the driver tilts the vehicle himself, the tilting of an ntv should be automatic. the control objective in this paper is to reduce the perceived lateral acceleration through tilting, what would increase lateral stability and also the driver comfort with regard to centrifugal acceleration. a two degree of freedom tilting controller is proposed in the paper, using all the measurement available, controlling directly the perceived lateral acceleration (measured), while considering the steering angle as the main disturbance source. the control problem is recast as an optimal h2 problem, considering a model of the typical steering angle in addition to the vehicle model. the steering angle model allows predicting the near future of the vehicle trajectory, leading to an improved result in terms of energy requirements. although initially designed for a given constant speed, the lpv controller is finally proposed with guaranties of good performance for a large range of speed and acceleration.
trajectory generation for high speed pick-and-place robots. null
a model-driven traceability framework for software product lines. software product line (spl) engineering is a recent approach to software development where a set of software products are derived for a well defined target application domain, from a common set of core assets using analogous means of production (for instance, through model driven engineering). therefore, such family of products are built from reuse, instead of developed individually from scratch. spl promise to lower the costs of development, increase the quality of software, give clients more flexibility and reduce time to market. these benefits come with a set of new problems and turn some older problems possibly more complex. one of these problems is traceability management. in the europe an ample project we are creating a common traceability framework across the various activities of the spl development. we identified four orthogonal traceability dimensions in spl development, one of which is an extension of what is often considered as "traceability of variability". this constitutes one of the two contributions of this paper. the second contribution is the specification of a metamodel for a repository of traceability links in the context of spl and the implementation of a respective traceability framework. this framework enables fundamental traceability management operations, such as trace import and export, modification, query and visualization. the power of our framework is highlighted with an example scenario.
elasto-dynamic model of robotic milling process considering interaction between tool and workpiece. null
radiative and collisional energy loss of heavy quarks in deconfined matter. we extend our recently advanced model on collisional energy loss of heavy quarks in a quark gluon plasma (qgp) by including radiative energy loss. we discuss the approach and present calculations for pbpb collisions at $\sqrt{s}=2.76 tev$. the transverse momentum spectra, raa, and the elliptic flow $v_2$ of heavy quarks have been obtained using the model of kolb and heinz for the hydrodynamical expansion of the plasma.
supersymmetric proof of the hirzebruch-riemann-roch theorem for non-kähler manifolds. we present the proof of the hrr theorem for a generic complex compact manifold by evaluating the functional integral for the witten index of the appropriate supersymmetric quantum mechanical system.
redox-active phases and radionuclide equilibrium valence state in subsurface environments - new insights from 6th ec fp ip funmig. within the 6th ec fp integrated project "fundamental processes of radionuclide migration" (funmig), progress has been made to improve knowledge about the phases and reaction mechanisms involved in complex reduction processes of radionuclide contaminants in natural subsurface environments. this review paper gives an overview of the achievements made by the research groups involved in this project, and puts the scope and results of the studies in a more global context. firstly, both thermodynamic and experimental evidence show that green rust is present and reactive in subsurface groundwater with a composition that spans the fe(ii)/fe(iii) redox boundary. green rust has been shown to reduce np(v), se(vi) and se(iv), but the pathways for the redox processes and the reaction products that result are complicated, and change as a function of the reaction parameters. secondly, considerable evidence has emerged that se(iv) is reduced on fe(ii)-bearing minerals which are ubiquitous in subsurface environments. the stable se valence state in the presence of fes2 has been shown to be se(0). also, natural dissolved humic substances that contain sufficient electron donating capacity are capable of interacting with, and possibly reducing, se(iv) to lower valence states. thirdly, the influence of hco-3 and organic ligands on the uptake and reduction of u(vi) on fe(ii)-bearing minerals was investigated. while it appeared that hco-3 decreased the extent of u(vi) uptake by the reducing surface, the fraction of reduced u(iv) in the solid phase increased with increasing hco-3 concentration. in contrast with the observations for hco-3, organic ligands decreased both the extent of u uptake, as well as the fraction of u(iv) found in the solid phase. the studies performed within funmig show that investigating reduction-oxidation mechanisms require (1) a detailed control over reaction conditions (anoxic atmosphere, purification of solid phases, initial radionuclide speciation), (2) a rigorous follow-up of reaction products (both solution chemistry and spectroscopic methods), and (3) the consideration of slow kinetics in the setting up of an experiment. these requirements make the study and assessment of redox processes one of the most demanding scientific challenges for geochemists who are asked to make predictions for radionuclide transport behaviour in the environment.
directed and elliptic flow of charged particles in cu+cu collisions at $\sqrt{\bm {s_{nn}}} =$ 22.4 gev. this paper reports results for directed flow $v_{1}$ and elliptic flow $v_{2}$ of charged particles in cu+cu collisions at $\sqrt{s_{nn}}=$ 22.4 gev at the relativistic heavy ion collider. the measurements are for the 0-60% most central collisions, using charged particles observed in the star detector. our measurements extend to 22.4 gev cu+cu collisions the prior observation that $v_1$ is independent of the system size at 62.4 and 200 gev, and also extend the scaling of $v_1$ with $\eta/y_{\rm beam}$ to this system. the measured $v_2(p_t)$ in cu+cu collisions is similar for $\sqrt{s_{nn}} = 22.4-200$ gev. we also report a comparison with results from transport model (urqmd and ampt) calculations. the model results do not agree quantitatively with the measured $v_1(\eta), v_2(p_t)$ and $v_2(\eta)$.
system size and energy dependence of near-side di-hadron correlations. two-particle azimuthal ($\delta\phi$) and pseudorapidity ($\delta\eta$) correlations using a trigger particle with large transverse momentum ($p_t$) in $d$+au, cu+cu and au+au collisions at $\sqrt{s_{{nn}}}$ =\xspace 62.4 gev and 200~gev from the star experiment at rhic are presented. the \ns correlation is separated into a jet-like component, narrow in both $\delta\phi$ and $\delta\eta$, and the ridge, narrow in $\delta\phi$ but broad in $\delta\eta$. both components are studied as a function of collision centrality, and the jet-like correlation is studied as a function of the trigger and associated $p_t$. the behavior of the jet-like component is remarkably consistent for different collision systems, suggesting it is produced by fragmentation. the width of the jet-like correlation is found to increase with the system size. the ridge, previously observed in au+au collisions at $\sqrt{s_{{nn}}}$ = 200 gev, is also found in cu+cu collisions and in collisions at $\sqrt{s_{{nn}}}$ =\xspace 62.4 gev, but is found to be substantially smaller at $\sqrt{s_{{nn}}}$ =\xspace 62.4 gev than at $\sqrt{s_{{nn}}}$ = 200 gev for the same average number of participants ($ \langle n_{\mathrm{part}}\rangle$). measurements of the ridge are compared to models.
theory of heavy quark energy loss. we briefly review some of the models and theoretical schemes established to describe heavy quark quenching in ultrarelativistic heavy ions collisions. some lessons are derived from rhic and early lhc data, especially as for the contraints they impose on those models.
$k^-$ and $\bar p$ spectra for au+au collisions at $\sqrt{s}$ = 200 gev from star, phenix and brahms in comparison to core-corona model predictions. based on results obtained with event generators we have launched the core-corona model. it describes in a simplified way but quite successfully the centrality dependence of multiplicity and $$ of identified particles observed in heavy-ion reaction at beam energies between $\sqrt{s}$ = 17 gev and 200 gev. also the centrality dependence of the elliptic flow, $v_2$, for all charged and identified particles could be explained in this model. here we extend this analysis and study the centrality dependence of single particle spectra of $k^-$ and ${\bar p}$ measured by the phenix, star and brahms collaborations. we find that also for these particles the analysis of the spectra in the core-corona model suffers from differences in the data published by the different experimental groups, notably for the pp collisions. as for protons and $k^+$ for each experience the data agree well with the prediction of the core-corona model but the value of the two necessary parameters depends on the experiments. we show as well that the average momentum as a function of the centrality depends in a very sensitive way on the particle species and may be quite different for particles which have about the same mass. therefore the idea to interpret this centrality dependence as a consequence of a collective expansion of the system, as done in blast way fits may be premature.
training effects of a visual aid on haptic sensitivity in a needle insertion task. this paper describes an experiment conducted to measure human's haptic sensitivity and the effects of haptic training with and without visual aid on a needle insertion task. the haptic training protocol consisted of a needle insertion task using dual-layer silicon samples. a visual aid was provided as a multimodal cue for haptic perception. results show that for a novices' group, training with a visual aid inhibited haptic perception. hence, haptic skills must be trained differently from visuo-motor skills.
robotics modelling and tilting control of an innovative urban vehicle. modeling and simulating are fundamental tools to develop new vehicles. the aim of this thesis is to model and simulate a urban narrow tilting car whose structure contains closed mechanical chains. hence the goal is to build a physical model more precise and realistic than the bicycle model or quarter vehicle model used usually for some control purposes. the modeling approach is based on the modified denavit&amp;hartenberg description, commonly used in robotics, by considering the vehicle as a multi-body poly-articulated system whose the terminal links are the wheels. this description allows calculating automatically the symbolic expression of the geometric, kinematic and dynamic models, by using robotics techniques and a symbolic software package named symoro+. the dynamic model is calculated recursively thanks to the newton-euler algorithm. simulations of different dynamical model of vehicles have been performed, analyzed and compared. they validate in some sense the modeling methodology presented as an efficient way to get realistic model of non-standard vehicles.
effects of ca2+ on supramolecular aggregation of natural organic matter in aqueous solutions: a comparison of molecular modeling approaches. natural organic matter (nom) represents a complex molecular system that cannot be fully characterized compositionally or structurally in full atomistic detail. this makes the application of molecular modeling approaches very difficult and significantly hinders quantitative investigation of nom properties and behavior by these otherwise very efficient computational techniques. here we report and analyze three molecular dynamics (md) simulations of ca2+ complexation with nom in aqueous solutions in an attempt to quantitatively assess possible effects of model- and system size-dependence in such simulations. despite some obvious variations in the computed results that depend on the size of the simulated system and on the parameters of the force field models used, all three simulations are quite robust and consistent. they show ca2+ ions associated with 35-50% of the nom carboxylic groups at near-neutral ph and point to a strong preference for the stability of bidentate-coordinated contact ion pairs. the degree and potential mechanisms of nom supramolecular aggregation in the presence of ca2+ ions in solution are also assessed on a semi-quantitative level from two larger-scale md simulations.
new insights in elf analysis using relativistic two-component approach: an application to astatine compounds. null
sorption of model radionuclides on callovian-oxfordian clayey formation. null
a monadic interpretation of execution levels and exceptions for aop. aspect-oriented programming (aop) started ten years ago with the remark that modularization of so-called crosscutting functionalities is a fundamental problem for the engineering of large-scale applications. originating at xerox parc, this observation has sparked the development of a new style of programming featured that is gradually gaining traction. however, aop lacks theoretical foundations to clarify new ideas showing up in its wake. this paper proposes to put a bridge between aop and the notion of 2-category to enhance the conceptual understanding of aop. starting from the connection between the λ-calculus and the theory of categories, we provide an internal language for 2-categories and show how it can be used to deﬁne the ﬁrst categorical semantics for a realistic functional aop language, called minaml. we then take advantage of this new categorical framework to introduce the notion of computational 2-monads for aop. we illustrate their conceptual power by deﬁning a 2-monad for éric tanter's execution levels--which constitutes the ﬁrst algebraic semantics for execution levels--and then introducing the ﬁrst exception monad transformer speciﬁc to aop that gives rise to a non-ﬂat semantics for exceptions by taking levels into account.
invertible program restructurings for continuing modular maintenance. when one chooses a main axis of structural decompostion for a software, such as function- or data-oriented decompositions, the other axes become secondary, which can be harmful when one of these secondary axes becomes of main importance. this is called the tyranny of the dominant decomposition. in the context of modular extension, this problem is known as the expression problem and has found many solutions, but few solutions have been proposed in a larger context of modular maintenance. we solve the tyranny of the dominant decomposition in maintenance with invertible program transformations. we illustrate this on the typical expression problem example. we also report our experiments with java and haskell programs and discuss the open problems with our approach.
toward systems biology in brown algae to explore acclimation and adaptation to the shore environment. brown algae belong to a phylogenetic lineage distantly related to land plants and animals. they are almost exclusively found in the intertidal zone, a harsh and frequently changing environment where organisms are submitted to marine and terrestrial constraints. in relation with their unique evolutionary history and their habitat, they feature several peculiarities, including at the level of their primary and secondary metabolism. the establishment of ectocarpus siliculosus as a model organism for brown algae has represented a framework in which several omics techniques have been developed, in particular, to study the response of these organisms to abiotic stresses. with the recent publication of medium to high throughput profiling data, it is now possible to envision integrating observations at the cellular scale to apply systems biology approaches. as a first step, we propose a protocol focusing on integrating heterogeneous knowledge gained on brown algal metabolism. the resulting abstraction of the system will then help understanding how brown algae cope with changes in abiotic parameters within their unique habitat, and to decipher some of the mechanisms underlying their (1) acclimation and (2) adaptation, respectively consequences of (1) the behavior or (2) the topology of the system resulting from the integrative approach.
pollutants transfer in urban stormwater runoff basin - additional biological treatment. the objective of this work is to evaluate the heavy metal transfer into an urban stormwater treatment device and to develop a treatment process which would allow improving the treatment of these waters. the scientific approach consisted first to characterize the metal loads forwarded into a retention pond receiving stormwater runoff coming from a highway and to evaluate the transfer of this metal pollution to the surrounding vegetation. the results of this preliminary study demonstrated: (1) a heavy metal transfer (cd, ni and zn) from the water and soil compartments to the aquatic macrophytes present on the studied site, (2) the capacity of these macrophytes to accumulate metal pollutants into their tissues and (3) the bioindicator value of these macrophytes for biomonitoring of stormwater metal pollution. regarding the capacity of plants to accumulate metals, especially in their roots, a phytoremediation process called floating treatment wetlands was proposed to improve urban stormwater quality. the treatment performances of these systems were evaluated through a microcosm experiment and the technical feasibility for implanting such floating systems directly on the surface of existing ponds was also evaluated. the results showed that floating treatment wetlands can be operated on the surface of retention pond provided that the material chosen for the construction of these systems are well adapted to environmental conditions. the results also brought to light the efficiency of floating treatment wetlands for metal uptake from water as well as the importance of the root system on pollutant retention and the filtration of fine suspended particles. finally, this study showed that floating treatment wetlands can be considered as sustainable treatment systems that need low maintenance.
heavy flavour decay muon production at forward rapidity in proton--proton collisions at \sqrt(s) = 7 tev. the production of muons from heavy flavour decays is measured at forward rapidity in proton--proton collisions at \sqrt(s) = 7 tev collected with the alice experiment at the lhc. the analysis is carried out on a data sample corresponding to an integrated luminosity l_{int} = 16.5 nb^{-1}. the transverse momentum and rapidity differential production cross sections of muons from heavy flavour decays are measured in the rapidity range 2.5 &lt; y &lt; 4, over the transverse momentum range 2 &lt; p_{t} &lt; 12 gev/c. the results are compared to predictions based on perturbative qcd calculations.
sorption speciation of nickel(ii) onto ca-montmorillonite: batch, exafs techniques and modeling. a.
determination of ni(ii) uptake mechanisms on mordenite surfaces: a combined macroscopic and microscopic approach. a.
chemical dosimetry during alpha irradiation: a specific system for uv-vis in situ measurement. null
measurement of event background fluctuations for charged particle jet reconstruction in pb-pb collisions at $\sqrt{s_{nn}} = 2.76$ tev. the effect of event background fluctuations on charged particle jet reconstruction in pb-pb collisions at sqrt(s_nn) = 2.76 tev has been measured with the alice experiment. the main sources of non-statistical fluctuations are characterized based purely on experimental data with an unbiased method, as well as by using single high p_t particles and simulated jets embedded into real pb-pb events and reconstructed with the anti-kt jet finder. the influence of a low transverse momentum cut-off on particles used in the jet reconstruction is quantified by varying the minimum track p_t between 0.15 gev/c and 2 gev/c. for embedded jets reconstructed from charged particles with p_t &gt; 0.15 gev/c, the uncertainty in the reconstructed jet transverse momentum due to the heavy-ion background is measured to be 11.3 gev/c (standard deviation) for the 10% most central pb-pb collisions, slightly larger than the value of 11.0 gev/c measured using the unbiased method. for a higher particle transverse momentum threshold of 2 gev/c, which will generate a stronger bias towards hard fragmentation in the jet finding process, the standard deviation of the fluctuations in the reconstructed jet transverse momentum is reduced to 4.8-5.0 gev/c for the 10% most central events. a non-gaussian tail of the momentum uncertainty is observed and its impact on the reconstructed jet spectrum is evaluated for varying particle momentum thresholds, by folding the measured fluctuations with steeply falling spectra.
description of atmospheric conditions at the pierre auger observatory using the global data assimilation system (gdas). atmospheric conditions at the site of a cosmic ray observatory must be known for reconstructing observed extensive air showers. the global data assimilation system (gdas) is a global atmospheric model predicated on meteorological measurements and numerical weather predictions. gdas provides altitude-dependent profiles of the main state variables of the atmosphere like temperature, pressure, and humidity. the original data and their application to the air shower reconstruction of the pierre auger observatory are described. by comparisons with radiosonde and weather station measurements obtained on-site in malargüe and averaged monthly models, the utility of the gdas data is shown.
the neutrons for science facility at spiral‐2. the "neutrons for science" (nfs) facility will be a component of spiral‐2, the future accelerator dedicated to the production of very intense radioactive ion beams, under construction at ganil in caen (france). nfs will be composed of a pulsed neutron beam for in‐flight measurements and irradiation stations for cross‐section measurements and material studies. continuous and quasi‐monokinetic energy spectra will be available at nfs respectively produced by the interaction of deuteron beam on thick a be converter and by the 7li(p,n) reaction on a thin converter. the flux at nfs will be up to 2 orders of magnitude higher than those of other existing time‐of‐flight facilities in the 1 mev to 40 mev range. nfs will be a very powerful tool for physics and fundamental research as well as applications like the transmutation of nuclear waste, design of future fission and fusion reactors, nuclear medicine or test and development of new detectors.
indication of reactor electron antineutrinos disappearance in the double chooz experiment. the double chooz experiment presents an indication of reactor electron antineutrino disappearance consistent with neutrino oscillations. an observed-to-predicted ratio of events of 0.944±0.016(stat)±0.040(syst) was obtained in 101 days of running at the chooz nuclear power plant in france, with two 4.25gwth reactors. the results were obtained from a single 10m3 fiducial volume detector located 1050 m from the two reactor cores. the reactor antineutrino flux prediction used the bugey4 flux measurement after correction for differences in core composition. the deficit can be interpreted as an indication of a nonzero value of the still unmeasured neutrino mixing parameter sin⁡22θ13. analyzing both the rate of the prompt positrons and their energy spectrum, we find sin⁡22θ13=0.086±0.041(stat)±0.030(syst), or, at 90% c.l., 0.017.
aspectualizing component models : implementation and interferences analysis. using aop to model non-modular concerns in cbse ensures better modularity and reusability of components. in this thesis, we provide a model independent approach for modeling aspects in component models. in the approach we model aspects as wrappers on views of component systems. a view describes an adequate component system configuration where all the components of interest of an aspect are encapsulated in the same composite.for declarative definition of views, we provide a declarative language vil. we illustrate how views are implemented in component models(e.g., fractal). we provide a formal framework for aspect interferences analysis. in the framework component systems and aspects are modeled as automata and uppaal model checker is used for the detection of aspect interferences. for interferences resolution, we provide a set of composition operators as templates to be instantiated for any two arbitrary aspects. our approach is illustrated with an airport wireless access example.
surrounding effects and sensitivity of the codalema experiment. future autonomous systems of cosmic ray radiodetection will be installed over large areas, encountering various environmental and noise conditions. it is thus essential to check and evaluate the influence of the vicinity on the sensitivity of detection. in this paper, the main environmental influences on the performances of the codalema experiment are presented. it will be shown that the performances and sensitivity of the detector are not affected by the environment, and that the new codalema autonomous detection station can reach the ultimate accessible sensitivity even in a quite noisy environment. this allows deconvolving the detector's response and recovering the real spectral characteristics of the cosmic ray air showers.
radio emission modelization - observables and interpretation. null
haptic sensitivity in needle insertion: the effects of training and visual aid. this paper describes an experiment conducted to measure haptic sensitivity and the effects of haptic training with and without visual aid. the protocol for haptic training consisted of a needle insertion task using dual-layer silicon samples. a visual aid was provided as a multimodal cue for the haptic perception task. results showed that for a group of novices (subjects with no previous experience in needle insertion), training with a visual aid resulted in a longer time to task completion, and a greater applied force, during post-training tests. this suggests that haptic perception is easily overshadowed, and may be completely replaced, by visual feedback. therefore, haptic skills must be trained differently from visuomotor skills.
haptic communication for a 2-d pointing task in a virtual environment. this paper examines the properties of haptic communication between two human operators using kinesthetic haptic devices in a collaborative task in a virtual environment. twenty subjects, divided into 10 dyads, participated in a 2d pointing task. each dyad consisted of a supervisor and an acting agent. the supervisor's role was to guide the acting agent towards a target in the virtual environment through either verbal or haptic communication only. verbal communication was found to be the most efficient means of communication, but collaboration was also effective using haptic communication. several different haptic communication strategies were observed, all with equal effectiveness as measured by task completion time. these strategies followed the same pattern as the verbal strategies. these results suggest that haptic communication in a virtual environment is possible, allowing for future designs of haptically enhanced collaborative work in virtual environments.
a programming model integrating classes, events and aspects. object-oriented programming (oop) has become the de facto programming paradigm. event-based programming (ebp) and aspect-oriented programming (aop) complement oop, covering some of its deficiencies when building complex software. today's applications combine the three paradigms. however, oop, ebp and aop have not yet been properly integrated. their underlying concepts are in general provided as distinct language constructs, whereas they are not completely orthogonal. this lack of integration and orthogonality complicates the development of software as it reduces its understandability, its composability and increases the required glue code. this thesis proposes an integration of oop, ebp and aop leading to a simple and regular programming model. this model integrates the notions of class and aspect, the notions of event and join point, and the notions of piece of advice, method and event handler. it re- duces the number of language constructs while keeping expressiveness and offering additional programming options. we have designed and implemented two programming languages based on this model: ejava and ecaesarj. ejava is an extension of java implementing the model. we have validated the expressiveness of this language by implementing a well-known graphical editor, jhotdraw, reducing its glue code and improving its design. ecaesarj is an extension of caesarj that combines our model with mixins and language support for state machines. this combination was shown to greatly facilitate the implementation of a smart home application, an industrial- strength case study that aims to coordinate different devices in a house and automatize their behaviors.
search for ultrahigh energy neutrinos in highly inclined events at the pierre auger observatory. the surface detector of the pierre auger observatory is sensitive to neutrinos of all flavors above 0.1 eev. these interact through charged and neutral currents in the atmosphere giving rise to extensive air showers. when interacting deeply in the atmosphere at nearly horizontal incidence, neutrinos can be distinguished from regular hadronic cosmic rays by the broad time structure of their shower signals in the water-cherenkov detectors. in this paper we present for the first time an analysis based on down-going neutrinos. we describe the search procedure, the possible sources of background, the method to compute the exposure and the associated systematic uncertainties. no candidate neutrinos have been found in data collected from 1 january 2004 to 31 may 2010. assuming an e-2 differential energy spectrum the limit on the single-flavor neutrino is e2dn/de&lt;1.74×10-7gevcm-2s-1sr-1 at 90% c.l. in the energy range 1×1017ev.
how to deal with your it legacy? what is coming up in modisco. null
btrscript : a safe management system for virtualized data center. virtual machine management in data centers is more and more complex and this is due to the increasing total number of virtual machines. virtual machine resources and scheduled policies (e.g., consolidation) define the virtual machine placement. this placement is difficult to compute for large infrastructures. administrators maintain a correct placement by performing actions (e.g., migrate virtual machines, power off servers...) and some time using autonomic schedulers. we propose btrscript: a safe autonomic system for virtual machine management that includes actions and placement rules. actions are imperative operations to reconfigure the data center and declarative rules specify the virtual machine placement. administrators schedule both actions and rules, to manage their data center(s). they can also interact with the btrscript system in order to monitor the data center and compute the correct virtual machine placement.
[acute hemiparesis revealing a neuroborreliosis in a child]. we report on a 11-year-old boy who had 2 acute hemiparesis episodes over a period of 1 month. he suffered from headache and fatigue since 1 year. he could not remember neither a tick bite nor a local erythematous skin lesion. the diagnosis of neuroborreliosis was based on intrathecal production of specifics antibodies. furthermore, the csf/blood glucose ratio was decreased (0.14), which was rarely described. cranial mri showed left capsulothalamic inflammation and a vasculitis. the patient was successfully treated by ceftriaxone. neuroborreliosis should be considered in all children with stroke-like episode, even in the absence of a history of a tick bite.
contribution to the emergy theory : application to recycling. the continuous development of tools to measure sustainability led to the emergy theory. the emergy of a resource or product is defined by converting all resource (raw materials) and energy inputs in the form of solar energy equivalents (solar energy unit, sej), cf odum (1996, 2000). the main objective of this thesis is to adapt the method of emergy evaluation to industrial recycling practices. the principal scientific contribution from the study can be summarized as: contribution to th emergy theory in discrete time applied to recycling. under certain assumptions, the emergy of a recycled product can be expressed in the form of a geometric series. if the emergy of a product deteriorates, there is a cost to the emergy of recycling with similarities to the carnot principle. as a result, a 'factor' is introduced which could be included on emergy evaluation tables to reflect increases in transformity due to multiple recycling. fi nally, the developed approach is successfully applied to the use of recycle materials in a low energy building.
directed flow of identified particles in au + au collisions at $\sqrtsnn = 200$ gev at rhic. star's measurements of directed flow ($v_1$) around midrapidity for $\pi^{\pm}$, k$^{\pm}$, k$_s^0$, $p$ and $\bar{p}$ in au + au collisions at $\sqrtsnn = 200$ gev are presented. a negative $v_1(y)$ slope is observed for most of produced particles ($\pi^{\pm}$, k$^{\pm}$, k$_{s}^{0}$ and $\bar{p}$). the proton $v_1(y)$ slope is found to be much closer to zero compared to antiprotons. a sizable difference is seen between $v_1$ of protons and antiprotons in 5-30% central collisions. the $v_1$ excitation function is presented. comparisons to model calculations (rqmd, urqmd, ampt, qgsm with parton recombination, and a hydrodynamics model with a tilted source) are made. anti-flow alone cannot explain the centrality dependence of the difference between the $v_1(y)$ slopes of protons and antiprotons.
statistical method for the determination of the ignition energy of dust cloud-experimental validation. powdery materials such as metallic or polymer powders play a considerable role in many industrial processes. their use requires the introduction of preventive safeguard to control the plants safety. the mitigation of an explosion hazard, according to the atex 137 directive (1999/92/eu), requires, among other things, the assessment of the dust ignition sensitivity. prisme laboratory (university of orléans) has developed an experimental set-up and methodology, using the langlie test, for the quick determination of the explosion sensitivity of dusts. this method requires only 20 shots and ignition sensitivity is evaluated through the e50 (energy with an ignition probability of 0.5). a hartmann tube, with a volume of 1.3 l, was designed and built. many results on the energy ignition thresholds of partially oxidised previous termaluminiumnext term were obtained using this experimental device (baudry, 2007) and compared to literature. e50 evolution is the same as previous mie but their respective values are different and previous mie is lower than e50 however the link between e50 and previous mie has not been elucidated. in this paper, the langlie method is explained in detail for the determination of the parameters (mean value e50 and standard deviation σ) of the associated statistic law. the ignition probability versus applied energy is firstly measured for lycopodium in order to validate the method. a comparison between the normal and the lognormal law was achieved and the best fit was obtained with the lognormal law. in a second part, the langlie test was performed on different dusts such as previous aluminium, cornstarch, lycopodium, coal, and pa12 in order to determine e50 and σ for each dust. the energies e05 and e10 corresponding respectively to an ignition probability of 0.05 and 0.1 are determined with the lognormal law and compared to previous mie find in literature. e05 and e10 values of ignition energy were found to be very close and were in good agreement with previous mie in the literature.
mie and flame velocity of partially oxidised aluminium dust. this work presents experimental tools for the determination of minimum ignition energy (mie) and results concerning the influence of an initial oxidation state on ignition threshold energies and flame velocity. these studies are carried out with micrometric aluminium particles which are oxidised using an anodising process. the first part of this work concerns the description of the experimental devices (hartmann tube, for mie measurements, and constant volume combustion chamber for flame velocity measurement with using high speed recording shadowgraphy). in the second part, a review of some results obtained for the sensitivity (mie) of aluminium particle evolution versus particle diameter, air-fuel equivalence ratio and oxide content is presented. the effect of the oxide content is demonstrated: the mie increases with the initial oxide content. the sensitivity of oxidised dust remains relatively high for high oxide contents (17.1wt%). the flame velocity is also modified and decreases as the oxide content increases. the most important result seems to be the role of the water content contained in the oxide shell which increases the reactivity of the oxidised aluminium dust.
measurement of the $w \to e \nu$ and $z/\gamma^* \to e^+e^-$ production cross sections at mid-rapidity in proton-proton collisions at $\sqrt{s}$ = 500 gev. we report measurements of the charge-separated $w^{+(-)} \to e^{+(-)} + \nu_e(\bar{\nu}_e)$ and $z/\gamma^* \to e^+e^-$ production cross sections at mid-rapidity in proton-proton collisions at $\sqrt{s}$ = 500 gev. these results are based on 13.2 pb$^{-1}$ of data recorded in 2009 by the star detector at rhic. production cross sections for w bosons that decay via the $e \nu$ channel were measured to be $\sigma(pp \to w^+ x) \cdot br(w^+ \to e^+ \nu_e)$ = 117.3 \pm 5.9(stat) \pm 6.2(syst) \pm 15.2(lumi) pb, and $\sigma(pp \to w^- x) \cdot br(w^- \to e^- \bar{\nu}_e)$ = 43.3 \pm 4.6(stat) \pm 3.4(syst) \pm 5.6(lumi) pb. for $z/\gamma^*$ production, $\sigma(pp \to z/\gamma^* x) \cdot br(z/\gamma^* \to e^+ e^-)$ = 7.7 \pm 2.1(stat) $^{+0.5}_{-0.9}$(syst) \pm 1.0(lumi) pb for di-lepton invariant masses $m_{e^+e^-}$ between 70 and 110 gev/$c^2$. first measurements of the w cross section ratio, $\sigma(pp \to w^+ x) / \sigma(pp \to w^- x)$, at $\sqrt{s}$ = 500 gev are also reported. theoretical predictions, calculated using recent parton distribution functions, are found to agree with the measured cross sections.
light vector meson production in pp collisions at sqrt(s) = 7 tev. the alice experiment has measured low-mass dimuon production in pp collisions at \sqrt{s} = 7 tev in the dimuon rapidity region 2.5.
probing the microscopic nuclear matter self-organization processes in the neutron star crust. we investigate microscopic self-organization processes of nuclear matter in the outermost layers of neutron star crusts with the dywan model. in this framework a pure mean-field description of nuclear dynamics has been performed. starting from initial crystalline lattices, which are expected in the most external regions, the system organizes itself in exotic structures. the present work focuses on the effects of both the initial lattice symmetries and the nuclear species on the morphology and on the evolution of those structures. the response of the system is analyzed when it is subjected to random fluctuations of the initial lattice.
solution controls for dissolved silica at 25, 50 and 90 °c for quartz, callovo-oxfordian claystone, illite and mx80 bentonite. clay host rock and engineered barrier systems are the key elements in the concept adopted by several countries to isolate the high level nuclear waste from the biosphere. mainly composed by illite, mixed-layer illite-smectite (i/s) and montmorillonite, these clays are characterized by their properties of high retention and low permeability for released radionuclides. after closure of the repository in deep geological formation, groundwater in equilibrium with the host rock, forming the pore water, will saturate the engineered barrier system and come in contact with the nuclear glass waste package. the leaching of the different silicate minerals as well as the uptake of dissolved silicic acid by these phases is an important factor in the dissolution of the nuclear glass waste. to understand how the si interacts with clay materials, this study aims at evaluating the solubility as well as the dynamics of solid/solution exchange reactions, which control the dissolved silicic acid concentration in solution in contact with callovo-oxfordian claystone. the results were compared with the dissolution of illite, bentonite and quartz at 25, 50 and 90 °c in pore water. the experiments were conducted in batch system and in controlled atmosphere conditions and were followed in continue until the equilibrium between the solid and solution is reached. the results present a stabilization of the ph-values at 8.2 after 211 days for all the samples. the nature of solids and the temperature were not the determining factors on the ph-value but the chemical composition of the pore water and the working atmosphere (here, nitrogen). dissolution and precipitation rates were calculated from the concentration of si released from solids and the activity of 32si-radiotracer added in solution, respectively. dissolution rates were in the range of (8.7 ± 0.4) × 10−12-(6.8 ± 0.3) × 10−11 mol si/m2/s for quartz, (1.6 ± 0.1) × 10−13-(6.4 ± 0.3) × 10−13 mol si/m2/s for callovo-oxfordian claystone, (2.4 ± 0.1) × 10−13-(9.4 ± 0.5) × 10−13 mol si/m2/s for illite du puy and (1.2 ± 0.6) × 10−12-(9.1 ± 0.5) × 10−12 mol si/m2/s for bentonite. precipitation rates were in the range of (8.4 ± 0.4) × 10−12-(2.0 ± 0.1) × 10−11 mol si/m2/s for quartz, (2.0 ± 0.1) × 10−13-(2.5 ± 0.1) × 10−12 for callovo-oxfordian claystone, (2.4 ± 0.1) × 10−12-(5.2 ± 0.3) × 10−12 mol si/m2/s for illite and (1.9 ± 0.1) × 10−13-(6.9 ± 0.3) × 10−13 mol si/m2/s for mx80 bentonite. the plot of dissolution rates versus precipitation rates gave a slope near one indicating a dynamic process of dissolution/precipitation. through the experiment with the addition of a spike of 32si-radiotracer in solution, we have showed that the activity of 32si decreases in contact with clay containing at least 6% of fe and al, as for example illite, which seems to be a necessary condition. finally, in the condition of nuclear glass waste disposal, the callovo-oxfordian claystone would be the phase controlling the solubility of dissolved si from the nuclear waste and clay with a solubility value of 4.8 × 10−4 mol/l at 90 °c in pore water of bure site.
h2 production by γ and he ions water radiolysis, effect of presence tio2 nanoparticles. the effect of tio2 particles on the yield of h2 formation under water radiolysis is measured. irradiations were performed using a 60co γ−ray source as well as with he ions particles (4he2+) generated by a cyclotron with an external beam energy of 6 mev. the resulting hydrogen as a stable product of radiolysis was measured by mass spectrometry. g(h2) obtained for water radiolysis by he ions−irradiation in aerated and argon water are found to be 1.91 × 10−7 and 1.35 × 10−7 mol j−1, respectively. in the presence of titanium oxide anatase−type dispersed in water, under he ions−irradiation, g(h2) is found to increase slightly from 1.04 × 10−7 to 1.35 × 10−7 mol j−1 by increasing the specific surface from 8 to 253 m2/g, respectively. under γ-irradiation, g(h2) is found to be 0.41 × 10−7 mol j−1 close to primary yield of hydrogen in presence of oh. radical scavenger. in addition, radiolysis of water adsorbed in the titanium oxide with low water content, which corresponds to a few layers of water sorbed onto the solid surface gives a huge values of the g(h2). for the same amount of water, with using the dose absorbed by tio2 particles, for he ions-irradiation, g(h2) increases from 14.5 × 10−7 to 35 × 10−7 mol j-1 by increasing the surface area of tio2 nanoparticles from 4 to 52 m2/g, respectively. for γ−irradiation g(h2) is found to be 5.25 × 10−7 mol j-1 for the sample with 8 m2/g specific surface area.
studies of (cs,ba)-hollandite dissolution under gamma irradiation at 95 °c and at ph 2.5, 4.4 and 8.6. in the frame of the former french 1991-law on waste management, which was extended in 2006-law, hollandite ceramic was studied as a potential specific conditioning matrix for caesium isotopes (long-life radionuclide 135cs and the strong heat generating radionuclide 137cs). in this general study of cs-containment in a ceramic matrix, the chemical durability was pointed out as a key property. leaching experiments in static mode were conducted during 240 days at various ph-values from acidic to alkaline range. the initial leaching rates between 0 and 45 days are faster for cs than for ba and the average for the caesium are (1.4 ± 0.1) × 10−4 g/m2/d (ph 2.5), (6.4 ± 0.9) × 10−5 g/m2/d (ph 4.4) and (3.1 ± 0.6) × 10−5 g/m2/d (ph 8.6), and for the barium (6 ± 1) × 10−5 g/m2/d (ph 2.5), (2.8 ± 0.3) × 10−5 g/m2/d (ph 4.4), and (2 ± 2) × 10−6 g/m2/d (ph 8.6). at the equilibrium between 45 and 240 days, the normalised mass losses average for caesium are (8.2 ± 0.3) × 10−3 g/m2 (ph 2.5), (5.2 ± 0.4) × 10−3 g/m2 (ph 4.4) and (4.1 ± 0.2) × 10−3 g/m2 (ph 8.6), and for barium (3.7 ± 0.4) × 10−3 g/m2 (ph 2.5), (2.0 ± 0.1) × 10−3 g/m2 (ph 4.4) and (4 ± 2) × 10−4 g/m2 (ph 8.6). caesium and barium are incongruently released in solution with a correlation slope close to 0.5 at ph 2.5 and ph 4.4 and very low (near 0.02) in alkaline solution. sorption experiments with radioactive isotopes (137cs and 133ba) were conducted on hollandite pre-leached in aqueous solutions. caesium and barium release is controlled by the surface reactions. leaching experiments and isotopic addition experiments (137cs- and 133ba-radiotracer) indicate that caesium behaviour is independent on ph-values, whereas barium behaviour is strongly dependent. additional experiments in the presence of gamma irradiation (60co source) did not show any significant effect on hollandite leaching process.
discrepancies in thorium oxide solubility values: study of attachment/detachment processes at the solid/solution interface. the solubility of thorium under oxide and/or hydroxide forms has been extensively studied for many years. nevertheless, a large discrepancy in the solubility values is noticed in the literature. we study th atom exchange between thorium oxide surfaces and various aqueous solutions (0.01 mol*l−1 nacl for 0.0 &lt; ph &lt; 5.2) to address this issue. by solid-state characterization [x-ray photoelectron spectroscopy (xps), scanning electron microscopy, and atomic force microscopy], we determined that 80% of the xps accessible near the surface region of sintered thorium oxide is represented by the less reactive tho2(cr) grains. the remaining 20% corresponds to thox(oh)y(h2o)z, which is largely associated with grain boundaries. only the latter fraction is involved in solid/solution exchange mechanisms. local conditions (thorium concentrations, ph values, etc.) in grain boundaries lead to an adjustment of the "local solubility constraints" and explain the thorium concentration measured in our experiments. for ph &lt;5.2, the thorium concentration and ph gradient between the bulk solution and grain-boundary regions imply that the solubility values mainly depend on the availability and accessibility of thox(oh)y(h2o)z. we have performed two solubility experiments with a 232tho2(cr) solid in a 0.01 mol*l−1 nacl solution for 300 days. in a first experiment, we measured 232th concentrations in dissolution experiments in order to determine the detachment rates of th atoms from the solid surface. in a subsequent step, we added 229th to the solution in order to measure the surface attachment rate for dissolved th atoms. this allowed an assessment of the net balance of th atom exchange at the solid/solution interface. the empirical solubility data do not correspond to the thermodynamic bulk phase/solution equilibrium because measured solution concentrations are controlled by site-specific exchange mechanisms at the solid/solution interface. therefore, for sparingly soluble solids, one needs to quantify site-specific surface attachment and detachment rates if one wants to assess solubility constraints.
underlying event measurements in pp collisions at sqrt(s) = 0.9 and 7 tev with the alice experiment at the lhc. we present measurements of underlying event observables in pp collisions at sqrt(s) = 0.9 and 7 tev. the analysis is performed as a function of the highest charged-particle transverse momentum pt,lt in the event. different regions are defined with respect to the azimuthal direction of the leading (highest transverse momentum) track: toward, transverse and away. the toward and away regions collect the fragmentation products of the hardest partonic interaction. the transverse region is expected to be most sensitive to the underlying event activity. the study is performed with charged particles above three different pt thresholds: 0.15, 0.5 and 1.0 gev/c. in the transverse region we observe an increase in the multiplicity of a factor 2-3 between the lower and higher collision energies, depending on the track pt threshold considered. data are compared to pythia 6.4, pythia 8.1 and phojet. on average, all models considered underestimate the multiplicity and summed pt in the transverse region by about 10-30%.
one step purification process for no-carrier-added (64)cu produced using enriched nickel target. null
the objective sum constraint. constraint toolkits generally propose a sum constraint where a global objective variable should be equal to a sum of local objective variables, on which bound-consistency is achieved. to solve optimization problems this propagation is poor. therefore, ad-hoc techniques are designed for pruning the global objective variable by taking account of the interactions between constraints defined on local objective variables. each technique is specific to each (class of) practical problem. in this paper, we propose a new global constraint which deals with this issue in a generic way. we propose a sum constraint which exploits the propagation of a set of constraints defined on local objective variables.
inner regions and interval linearizations for global optimization. researchers from interval analysis and constraint (logic) programming communities have studied intervals for their ability to manage infinite solution sets of numerical constraint systems. in particular, inner regions represent subsets of the search space in which all points are solutions. our main contribution is the use of recent and new inner region extraction algorithms in the upper bounding phase of constrained global optimization. convexification is a major key for efficiently lower bounding the objective function. we have adapted the convex interval taylorization proposed by lin &amp; stadtherr for producing a reliable outer and inner polyhedral approximation of the solution set and a linearization of the objective function. other original ingredients are part of our optimizer, including an efficient interval constraint propagation algorithm exploiting monotonicity of functions. we end up with a new framework for reliable continuous constrained global optimization. our interval b&amp;b is implemented in the interval-based explorer ibex and extends this free c++ library. our strategy significantly outperforms the best reliable global optimizers.
the effect of the geomagnetic field on cosmic ray energy estimates and large scale anisotropy searches on data from the pierre auger observatory. we present a comprehensive study of the influence of the geomagnetic field on the energy estimation of extensive air showers with a zenith angle smaller than $60^\circ$, detected at the pierre auger observatory. the geomagnetic field induces an azimuthal modulation of the estimated energy of cosmic rays up to the ~2% level at large zenith angles. we present a method to account for this modulation of the reconstructed energy. we analyse the effect of the modulation on large scale anisotropy searches in the arrival direction distributions of cosmic rays. at a given energy, the geomagnetic effect is shown to induce a pseudo-dipolar pattern at the percent level in the declination distribution that needs to be accounted for.
nonlinear motion control of cpg-based movement with applications to a class of swimming robots. in bio-inspired robotics, use of a central pattern generator (cpg) to coordinate actuation is fairly common. the gait achieved depends on a number of cpg parameters, which can be adjusted to control the robot's motion. this paper presents an output feedback motion control framework, addressing issues encountered when dealing with this type of control problem, including partial state measurements and system uncertainty. efficacy of the presented approach is illustrated by results of numerical simulations in the case of a swimming robot.
preface (ocl 2011 proceedings). null
the mde diploma: first international postgraduate specialization in model-driven engineering. model-driven engineering (mde) is changing the way we build, operate, and maintain our software-intensive systems. several projects using mde practices are reporting signi cant improvements in quality and perfor- mance but, to be able to handle these projects, software engineers need a set of technical and interpersonal skills that are currently not widely available. the mde postgraduate diploma intends to ll this gap by o ering a full-time one year formation on mde. the course syllabus is designed to teach students how to work at a higher abstraction level by the rigorous use of (software) models as the main artifacts in all software engineering activities. contents include the conceptual framework of mde plus all techniques and tools (e.g., for de ning modeling languages, models, model transformations and so on) required to successfully complete software engineering projects following a mde approach. the organizational impact (and challenges) of adopting mde are exempli ed using real industrial experiences. this paper describes the mde diploma and the lessons that we learned after completing its first edition.
discovery, beyond the clouds - distributed and cooperative framework to manage virtual environments autonomically: a prospective study. although the use of virtual environments provided by cloud computing infrastructures is gaining consensus from the scienti c community, running applications in these environments is still far from reaching the maturity of more usual computing facilities such as clusters or grids. indeed, current solutions for managing virtual environments are mostly based on centralized approaches that barter large-scale concerns such as scalability, reliability and reactivity for simplicity. however, considering current trends about cloud infrastructures in terms of size (larger and larger) and in terms of usage (cross-federation), every large-scale concerns must be addressed as soon as possible to e ciently manage next generation of cloud computing platforms. in this work, we propose to investigate an alternative approach leveraging distributed and cooperative mechanisms to manage virtual environments autonomically (discovery). this initiative aims at overcoming the main limitations of the traditional server-centric solutions while integrating all mandatory mechanisms into a uni ed distributed framework. the system we propose to implement, relies on a peer-to-peer model where each agent can e ciently deploy, dynamically schedule and periodically checkpoint the virtual environments they manage. the article introduces the global design of the discovery proposal and gives a preliminary description of its internals.
revisiting the tree constraint. this paper revisits the tree constraint introduced in [2] which partitions the nodes of a n-nodes, m-arcs directed graph into a set of node-disjoint anti-arborescences for which only certain nodes can be tree roots. we introduce a new filtering algorithm that enforces generalized arc-consistency in o(n + m) time while the original filtering algorithm reaches o(nm) time. this result allows to tackle larger scale problems involving graph partitioning.
reusing legacy software in a self-adaptive middleware framework. software that adapts its behavior to an operational context and/or feedback from within is self-adaptive. for instance, a computer vision system to detect people may change its behavior due to change in context such as nightfall. this may entail automatic change in architecture, software com- ponents and their parameters at runtime. legacy software components do not possess this ability. therefore we ask, can legacy software be successfully cast into a self-adaptive middleware framework ? we present tekio, a self-adaptive middleware platform to dynamically compose legacy soft- ware behavior. tekio is based on dynamic component load- ing available in a java implementation of open service gate- way interface (osgi). tekio contains generic components to capture context/feedback, plan an adaptation strategy, and reconfigure domain-specific components. the domain- specific components encapsulate legacy behavior implemented possibly in native languages such as c/c++. we implement a self-adaptive vision system in tekio as a case study. we perform experiments to validate that the self-adaptive layer based on osgi has negligible effects on the performance of the legacy library namely opencv. we also demonstrate that the self-adaptive middleware can handle about 30 adap- tations in a span of 2 seconds while producing meaningful output.
a vpa-based aspect language. this thesis focuses on the development of an advanced history-based aspect language and approaches to certain related issues ranging from applications to analysis methods. the aspect language, namely vpa-based aspect language, is defined upon visibly pushdown au- tomata (vpas) [21]. this language is essentially an extension from an existing framework [47] of regular aspect languages. it features vpa-based pointcuts and provides, in particu- lar, constructors for the declarative definition of pointcuts based on regular and non-regular structures. we have also extended and developed the technique for detecting automatically potential interactions among vpa-based aspects. despite several advantages of the class of visibly pushdown automata, there has been no practical support for them available. therefore, we have realized a library called vpalib that provides the implementation of essential data structures and operations for the vpa. this library is essential to enable the construction and analysis of vpa-based aspects. for instance, we have successfully performed certain analysis for detecting interactions among aspects using this library. in order to motivate the use of vpa-based aspects, we have studied two basic kinds of distributed applications, one representing typical systems with nested login sessions, and the other representing a grid computing system over peer-to-peer network. we have shown how vpa-based aspects can be useful for the realization of certain functionalities of these typical distributed applications. thanks to their highly expressive pointcuts, another important application of vpa-based aspects is to define evolution on component-based systems, especially those with explicit component protocols. the use of aspects over component protocols, however, may break the coherence between the components of the system. we have further developed proof methods to establish the preservation of fundamental correctness properties, such as compatibility and substitutability relations between software components after the application of vpa-based aspects. finally, we have considered the use of model checking techniques to verify systems that are modified by aspects. the goal of the verification is to check whether an aspect violates the global properties of a base system or the properties of other aspects. we have chosen the approach in which we create an abstract model from the vpa model and then run a model checker that is capable of checking the abstract model against the properties. we formally define the abstraction process and demonstrate our model checking approach via examples.
cross-layer sla selection for cloud services. cloud computing is a paradigm for enabling remote, on-demand access to a set of configurable computing resources as a service. the pay-per-use model enables service providers to offer their services to customers in different quality-of-service (qos) levels. these qos parameters are used to compose some bipartisan service level agreement (sla) between a service provider and a service consumer. a main challenge for a service provider is to manage slas for its service consumers, i.e. automatically determine the appropri- ate resources required from the lower layer in order to respect the qos requirements of his consumers. this paper proposes an optimization framework driven by consumer preferences to address the sla dependencies problem across the different cloud layers as well as the need of flexibility and dynamicity required by the domain of cloud computing. our approach aims to select the optimal vertical business process designed by cross-layer cloud services, enforcing sla dependen- cies between layers. based on constraint programming (cp), our approach can take into account dynamic qos parameters in a flexible manner to compose the best vertical business process. experimental results demonstrate the flexibility and effectiveness of our approach.
study of the composition of ultra-high energy cosmic rays detected by the pierre auger observatory and analysis of the associated hadronic mechanisms. ultra high energy cosmic rays (uhecr), i.e. e &gt; 1 eev, raise many questions about their origin and constitute a challenge to modern physics. these cosmic rays entering the atmosphere dissipate their huge energy by generating a shower of secondary particles whose development is significantly different depending on the nature of the primaries. the study of the composition of uhecr is therefore a major interest both in understanding the hadronic processes which govern the evolution of showers and in identifying the sources of this radiation. given its hybrid structure and the size of its unmatched network of ground detectors, the pierre auger observatory can provide clear answers to the issues raised by uhecr. in this thesis, we are particularly interested in the muon component of air showers. first, we show how the hadronic parameters define the production of muons. then we present an original method to extract this muon component and deduce the implications on the composition of uhecr. the results of this approach suggest a transition from a heavy composition to a light one when the energy increases. finally, we address the measurement of cosmic-air cross section and present the first results derived from the pierre auger observatory data.
api2mol: automating the building of bridges between apis and model-driven engineering. context: a software artefact typically makes its functionality available through a specialized application programming interface (api) describing the set of services offered to client applications. in fact, building any software system usually involves managing a plethora of apis, which complicates the development process. in model-driven engineering (mde), where models are the key elements of any software engineering activity, this api management should take place at the model level. therefore, tools that facilitate the integration of apis and mde are clearly needed. objective: our goal is to automate the implementation of api-mde bridges for supporting both the creation of models from api objects and the generation of such api objects from models. in this sense, this paper presents the api2mol approach, which provides a declarative rule-based language to easily write mapping definitions to link api specifications and the metamodel that represents them. these definitions are then executed to convert api objects into model elements or vice versa. the approach also allows both the metamodel and the mapping to be automatically obtained from the api specification (bootstrap process). method: after implementing the api2mol engine, its correctness was validated using several apis. since apis are normally large, we then developed a tool to implement the bootstrap process, which was also validated. results: we provide a toolkit (language and bootstrap tool) for the creation of bridges between apis and mde. the current implementation focuses on java apis, although its adaptation to other statically typed object-oriented languages is straightforward. the correctness, expressiveness and completeness of the approach have been validated with the swing, swt and jtwitter apis. conclusion: api2mol frees developers from having to manually implement the tasks of obtaining models from api objects and generating such objects from models. this helps to manage api models in mde-based solutions.
measurements of inelastic neutron scattering at 96 mev from carbon, iron, yttrium and lead. inelastic neutron scattering for (12)c, (58)fe, (89)y and (208)pb have been measured at 96 mev at the the svedberg laboratory in uppsala and double-differential cross sections are reported. data cover an excitation energy range of 0-45 mev and the angular intervals are 28 - 58 degrees for (12)c, 26 - 65 degrees for (58)fe and 26 - 52 degrees for (89)y and (208)pb. in this experiment, neutron detection is based on conversion to protons in an active scintillator converter. an analysis technique in which the neutron spectra have been obtained through a folding procedure using the response of the detector system has been used. the results are compared to and are in reasonable agreement with several model predictions and with inelastic neutron scattering data at 65 mev from university of california, davis, usa.
towards a general composition semantics for rule-based model transformation. as model transformations have become an integral part of the automated software engineering lifecycle, reuse, modularisation, and composition of model transformations becomes important. one way to compose model transformations is to compose modules of transformation rules, and execute the composition as one transformation (internal composition). this kind of composition can provide richer semantics, as it is part of the transformation language. this paper aims to generalise two internal composition mechanisms for rule-based transformation languages, module import and rule inheritance, by providing executable semantics for the composition mechanisms within a virtual machine. the generality of the virtual machine is demonstrated for different rule-based transformation languages by compiling those languages to, and executing them on this virtual machine. we will discuss how atl and graph transformations can be mapped to modules and rules inside the virtual machine.
self-management of applications qos for energy optimization in datacenters. as a direct consequence of the increasing popularity of cloud computing solutions, data centers are amazingly growing and hence have to urgently face with the energy consumption issue. available solutions rely on cloud computing models and virtualization techniques to scale up/down application based on their performance metrics. although those proposals can reduce the energy footprint of applications and by transitivity of cloud infrastructures, they do not consider the internal characteristics of applications to finely de fine a trade-off between applications quality of service and energy footprint. in this paper, we propose a self-adaptation approach that considers both application internals and system to reduce the energy footprint in cloud infrastructure. each application and the infrastructure are equipped with their own control loop, which allows them to autonomously optimize their executions. simulations show that the approach may lead to appreciable energy savings without interfering on application provider revenues.
alendronate-doped apatitic cements as a potential technology for the prevention of osteoporotic hip fractures. null
an open source-based approach for industrializing research tools. from a business perspective, software engineering is an enormous market which is always evolving and progressing from both economical and technical sides. such a market requires constant adaptation, which is often made possible via the regular adoption of significant innovations. thus, research labs, as priority innovation providers, are key actors of this market. unfortunately industrialization of research works is a very challenging process, thus (too) few research prototypes end up as successful and popular largely used products. based on our concrete experience and different feedback collected in this area, we have proposed a pragmatic business model for transforming the results of scientific experimentation into practical industrial solutions. the two key elements in this collaborative model are: 1) the introduction of a third entity, a technology provider, as an interface between the two classic partners in such a process (i.e. the research lab and the big company or community playing the role of the actual end user); 2) the fact that we rely on the use of open source and its ecosystem (i.e. the eclipse foundation) to largely facilitate the communication and exchanges between all the involved partners. we will illustrate our business model by describing our successful collaboration experiences within the context of the atl and modisco eclipse "modeling" projects.
virtual emf - transparent composition, weaving and linking of models. when using the eclipse modeling framework (emf), one frequently faces the problem of having to deal with several large heterogeneous and interrelated models. the information relevant for a specific user at a given time is often scattered across those models. therefore, we often have the need for composing, weaving or simply linking (parts of) these models in order to provide a more unified and usable view of the modeled system(s). with the currently available technologies, this is not a trivial task. ideally, we would like to have a kind of virtual emf resource offering a centralized and transparent access point to a global view on these different interconnected models. it should be implemented in a way such that: 1) the virtual emf resource behaves as a normal model, so interoperability (compatibility with existing emf-based solutions/tools) is guaranteed; 2) there is a perfect synchronization between the composed view (virtual resource) and the original models; 3) performance is not an issue, because neither creating nor accessing the global view results in additional costs (loading time, memory usage, etc.). as a solution, this talk introduces the brand new virtual emf tool which enables users to efficiently access, handle and combine a set of interrelated emf models in a completely transparent way.
j/psi polarization in pp collisions at sqrt(s)=7 tev. the alice collaboration has studied j/psi production in pp collisions at sqrt(s)=7 tev at the lhc through its muon pair decay. the polar and azimuthal angle distributions of the decay muons were measured, and results on the j/psi polarization parameters lambda_theta and lambda_phi were obtained. the study was performed in the kinematic region 2.5.
dark matter results from 100 live days of xenon100 data. we present results from the direct search for dark matter with the xenon100 detector, installed underground at the laboratori nazionali del gran sasso of infn, italy. xenon100 is a two-phase time-projection chamber with a 62 kg liquid xenon target. interaction vertex reconstruction in three dimensions with millimeter precision allows the selection of only the innermost 48 kg as the ultralow background fiducial target. in 100.9 live days of data, acquired between january and june 2010, no evidence for dark matter is found. three candidate events were observed in the signal region with an expected background of (1.8±0.6) events. this leads to the most stringent limit on dark matter interactions today, excluding spin-independent elastic weakly interacting massive particle (wimp) nucleon scattering cross sections above 7.0×10-45  cm2 for a wimp mass of 50  gev/c2 at 90% confidence level.
implications on inelastic dark matter from 100 live days of xenon100 data. the xenon100 experiment has completed a dark matter search with 100.9 live days of data, taken from january to june 2010. events with energies between 8.4 and 44.6  kevnr in a fiducial volume containing 48 kg of liquid xenon have been analyzed. a total of three events have been found in the predefined signal region, compatible with the background prediction of (1.8±0.6) events. based on this analysis we present limits on the wimp-nucleon cross section for inelastic dark matter. with the present data we are able to rule out the explanation for the observed dama/libra modulation as being due to inelastic dark matter scattering off iodine, at a 90% confidence level.
shear and bulk viscosities of the gluon plasma in a quasiparticle description. shear and bulk viscosities of deconfined gluonic matter are investigated within an effective kinetic theory by describing the strongly interacting medium phenomenologically in terms of quasiparticle excitations with medium-dependent self-energies. we show that the resulting transport coefficients reproduce the parametric dependencies on temperature and coupling obtained in perturbative qcd at large temperatures and small running coupling. the extrapolation into the nonperturbative regime results in a decreasing specific shear viscosity with decreasing temperature, exhibiting a minimum in the vicinity of the deconfinement transition, while the specific bulk viscosity is sizable in this region, falling off rapidly with increasing temperature. the temperature dependence of specific shear and bulk viscosities found within this quasiparticle description of the pure gluon plasma is in agreement with available lattice qcd results.shear and bulk viscosities of deconfined gluonic matter are investigated within an effective kinetic theory by describing the strongly interacting medium phenomenologically in terms of quasiparticle excitations with medium-dependent self-energies. we show that the resulting transport coefficients reproduce the parametric dependencies on temperature and coupling obtained in perturbative qcd at large temperatures and small running coupling. the extrapolation into the nonperturbative regime results in a decreasing specific shear viscosity with decreasing temperature, exhibiting a minimum in the vicinity of the deconfinement transition, while the specific bulk viscosity is sizable in this region, falling off rapidly with increasing temperature. the temperature dependence of specific shear and bulk viscosities found within this quasiparticle description of the pure gluon plasma is in agreement with available lattice qcd results.
measurement of charm production at central rapidity in proton-proton collisions at sqrt(s) = 7 tev. the pt-differential inclusive production cross sections of the prompt charmed mesons d0, d+, and d*+ in the rapidity range |y|&lt;0.5 were measured in proton-proton collisions at sqrt(s) = 7 tev at the lhc using the alice detector. reconstructing the decays d0-&gt;k-pi+, d+-&gt;k-pi+pi+, d*+-&gt;d0pi+, and their charge conjugates, about 8,400 d0, 2,900 d+, and 2,600 d*+ mesons with 1.
aspects preserving properties. aspect oriented programming can arbitrarily distort the semantics of programs. in particular, weaving can invalidate crucial safety and liveness properties of the base program. in this article, we identify categories of aspects that preserve some classes of properties. specialized aspect languages can be then designed to ensure that aspects belong to a specific category and therefore that woven programs will preserve the corresponding properties. our categories of aspects, inspired by katz's, comprise observers, aborters and confiners. observers introduce new instructions and a new local state but they do not modify the base program's state and control-flow. aborters are observers which may also abort executions. confiners only ensure that executions remain in the reachable states of the base program. % these categories (along with three other) are defined precisely based on a language independent abstract semantics framework. the classes of preserved properties are defined as subsets of ltl for deterministic programs and ctl* for non-deterministic ones. we can formally prove that, for any program, the weaving of any aspect in a category preserves any property in the related class. we present, for most aspect categories, a specialized aspect language which ensures that any aspect written in that language belongs to the corresponding category. it can be proved that these languages preserve the corresponding classes of properties by construction. the aspect languages share the same expressive pointcut language and are designed \wrt a common imperative base language. each category and language are illustrated by simple examples. the appendix provides semantics and examples of proofs: the proof of preservation of properties by a category and the proof that all aspects written in a language belong to the corresponding category.
extended h2 controller synthesis for continuous descriptor systems. this paper presents a complete solution to the nonstandard h2 output feedback control problem for continuous descriptor systems where unstable and nonproper weighting functions are used. in such a problem, the desired controller has to satisfy two conditions simultaneously: (i) the closed-loop is admissible and has a minimum h2 norm, (ii) only the internal stability of a part of the closed-loop is sought. the condition of the existence of such a controller is deduced. an explicit characterization of the optimal solution is also formulated, based on two generalized algebraic riccati equations (gares) and two generalized sylvester equations. a numerical example is included to illustrate the validity of the proposed results.
bilayer graphene inclusions in rotational-stacked multilayer epitaxial graphene. additional component in multi-layer epitaxial graphene grown on the c-terminated surface of sic, which exhibits the characteristic electronic properties of a ab-stacked graphene bilayer, is identified in magneto-optical response of this material. we show that these inclusions represent a well-defined platform for accurate magneto-spectroscopy of unperturbed graphene bilayers.
clusterization in the shape isomers of the 56ni nucleus. the interrelation of the quadrupole deformation and clusterization is investigated in the example of the 56ni nucleus. the shape isomers, including superdeformed and hyperdeformed states, are obtained as stability regions of the quasidynamical u(3) symmetry based on a nilsson calculation. their possible binary clusterizations are investigated by considering both the consequences of the pauli exclusion principle and the energetic preference.
assault frequency and preformation probability of the alpha emission process. a study of the assault frequency and preformation factor of the α-decay description is performed from the experimental α-decay constant and the penetration probabilities calculated from the generalized liquid-drop model (gldm) potential barriers. to determine the assault frequency a quantum-mechanical method using a harmonic oscillator is introduced and leads to values of around 1021 s−1, similar to the ones calculated within the classical method. the preformation probability is around 10−1-10−2. the results for even-even po isotopes are discussed for illustration. while the assault frequency presents only a shallow minimum in the vicinity of the magic neutron number 126, the preformation factor and mainly the penetrability probability diminish strongly around n=126.
neutron production in neutron-induced reactions at 96 mev on 56fe and 208pb. null
search for signatures of magnetically-induced alignment in the arrival directions measured by the pierre auger observatory. we present the results of an analysis of data recorded at the pierre auger observatory in which we search for groups of directionally-aligned events (or 'multiplets') which exhibit a correlation between arrival direction and the inverse of the energy. these signatures are expected from sets of events coming from the same source after having been deflected by intervening coherent magnetic fields. the observation of several events from the same source would open the possibility to accurately reconstruct the position of the source and also measure the integral of the component of the magnetic field orthogonal to the trajectory of the cosmic rays. we describe the largest multiplets found and compute the probability that they appeared by chance from an isotropic distribution. we find no statistically significant evidence for the presence of multiplets arising from magnetic deflections in the present data.
robust optimization of inventory routing for bulk gas distribution. we address the 'rich' (i.e., with real-world features and constraints) inventory routing problem for bulk gas distribution under uncertainty. we consider that the uncertainty occurs on the supply side and consists of outages at the production plant. we propose a general methodology for generating, classifying and selecting 'robust' solutions: solutions that are less impacted when uncertain events occur such asplant outages. this methodology is applied to real data provided by the air liquide company in the context of bulk gas distribution, and we show that for a relatively small increase in cost, the robustness of routes and schedules for the bulk gas distribution with regard to possible plant outages is improved. results show that we can reduce the extra cost induced by plant outage, while only slightlyincreasing the cost in the cases where no outages occur.
environmental impact assessment of urban mobility plan : a methodology including socio-economic consequences. the project objective is to develop an optimized methodology to assess the environmental impacts of urban mobility plans (ump, in french pdu),taking into account their social and economic consequences. the main proposed methodology is based on a systemic approach : multi-factor (air quality, noise, energy consumption, greenhouse gas emission) numerical simulations with a chain of physically-based models, based on alternative and comparative scenarios. the social and economic consequences of these alternative simulations are assessed by means of econometric models. two alternative approaches are explored: (i) the use of composite environmental indicators to correlate the sources to the impacts, especially health impacts, and (ii) the analysis of sample surveys on what makes inhabitants'quality of life, well-being and territorial satisfaction and on citizens'behavioral changes linked to transport offer changes.
design of structured control laws (hierarchical, decentralized) applied to powertrains. a conventional engine control unit (ecu) consists of two levels, called powertrain and engine levels. historically, the design of this device is based on an organic approach. thus, developments concerning gasoline and diesel control are today decoupled. based on a functional approach, the aim of this thesis is to propose a generic architecture and a cross level consistency that could be used with diesel, gasoline or hybrid structures, this reduces costs and design time. achieving this aim, we propose a new ecu hierarchical architecture consistent with control schemes and based on optimal and predictive/preview control. moreover, we consider a simplified torque and cruise control problems to illustrate the feasibility and the relevance of the proposition.
optimal resurfacing decisions for road maintenance : a pomdp perspective. we develop an optimal maintenance policy for a road section to minimize the total maintenance cost over the infinite horizon when some deterioration and decision parameters are not observable. both perfect and imperfect maintenance actions are possible through the application of various thicknesses of resurfacing layers. we use a two-phase deterioration process based on two parameters: the longitudinal cracking percentage and the deterioration growth rate. our deterioration model is a state-based model based on the state-dependent gamma process for the longitudinal cracking percentage and the bilateral gamma process for the deterioration growth rate. moreover the maintenance decision is constrained by a maximum road thickness that makes the maintenance decisions more complex as it becomes how much surface layer to add as well as to remove. because only one of the two deterioration parameters is observable, we formulate the problem as a partially observed markov decision process and solve it using a grid-based value iteration algorithm. numerical examples have shown that our model provides a preventive maintenance policy that slows down the initiation as well as the propagation of longitudinal cracks and that may ameliorate the road state to a better than as-good-as-new one by altering its composition through additive resurfacing layers.
optimizing road milling and resurfacing actions. a condition-based maintenance optimization approach is developed for the road-cracking problem in order to derive optimal maintenance policies that minimize a total discounted maintenance cost. the approach is based on a markov decision process that takes into ac- count multiple actions with varying effects on future road performance. maintaining the road consists of adding a new asphalt layer; however, as resurfacing actions are constrained by a maximum total road thickness, the maintenance decision is not only how thick a layer to apply, but also how much old road to remove. each combination of these actions leads to different maintenance costs and different future degradation behaviours. the road state is modelled by a dependent bivariate deterioration variable (the longitudinal cracking percentage and the deterioration growth rate), for taking these diﬀerent changes in the cracking patterns into account. moreover, the sensitivity to cracking for existing roads can be reduced with the addition of new layers, and thus actions that can lead to states better than good-as-new have to be considered. a numerical analysis is provided to illustrate the benefits of the introduction of the deterioration speed in the decision framework, as well as the belief that initially building a road to its maximum thickness is not optimal. the trade-oﬀs in the design decisions and the exploitation/maintenance costs are also explored.
j/psi production at forward rapidity in pb-pb collisions at sqrt(s_nn) = 2.76 tev, measured with the alice detector. in the alice experiment, at forward rapidity (2.5 &lt; y &lt; 4), the production of heavy quarkonium states is measured via their mu+ mu- decay channels. we present the first measurement of inclusive j/psi production, down to pt = 0, from pb-pb data collected at the lhc at sqrt(s_nn) = 2.76 tev. preliminary results on the nuclear modification factor (r_aa) and the central to peripheral nuclear modification factor (r_cp) show j/psi suppression with no significant centrality dependence and an integrated r_aa(0-80%) = 0.49 \pm 0.03(stat.) \pm 0.11(syst.).
multi-physics model of an electric fish-like robot : numerical aspects and application to obstacle avoidance. the paper deals with the modeling of a fish- like robot equipped with the electric sense, suited to study sensorimotor loops. the proposed multi-physics model merges a swimming dynamic model of a fish-like robot with an electric model of an embedded electrolocation sensor. based on a tcp- ip and threaded framework, the resulting simulator works in real time. after presenting the modeling aspects of this work, this article focuses on two numerical studies. in the first, the in- teractions between body deformations and perception variables are studied and a current correction process is proposed. in the second study, an electric exteroceptive feedback loop based on a direct current measurement method is designed and tested for obstacle avoidance.
modelling and control of flexible manipulators. null
recursive inverse dynamics of multibody systems with joints and wheels. null
three-dimensional extension of lighthill's large-amplitude elongated-body theory of fish locomotion . null
geometreically exact kirchhoff beam theory : application to cable dynamics. null
poincaré-cosserat equations for lighthill three-dimensional dynamic model of a self propelled eel devoted to robotics. in this article, we propose a dynamic model of the three-dimensional eel swim. this model is analytical and suited to the on-line control of eel-like robots. the proposed solution is based on the large amplitude elongated body theory of lighthill and a working frame recently proposed in [1] for the dynamic modeling of hyper-redundant robots. this working frame was named "macro-continuous" since at this macroscopic scale, the robot (or the animal) is considered as a cosserat beam internally (and continuously) actuated. this article proposes new results in two directions. firstly, it achieves an extension of the lighthill theory to the case of a self propelled body swimming in three dimensions, while including a model of the internal control torque. secondly, this generalization of the lighthill model is achieved due to a new set of equations which is also derived in this article. these equations generalize the poincaré equations of a cosserat beam to the case of an open system containing a fluid stratified around the slender beam.
fast dynamics of a three dimensional eel-like robot: comparisons with navier-stokes simulations. this article proposes a dynamic model of the swim of elongated ﰣshes suited to the on-line control of bio-mimetic eel-like robots. the approach is analytic and can be considered as an extension of the original reactive "large-elongated-body-theory" of lighthill to the three dimensional self propulsion augmented of a resistive empirical model. while all the mathematical fundamentals are detailed in [1], this article essentially focuses on the numerical validation and calibration of the model and the study of swimming gaits. the proposed model is coupled to an algorithm allowing us to compute the motion of the ﰣsh head and the ﰣeld of internal control torque from the knowledge of the imposed internal strain ﰣelds. based on the newton-euler formalism of robots dynamics, this algorithm works faster than real time. as far as precision is concerned, many tests obtained with several planar and three dimensional gaits are reported and compared (in the planar case) with a navier-stokes solver, devoted until today to the planar swim. the comparisons obtained are very encouraging since in all the cases we tested, the diﰢerences between our simpliﰣed and reference simulations do not exceed ten per cent.
dynamic modeling and simulation of a 3-d serial eel-like robot. null
further results on the controllability of the satellite with two rotors: open-loop control and path planning. null
macro-continuous computed torque algorithm for a three-dimensional eel-like robot. this paper presents the dynamic modeling of a continuous three-dimensional swimming eel-like robot. the modeling approach is based on the "geometrically exact beam theory" and on that of newton-euler, as it is well known within the robotics community. the proposed algorithm allows us to compute the robot's galilean movement and the control torques as a function of the expected internal deformation of the eel's body.
fast motions in biomechanics and robotics". springer, chapitre: "re-injecting the structure in nmpc schemes: application to the constrained stabilization of a snakeboard. null
a moving cable simulator for robotics. null
finite element of slender beams in finite transformations a geometrically exact approach. null
fast generation of attractive trajectories for an under-actuated satellite, application to feedback control design. null
flexible links manipulators : from modelling to control. null
flexible multibody dynamics based on nonlinear euler-bernoulli kinematics. null
kinematic model of a multi-beam structure undergoing large elastic displacement and rotations. part two: kinematic model of an open chain. null
kinematic model of a multi-beam structure undergoing large elastic displacement and rotations. part one: model of an isolated beam. null
simulation of an open chain of flexible links in a mixed formalism. null
the pierre auger observatory iv: operation and monitoring. technical reports on operations and monitoring of the pierre auger observatory.
the pierre auger observatory iii: other astrophysical observations. astrophysical observations of ultra-high-energy cosmic rays with the pierre auger observatory.
the pierre auger observatory ii: studies of cosmic ray composition and hadronic interaction models. studies of the composition of the highest energy cosmic rays with the pierre auger observatory, including examination of hadronic physics effects on the structure of extensive air showers.
spectroscopy of $^{18}$na: bridging the two-proton radioactivity of $^{19}$mg. the unbound nucleus $^{18}$na, the intermediate nucleus in the two-proton radioactivity of $^{19}$mg, was studied by the measurement of the resonant elastic scattering reaction $^{17}$ne(p,$^{17}$ne)p performed at 4 a.mev. spectroscopic properties of the low-lying states were obtained in a r-matrix analysis of the excitation function. using these new results, we show that the lifetime of the $^{19}$mg radioactivity can be understood assuming a sequential emission of two protons via low energy tails of $^{18}$na resonances.
modelling a maintenance scheduling problem with alternative resources. effective management of maintenance in buildings can have a signi cant impact on the total life cycle costs and on the building energy use. nevertheless, the building maintenance scheduling problem has been infrequently studied. in this paper, we present constraint-based scheduling models for the building maintenance scheduling problem, where each activity has a set of alternative resources. we consider two di erent models, one using basic constraints, and the other using our new and modi fied global constraints, which handle alternative disjunctive resources for each activity to allow propagation before activities are assigned to resources. we evaluate these models on randomly generated problems and show that while the basic model is faster on smaller problems, the global con- straint model scales better.
modeling the visual and motor control of steering with an eye to shared-control automation. null
closing the open shop: contradicting conventional wisdom. this paper describes a new approach for solving disjunctive temporal problems such as the open shop and job shop scheduling domains. much previous research in systematic search approaches for these problems has focused on developing problem specific constraint propagators and ordering heuristics. indeed, the common belief is that many of these problems are too difficult to solve without such domain specific models. we introduce a simple constraint model that combines a generic adaptive heuristic with naive propagation, and show that it often outperforms state-of-the-art solvers for both open shop and job shop problems.
shop and batch scheduling with constraints. solving a scheduling problem consists of organizing a set of tasks, that is assigning their starting and ending times and allocating resources such that all constraints are satisfied. in this thesis, we propose new constraint programming approaches for two categories of np-hard scheduling problems which are validated experimentally by the implementation of a set of new features within the constraint solver choco. in shop scheduling, a set of n jobs, consisting each of m tasks, must be processed on m distinct machines. a machine can process only one task at a time. the processing orders of tasks which belong to a job can vary (global order, order per job, no order). we consider the construction of non-preemptive schedules of minimal makespan. we first propose a study and a classification of different constraint models and search algorithms. then, we introduce a new flexible approach for these classical problems. a batch processing machine can process several jobs simultaneously as a batch. the starting and ending times of a task are the ones of the batch to which they belong. the studied problem consists of minimizing the maximal lateness for a batch processing machine on which a finite number of tasks of non-identical sizes must be scheduled. the sum of the sizes of the jobs that are in a batch should not exceed the capacity b of the machine. we propose, within this context, a constraint model based on a decomposition of the problem. then, we define a new optimization constraint based on the resolution of a relaxed problem enhanced by cost-based domain filtering techniques which improves the resolution.
spectral ratio: an observable to determine $k^{+}$ nucleus potential and $k^{+}$ n scattering cross section. here we aim to show that the ratio of the momentum spectra of $k^{+}$ at small transverse momentum measured for symmetric systems of different sizes can be such an observable.
$k^{+}$ and $k^{-}$ potentials in hadronic matter can be observed. we aim to show that k+ and k- spectra at low transverse momentum measured in light symmetric systems at around 2agev depend strongly on the k potential. the ratio of the spectra can allow therefore for a direct determination of the strength of the k+ as well as that of the k- potential in a hadronic environment.
in-medium effects on $k^{+}$ and $k^{-}$ spectra in lighter systems. we aim to explore the in-medium effects on the transverse momentum ($p_{t}$) spectra of $k^{+}$ and $k^{-}$ in lighter mass system $^{12}c+^{12}c$.
identified hadron compositions in p+p and au+au collisions at high transverse momenta at $\sqrt{s_{_{nn}}} = 200$ gev. we report transverse momentum ($p_{t} \leq15$ gev/$c$) spectra of $\pi^{\pm}$, $k^{\pm}$, $p$, $\bar{p}$, $k_{s}^{0}$, and $\rho^{0}$ at mid-rapidity in p+p and au+au collisions at $\sqrt{s_{_{nn}}}$ = 200 gev. perturbative qcd calculations are consistent with $\pi^{\pm}$ spectra in p+p collisions but do not reproduce $k$ and $p(\bar{p})$ spectra. the observed decreasing antiparticle-to-particle ratios with increasing $p_t$ provide experimental evidence for varying quark and gluon jet contributions to high-$p_t$ hadron yields. the relative hadron abundances in au+au at $p_{t}{}^{&gt;}_{\sim}8$ gev/$c$ are measured to be similar to the p+p results, despite the expected casimir effect for parton energy loss.
rotating hyperdeformed states in light nuclear systems. the existence of rotating quasimolecular hyperdeformed states formed in the entrance channel of capture reactions of light nuclei is predicted within a rotational liquid-drop model, including the nuclear proximity energy. the l-dependent capture barrier heights and positions, as well as the angular momentum, the energy, and the moment of inertia ranges of these very deformed high-spin states, are given for the reactions 13c + 13c, 16o + 16o, 28si + 12c, 28si + 16o, 24mg + 24mg, 28si + 24mg, 28si + 28si, 28si + 40ca, 40ca + 40ca, 40ca + 48ca, 48ca + 48ca, and 58ni + 58ni. analytical formulas are provided for any reaction between light nuclei.
azimuthal correlations of electrons from heavy-flavor decay with hadrons in p+p and au+au collisions at sqrt(s_nn)=200 gev. measurements of electrons from the decay of open-heavy-flavor mesons have shown that the yields are suppressed in au+au collisions compared to expectations from binary scaled p+p collisions. these measurements indicate that charm and bottom quarks interact with the hot-dense matter produced in heavy-ion collisions much more than expected. here we extend these studies to two-particle correlations where one particle is an electron from the decay of a heavy-flavor meson and the other is a charged hadron from either the decay of the heavy meson or from jet fragmentation. these measurements provide more detailed information about the interactions between heavy quarks and the matter, such as whether the modification of the away-side-jet shape seen in hadron-hadron correlations is present when the trigger particle is from heavy-meson decay and whether the overall level of away-side jet suppression is consistent. we statistically subtract correlations of electrons arising from background sources from the inclusive electron-hadron correlations and obtain two-particle azimuthal correlations at sqrt(s_nn)=$200 gev between electrons from heavy-flavor decay with charged hadrons in p+p and also first results in au+au collisions. we find the away-side-jet shape and yield to be modified in au+au collisions compared to p+p collisions.
an efficient calculation of flexible manipulator inverse dynamics. null
symbolic modelling of a flexible manipulator via assembling of its generalized newton-euler model. null
generalisation of newton-euler model for flexible manipulators. null
particle-yield modification in jet-like azimuthal di-hadron correlations in pb-pb collisions at $\sqrt(s_nn)$ = 2.76 tev. the yield of charged particles associated with high-pt trigger particles (8 &lt; pt &lt; 15 gev/c) is measured with the alice detector in pb-pb collisions at sqrt(s_nn) = 2.76 tev relative to proton-proton collisions at the same energy. the conditional per-trigger yields are extracted from the narrow jet-like correlation peaks in azimuthal di-hadron correlations. in the 5% most central collisions, we observe that the yield of associated charged particles with transverse momenta pt &gt; 3 gev/c on the away-side drops to about 60% of that observed in pp collisions, while on the near-side a moderate enhancement of 20-30% is found.
transport coefficients in gluodynamics: from weak coupling towards the deconfinement transition. we study the ratio of bulk to shear viscosity in gluodynamics within a phenomenological quasiparticle model. we show that at large temperatures this ratio exhibits a quadratic dependence on the conformality measure as known from weak coupling perturbative qcd. in the region of the deconfinement transition, however, this dependence becomes linear as known from specific strongly coupled theories. the onset of the strong coupling behavior is located near the maximum of the scaled interaction measure. this qualitative behavior of the viscosity ratio is rather insensitive to details of the equation of state.
"disrupted in renal carcinoma 2" (dirc2) - a novel transporter of the lysosomal membrane - is proteolytically processed by cathepsin l. "disrupted in renal carcinoma 2" (dirc2) has been initially identified as a breakpoint spanning gene in a chromosomal translocation putatively associated with the development of renal cancer. the dirc2 protein belongs to the major facilitator superfamily (mfs) and has been previously detected by organellar proteomics as a tentative constituent of lysosomal membranes. in the present study, lysosomal residence of overexpressed as well as endogenous dirc2 was shown by several approaches. dirc2 is proteolytically processed into a n-glycosylated n-terminal and a non-glycosylated c-terminal fragment, respectively. proteolytic cleavage occurs in lysosomal compartments and critically depends on the activity of cathepsin l which was found to be indispensable for this process in murine embryonic fibroblasts. the cleavage site within dirc2 was mapped between amino acid residues 214 and 261 using internal epitope tags and is presumably located within the tentative fifth intralysosomal loop assuming the typical mfs topology. lysosomal targeting of dirc2 was demonstrated to be mediated by a n-terminal dileucine motif. by disrupting this motif, dirc2 can be redirected to the plasma membrane. finally, in a whole-cell electrophysiological assay based on heterologous expression of the targeting mutant at the plasma membrane of xenopus oocytes, the application of a complex metabolic mixture evokes an outward current associated with the surface expression of full-length dirc2. taken together, these data strongly support the idea that dirc2 is an electrogenic lysosomal metabolite transporter which is subjected to and presumably modulated by limited proteolytic processing.
a new experimental setup for a high performance double electropneumatic actuators system. this paper presents design, modelization and control of a new electropneumatic test bench. this latter has been designed for many applications given that it allows high accurancy control and dynamic perturbation force. in fact, the main originality (with respect to previous test benchs) of this test bench is that it is composed by two actuators, the first one being controlled in position, the second one generating perturbation forces. this latter one allows to evaluate the performance of control laws with respect to dynamical forces.
estimation of absolute orientation for a bipedal robot: experimental results. this paper deals with a planar biped. the aim is the estimation, during the imbalance phases of a walking cyclic gait, of its absolute orientation by only using the measurement of the actuated joint variables. the main contribution is the experimental evaluation of an original finite-time convergent posture observer.
exploring membranes for controlling aspects. in most aspect-oriented languages, aspects have an unrestricted global view of computation. several approaches for aspect scoping and more strongly encapsulated modules have been formulated to restrict this controversial power of aspects. this paper proposes to leverage the concept of programmable membranes developed by boudol, schmitt and stefani, as a means to tame aspects by customizing the semantics of aspect weaving locally. membranes subsume previous proposals in a uniform framework. because membranes give structure to computation, they enable ﬂexible scoping of aspects; because they are programmable, they make it possible to deﬁne visibility and safety constraints, both for the advised program and for the aspects. we ﬁrst de- scribe membranes for aop without committing to any speciﬁc language design. in addition, we then illustrate an extension of aspectscheme with membranes, and explore the instantiation of programmable membranes in the kell calculus. the power and simplicity of membranes open interesting perspectives to unify multiple approaches that tackle the unrestricted power of aspect-oriented programming.
measurement of the neutrino velocity with the opera detector in the cngs beam. the opera neutrino experiment at the underground gran sasso laboratory has measured the velocity of neutrinos from the cern cngs beam over a baseline of about 730 km with much higher accuracy than previous studies conducted with accelerator neutrinos. the measurement is based on high-statistics data taken by opera in the years 2009, 2010 and 2011. dedicated upgrades of the cngs timing system and of the opera detector, as well as a high precision geodesy campaign for the measurement of the neutrino baseline, allowed reaching comparable systematic and statistical accuracies. an early arrival time of cngs muon neutrinos with respect to the one computed assuming the speed of light in vacuum of (60.7 \pm 6.9 (stat.) \pm 7.4 (sys.)) ns was measured. this anomaly corresponds to a relative difference of the muon neutrino velocity with respect to the speed of light (v-c)/c = (2.48 \pm 0.28 (stat.) \pm 0.30 (sys.)) \times 10-5.
design and optimization of an hybrid sailboat by a power modeling approach. in this paper an original modeling approach is proposed. this approach is based on power exchanges and highlights the most influential parameters of a complex system, without having to choose particular technological solutions. this is thus a particularly well suited solution for a first and global design or for the validation of a concept. this approach has been efficiently used for the optimal sizing and control of a hybrid sailboat which comprises a thermal engine and two electric motors in serie configuration associated with sails. the control strategy is organized in two levels: a global one, which defines power objectives, and a local one, which maximizes the efficiency, while guaranteeing the security of each component.
a multivariable centralized controller design methodology for a steer-by-wire system. this paper presents a multivariable centralized control law for a steer-by-wire system. the controller is designed by solving an h2 problem leaning on a model-matching approach. a particular steering column model is used to define the expected performances for the steer-by-wire system. a driver-vehicle model is used to evaluate the behaviour of the steering system (through the h2 criterion) in a realistic way . this make the approach proposed original and the resulting steer-by-wire system robust.
power modeling for the optimization of a marine hybrid propulsion. in this paper an original modeling approach is proposed. this approach is based on power exchanges and highlights the most influential parameters of a complex system, without having to choose particular technological solutions. this is thus a perfect solution for a first and global design or for the validation of a concept. this approach has been efficiently employed for the optimal sizing and control of a marine hybrid propulsion which comprises a thermal engine and two electric motors in series configuration associated with sails. the control strategy is organized in two levels: a global one, which defines power objectives, and a local one which maximizes the efficiency and guarantees the component security.
parametrization of extended stabilizing controllers for continuous-time descriptor systems. this paper investigates the extended stabilization control problem for continuous-time descriptor systems (also refereed to as singular systems, implicit systems or generalized state-space systems) where unstable and nonproper weighting filters are used. in such nonstandard problem, the desired controller, called the extended stabilizing controller, has to satisfy two conditions simultaneously: (i) the closed-loop is admissible; (ii) only the internal stability of a part of the closed-loop is required. in terms of two generalized sylvester-type equations, necessary and sufficient conditions for the existence of an observer-based extended stabilizing controller are given. moreover, a parametrization of the class of extended stabilizing controllers is formulated.
heavy quark energy loss in high multiplicity proton proton collisions at lhc. one of the most promising probes to study deconfined matter created in high energy nuclear collisions is the energy loss of (heavy) quarks. it has been shown in experiments at the relativistic heavy ion collider (rhic) that even charm and bottom quarks, despite their high mass, experience a remarkable medium suppression in the quark gluon plasma. in this exploratory investigation we study the energy loss of heavy quarks in high multiplicity proton-proton collisions at lhc energies. although the colliding systems are smaller than compared to those at rhic (p+p vs. au+au) the higher energy might lead to multiplicities comparable to cu+cu collisions at rhic. recently the cms collaboration has shown that these particles likely interact among each other. the interaction of charm quarks with this environment gives rise to a non-negligible suppression of high momentum heavy quarks in elementary collisions.
on the thermal phase structure of qcd at vanishing chemical potentials. the hypothesis is investigated, that the thermal structure of qcd phases at and near zero chemical potentials is determined by long range coherence, inducing the gauge boson pair condensate, and its thermal extension, representing a fundamental order parameter. a consistent model for thermal behavior including interactions is derived in which the condensate does not produce any latent heat as it vanishes at the critical temperature inducing a second-order phase transition with respect to energy density neglecting eventual numerically small critical exponents. localization and delocalization of color fields are thus separated by a unique critical temperature.
stability of the fragments and thermalization at the peak centre-of-mass energy. we simulated the central reactions of nearly symmetric and asymmetric systems, for energies at which maximum production of intermediate mass fragments (imfs) occurred (e(c.m.)(peak)). this study was carried out using hard eos along with cugnon cross-section employing mstb method for clusterization. we studied the various properties of fragments. the stability of fragments was checked through persistence coefficient and gain term. the information about the thermalization and stopping in heavy-ion collisions was obtained via relative momentum, anisotropy ratio and rapidity distribution. we found that for a complete stopping of incoming nuclei very heavy systems are required. the mass dependence of various quantities (such as average and maximum central density, collision dynamics as well as the time zone for hot and dense nuclear matter) was also presented. in all cases (i.e., average and maximum central density, collision dynamics as well as the time zone for hot and dense nuclear matter) a power-law dependence was obtained.
$\rho^{0}$ photoproduction in auau collisions at $\sqrt{s_{nn}}$=62.4 gev with star. vector mesons may be photoproduced in relativistic heavy-ion collisions when a virtual photon emitted by one nucleus scatters from the other nucleus, emerging as a vector meson. the star collaboration has previously presented measurements of coherent $\rho^0$ photoproduction at center of mass energies of 130 gev and 200 gev in auau collisions. here, we present a measurement of the cross section at 62.4 gev; we find that the cross section for coherent $\rho^0$ photoproduction with nuclear breakup is $10.5\pm1.5\pm 1.6$ mb at 62.4 gev. the cross-section ratio between 200 gev and 62.4 gev is $2.8\pm0.6$, less than is predicted by most theoretical models. it is, however, proportionally much larger than the previously observed $15\pm 55$% increase between 130 gev and 200 gev.
scheduling of a speed-dating event. null
on the technician routing and scheduling problem. the technician routing and scheduling problem consists in routing and scheduling a crew of technicians in order to attend a set of service requests, subject to skill, tool, and spare part constraints. in this study we propose a formal definition of the problem and present a constructive heuristic and a large neighborhood search optimization algorithm.
new constructive heuristics for the multi-compartment vehicle routing problem with stochastic demands. null
a dynamic approach for the vehicle routing problem with stochastic demands. the vehicle routing problem with stochastic demands (vrpsd) is a variation of the classical capacitated vehicle routing problem (cvrp). in contrast to the deterministic cvrp, in the vrpsd the demand of each customer is modeled as a random variable and its realization is only known upon vehicle arrival to the customer site. under this uncertain scenario, a possible outcome is that the demand of a customer ends up exceeding the remaining capacity of the vehicle, leading to a route failure. in this study we will focus on the single vehicle vrpsd in which the fleet is limited to one vehicle with finite capacity, that can execute various routes sequentially. the present work is based on an adaptation of an optimization framework developed initially for the vehicle routing problem with dynamic customers (i.e., customers appear while the vehicles are executing their routes).
a decomposition approach to the batch processing problem. null
dynamic vehicle routing problems: state of the art and prospects. this scientific report summarizes the results of a literature review on dynamic vehicle routing problems. after a brief description of vehicle routing problems in general, a classification is introduced to distinguish between static and dynamic problems. then a more precise definition of dynamism is presented, supported by example of real-world applications of such problems. finally, a detailed study of the current state of the art in dynamic vehicle routing optimization is drawn.
a constraint programming approach for a batch processing problem with non-identical job sizes. null
solving the vehicle routing problem with stochastic demands with a multiple scenario approach. traditional approaches for the vrpsd aim at designing a-priori robust plans that avoid potential route failures. however, the widespread and inexpensive real-time communication and geolocalization technologies have opened promising perspectives in this field. we illustrate on the vrpsd the flexibility of jmsa, a generic framework for multiple scenario approach. preliminary results show that a continuous re-optimization leads to reductions in route failures and improvements in cost efficiency.
constructive heuristics for the multicompartment vehicle routing problem with stochastic demands. the vehicle routing problem with stochastic demands (vrpsd) consists of designing transportation routes of minimal expected cost to satisfy a set of customers with random demands of known probability distribution. this paper tackles a generalization of the vrpsd known as the multicompartment vrpsd (mc-vrpsd), a problem in which each customer demands several products that, because of incompatibility constraints, must be loaded in independent vehicle compartments. to solve the problem, we propose three simple and effective constructive heuristics based on a stochastic programming with recourse formulation. one of the heuristics is an extension to the multicompartment scenario of a savings-based algorithm for the vrpsd; the other two are different versions of a novel look-ahead heuristic that follows a route-first, cluster-second approach. in addition, to enhance the performance of the heuristics these are coupled with a post-optimization procedure based on the classical 2-opt heuristic. the three algorithms were tested on instances of up to 200 customers from the mc-vrpsd and vrpsd literature. the proposed heuristics unveiled 26 and 12 new best known solutions for a set of 180 mc-vrpsd problems and a 40-instance testbed for the vrpsd, respectively.
clone evolution: a systematic review. detection of code clones - similar or identical source code fragments - is of concern both to researchers and to practitioners. an analysis of the clone detection results for a single source code version provides a developer with information about a discrete state in the evolution of the software system. however, tracing clones across multiple source code versions permits a clone analysis to consider a temporal dimension. such an analysis of clone evolution can be used to uncover the patterns and characteristics exhibited by clones as they evolve within a system. developers can use the results of this analysis to understand the clones more completely, which may help them to manage the clones more effectively. thus, studies of clone evolution serve a key role in understanding and addressing issues of cloning in software. in this paper we present a systematic review of the literature on clone evolution. in particular, we present a detailed analysis of 30 relevant papers that we identified in accordance with our review protocol. the review results are organized to address three research questions. through our answers to these questions, we present the methods that researchers have used to study clone evolution, the patterns that researchers have found evolving clones to exhibit, and the evidence that researchers have established regarding the extent of inconsistent change undergone by clones during software evolution. overall, the review results indicate that while researchers have conducted several empirical studies of clone evolution, there are contradictions among the reported findings, particularly regarding the lifetimes of clone lineages and the consistency with which clones are changed during software evolution. we identify human-based empirical studies and classification of clone evolution patterns as two areas in particular need of further work.
harmonic decomposition of two-particle angular correlations in pb--pb collisions at $\mathbf{\sqrt{s_{\rm nn}} = 2.76}$ tev. angular correlations between unidentified charged trigger ($t$) and associated ($a$) particles are measured by the alice experiment in \pbpb\ collisions at $\snn=2.76$ tev for transverse momenta $0.25 &lt; p_{t}^{t,\, a} &lt; 15$ gev/$c$, where $p_{t}^t &gt; p_{t}^a$. the shapes of the pair correlation distributions are studied in a variety of collision centrality classes between 0 and 50% of the total hadronic cross section for particles in the pseudorapidity interval $|\eta| &lt; 1.0$. distributions in relative azimuth $\delta\phi \equiv \phi^t - \phi^a$ are analyzed for $|\delta\eta| \equiv |\eta^t - \eta^a| &gt; 0.8$, and are referred to as "long-range correlations". fourier components $v_{n\delta} \equiv &lt;\cos(n\delta\phi)&gt;$ are extracted from the long-range azimuthal correlation functions. if the particle pair correlation arises dominantly from production mechanisms that distribute according to a common plane of symmetry, then the pair $\vnd$ coefficients are expected to factorize as the product of single-particle anisotropies $v_n (\pt)$, i.e. $v_{n\delta}(p_{t}^t, p_{t}^a) = v_n(p_{t}^t) \, v_n(p_{t}^a)$. this expectation is tested for $1 \leq n \leq 5$ by applying a global fit of all $\vnd (p_{t}^t, p_{t}^a)$ to obtain the best values $\vngf (\pt)$. it is found that for $2 \leq n \leq 5$, the factorization holds for associated particle momenta up to $\pta \sim 3$-4 gev/$c$, with a trend of increasing deviation between the data and the factorization hypothesis as $p_{t}^t$ and $p_{t}^a$ are increased or as collisions become more peripheral. $v_{1\delta}$ does not factorize precisely at any $\pt$ or centrality, as indicated by the lack of a good global fit over the full $\ptt, \pta$ range. the $\vngf$ values for $2 \leq n \leq 5$ from the global fit are in close agreement with previous measurements.
cooperative dynamic scheduling of virtual machines in distributed systems. cloud computing aims at outsourcing data and applications hosting and at charging clients on a per-usage basis. these data and ap- plications may be packaged in virtual machines (vm), which are them- selves hosted by nodes, i.e. physical machines. consequently, several frameworks have been designed to manage vms on pools of nodes. unfortunately, most of them do not efficiently address a common objective of cloud providers: maximizing system utilization while ensuring the quality of service (qos). the main reason is that these frameworks schedule vms in a static way and/or have a centralized design. in this article, we introduce a framework that enables to schedule vms cooperatively and dynamically in distributed systems. we evaluated our prototype through simulations, to compare our approach with the cen- tralized one. preliminary results showed that our scheduler was more reactive. as future work, we plan to investigate further the scalability of our framework, and to improve reactivity and fault-tolerance aspects.
component types qualification in java legacy code driven by communication integrity rules. to fight software architectural erosion, new languages and development methods are proposed that make explicit the architectural decisions in the source code for the benefit of the programmers. component based software engineering is a way to improve software modularization and to embed architectural concerns. to restructure legacy code with components in mind we need tools to asses the compliance with component programming principles. the communication integrity property is one of the major principles to implement software architectures. however, there is a lack of tooling for assessing the quality of components codes. to cope with this issue, we define a component model in java and a tool for identifying component types. the tool relies on a set of rules to statically check potential violations of the communication integrity property in java source code. we illustrate its application on a case study and report the results of our experiments with the tool.
managing information flow in spl development processes. null
introduction. null
aspect-oriented, model-driven software product lines the ample way. software product lines provide a systematic means of managing variability in a suite of products. they have many benefits but there are three major barriers that can prevent them from reaching their full potential. first, there is the challenge of scale: a large number of variants may exist in a product line context and the number of interrelationships and dependencies can rise exponentially. second, variations tend to be systemic by nature in that they affect the whole architecture of the software product line. third, software product lines often serve different business contexts, each with its own intricacies and complexities. the ample (http://www.ample-project.net/) approach tackles these three challenges by combining advances in aspect-oriented software development and model-driven engineering. the full suite of methods and tools that constitute this approach are discussed in detail in this edited volume and illustrated using three real-world industrial case studies.
nuclear reaction measurements of 95mev/u 12c interactions on pmma for hadrontherapy. the ion dose deposition in tissues is characterized by a favorable depth dose profile (i.e. bragg peak) and a small lateral spread. in order to keep these benefits of ions in cancer treatments, a very high accuracy is required on the dose deposition (±3%). for given target stoechiometry and geometry, the largest uncertainty on the physical dose deposition is due to the ion nuclear fragmentation. we have performed an experiment at ganil with a 95mev/u 12c beam on thick tissue equivalent pmma targets (thicknesses: 5, 10, 15, 20 and 25mm). the main goals of this experiment are to provide experimental fragmentation data for benchmarking the physical models used for treatment planning. production rates, energy and angular distributions of charged fragments have been measured. the purpose of this paper is to present the results of this experiment.
cloning in dsls: experiments with ocl. code cloning (i.e., similar code fragments) in general purpose languages has been a major focus of the research community. for domain specific languages (dsls), cloning related to domain-specific graphical languages has also been considered. this paper focuses on domain-specific textual languages in an effort to evaluate cloning in these dsls where instances of such dsls allow for less code to express domain-specific features, but potentially more frequently used code constructs. we suggest potential application scenarios of using clone detection for the maintenance of dsl code. we introduce a clone detection mechanism using a model driven engineering (mde) based approach to evaluate the extent of cloning in an initial dsl (i.e., the object constraint language (ocl)). the evaluation reveals the existence of cloning in ocl, which suggests the relevance and potential applications of clone detection and analysis in dsls.
model for a sensor inspired by electric fish. this article reports the first results from a programme of work aimed at developing a swimming robot equipped with electric sense. after having presented the principles of a bio- inspired electric sensor, now working, we will build the models for electrolocation of objects that are suited to this kind of sensor. the produced models are in a compact analytical form in order to be tractable on the onboard computers of the future robot. these models are tested by comparing them with numerical simulations based on the boundary elements method. the results demonstrate the feasibility of the approach and its compatibility with online objects electrolocation, another parallel programme of ours.
assessment of an effective quasirelativistic methodology designed to study astatine chemistry in aqueous solution. a cost-effective computational methodology designed to study astatine (at) chemistry in aqueous solution has been established. it is based on two-component spin-orbit density functional theory calculations and solvation calculations using the conductor-like polarizable continuum model in conjunction with specific astatine cavities. theoretical calculations are confronted with experimental data measured for complexation reactions between metallic forms of astatine (at+ and ato+) and inorganic ligands (cl-, br- and scn-). for each reaction, both 1:1 and 1:2 complexes are evidenced. the experimental trends regarding the thermodynamic constants (k) can be reproduced qualitatively and quantitatively. the mean signed error on computed log k values is -0.4, which corresponds to a mean signed error smaller than 1 kcal mol-1 on free energies of reaction. theoretical investigations show that the reactivity of cationic species of astatine is highly sensitive to spin-orbit coupling and solvent effects. at the moment, the presented computational methodology appears to be the only tool to gain an insight into astatine chemistry at a molecular level.
the pierre auger observatory v: enhancements. ongoing and planned enhancements of the pierre auger observatory.
the pierre auger observatory i: the cosmic ray energy spectrum and related measurements. studies of the cosmic ray energy spectrum at the highest energies with the pierre auger observatory.
state feedback h2 optimal controllers under regulation constraints for descriptor systems. this paper is concerned with a non-standard multi-objective state feedback control problem for continuous descriptor systems. in this problem an output is to be regulated asymptotically with presence of an infinite-energy exo-system, while a desired h2 performance from a finite external disturbance to a tracking error has also to be satisfied. thanks to the descriptor framework, not only unstable but also nonproper behaviors can be treated. a parametrization of all optimal dynamic and static controllers solving the proposed multi-objective control problem is given. moreover, an application to the non-standard lqr problem is investigated. a numerical example shows the efficiency of the proposed results.
automatic segmentation of portal vein in ct-scans of the liver. null
liver segmentation in contrast enhanced helical ct-scans. null
how to deal with your it legacy? reverse engineering using models - modisco in a nutshell!. free download of the full journal issue from http://jaxenter.com/java-tech-journal/jtj-2011-06.
urban mobility plan environmental impacts assessment: a methodology including socio-economic consequences - the eval-pdu project. the project objective is to develop an optimized methodology to assess the environmental impacts of urban mobility plans (ump, in french pdu), taking into account their social and economic consequences. the main proposed methodology is based on a systemic approach: multi-factor (air quality, noise, energy consumption, greenhouse gas emission) numerical simulations with a chain of physically-based models, based on alternative and comparative scenarios. the social and economic consequences of these alternative simulations are assessed by means of econometric models. two alternative approaches are explored: (i) the use of composite environmental indicators to correlate the sources to the impacts, especially health impacts, and (ii) the analysis of sample surveys on what makes inhabitants' quality of life, well-being and territorial satisfaction and on citizens' behavioral changes linked to transport offer changes.
the lateral trigger probability function for the ultra-high energy cosmic ray showers detected by the pierre auger observatory. in this paper we introduce the concept of lateral trigger probability (ltp) function, i.e., the probability for an extensive air shower (eas) to trigger an individual detector of a ground based array as a function of distance to the shower axis, taking into account energy, mass and direction of the primary cosmic ray. we apply this concept to the surface array of the pierre auger observatory consisting of a 1.5 km spaced grid of about 1600 water cherenkov stations. using monte carlo simulations of ultra-high energy showers the ltp functions are derived for energies in the range between 1017 and 1019 ev and zenith angles up to 65°. a parametrization combining a step function with an exponential is found to reproduce them very well in the considered range of energies and zenith angles. the ltp functions can also be obtained from data using events simultaneously observed by the fluorescence and the surface detector of the pierre auger observatory (hybrid events). we validate the monte-carlo results showing how ltp functions from data are in good agreement with simulations.
evolving security requirements in multi-layered service-oriented-architectures. due to today's rapidly changing corporate environments, business processes are increasingly subject to dynamic configuration and evolution. the evolution of new deployment architectures, as illustrated by the move towards mobile platforms and the internet of services, and the introduction of new security regulations (imposed by national and international regulatory bodies, such as sox4 or basel5) are an im- portant constraint in the design and development of business processes. in such context, it is not sufficient to apply the corresponding adapta- tions only at the service orchestration or at the choreography level; there is also the need for controlling the impact of new security requirements to several architectural layers, specially in cloud computing, where the notion of platforms as services and infrastructure as services are fun- damental. in this paper we survey several research questions related to security cross-domain and cross-layer security functionality in service oriented architectures, from an original point of view. we provide the first insights on how a general service model empowered with aspect oriented programming capabilities can provide clean modularization to such cross-cutting security concerns.
on qgp formation in pp collisions at 7 tev. the possibility of qgp formation in central pp collisions at ultra-high collision energy is discussed. centrality-dependent $\pt$-spectra and (pseudo)rapidity spectra of thermal photons (charged hadrons) from pp collisions at 7 tev are presented (addressed). minimal-bias $\pt$-spectrum of direct photons and charged hadrons is compared under the framework with and without hydrodynamical evolution process.
quarkonium production measurements with the alice detector at the lhc. in this new energy regime, quarkonium provides a unique probe to study the properties of the high-density, strongly interacting system formed in the early stages of high-energy heavy-ion collisions. in alice, quarkonium states are reconstructed down to p_t=0 via their mu+mu- decay channel in the muon spectrometer (2.5.
a general approach for optimizing regular criteria in the job-shop scheduling problem. even though a very large number of solution methods has been developed for the job-shop scheduling problem, a majority has been designed for the makespan criterion. in this paper, we propose a general approach for optimizing any regular criterion in the job-shop scheduling problem. the approach is a local search method that uses a disjunctive graph model and neighborhoods generated by swapping critical arcs. the connectivity property of the neighborhood structure is proved and a novel efficient method for evaluating moves is presented. besides its generality, another prominent advantage of the proposed approach is its simple implementation that only requires to tune the range of one parameter. extensive computational experiments carried out on various criteria (makespan, total weighted flow time, total weighted tardiness, weighted sum of tardy jobs, maximum tardiness) show the efficiency of the proposed approach. best results were obtained for some problem instances taken from the literature.
geomagnetic origin of the radio emission from cosmic ray induced air showers observed by codalema. the new setup of the codalema experiment installed at the radio observatory in nançay, france, is described. it includes broadband active dipole antennas and an extended and upgraded particle detector array. the latter gives access to the air shower energy, allowing us to compute the efficiency of the radio array as a function of energy. we also observe a large asymmetry in counting rates between showers coming from the north and the south in spite of the symmetry of the detector. the observed asymmetry can be interpreted as a signature of the geomagnetic origin of the air shower radio emission. a simple linear dependence of the electric field with respect to ∧ is used which reproduces the angular dependencies of the number of radio events and their electric polarity.
revisiting scaling properties of medium-induced gluon radiation. discussing the general case of a hard partonic production process, we show that the notion of parton energy loss is not always sufficient to fully address medium-induced gluon radiation. the broader notion of gluon radiation associated to a hard process has to be used, in particular when initial and final state radiation amplitudes interfere, making the medium-induced radiated energy different from the energy loss of any well-identified parton. our arguments are first presented in an abelian qed model, and then applied to large-xf quarkonium hadroproduction. in this case, we show that the medium-induced radiated energy is qualitatively similar (but not identical) to the radiative energy loss of an "asymptotic massive parton" undergoing transverse momentum broadening when travelling through the nucleus. in particular, it scales as the incoming parton energy, which suggests to reconsider gluon radiation as a possible explanation of large-xf quarkonium suppression in p-a collisions. we expect a similar effect in open heavy-flavour and possibly light-hadron hadroproduction at large xf, depending on the precise definition of the nuclear suppression factor in the latter case.
considering clay rock heterogeneity in radionuclide retention. null
the pourbaix diagram of astatine in aqueous medium. null
advances in the development of astatine-radiolabelling protocols: exploring the metallic character of astatine. null
temperature effect on u(vi) sorption onto na-bentonite. u(vi) sorption on a purified na-bentonite was investigated from 298±2 to 353±2 k by using a batch experimental method as a function of ph, u(vi) concentration, carbonate concentration and solid-to-liquid ratio (m/v). the data at 298±2 k could be well described by a surface complexation model (scm) with a complex located on layer sites (x2uo2) and three complexes located on edge sites (≡souo2+, ≡so(uo2)3(oh)5, and ≡so(uo2)3(oh)72-). the intrinsic equilibrium constants (kint) of the surface reactions at 333±2 k and 353±2 k were obtained by fitting u(vi) sorption curves versus ph on the na-bentonite. the model enables u(vi) sorption in the presence of carbonate ( =10-3.58 atm) to be described without considering any ternary surface complexes involving carbonate, except for underestimation around ph 7 (6 &lt; ph &lt; 7.5). the standard enthalpy changes ( ) of the surface reactions were evaluated from the kint values obtained at three temperatures (298±2, 333±2 and 353±2 k) via the van't hoff equation. the proposed scm and of the surface reactions enable u(vi) sorption on the na-bentonite at other temperatures to be predicted.
a rapid microwave-assisted procedure for easy access to nx polydentate ligands for potential application in α-rit. heterocycles bearing a hydrazine moiety react with bisaldehydes or bisketones to afford new nx polydentate ligands suitable for α-radioimmunotherapy. we developed a fast and efficient method using microwave-assisted technology to obtain chelators with variable size and number of coordination centres which were fully characterized. the complexation efficiency with astatine will be assessed.
quarkonium production in high energy proton-proton and proton-nucleus collisions. null
from uml profiles to emf profiles and beyond. null
theory and practice of model transformations - 4th international conference, icmt 2011, zurich, switzerland, june 27-28, 2011. proceedings. null
spy on your models. emf is now widely used by various kinds of systems based on eclipse modeling project components. as emf models become the heart of these systems, it becomes critical to be able to inspect them very precisely. it is the main objective of the modisco model browser to provide a good insight of the content of any emf model, especially when these models are large and complex. this talk will present the main features of this tool: 1) directly access to instances of a given eclass 2) navigate through the relations between model elements 3) dynamically customize the rendering of model elements 4) edit the model elements with a tabular view 5) integrate the browser components with the common navigation framework.
cartesian stiffness matrix of manipulators with passive joints: analytical approach. the paper focuses on stiffness matrix computation for manipulators with passive joints. it proposes both explicit analytical expressions and an efficient recursive procedure that are applicable in general case and allow obtaining the desired matrix either in analytical or numerical form. advantages of the developed technique and its ability to produce both singular and non-singular stiffness matrices are illustrated by application examples that deal with stiffness modeling of two stewart-gough platforms.
lightweight verification of executable models. executable models play a key role in many development methods by facilitating the immediate simulation/implementation of the software system under development. this is possible because executable models include a fine-grained specification of the system behaviour. unfortunately, a quick and easy way to check the correctness of behavioural specifications is still missing, which compromises their quality (and in turn the quality of the system generated from them). in this paper, a lightweight verification method to assess the strong executability of fine-grained behavioural specifications (i.e. operations) at design-time is provided. this method suffices to check that the execution of the operations is consistent with the integrity constraints defined in the structural model and returns a meaningful feedback that helps correcting them otherwise.
two basic correctness properties for atl transformations: executability and coverage. model transformations play a cornerstone role with the emergence of model driven engineering (mde), where models are transformed from higher to lower levels of abstraction. unfortunately, a quick and easy way to check the correctness of model transformations is still missing, which compromises their quality (and in turn, the quality of the target models generated from them). in this paper we propose a lightweight and efficient method that performs a static analysis of the atl rules with respect to two correctness properties we define: (1) weak executability, which determines if there is some scenario in which an atl rule can be safely applied without breaking the target metamodel integrity constraints; and (2) coverage, which ensures a set of atl rules allow addressing all elements of the source and target metamodels. in both cases, our method returns meaningful feedback that helps repairing the possible detected inconsistencies.
lightweight executability analysis of graph transformation rules. domain specific visual languages (dsvls) play a cornerstone role in model-driven engineering (mde), where (domain specific) models are used to automate the production of the final application. graph transformation is a formal, visual, rule-based technique, which is increasingly used in mde to express in-place model transformations like refactorings, animations and simulations. however, there is currently a lack of methods able to perform static analysis of rules, taking into account the dsvl meta-model integrity constraints. in this paper we propose a lightweight, efficient technique that performs static analysis of the weak executability of rules. the method determines if there is some scenario in which the rule can be safely applied, without breaking the meta-model constraints. if no such scenario exists, the method returns meaningful feedback that helps repairing the detected inconsistencies.
moscript: a dsl for querying and manipulating model repositories. abstract. growing adoption of model-driven engineering has hugely increased the number of modelling artefacts (models, metamodels, trans- formations, ...) to be managed. therefore, development teams require ap- propriate tools to search and manipulate models stored in model repos- itories, e.g. to find and reuse models or model fragments from previous projects. unfortunately, current approaches for model management are either ad-hoc (i.e., tied to specific types of repositories and/or models), do not support complex queries (e.g., based on the model structure and its relationship with other modelling artefacts) or do not allow the manipu- lation of the resulting models (e.g., inspect, transform). this hinders the probability of efficiently reusing existing models or fragments thereof. in this paper we introduce moscript, a textual domain-specific language for model management. with moscript, users can write scripts containing queries (based on model content, structure, relationships, and behaviour derived through on-the-fly simulation) to retrieve models from model repositories, manipulate them (e.g., by running transformations on sets of models), and store them back in the repository. moscript relies on the megamodeling concept to provide a homogeneous model-based interface to heterogeneous repositories.
lazy execution of model-to-model transformations. the increasing adoption of model-driven engineering in in- dustrial contexts highlights scalability as a critical limitation of several mde tools. most of the current model-to-model transformation engines have been designed for one-shot translation of input models to output models, and present efficiency issues when applied to very large models. in this paper, we study the application of a lazy-evaluation approach to model transformations. we present a lazy execution algorithm for atl, and we empirically evaluate a prototype implementation. with it, the elements of the target model are generated only when (and if) they are accessed, enabling also transformations that generate infinite target models. we achieve our goal on a significant subset of atl by extending the atl compiler.
extending atl for native uml profile support: an experience report 49-62. with the rise of model-driven engineering (mde) the ap- plication field of model transformations broadens drastically. current model transformation languages provide appropriate support for stan- dard mde scenarios such as model-to-model transformations specified between metamodels. however, for other transformation scenarios often the escape to predefined apis for handling specific model manipulations is required such as is the case for supporting uml profiles in transforma- tions. thus, the need arises to extend current transformation languages for natively supporting such additional model manipulations. in this paper we report on extending atl for natively supporting uml profiles in transformations. the extension is realized by providing an extended atl syntax comprising keywords for handling uml profiles which is reduced by a preprocessor based on a higher-order transfor- mation (hot) again to the standard atl syntax. in particular, we elab- orate on our methodology of extending atl by presenting the extension process step-by-step as well as reporting on lessons learned. with this experience report we aim at providing design guidelines for extending atl as well as stimulating the research of providing further extensions for atl.
investigation of alendronate-doped apatitic cements as a potential technology for the prevention of osteoporotic hip fractures: critical influence of the drug introduction mode on the in vitro cement properties. null
product line implementation with ecaesarj. this chapter takes a closer look at the difficulties of feature-oriented modularisation of product lines and demonstrate how a better modularisation can be achieved with the ecaesarj programming language, through a type-safe and stable decomposition of a broad spectrum of software abstractions: classes, methods, events, and state machines, based on late binding and mixin composition.
representing clones in a localized manner. code clones (i.e., duplicate sections of code) can be scattered throughout the source files of a program. manually evaluating a group of such clones requires observing each clone in its original location (i.e., opening each file and finding the source location of each clone), which can be a time-consuming process. as an alternative, this paper introduces a technique that localizes the representation of code clones to provide a summary of the properties of two or more clones in one location. in our approach, the results of a clone detection tool are analyzed in an automated manner to determine the properties (i.e., similarities and differences) of the clones. these properties are visualized directly within the source editor. the localized representation is realized as part of the features of an eclipse plug-in called cedar.
virtual composition of emf models. model composition is a very important modeling task as it allows to combine various perspectives of a system (represented by various models) into a single specialized view (a composed model). several approaches have been proposed to tackle this problem, but they present some important limitations concerning efficiency, interoperability, and/or synchronization issues (mainly due to the element cloning mechanism used to create the composed model). in this paper we propose a new model composition method based on the virtualization of the composition mechanism. in our approach, the composed model is in fact created as a virtual model that redirects all its model access and manipulation requests directly to the set of base models from which it was generated. this is done transparently for the designer. our mechanism improves the composition process with relation to the limitations mentioned above. the solution has been implemented and validated in a prototype tool on top of emf.
static analysis of aspect interaction and composition in component models. component based software engineering and aspect orientation are claimed to be two complementary approaches. while the former ensures the modularity and the reusability of software entities, the latter enables the modularity of crosscutting concerns that cannot be modularized by regular components. nowadays, several approaches and frameworks are dedicated to integrate aspects into component models. however, when several aspects are woven, interferences may appear which results on undesirable behaviors. the contribution of this paper is twofold. first, we show how aspectualized component models can be formally modeled in uppaal model checker in order to detect potential interferences among aspects. second, we provide an extendible catalog of composition operators used for aspect composition. we illustrate our general approach with an airport internet service example.
composable controllers in fractal: implementation and interference analysis. fractal component model provides controllers for adding extra-functional capabilities to component behaviors. however, controllers may interfere one with another and their composition is still a challenge. in this article, we extend fractal with a support for composing controllers with reusable operators. then, we discuss how to formally model and analyze, in uppaal, fractal systems with several controllers. this enables us to detect when controllers interfere and to check whether their composition is interference-free.
architecture for the next generation system management tools. to get more results or greater accuracy, computational scientists execute their applications on distributed computing platforms such as clusters, grids and clouds. these platforms are different in terms of hardware and software resources as well as locality: some span across multiple sites and multiple administrative domains whereas others are limited to a single site/domain. as a consequence, in order to scale their applications up the scientists have to manage technical details for each target platform. from our point of view, this complexity should be hidden from the scientists who, in most cases, would prefer to focus on their research rather than spending time dealing with platform configuration concerns. in this article, we advocate for a system management framework that aims to automatically setup the whole run-time environment according to the applications' needs. the main difference with regards to usual approaches is that they generally only focus on the software layer whereas we address both the hardware and the software expectations through a unique system. for each application, scientists describe their requirements through the definition of a virtual platform (vp) and a virtual system environment (vse). relying on the vp/vse definitions, the framework is in charge of: (i) the configuration of the physical infrastructure to satisfy the vp requirements, (ii) the setup of the vp, and (iii) the customization of the execution environment (vse) upon the former vp. we propose a new formalism that the system can rely upon to successfully perform each of these three steps without burdening the user with the specifics of the configuration for the physical resources, and system management tools. this formalism leverages goldberg's theory for recursive virtual machines by introducing new concepts based on system virtualization (identity, partitioning, aggregation) and emulation (simple, abstraction). this enables the definition of complex vp/vse configurations without making assumptions about the hardware and the software resources. for each requirement, the system executes the corresponding operation with the appropriate management tool. as a proof of concept, we implemented a first prototype that currently interacts with several system management tools (e.g. oscar, the grid'5000 toolkit, and xtreemos) and that can be easily extended to integrate new resource brokers or cloud systems such as nimbus, opennebula or eucalyptus for instance.
impact of hot carrier stress on small signal mosfet rf parameters. null
from dc to rf mosfet reliability (45mn). null
using traceability links and higher order transformations for easing regression testing of web applications. null
probing pre-formed alpha particles in the ground state of nuclei. in this proceeding we report on alpha particle emission through the nuclear break-up in the reaction 40ca on a 40ca target at 50a mev. it is observed that alpha particles are emitted to the continuum with very specific angular distribution during the reac- tion. the alpha particle properties seem to be compatible with an alpha cluster in the daughter nucleus that is perturbed by the short range nuclear attraction of the collision partner and emitted as described by a time-dependent theory. this mechanism offers new possibilities to study alpha particle properties in the nuclear medium.
anisotropy and chemical composition of ultra-high energy cosmic rays using arrival directions measured by the pierre auger observatory. the pierre auger collaboration has reported evidence for anisotropy in the distribution of arrival directions of the cosmic rays with energies e &gt; eth = 5.5 × 1019 ev. these show a correlation with the distribution of nearby extragalactic objects, including an apparent excess around the direction of centaurus a. if the particles responsible for these excesses at e &gt; eth are heavy nuclei with charge z, the proton component of the sources should lead to excesses in the same regions at energies e/z. we here report the lack of anisotropies in these directions at energies above eth/z (for illustrative values of z = 6,13,26). if the anisotropies above eth are due to nuclei with charge z, and under reasonable assumptions about the acceleration process, these observations imply stringent constraints on the allowed proton fraction at the lower energies.
thomas-fermi approximation to pairing in finite fermi systems. the weak coupling regime. we present a new semiclassical theory for describing pairing in finite fermi systems. it is based on taking the $\hbar \to 0$, i.e. thomas-fermi, limit of the gap equation written in the basis of the mean field (weak coupling). in addition to the position dependence of the fermi momentum, the size dependence of the pairing force is also taken into account in this theory. along isotopic chains the thomas-fermi gaps average the well known arch structure shown by the quantal gaps. this structure can be almost recovered in our formalism if some shell fluctuations are included in the level density. we point out that at the drip line nuclear pairing is strongly reduced. this fact is illustrated with the behavior of the gap in the inner crust of neutron stars.
multi-agent electro-location and the among constraint. in this paper, we give a new approach for the localization of several autonomous fish robots equipped with the electric sense. the approach is based on interval arithmetic and constraint programming. it introduces a generalization of the among constraint which was used so far in the different context of resource allocation in logistics. we prove that the bound consistency for a conjunction of among constraints with interval domains is np-complete in the vector case and polynomial in the one-dimensional case. although it was designed for electric fish robots, this work is not restricted to this type of systems. the approach can be easily extended to other multi-agent systems with low-range sensing.
on the mobility and potential retention of iodine in the callovovian-oxfordian formation. iodide sorption experiments were conducted on clay stone samples originating from the callovian-oxfordian formation under experimental conditions as close as possible to in situ conditions. the total natural iodine content of the formation is shown to be very constant throughout the formation, ranging from 2 to 3 ppm. this range is in agreement with a past iodine accumulation in the marine organic matter of the sediment before and during deposition, and early diagenesis. at variance with total iodine, the leached iodine concentrations are variable. if leached iodine is considered to represent porewater solute iodine, its concentration can be calculated and ranges from 0 (below detection limit) to ~60 µmol/l and represents 0 to 25 % of the total iodine. the reason for this variability is not understood. sorption isotherms were determined either for natural 127i- solutions or for 131i- spiked 127i- solutions, with concentrations ranging from 10-9 to 10-3 mol/l at solid to liquid ratios from 10 to 200 g/l. no or little sorption was encountered, kd values being in the range 0-0.5 l/kg with statistical and analytical error bands being greater than the kd values, with the exception of one experiment at low solid to liquid ratio (10 g/l), showing significant kd values of ~25 l/kg. in sorption experiments with natural 127i- and at the lowest added iodide concentrations (&lt; 10-6 mol/l) an apparent negative kd was obtained due to the iodide content in the solid porewater that was leached once the solid was suspended. the low affinity of iodide for argilite is thus confirmed. however, based only on these results and given the extent of the error bands, one cannot discard a limited iodide uptake. literature data on iodide diffusion on similar rock materials have already shown that iodide does not behave like chloride. the retention mechanism of radio-iodide is discussed in the light of the present results and diffusion data. a model involving isotopic exchange between the natural iodine content of the geological formation and radio-iodine allows all of the results to be described. not all the iodine in the formation appears to participate in isotopic exchange reactions with the solution. a quantification of the isotopically labile fraction of iodine would allow the effect of isotopic exchange on radio-iodide migration throughout the callovian-oxfordian formation to be assessed and predicted.
analytic relations for partial alpha decay half-lives and barrier heights and positions. from an adjustment on a recent selected data set of partial α-decay half-lives of 344 ground state to ground state transitions, analytic formulae are proposed depending on the angular momentum of the α particle. in particular, an expression allows to reproduce precisely the partial α-decay half-lives of even-even heavy nuclei and, then, to predict accurately the partial α-decay half-lives of other very heavy elements from the experimental or predicted qα. simple expressions are also provided to calculate the potential barrier radius and height.
community of inquiry in e-learning : a critical analysis of garrison and anderson model. this article is based on a constructively critical analysis of the model of community of inquiry developed by garrison and anderson (2003) as part of a research conducted in the area of e-learning. the authors claim that certain collaborative interactions create "distant presence" fostering the emergence of a community of inquiry which has a positive influence on individual and collective learning. more specifically, the article points out that until now, the model's theoretical foundations had not been made explicit and provides important insights concerning these epistemological considerations. it also suggests a number of theoretical perspectives which strengthen the authors' presentation of the conceptual anchorings of the model. thus the major contribution of this article is to show the potential of garrison and anderson's model for the research in the field of e-learning.
higher harmonic anisotropic flow measurements of charged particles in pb-pb collisions at 2.76 tev. we report on the first measurement of the triangular v3, quadrangular v4, and pentagonal v5 charged particle flow in pb-pb collisions at 2.76 tev measured with the alice detector at the cern large hadron collider. we show that the triangular flow can be described in terms of the initial spatial anisotropy and its fluctuations, which provides strong constraints on its origin. in the most central events, where the elliptic flow v2 and v3 have similar magnitude, a double peaked structure in the two-particle azimuthal correlations is observed, which is often interpreted as a mach cone response to fast partons. we show that this structure can be naturally explained from the measured anisotropic flow fourier coefficients.
influence of haptic communication on a shared manual task in a collaborative virtual environment. with the advent of new haptic feedback devices, researchers are giving serious consideration to the incorporation of haptic communication in collaborative virtual environments. for instance, haptic interactions based tools can be used for medical and related education whereby students can train in minimal invasive surgery using virtual reality before approaching human subjects. to design virtual environments that support haptic communication, a deeper understanding of humans' haptic interactions is required. in this paper, human's haptic collaboration is investigated. a collaborative virtual environment was designed to support performing a shared manual task. to evaluate this system, 60 medical students participated to an experimental study. participants were asked to perform in dyads a needle insertion task after a training period. results show that compared to conventional training methods, a visual-haptic training improves user's collaborative performance. in addition, we found that haptic interaction influences the partners' verbal communication when sharing haptic information. this indicates that the haptic communication training changes the nature of the users' mental representations. finally, we found that haptic interactions increased the sense of copresence in the virtual environment: haptic communication facilitates users' collaboration in a shared manual task within a shared virtual environment. design implications for including haptic communication in virtual environments are outlined.
measuring cosmic ray radio signals at the pierre auger observatory. the recent results of the lopes and codalema experiments open the door to a renewal of the radio technique for cosmic ray induced shower measurements. the demonstration has been done of its potential and performances at energies below 1018 ev, this upper limit being due to the small scale of the current experiments. a natural stage toward the improvement of the method is thus to install radio detectors in association with a large cosmic ray detector such as auger. besides surface and fluorescence detection, radio detection could be an alternative method, providing a complementary information. the pierre auger collaboration has thus engaged a r&amp;d effort which will lead to the installation of a radio engineering array covering 20 km2 on its southern site. outline of the technique, results of the first phase of the tests and current plans for the future engineering array will be presented.
development of a cryogenic gaseous photomultiplier dedicated to a liquid-xenon compton telescope for medical imaging. a novel imaging technique based on the tridimensional localization of a (beta, gamma) radioisotope emitter with a liquid-xenon compton telescope was proposed at subatech in 2003. this technique named 3 gamma imaging combines a classical positron emission tomography device and a compton telescope for the reconstruction of two back-to-back annihilation gamma-rays and the third one respectively. the interaction of the last one with liquid-xenon induces a scintillation signal read by a vacuum photomultiplier tube to trigger the acquisition of the simultaneous ionization signal read by a micromegas (micro mesh gaseous structure) which allows the measurement of each interaction position and corresponding energy. in this experimental framework, we propose an original way of scintillation reading, replacing the traditionnal photomultiplier tubes devices by a large-area cryogenic gaseous photomultiplier. this photodetector consists of a reflective solid cesium iodide photocathode for the photoconversion of uv light and a combination of three micro-pattern gaseous detectors : the thgem (thick gaseous electron multiplier), the micromegas and the pim (parallel ionization multiplier). it should allow a virtual segmentation of the liquid xenon volume to reduce the telescope occupancy. first results obtained with a small area prototype at liquid xenon temperature (173 k) are presented.
the journey of biorthogonal logical relations to the realm of assembly code. logical relations appeared to be very fruitful for the development of modular proofs of compiler correctness. in this field, logical relations are parametrized by a high-level type system, and are even sometimes directly relating low level pieces of code to high-level programs. all those works rely crucially on biorthogonality to get extensionality and compositionality properties. but the use of biorthogonality in the definitions also complicates matters when it comes to operational correctness. most of the time, such correctness results amount to show an unfolding lemma that makes reduction more explicit than in a biorthogonal definition. unfortunately, unfolding lemmas are not easy to derive for rich languages and in particular for assembly code. in this paper, we focus on three different situations that enable to reach step-by-step the assembly code universe: the use of curry-style polymorphism, the presence of syntactical equality in the language and finally an ideal assembly code with a notion of code pointer.
k+ and k- potentials in hadronic matter are observable quantities. the comparison of $k^+$ and $k^-$ spectra at low transverse momentum in light symmetric heavy ion reactions at energies around 2 agev allows for a direct experimental determination of the strength of the $k^+$ as well as of t he $k^-$ nucleus potential. other little known or unknown input quantities like the production or rescattering cross sections of $k^+$ and $k^-$ mesons do not spoil this signal. this result, obtained by simulations of these reactio ns with the isospin quantum molecular dynamics (iqmd) model, may solve the longstanding question of the behaviour of the $k^-$ in hadronic matter and especially whether a $k^-$ condensate can be formed in heavy ion collisions.
ground and excited charmonium state production in p+p collisions at sqrt(s)=200 gev. we report on charmonium measurements [j/psi(1s), psi'(2s), and chi_c(1p)] in p+p collisions at sqrt(s)=200 gev. we find that the fraction of j/psi coming from the feed-down decay of psi' and chi_c in the midrapidity region ($|\eta|&lt;0.35$) is 9.6+/-2.4% and 32+/-9%, respectively. we also report new, higher statistics p_t and rapidity dependencies of the j/psi yield via dielectron decay in the same midrapidity range and at forward rapidity (1.2&lt;|eta|&lt;2.4) via dimuon decay. these results are compared with measurements from other experiments and discussed in the context of current charmonium production models.
stability and stabilizability of mixed retarded-neutral type systems. we analyze the stability and stabilizability properties of mixed retarded-neutral type systems when the neutral term is allowed to be singular. considering an operator model of the system in a hilbert space we are interesting in the critical case when there exists a sequence of eigenvalues with real parts approaching to zero. in this case the exponential stability is not possible and we are studying the strong asymptotic stability property. the behavior of spectra of mixed retarded-neutral type systems does not allow to apply directly neither methods of retarded system nor the approach of neutral type systems for analysis of stability. in this paper two technics are combined to get the conditions of asymptotic non-exponential stability: the existence of a riesz basis of invariant finite-dimensional subspaces and the boundedness of the resolvent in some subspaces of a special decomposition of the state space. for unstable systems the technics introduced allow to analyze the concept of regular strong stabilizability for mixed retarded-neutral type systems. the present paper extends the results by r.~rabah, g.m.~sklyar, a.v.~rezounenko on stability obtained in [j. diff. equat., 214(2005), no. 2, 391-428] and on stabilizability from [j. diff. equat., 245(2008), no. 3, 569-593].
modelling and solving a practical flexible job-shop scheduling problem with blocking constraints. this paper presents a study of a practical job-shop scheduling problem modelled and solved when helping a company to design a new production workshop. the main characteristics of the problem are that some resources are flexible, and blocking constraints have to be taken into account. the problem and the motivation for solving it are detailed. the modelling of the problem and the proposed resolution approach, a genetic algorithm, are described. numerical experiments using real data are presented and analysed. we also show how these results were used to support choices in the design of the workshop.
an integrated production and maintenance planning model with time windows and shortage cost. in this paper, we tackle the problem of integrating production and maintenance. production problem addresses the issue of determining the production lot sizes of various items. preventive maintenance is carried out in time windows to restore the production line to an 'as-good-as-new' status, and when a production line fails, a minimal repair is carried out to restore it to an 'as-bad-as-old' status. the resulting problem is modelled as a linear mixed-integer program. it takes into account demand shortage and the reliability of the production line. computational experiments are carried out to show the effectiveness of the integrated model compared to classical separate model for different instances, and the obtained results are analyzed in detail.
rapidity and transverse momentum dependence of inclusive j/psi production in pp collisions at sqrt(s) = 7 tev. the alice experiment at the lhc has studied inclusive j/psi production at central and forward rapidities in pp collisions at sqrt(s) = 7 tev. in this letter, we report on the first results obtained detecting the j/psi through its dilepton decay into e+e- and mu+mu- pairs in the rapidity range |y|&lt;0.9 and 2.5.
typing artifacts in megamodeling. null
a model-driven framework for aspect weaver construction. null
a probabilistic study of bound consistency for the alldifferent constraint. this paper introduces a mathematical model for bound consistency of the con- straint alldifferent. it allows us to compute the probability that the filtering algorithm effectively removes at least one value in the variable domains. a complete study of the bound consistency properties is then proposed. it identifies several behaviors depending on some macroscopic quantities related to the variables and the domains. finally, it is shown that the probability for an alldifferent constraint to be bound consistent can be asymptotically estimated in constant time. the experiments illustrate that the precision is good enough for a practical use in constraint programming.
sensitivity of the transverse flow towards symmetry energy. we study the sensitivity of transverse flow towards symmetry energy in the fermi energy region as well as at high energies. we find that transverse flow is sensitive to symmetry energy as well as its density dependence in the fermi energy region. we also show that the transverse flow can address the symmetry energy at densities about twice the saturation density, however it shows the insensitivity towards the symmetry energy at densities $\rho/\rho_{0}$ $&gt;$ 2. the mechanism for the sensitivity of transverse flow towards symmetry energy as well as its density dependence is also discussed.
semiclassical description of average pairing properties in nuclei. we present a new semiclassical theory for describing pairing in finite fermi systems. it is based on taking the ħ → 0, i.e. thomas-fermi, limit of the gap equation written in the basis of the mean field (weak coupling). in addition to the position dependence of the fermi momentum, the size dependence of the matrix elements of the pairing force is also taken into account in this theory. an example typical for the nuclear situation shows the improvment of this new approach over the standard local density approximation. we also show that if in this approach some shell fluctuations are introduced in the level density, the arch structure displayed by the quantal gaps along isotopic chains is almost recovered. we also point out that in heavy drip line nuclei pairing is strongly reduced.
ridges and soft jet components in untriggered di-hadron correlations in pb+pb collisions at 2.76 tev. we study untriggered di-hadron correlations in pb+pb at 2.76 tev, based on an event-by-event simulation of a hydrodynamic expansion starting from flux tube initial conditions. the correlation function shows interesting structures as a function of the pseudorapidity difference $\delta\eta$ and the azimuthal angle difference $\delta\phi$, in particular comparing different centralities. we can clearly identify a peak-like nearside structure associated with very low momentum components of jets for peripheral collisions, which disappears towards central collisions. on the other hand, a very broad ridge structure from asymmetric flow seen at central collisions, gets smaller and finally disappears towards peripheral collisions.
density dependence of symmetry energy and collective transverse in-plane flow. we study the sensitivity of the collective transverse in-plane flow to the symmetry energy and its density dependence at fermi energies and higher incident energies. we find that collective transverse in-plane flow is sensitive to the symmetry energy and its density dependence at fermi energies whereas it shows insensitivity at higher incident energies.
bose-einstein correlations in a fluid dynamical scenario for prototon-proton scattering at 7 tev. using a fluid dynamical scenario for pp scattering at 7 tev, we compute correlation functions for pi+pi+ paires. femtoscopic radii are extracted based on threedimensional parameterizations of the correlation functions. we study the radii as a function of the transverse momenta of the pairs, for different multiplicity classes, corresponding to recent experimental results from alice. we find the same decrease of the radii with k_t, more an more pronounced with increasing multiplicity, but absent for the lowest multiplicities. in the model we understand this as transition from string expansion (low multiplicity) towards a threedimensional hydrodynamical expansion (high multiplicity).
decomposing logical relations with forcing. logical relations have now the maturity to deal with program equivalence for realistic programming languages with features likes recursive types, higher-order references and first-class continuations. however, such advanced logical relations---which are defined with technical developments like step-indexing or heap abstractions using recursively defined worlds---can make a proof tedious. a lot of work has been done to hide step-indexing in proofs, using gödel-löb logic. but to date, step-indexes have still to appear explicitely in particular constructions, for instance when building recursive worlds in a stratified way. in this paper, we go one step further, proposing an extension of abadi-plotkin logic with forcing construction which enables to encapsulate reasoning about step-indexing or heap in different layers. moreover, it gives a uniform and abstract management of step-indexing for recursive terms or types and for higher-order references.
refining models with rule-based model transformations. several model-to-model transformation languages have been primarily designed to easily address the syntactic and semantic translation of read-only input models towards write-only output models. while this approach has been proven successful in many practical cases, it is not directly applicable to transformations that need to modify their source models, like refactorings. in this paper we investigate the application of a model-to-model transformation language to in-place transformations, by providing a systematic view of the problem, comparing alternative solutions and proposing a transformation semantics to address this problem in atl.
first detection of extensive air showers by the trend self-triggering radio experiment. an antenna array devoted to the autonomous radio-detection of high energy cosmic rays is being deployed on the site of the 21 cm array radio telescope in xinjiang, china. thanks in particular to the very good electromagnetic environment of this remote experimental site, self-triggering on extensive air showers induced by cosmic rays has been achieved with a small scale prototype of the foreseen antenna array. we give here a detailed description of the detector and present the first detection of extensive air showers with this prototype.
structured and flexible gray-box composition using invasive distributed patterns. the evolution of complex distributed software systems often requires intricate composition operations in order to adapt or add functionalities, to react to unanticipated changes, or to apply performance improvements that cannot be modularized in terms of existing services and components. these evolutions often need controlled access to selected parts of the implementation, e.g., to manage exceptional situations and crosscutting within services and their compositions. however, existing composition techniques typically support only interface-level (black-box) composition or arbitrary access to the implementation (gray-box or white-box composition). in this paper, we present a structured approach to the composition of complex software systems that require invasive modifications. concretely, we provide three contributions: (i) we present a small kernel composition language for structured gray-box composition using invasive distributed patterns; (ii) we motivate that gray-box composition approaches should be defined and evaluated in terms of the flexibility and control they provide, a notion of degrees of invasiveness is introduced to help assess this trade-off; (iii) we apply our approach to a new case study of evolution and evaluate it in the context of two previous studies involving two real-world software systems: benchmarking of grid algorithms with nasgrid and transactional replication with jboss cache. as a main result, we show that gray-box composition using invasive distributed patterns allows the declarative and modular definition of evolutions of real-world applications that need moderate to high degrees of invasive modifications.
aspect oriented programming: a language for 2-categories. aspect oriented programming (aop) started ten years ago with the remark that modularization of so-called crosscutting functionalities is a fundamental problem for the engineering of large-scale applications. originating at xerox parc, this observation has sparked the development of a new style of programming featured that is gradually gaining traction, as it is the case for the related concept of code injection, in the guise of frameworks such as swing and google guice. however, aop lacks theoretical foundations to clarify this new idea. this paper proposes to put a bridge between aop and the notion of 2-category to enhance the conceptual understanding of aop. starting from the connection between the $\lambda$-calculus and the theory of categories, we propose to see an aspect as a morphism between morphisms---that is as a program that transforms the execution of a program. to make this connection precise, we develop an advised lambda-calculus that provides an internal language for 2-categories and show how it can be used as a base for the definition of the weaving mechanism of a realistic functional aop language, called minaml. finally, we advocate for a formalization of more complex aop languages (eg. with references or exceptions) using the notion of enriched lawvere theories.
high $p_{t}$ non-photonic electron production in $p$+$p$ collisions at $\sqrt{s}$ = 200 gev. we present the measurement of non-photonic electron production at high transverse momentum ($p_t &gt; $ 2.5 gev/$c$) in $p$+$p$ collisions at $\sqrt{s}$ = 200 gev using data recorded during 2005 and 2008 by the star experiment at the relativistic heavy ion collider (rhic). the measured cross-sections from the two runs are consistent with each other despite a large difference in photonic background levels due to different detector configurations. we compare the measured non-photonic electron cross-sections with previously published rhic data and pqcd calculations. using the relative contributions of b and d mesons to non-photonic electrons, we determine the integrated cross sections of electrons ($\frac{e^++e^-}{2}$) at 3 gev/$c &lt; p_t &lt;~$10 gev/$c$ from bottom and charm meson decays to be ${d\sigma_{(b\to e)+(b\to d \to e)} \over dy_e}|_{y_e=0}$ = 4.0$\pm0.5$({\rm stat.})$\pm1.1$({\rm syst.}) nb and ${d\sigma_{d\to e} \over dy_e}|_{y_e=0}$ = 6.2$\pm0.7$({\rm stat.})$\pm1.5$({\rm syst.}) nb, respectively.
studies of di-jet survival and surface emission bias in au+au collisions via angular correlations with respect to back-to-back leading hadrons. we report first results from an analysis based on a new multi-hadron correlation technique, exploring jet-medium interactions and di-jet surface emission bias at rhic. pairs of back-to-back high transverse momentum hadrons are used for triggers to study associated hadron distributions. in contrast with two- and three-particle correlations with a single trigger with similar kinematic selections, the associated hadron distribution of both trigger sides reveals no modification in either relative pseudo-rapidity or relative azimuthal angle from d+au to central au+au collisions. we determine associated hadron yields and spectra as well as production rates for such correlated back-to-back triggers to gain additional insights on medium properties.
enhanced stiffness modeling of manipulators with passive joints. the paper presents a methodology to enhance the stiffness analysis of serial and parallel manipulators with passive joints. it directly takes into account the loading influence on the manipulator configuration and, consequently, on its jacobians and hessians. the main contributions of this paper are the introduction of a non-linear stiffness model for the manipulators with passive joints, a relevant numerical technique for its linearization and computing of the cartesian stiffness matrix which allows rank-deficiency. within the developed technique, the manipulator elements are presented as pseudo-rigid bodies separated by multidimensional virtual springs and perfect passive joints. simulation examples are presented that deal with parallel manipulators of the ortholide family and demonstrate the ability of the developed methodology to describe non-linear behavior of the manipulator structure such as a sudden change of the elastic instability properties (buckling).
modelling and simulation of a two wheeled vehicle with suspensions by using robotic formalism. models, simulators and control strategies are required tools for the conception of secure and comfortable vehicles. the aim of this paper is to present an efficient way to develop models for dynamic vehicle, focusing on a two wheeled vehicles whose body involves six degrees of freedom. the resulting model is sufficiently generic to perform simulation of realistic cornering and accelerating behavior in various situations. it may be used in the context of motorcycle modeling, but also in various situations (e.g. for control application) as simplified model for 3 or 4 wheeled (tilting) cars. the approach is based on considering the vehicle as a multi-body poly-articulated system and the modeling is carried out using the robotics formalism based on the modified denavit-hartenberg geometric description. in that way, the dynamic model is easy to implement and the system can be used for control applications.
melo 2011 - 1st workshop on model-driven engineering, logic and optimization. the main goal of this workshop is to bring together two different communities: the model-driven engineering (mde) community and the logic programming community, to explore how each community can benefit from the techniques of the other. are both communities friends or foes?.
observation of the antimatter helium-4 nucleus. high-energy nuclear collisions create an energy density similar to that of the universe microseconds after the big bang, and in both cases, matter and antimatter are formed with comparable abundance. however, the relatively short-lived expansion in nuclear collisions allows antimatter to decouple quickly from matter, and avoid annihilation. thus, a high energy accelerator of heavy nuclei is an efficient means of producing and studying antimatter. the antimatter helium-4 nucleus ($^4\bar{he}$), also known as the anti-{\alpha} ($\bar{\alpha}$), consists of two antiprotons and two antineutrons (baryon number b=-4). it has not been observed previously, although the {\alpha} particle was identified a century ago by rutherford and is present in cosmic radiation at the 10% level. antimatter nuclei with b &lt; -1 have been observed only as rare products of interactions at particle accelerators, where the rate of antinucleus production in high-energy collisions decreases by about 1000 with each additional antinucleon. we present the observation of the antimatter helium-4 nucleus, the heaviest observed antinucleus. in total 18 $^4\bar{he}$ counts were detected at the star experiment at rhic in 10$^9$ recorded au+au collisions at center-of-mass energies of 200 gev and 62 gev per nucleon-nucleon pair. the yield is consistent with expectations from thermodynamic and coalescent nucleosynthesis models, which has implications beyond nuclear physics.
an mde-based approach for solving configuration problems: an application to the eclipse platform. most of us have experienced configuration issues when installing new software applications. finding the right configuration is often a challenging task since we need to deal with many dependencies between plug-ins, components, libraries, packages, etc; sometimes even regarding specific versions of the involved artefacts. right now, most configuration engines are adhoc tools designed for specific configuration scenarios. this makes their reuse in different contexts very difficult. in this paper we report on our experience in following a mde-based approach to solve configuration problems. in our approach, the configuration problem is represented as a model that abstracts all irrelevant technological details and facilitates the use of generic (constraint) solvers to find optimal solutions. this approach has been applied by an industrial partner to the management of plug-ins in the eclipse framework, a big issue for all the technology providers that distribute eclipse-based tools.
on controllabilty and stabilizability of linear neutral type systems. linear systems of neutral type are considered using the infinite dimensional approach. conditions for exact controllability and regular asymptotic stabilizability are given. the main tools are the moment problem approach and the existence of a riesz basis of invariant subspaces.
on exact controllability of linear time delay systems of neutral type. the problem of exact null controllability is considered for linear neutral type systems with distributed delay. a characterization of this problem is given. the minimal time of controllability is precised. the results are based on the analysis of the riesz basis property of eigenspaces in hilbert space. recent results on the moment problem and properties of exponential families are used.
the analysis of exact controllability of neutral type systems by the moment problem approach. the problem of exact null-controllability is considered for a wide class of linear neutral-type systems with distributed delay. the main tool of the analysis is the application of the moment problem approach and the theory of the basis property of exponential families. a complete characterization of this problem is given. the minimal time of controllability is specified. the results are based on the analysis of the riesz basis property of eigenspaces of the neutral-type systems in hilbert space. key words. neutral-type systems, exact controllability, moment problem, riesz basis, distributed delays ams subject classifications. 93b05, 93c23, 93c25 doi. 10.1137/060650246 1.
stability analysis of mixed retarded-neutral type systems in hilbert space. we investigate the stability property of mixed retarded-neutral type systems. considering an operator model of the system in hilbert space we are interesting in the critical case when the spectrum of the operator belongs to the open left half-plane and there exists a sequence of eigenvalues with real parts approaching to zero. in this case the exponential stability is not possible and we are studying the strong asymp- totic stability property. the present paper extends the results obtained in [r. rabah, g.m. sklyar, a. v. rezounenko, stability analysis of neutral type systems in hilbert space. j. of dierential equations, 214(2005), no. 2, 391{428] in which stability of systems of neutral type was studied using the existence of a riesz basis of invariant finite-dimensional subspaces. however, for mixed retarded-neutral type systems such a basis may not exist for the whole state space. though the main result on stability remains the same for mixed retarded-neutral type systems, the technic of its proof had to be changed and it involves a proof of resolvent boundedness on some invariant subspace. we show that the property of asymptotic stability is determinated not only by the spectrum of the system but also depends on geometrical characteristics of its main neutral term which in our situation may be singular. we also give an explicit example of two systems having the same spectrum in the open left half-plane, but one of them is asymptotically stable while the other one is unstable.
on strong regular stabilizability for linear neutral type systems. the problem of strong stabilizability of linear systems of neutral type is investigated. we are interested in the case when the system has an infinite sequence of eigenvalues with vanishing real parts. this is the case when the main part of the neutral equation is not assumed to be stable in the classical sense. we discuss the notion of regular strong stabilizability and present an approach to stabilize the system by regular linear controls. the method covers the case of multivariable control and is essentially based on the idea of infinite-dimensional pole assignment proposed in [g.m. sklyar, a.v. rezounenko, a theorem on the strong asymptotic stability and determination of stabilizing controls, c. r. acad. sci. paris ser. i math. 333 (8) (2001) 807-812]. our approach is based on the recent results on the riesz basis of invariant finitedimensional subspaces and strong stability for neutral type systems presented in [r. rabah, g.m. sklyar, a.v. rezounenko, stability analysis of neutral type systems in hilbert space, j. differential equations 214 (2) (2005) 391-428].
on a vector moment problem arising in the analysis of neutral type systems. the solvability of some new vector moment problem related with the exact controllability of neutral type systems is investigated. condition of equivallence of this problem with the controllability of some system is analyzed. the time of solvability is precised.
on non-exponential stability of delay systems of neutral type with non-singular neutral term. condition of non exponential stability are given in the general case of neutral type with non singular matrix in the neutral term.
a multi-criteria large neighborhood search for the transportation of disabled people. this article addresses the problem of optimizing the transportation of disabled persons from home to specialized centers or schools. it is modeled as a dial a ride problem (darp), where several people share the same destination. particular emphasis is placed on the objective function in order to consider several potentially conflicting interests. we propose a multi-criteria model from multi-attribute utility theory based on the choquet integral. the darp is then solved with an adaptive large neighborhood search (alns) algorithm. this method includes classical destroy and repair heuristics as well as new operators exploiting the common delivery nodes aspect, as well as criterion-specic operators. the algorithm is evaluated on a set of 14 real life instances with up to 200 requests and 51 destination points.
a versatile palladium-triphosphane system for direct arylation of heteroaromatics with chlorides at low catalyst loading. put a ring on it: the use of an air-stable, robust palladium/tridentate phosphane catalyst in direct c[bond]h and c[bond]cl activation reactions is reported (see scheme; dmac=n,n-dimethylacetamide, tbab=tetra-n-butylammonium bromide). electron-rich, electron-poor, and polysubstituted furans (x=o), thiophenes (x=s), pyrroles (x=nr5), and thiazoles were arylated with chloroarenes in the presence of the catalyst.
isospin effects in the disappearance of flow as a function of colliding geometry. we study the effect of isospin degree of freedom on the balance energy (e$_{bal}$) as well as its mass dependence throughout the mass range 48-270 for two sets of isobaric systems with n/z = 1 and 1.4 at different colliding geometries ranging from central to peripheral ones. our findings reveal the dominance of coulomb repulsion in isospin effects on e$_{bal}$ as well as its mass dependence throughout the range of the colliding geometry. our results also indicate that the effect of symmetry energy and nucleon-nucleon cross section on e$_{bal}$ is uniform throughout the mass range and throughout the colliding geometry. we also present the counter balancing of nucleon-nucleon collisions and mean field by reducing the coulomb and the counter balancing of coulomb and mean filed by removing the nucleon-nucleon collisions.
the g0 experiment: apparatus for parity-violating electron scattering measurements at forward and backward angles. in the g0 experiment, performed at jefferson lab, the parity-violating elastic scattering of electrons from protons and quasi-elastic scattering from deuterons is measured in order to determine the neutral weak currents of the nucleon. asymmetries as small as 1 part per million in the scattering of a polarized electron beam are determined using a dedicated apparatus. it consists of specialized beam-monitoring and control systems, a cryogenic hydrogen (or deuterium) target, and a superconducting, toroidal magnetic spectrometer equipped with plastic scintillation and aerogel cerenkov detectors, as well as fast readout electronics for the measurement of individual events. the overall design and performance of this experimental system is discussed.
generating operation specifications from uml class diagrams: a model transformation approach. one of the more tedious and complex tasks during the specification of conceptual schemas (css) is modeling the operations that define the system behavior. this paper aims to simplify this task by providing a method that automatically generates a set of basic operations that complement the static aspects of the cs and suffice to perform all typical life-cycle create/update/delete changes on the population of the elements of the cs. our method guarantees that the generated operations are executable, i.e. their executions produce a consistent state wrt the most typical structural constraints that can be defined in css (e.g. multiplicity constraints). in particular, our method takes as input a cs expressed as a unified modeling language (uml) class diagram (optionally defined using a profile to enrich the specification of associations) and generates an extended version of the cs that includes all necessary operations to start operating the system. if desired, these basic operations can be later used as building blocks for creating more complex ones. we show the formalization and implementation of our method by means of model-to-model transformations. our approach is particularly relevant in the context of model driven development approaches.
likelihood approach to the first dark matter results from xenon100. many experiments that aim at the direct detection of dark matter are able to distinguish a dominant background from the expected feeble signals, based on some measured discrimination parameter. we develop a statistical model for such experiments using the profile likelihood ratio as a test statistic in a frequentist approach. we take data from calibrations as control measurements for signal and background, and the method allows the inclusion of data from monte carlo simulations. systematic detector uncertainties, such as uncertainties in the energy scale, as well as astrophysical uncertainties, are included in the model. the statistical model can be used to either set an exclusion limit or to make a discovery claim, and the results are derived with a proper treatment of statistical and systematic uncertainties. we apply the model to the first data release of the xenon100 experiment, which allows to extract additional information from the data, and place stronger limits on the spin-independent elastic wimp-nucleon scattering cross-section. in particular, we derive a single limit, including all relevant systematic uncertainties, with a minimum of 2.4x10^-44 cm^2 for wimps with a mass of 50 gev/c^2.
an application of dc programming approach for a logistics network design problem. null
designing a decision support system for the transportation of disabled persons. null
a linear relaxation heuristic for solving logistics network design and planning problems. null
a lagrangean relaxation based heuristic for logistics network design. we consider a logistics network design problem with multiple periods, echelons, facilities and commodities. the initial network has some operating facilities and a set of potential locations to settle new ones. over a strategic horizon, the optimisation model aims at locating operating facilities; planning the capacity, the production levels and the product flows. this problem is modelled as a mixed integer linear programme (milp) with binary variables associated to facility status and continuous variables associated to material flows. the proposed scheme is based on the lagrangean relaxation of some coupling constraints. a decomposition level is performed by considering the main echelons: suppliers / plants; plants / warehouses and customers; warehouses / customers. characteristics of the corresponding echelon the resolution of this decomposed problem provides a lower bound. on the other hand, the feasible solutions provide upper bounds that can be improved by applying a d.c. (difference of convex functions) algorithm for milp. a procedure for updating lagrangean multipliers is also proposed. all these elements are combined to build an iterative heuristic for solving the original problem. stopping conditions are based on the classical gap between the two bounds.
a tabu search algorithm for the dial-a-ride problem with transfers. the dial-a-ride problem (darp) consists in determining and scheduling routes serviced by a set of vehicles in order to satisfy transportation requests from pickup points to delivery points. this paper introduces a variant of the darp where the client can be transferred from one vehicle to another on specic points called \transfer points". solving this variant of the darp yields new algorithmic diculties. in this paper, we propose a tabu search algorithm for the dial-a-ride problem with transfers (darpt) with a special emphasis on checking whether a modication of the current solution is feasible or not. the method is evaluated over generated and real instances.
a linear relaxation-based heuristic for logistics network design. we address the problem of designing and planning a multi-period, multi-echelon, multicommodity logistics network with deterministic demands. this consists of making strategic and tactical decisions: opening, closing or expansion of facilities, supplier selection and definition of the products flows. we use a heuristic approach based on the linear relaxation of the original mixed integer linear problem (milp). the main idea is to fix as many binary variables as possible with the linear relaxation coupled with rounding procedures. the number of binary variables in the resulting milp is small enough to enable solving it with a commercial solver. we compare the computational time and the quality of the results obtained with the heuristic and the commercial solver.
a dynamic model for the facility location in supply chain design. in this paper, we propose a mixed integer linear program for the design and planning of a multi-echelon, multi-commodity production-distribution network with deterministic demands. within a strategic time horizon, this study aims to help strategic and tactical decisions for each period of time: opening, closing or enlargement of facilities, supplier election, flows along the supply chain network. several families of products, with bills of materials, are considered. we present one application of our model: how to plan the expansion of a company which has to face increasing demands. we report numerical experiments with a milp solver.
a dc programming heuristic applied to the logistics network design problem. this paper proposes a new heuristic method for the logistics network design and planning problem based on linear relaxation and dc (difference of convex functions) programming. we consider a multi-period, multi-echelon, multi-commodity and multi-product problem defined as a large scale mixed integer linear programming (milp) model. the method is experimented on data sets of various size. the numerical results validate the efficiency of the heuristic for instances with up to several dozens facilities, 18 products and 270 retailers.
portolan: a model-driven cartography framework. processing large amounts of data to extract useful information is an essential task within companies. to help in this task, visualization techniques have been commonly used due to their capacity to present data in synthesized views, easier to understand and manage. however, achieving the right visualization display for a data set is a complex cartography process that involves several transformation steps to adapt the (domain) data to the (visualization) data format expected by visualization tools. to maximize the benefits of visualization we propose portolan, a generic model-driven cartography framework that facilitates the discovery of the data to visualize, the specification of view definitions for that data and the transformations to bridge the gap with the visualization tools. our approach has been implemented on top of the eclipse emf modeling framework and validated on three different use cases.
particle swarm optimization for the design of h∞ static output feedbacks. the design of h∞ reduced order controllers is known to be a non convex optimization problem for which no generic solution exists. in this paper, the use of particle swarm optimization (pso) for the computation of h∞ static output feedbacks is investigated. two approaches are tested. in a first part, a probabilistic-type pso algorithm is defined for the computation of discrete sets of stabilizing static output feedback controllers. this method relies on a technique for random sample generation in a given domain. it is therefore used for computing a suboptimal h∞ static output feedback solution. in a second part, the initial optimization problem is solved by pso, the decision variables being the feedback gains. results are compared with standard reduced order problem solvers using the compleib benchmark examples and appear to be much than satisfactory, proving the great potential of pso techniques.
static output feedback control design for descriptor systems. the static output feedback (sof) synthesis problem for descriptor systems is considered in this paper. lmi-based algorithms are proposed to find potentially structured sof gains ensuring admissibility and even h∞ performance of the closed-loop system. these algorithms are then used to propose a (descriptor) observer-based h∞ controller design method. an alternative technique for determining such separated estimation/control structure, after the design step, is also proposed. several numerical examples, throughout the paper, demonstrate the effectiveness of the proposed algorithms.
mass predictions of exotic nuclei within a macro-microscopic model. different liquid drop model mass formulae have been studied. they include a coulomb diffuseness correction z2/a term and pairing and shell energies of the thomas-fermi model. the influence of the selected charge radius, the curvature energy and different forms of the wigner term has been investigated. their coefficients have been determined by a least square fitting procedure to 2027 experimental atomic masses. the different fits lead to a surface energy coefficient of 17-18 mev. a large equivalent rms radius (r0 = 1.22 − 1.24 fm) or a shorter central radius may be used. a rms deviation of 0.54 mev can be reached between the experimental and theoretical masses. the remaining differences come from the determination of the shell and pairing energies. mass predictions are given for exotic nuclei.
study of selenium nanoparticles dissolution in environmental and human fluids. null
a lateral control strategy for narrow tilting commuter vehicle based on the perceived lateral acceleration. narrow tilting commuter vehicles are expected to be the new generation of city cars, considering their practical dimensions and lower energy consumption. but their dimensions increase their tendency to overturn during cornering, facing lateral acceleration. this problem can be solved by tilting the vehicle in a way that reduces the perceived lateral acceleration at the cabin during cornering, as it is seen on two wheeled vehicles. in literature so far, the corresponding tilting angle is computed, and the control strategy aims at reaching this desired angle. this paper, presents another control approach, achieving a direct control of the perceived acceleration by appropriately tilting the vehicle. the strategy proposed is interesting in that it is both simple to implement, able to take advantage over accelerometer and gyroscope measures, and valid even on roads with non zero banking angle. as the lateral speed is not measured in practice, the paper proposes, for comparison purpose, different (original) ways to get it without deteriorating robustness. finally, the solution proposed reduces to a robust state feedback controller exploiting all the available measures.
a two level strategy for combustion engine starter robust control in the context of a hybrid power train. the main difficulty in controlling combustion engine start-up comes from its changing behaviour from load to motor. in the context of hybrid power trains considered here, the electric motor used for start-up is more powerful than a traditional starter. in order to get a robust control of the crankshaft speed, we propose to take inspiration from redundant control as formulated in the aeronautic domain (härkegard, 2003), (härkegard and glad, 2005), (buffington et al., 1996), (maciejowski, 1997), by considering that the engine and the electric motor are redundant torque providers. despite that the engine is the primary torque provider, it will be systematically seen as the failing actuator during the start-up stage. therefore, the electric motor has to compensate its deficiency.
towards a robust model for distributed aspects. in this paper, we present some of the problems we have found in distributed aspect models and introduce a set of criteria that we consider necessary for a robust distributed aspect system. we outline of a first version of model based on aspects and actors capable of meeting these criteria.
invasive composition for the evolution of a health information system. in this paper we show that some of the evolution tasks in openmrs, a health information system, may require the invasive modification of interfaces and implementations in order to offer an appropriate modularization. we introduce a new composition framework in java that supports the definition of expressive pattern-based invasive compositions. fur thermore, we show that the composition framework allows us to concisely define an evolution scenario of openmrs that supports the consolidation of patient data from differ- ent remote instances.
aspectizing java access control. it is inevitable that some concerns crosscut a sizeable application, resulting in code scattering and tangling. this issue is particularly severe for security-related concerns: it is difficult to be confident about the security of an application when the implementation of its security-related concerns is scattered all over the code and tangled with other concerns, making global reasoning about security precarious. in this study, we consider the case of access control in java, which turns out to be a crosscutting concern with a non-modular implementation based on runtime stack inspection. we describe the process of modularizing access control in java by means of aspect-oriented programming (aop). we first show a solution based on aspectj, the most popular aspect-oriented extension to java, that must rely on a separate automata infrastructure. we then put forward a novel solution via dynamic deployment of aspects and scoping strategies. both solutions, apart from providing a modular specification of access control, make it possible to easily express other useful policies such as the chinese wall policy. however, relying on expressive scope control results in a compact implementation, which, at the same time, permits the straightforward expression of even more interesting policies.
dynamic consolidation of highly available web applications. datacenters provide an economical and practical solution for hosting large scale n-tier web applications. when scalability and high availability are required, each tier can be implemented as multiple replicas, which can absorb extra load and avoid a single point of failure. realizing these benefits in practice, however, requires that replicas be assigned to datacenter nodes according to certain placement constraints. to provide the required quality of service to all of the hosted applications, the datacenter must consider of all of their specific constraints. when the constraints are not satisfied, the datacenter must quickly adjust the mappings of applications to nodes, taking all of the applications' constraints into account. this paper presents plasma, an approach for hosting highly available web applications, based on dynamic consolidation of virtual machines and placement constraint descriptions. the placement constraint descriptions allow the data- center administrator to describe the datacenter infrastructure and each appli- cation administrator to describe his requirements on the vm placement. based on the descriptions, plasma continuously optimizes the placement of the vms in order to provide the required quality of service. experiments on simulated configurations show that the plasma reconfiguration algorithm is able to man- age a datacenter with up to 2000 nodes running 4000 vms with 800 placement constraints. real experiments on a small cluster of 8 working nodes running 3 instances of the rubis benchmarks with a total of 21 vms show that con- tinuous consolidation is able to reach 85% of the load of a 21 working nodes cluster.
simultaneous hinf control for continuous-time descriptor systems. this study addresses the simultaneous hinf control problem for continuous-time descriptor systems, namely, a single controller is sought to stabilise a collection of descriptor systems with a prescribed hinf norm. first, this problem is transformed equivalently to the strong hinf stabilisation problem of an augmented system. then, a sufficient condition of the existence of strongly admissible hinf controllers is proposed in terms of strict linear matrix inequalities. finally, numerical examples are presented to show the effectiveness of the proposed method.
identified charged hadron production in p+p collisions at sqrt(s)=200 and 62.4 gev. transverse momentum distributions and yields for $\pi^{\pm}$, $k^{\pm}$, $p$ and $\bar{p}$ in $p+p$ collisions at $\sqrt{s}$=200 and 62.4 gev at midrapidity are measured by the phenix experiment at the relativistic heavy ion collider (rhic). these data provide important baseline spectra for comparisons with identified particle spectra in heavy ion collisions at rhic. we present the inverse slope parameter $t_{\rm inv}$, mean transverse momentum $$ and yield per unit rapidity $dn/dy$ at each energy, and compare them to other measurements at different $\sqrt{s}$ in $p+p$ and $p+\bar{p}$ collisions. we also present the scaling properties such as $m_t$ scaling, $x_t$ scaling on the $p_t$ spectra between different energies. to discuss the mechanism of the particle production in $p+p$ collisions, the measured spectra are compared to next-to-leading-order or next-to-leading-logarithmic perturbative quantum chromodynamics calculations.
direct photons at low transverse momentum -- a qgp signal in pp collisions at lhc. we predict that direct photon production in pp collisions at 7~tev will get at least 10 times enhanced compared to the next to leading order pqcd predictions, at low transverse momentum ($\pt$ $\lesssim$ 10~gev/c), due to the thermal photon emissions from a quark gluon plasma (qgp) formed in high multiplicity events. thus the enhancement of direct photon production at low $\pt$ can be a qgp signal in pp collisions.
the influence of bulk evolution models on heavy-quark phenomenology. we study the impact of different quark-gluon plasma expansion scenarios in heavy-ion collisions on spectra and elliptic flow of heavy quarks. for identical heavy-quark transport coefficients relativistic langevin simulations with different expansion scenarios can lead to appreciable variations in the calculated suppression and elliptic flow of the heavy-quark spectra, by up to a factor of two. a cross comparison with two sets of transport coefficients supports these findings, illustrating the importance of realistic expansion models for quantitative evaluations of heavy-quark observables in heavy-ion collisions. it also turns out that differences in freeze-out prescriptions and langevin realizations play a significant role in these variations. light-quark observables are essential in reducing the uncertainties associated with the bulk-matter evolution, even though uncertainties due to the freeze-out prescription persist.
generalised modal realisation as a practical and efficient tool for fwl implementation. finite word length (fwl) effects have been a critical issue in digital filter implementation for almost four decades. although some optimisations may be attempted to get an optimal realisation with regards to a particular effect, for instance the parametric sensitivity or the round-off noise gain, the purpose of this article is to propose an effective one, i.e. taking into account all the aspects. based on the specialised implicit form, a new effective and sparse structure, named rho-modal realisation, is proposed. this realisation meets simultaneously accuracy (low sensitivity, round-off noise gain and overflow risk), few and flexible computational efforts with a good readability (thanks to sparsity) and simplicity (no tricky optimisation is required to obtain it) as well. two numerical examples are included to illustrate the rho-modal realisation's interest.
a route feasibility algorithm for the dial a ride problem with transfers. null
towards autonomous radio detection of ultra high energy cosmic rays. the radiodetection of extensive air showers, investigated for the first time in the 1960's, obtained promising results but plagued by the technical limitations. at that time, h.r. allan summed up the state of the art in an extensive review article whose conclusions and predictions are still used today. set up in 2001 at the nancay observatory, the codalema experiment was built first as a demonstrator and successfully showed the feasibility of the radiodetection of extensive air showers. radically modified in 2005, it allowed to obtain a clear energy correlation, and put in evidence an unambiguous signature of the geomagnetic origin of the electric field emission process associated to the air shower. the switch towards large areas is the next step of the technique's development. therefore, the autonomy of the detectors becomes essential. after test prototypes installed in 2006 at the pierre auger observatory, a generation of new autonomous detectors was developed. their first results will be presented. this work is also dedicated to the issues related to the radiodetection technique : the antenna response, the sensitivity, the surrounding effects, the monitoring of a big array. the determination of the shower characteristics independently of other detectors such as the lateral distribution, the energy correlation and the frequency spectrum of the radio transient will be discussed.
the pierre auger observatory scaler mode for the study of solar activity modulation of galactic cosmic rays. since data-taking began in january 2004, the pierre auger observatory has been recording the count rates of low energy secondary cosmic ray particles for the self-calibration of the ground detectors of its surface detector array. after correcting for atmospheric effects, modulations of galactic cosmic rays due to solar activity and transient events are observed. temporal variations related with the activity of the heliosphere can be determined with high accuracy due to the high total count rates. in this study, the available data are presented together with an analysis focused on the observation of forbush decreases, where a strong correlation with neutron monitor data is found.
constraints from the first lhc data on hadronic event generators for ultra-high energy cosmic-ray physics. the determination of the primary energy and mass of ultra-high-energy cosmic-rays (uhecr) generating extensive air-showers in the earth's atmosphere, relies on the detailed modeling of hadronic multiparticle production at center-of-mass (c.m.) collision energies up to two orders of magnitude higher than those studied at particle colliders. the first large hadron collider (lhc) data have extended by more than a factor of three the c.m. energies in which we have direct proton-proton measurements available to compare to hadronic models. in this work we compare lhc results on inclusive particle production at energies sqrt(s) = 0.9, 2.36, and 7 tev to predictions of various hadronic monte carlo (mc) models used commonly in cosmic-ray (cr) physics (qgsjet, epos and sibyll). as a benchmark with a standard collider physics model we also show pythia (and phojet) predictions with various parameter settings. while reasonable overall agreement is found for some of the mc, none of them reproduces consistently the sqrt(s) evolution of all the observables. we discuss implications of the new lhc data for the description of cosmic-ray interactions at the highest energies.
creole: a universal language for creating, requesting, updating and deleting resources. in the context of service-oriented computing, applications can be developed following the rest (representation state transfer) architectural style. this style corresponds to a resource oriented model, where resources are manipulated via crud (create, request, update, delete) interfaces. the diversity of crud languages due to the absence of a standard leads to composition problems related to adaptation, integration and coordination of services. to overcome these problems, we propose a pivot architecture built around a universal language to manipulate resources, called creole, a crud language for resource edition. in this architecture, scripts written in existing crud languages, like sql, are compiled into creole and then executed over different crud interfaces. after stating the requirements for a universal language for manipulating resources, we formally describe the language and informally motivate its definition with respect to the requirements. we then concretely show how the architecture solves adaptation, integration and coordination problems in the case of photo management in flickr and picasa, two well-known service-oriented applications. finally, we propose a roadmap for future work.
views, program transformations, and the evolutivity problem in a functional language. we report on an experience to support multiple views of programs to solve the tyranny of the dominant decomposition in a functional setting. we consider two possible architectures in haskell for the classical example of the expression problem. we show how the haskell refactorer can be used to transform one view into the other, and the other way back. that transformation is automated and we discuss how the haskell refactorer has been adapted to be able to support this automated transformation. finally, we compare our implementation of views with some of the literature.
advanced functionality for radio analysis in the offline software framework of the pierre auger observatory. the advent of the auger engineering radio array (aera) necessitates the development of a powerful framework for the analysis of radio measurements of cosmic ray air showers. as aera performs "radio-hybrid" measurements of air shower radio emission in coincidence with the surface particle detectors and fluorescence telescopes of the pierre auger observatory, the radio analysis functionality had to be incorporated in the existing hybrid analysis solutions for fluoresence and surface detector data. this goal has been achieved in a natural way by extending the existing auger offline software framework with radio functionality. in this article, we lay out the design, highlights and features of the radio extension implemented in the auger offline framework. its functionality has achieved a high degree of sophistication and offers advanced features such as vectorial reconstruction of the electric field, advanced signal processing algorithms, a transparent and efficient handling of ffts, a very detailed simulation of detector effects, and the read-in of multiple data formats including data from various radio simulation codes. the source code of this radio functionality can be made available to interested parties on request.
hadron-hadron and cosmic-ray interactions at multi-tev energies. the workshop on "hadron-hadron and cosmic-ray interactions at multi-tev energies" held at the ect* centre (trento) in nov.-dec. 2010 gathered together both theorists and experimentalists to discuss issues of the physics of high-energy hadronic interactions of common interest for the particle, nuclear and cosmic-ray communities. qcd results from collider experiments -- mostly from the lhc but also from the tevatron, rhic and hera -- were discussed and compared to various hadronic monte carlo generators, aiming at an improvement of our theoretical understanding of soft, semi-hard and hard parton dynamics. the latest cosmic-ray results from various ground-based observatories were also presented with an emphasis on the phenomenological modeling of the first hadronic interactions of the extended air-showers generated in the earth atmosphere. these mini-proceedings consist of an introduction and short summaries of the talks presented at the meeting.
ratio of bulk to shear viscosity in a quasigluon plasma: from weak to strong coupling. the ratio of bulk to shear viscosity is expected to exhibit a different behaviour in weakly and in strongly coupled systems. this can be expressed by the dependence of the ratio on the squared sound velocity. in the high temperature qcd plasma at small running coupling, the viscosity ratio is uniquely determined by a quadratic dependence on the conformality measure, whereas in certain strongly coupled and nearly conformal theories this dependence is linear. employing an effective kinetic theory of quasiparticle excitations with medium-modified dispersion relation, we analyze the ratio of bulk to shear viscosity of the gluon plasma. we show that in this approach the viscosity ratio comprises both dependencies found by means of weak coupling perturbative and strong coupling holographic techniques.
improved predictions of reactor antineutrino spectra. we report new calculations of reactor antineutrino spectra including the latest information from nuclear databases and a detailed error budget. the first part of this work is the so-called ab initio approach where the total antineutrino spectrum is built from the sum of all beta-branches of all fission products predicted by an evolution code. systematic effects and missing information in nuclear databases lead to final relative uncertainties in the 10 to 20% range. a prediction of the antineutrino spectrum associated with the fission of 238u is given based on this ab initio method. for the dominant isotopes 235u and 239pu, we developed a more accurate approach combining information from nuclear databases and reference electron spectra associated with the fission of 235u, 239pu and 241pu, measured at ill in the 80's. we show how the anchor point of the measured total beta-spectra can be used to suppress the uncertainty in nuclear databases while taking advantage of all the information they contain. we provide new reference antineutrino spectra for 235u, 239pu and 241pu isotopes in the 2-8 mev range. while the shapes of the spectra and their uncertainties are comparable to that of the previous analysis of the ill data, the normalization is shifted by about +3% on average. in the perspective of the re-analysis of past experiments and direct use of these results by upcoming oscillation experiments, we discuss the various sources of errors and their correlations as well as the corrections induced by off equilibrium effects.
production of pions, kaons and protons in pp collisions at sqrt(s)= 900 gev with alice at the lhc. the production of pi+, pi-, k+, k-, p, and pbar at mid-rapidity has been measured in proton-proton collisions at sqrt(s) = 900 gev with the alice detector. particle identification is performed using the specific energy loss in the inner tracking silicon detector and the time projection chamber. in addition, time-of-flight information is used to identify hadrons at higher momenta. finally, the distinctive kink topology of the weak decay of charged kaons is used for an alternative measurement of the kaon transverse momentum (pt) spectra. since these various particle identification tools give the best separation capabilities over different momentum ranges, the results are combined to extract spectra from pt = 100 mev/c to 2.5 gev/c. the measured spectra are further compared with qcd-inspired models which yield a poor description. the total yields and the mean pt are compared with previous measurements, and the trends as a function of collision energy are discussed.
centrality dependence of the charged-particle multiplicity density at midrapidity in pb-pb collisions at √snn=2.76  tev. null
efficient algorithms for singleton arc consistency. in this paper, we propose two original and efficient approaches for enforcing singleton arc consistency. in the first one, the data structures used to enforce arc consistency are shared between all subproblems where a domain is reduced to a singleton. this new algorithm is not optimal but it requires far less space and is often more efficient in practice than the optimal algorithm sac-opt. in the second approach, we perform several runs of a greedy search (where at each step, arc consistency is maintained), possibly detecting the singleton arc consistency of several values in one run. it is an original illustration of applying inference (i.e., establishing singleton arc consistency) by search. using a greedy search allows benefiting from the incrementality of arc consistency, learning relevant information from conflicts and, potentially finding solution(s) during the inference process. we present extensive experiments that show the benefit of our two approaches.
femtoscopy of pp collisions at sqrt{s}=0.9 and 7 tev at the lhc with two-pion bose-einstein correlations. we report on the high statistics two-pion correlation functions from pp collisions at sqrt{s}=0.9 tev and sqrt{s}=7 tev, measured by the alice experiment at the large hadron collider. the correlation functions as well as the extracted source radii scale with event multiplicity and pair momentum. when analyzed in the same multiplicity and pair transverse momentum range, the correlation is similar at the two collision energies. a three-dimensional femtoscopic analysis shows an increase of the emission zone with increasing event multiplicity as well as decreasing homogeneity lengths with increasing transverse momentum. the latter trend gets more pronounced as multiplicity increases. this suggests the development of space-momentum correlations, at least for collisions producing a high multiplicity of particles. we consider these trends in the context of previous femtoscopic studies in high-energy hadron and heavy-ion collisions, and discuss possible underlying physics mechanisms. detailed analysis of the correlation reveals an exponential shape in the outward and longitudinal directions, while the sideward remains a gaussian. this is interpreted as a result of a significant contribution of strongly decaying resonances to the emission region shape. significant non-femtoscopic correlations are observed, and are argued to be the consequence of "mini-jet"-like structures extending to low p_t. they are well reproduced by the monte-carlo generators and seen also in pi^+ pi^- correlations.
two-pion bose-einstein correlations in central pbpb collisions at sqrt(s_nn) = 2.76 tev. the first measurement of two-pion bose--einstein correlations in central pbpb collisions at sqrt(s_nn) = 2.76 tev at the large hadron collider is presented. we observe a growing trend with energy now not only for the longitudinal and the outward but also for the sideward pion source radius. the pion homogeneity volume and the decoupling time are significantly larger than those measured at rhic.
a new consistent vehicle routing problem for the transportation of handicapped persons. null
study of the isolated photon production in proton-proton collisions with the emcal calorimeter of the alice experiment at lhc. prompt photons produced in the hard initial parton-parton scatterings in high-energy proton-proton collisions – such as those at the cern large hadron collider (lhc) – are an excellent tool to study perturbative quantum chromodynamics. in particular, high transverse momentum(pt) photons provide crucial information on the parton distribution functions (pdfs) of the proton. experimentally, in order to measure prompt photons, one needs first to get rid of various other photon sources, especially those from the large background due to pi0 decays. the electromagnetic calorimeter (emcal) of the alice (a large ion collider experiment) experiment at the lhc covers the central rapidities and provides improved capabilities to measure high-pt photons in alice.we present here studies of photon production in high-energy proton-proton collisions, as well as a complete analysis of their measurement with the emcal detector, giving details in particular on the methods developped to separate them from the pi0 decay photons with the help of isolation cuts.
reduced elastodynamic modelling of parallel robots for the computation of their natural frequencies. null
the parabasisphenoid complex in mesozoic turtles and the evolution of the testudinate basicranium. null
search for first harmonic modulation in the right ascension distribution of cosmic rays detected at the pierre auger observatory. we present the results of searches for dipolar-type anisotropies in different energy ranges above 2.5 × 10^17 ev with the surface detector array of the pierre auger observatory, reporting on both the phase and the amplitude measurements of the first harmonic modulation in the right-ascension distribution. upper limits on the amplitudes are obtained, which provide the most stringent bounds at present, being below 2% at 99% c.l. for eev energies. we also compare our results to those of previous experiments as well as with some theoretical expectations.
spiraclock: a continuous and non-intrusive display for upcoming events. in this paper, we present spiraclock, a new visualization technique for nearby events. spiraclock fills a gap between static calendar displays and pop-up reminders by giving the user a continuous and non-intrusive feedback on nearby events. events are displayed inside an analog clock that can be used as a regular computer clock. we used spiraclock for displaying bus schedules, and collected user feedback.
the magglite post-wimp toolkit: draw it, connect it and run it. this article presents magglite, a toolkit and sketch-based interface builder allowing fast and interactive design of post-wimp user interfaces. magglite improves design of advanced uis thanks to its novel mixed-graph architecture that dynamically combines scene-graphs with interaction- graphs. scene-graphs provide mechanisms to describe and produce rich graphical effects, whereas interaction-graphs allow expressive and fine-grained description of advanced interaction techniques and behaviors such as multiple pointers management, toolglasses, bimanual interaction, gesture, and speech recognition. both graphs can be built interactively by sketching the ui and specifying the interaction using a dataflow visual language. communication between the two graphs is managed at runtime by components we call interaction access points. while developers can extend the toolkit by refining built-in generic mechanisms, ui designers can quickly and interactively design, prototype and test advanced user interfaces by applying the magglite principle: "draw it, connect it and run it".
toward creative 3d modeling: an architects' sketches study. this paper describes our methodology in providing designers with a new advanced 3d modeling human-computer interaction. the main project - called gina, a french acronym for interactive and natural geometry - expect to introduce new intuitive and creative 3d modeling tools, relying on 2d perspective drawing reconstruction. we present here the user study we conducted and the implications of this study on the interface of gina.
architecture for the next generation system management tools. to get more results or greater accuracy, computational scientists execute parallel or distributed applications, and try to scale these applications up. clusters, grids and clouds are by nature different computing platforms: some span across multiple sites and multiple administrative domains, whereas others are single site/domain. as a consequence, scientists have to manage technical details related to both hardware and software constraints of each platform in order to execute their applications. from our point of view, scientists should focus on their research rather than dealing with platform considerations. in this article, we advocate for a system management framework that aims to automatically adapt both hardware and software resources to the applications' needs through a unique method. for each application, scientists describe the requirements through the definition of a virtual platform (vp) and a virtual system environment (vse). relying on these definitions, the framework is in charge of (i) the deployment and the configuration of the virtual platform upon the physical infrastructure, and (ii) the customization of the execution environment upon the former vp. furthermore, we present a refinement of the concept of emulation and virtualization introduced by goldberg. this allows scientists to define complex vps relying on different concepts based on system virtualization (identity, partitioning, aggregation) and emulation (simple, abstraction). leveraging several system management tools (such as oscar, the grid'5000 toolkit, or xtreemos), our current prototype follows a modular approach. we anticipate to be able to easily interface our architecture with recent cloud management solutions (such as opennebula or eucalyptus).
strange particle production in proton-proton collisions at $\sqrt{s}$ = 0.9 tev with alice at the lhc. the production of mesons containing strange quarks ($\kzs$, $\phi$) and both singly and doubly strange baryons ($\rmlambda$, $\rmalambda$, and $\xis$) are measured at central rapidity in pp collisions at $\sqrt{s}$ = 0.9 $\tev$ with the alice experiment at the lhc. the results are obtained from the analysis of about 250 k minimum bias events recorded in 2009. measurements of yields (dn/dy) and transverse momentum spectra at central rapidities for inelastic pp collisions are presented. for mesons, we report yields ($&lt; \dndy &gt;$) of $0.184 \pm 0.002 \stat \pm 0.006 \syst$ for $\kzs$ and $0.021 \pm 0.004 \stat \pm 0.003 \syst$ for $\phi$. for baryons, we find $&lt; \dndy &gt; = 0.048 \pm 0.001 \stat \pm 0.004 \syst$ for $\rmlambda$, $0.047 \pm 0.002 \stat \pm 0.005 \syst$ for $\rmalambda$ and $0.0101 \pm 0.0020 \stat \pm 0.0009 \syst$ for $\xis$. the results are also compared with predictions for identified particle spectra from qcd-inspired models and provide a baseline for comparisons with both future pp measurements at higher energies and heavy-ion collisions.
stiffness modelling of parallelogram-based parallel manipulators. the paper presents a methodology to enhance the stiffness analysis of parallel manipulators with parallelogram-based linkage. it directly takes into account the influence of the external loading and allows computing both the non-linear ``load-deflection" relation and relevant rank-deficient stiffness matrix. an equivalent bar-type pseudo-rigid model is also proposed to describe the parallelogram stiffness by means of five mutually coupled virtual springs. the contributions of this paper are highlighted with a parallelogram-type linkage used in a manipulator from the orthoglide family.
viscosities of the quasigluon plasma. we investigate bulk and shear viscosities of the gluon plasma within relaxation time approximation to an effective boltzmann-vlasov type kinetic theory by viewing the plasma as describable in terms of quasigluon excitations with temperature dependent self-energies. the found temperature dependence of the transport coefficients agrees fairly well with available lattice qcd results. the impact of some details in the quasigluon dispersion relation on the specific shear viscosity is discussed.
performance evaluation of parallel manipulators for milling application. this paper focuses on the performance evaluation of the parallel manipulators for milling of composite materials. for this application the most significant performance measurements, which denote the ability of the manipulator for the machining are defined. in this case, optimal synthesis task is solved as a multicriterion optimization problem with respect to the geometric, kinematic, kinetostatic, elastostostatic, dynamic properties. it is shown that stiffness is an important performance factor. previous models operate with links approximation and calculate stiffness matrix in the neighborhood of initial point. this is a reason why a new way for stiffness matrix calculation is proposed. this method is illustrated in a concrete industrial problem.
stiffness analysis of parallel manipulators with preloaded passive joints. the paper presents a methodology for the enhanced stiffness analysis of parallel manipulators with internal preloading in passive joints. it also takes into account influence of the external loading and allows computing both the non-linear ``load-deflection'' relation and the stiffness matrices for any given location of the end-platform or actuating drives. using this methodology, it is proposed the kinetostatic control algorithm that allows to improve accuracy of the classical kinematic control and to compensate position errors caused by elastic deformations in links/joints due to the external/internal loading. the results are illustrated by an example that deals with a parallel manipulator of the orthoglide family where the internal preloading allows to eliminate the undesired buckling phenomena and to improve the stiffness in the neighborhood of its kinematic singularities.
suppression of charged particle production at large transverse momentum in central pb--pb collisions at $\sqrt{s_{_{nn}}} = 2.76$ tev. inclusive transverse momentum spectra of primary charged particles in pb-pb collisions at $\sqrt{s_{_{nn}}}$ = 2.76 tev have been measured by the alice collaboration at the lhc. the data are presented for central and peripheral collisions, corresponding to 0-5% and 70-80% of the hadronic pb-pb cross section. the measured charged particle spectra in $|\eta|&lt;0.8$ and $0.3 &lt; p_t &lt; 20$ gev/$c$ are compared to the expectation in pp collisions at the same $\sqrt{s_{_{nn}}}$, scaled by the number of underlying nucleon-nucleon collisions. the comparison is expressed in terms of the nuclear modification factor $r_{aa}$. the result indicates only weak medium effects ($r_{aa} \approx $ 0.7) in peripheral collisions. in central collisions, $r_{aa}$ reaches a minimum of about 0.14 at $p_t=6$-7gev/$c$ and increases significantly at larger $p_t$. the measured suppression of high-$p_t$ particles is stronger than that observed at lower collision energies, indicating that a very dense medium is formed in central pb-pb collisions at the lhc.
optimal maintenance and replacement decisions under technological change. the requirement of equipment improvement in order to satisfy the safety and reliability of system motivates the development of technology. the presence or expectation of technologically better equipment will influence managerial decisions on whether to invest in the maintenance of current equipment, invest in replacement with an equivalent model, replacement with a higher technology model currently available on the market, or wait for a potentially even better technology to appear in the near future. hence, the consideration of technological change is a very important aspect for maintenance and replacement decisions. this paper aims to define a model that allows us to gain insight into how maintenance/replacement policies will be influenced by the expectation of future technology. we then use stochastic dynamic programming (i.e., markov decision process) to solve for the optimal maintenance and replacement policy of the equipment as a function of performance and cost. finally, we illustrate the problem through several numerical examples.
enabling tool reuse and interoperability through model-driven engineering. null
new developments of epos 2. since 2006, epos hadronic interaction model is being used for very high energy cosmic ray analysis. designed for minimum bias particle physics and used for having a precise description of sps and rhic heavy ion collisions, epos brought more detailed description of hadronic interactions in air shower development. thanks to this model it was possible to understand why there were less muons in air shower simulations than observed in real data. with the start of the lhc era, a better description of hard processes and collective effects is needed to deeply understand the incoming data. we will describe the basic physics in epos and the new developments and constraints which are taken into account in epos 2.
dealing with non-functional requirements in model-driven development,. null
automatic generation of basic behavior schemas from uml class diagrams. null
verification and validation of declarative model-to-model transformations through invariants. in this paper we propose a method to derive ocl invariants from declarative model-to-model transformations in order to enable their verification and analysis. for this purpose we have defined a number of invariant-based verification properties which provide increasing degrees of confidence about transformation correctness, such as whether a rule (or the whole transformation) is satisfiable by some model, executable or total. we also provide some heuristics for generating meaningful scenarios that can be used to semi-automatically validate the transformations. as a proof of concept, the method is instantiated for two prominent model-to-model transformation languages: triple graph grammars and qvt.
adopting agile methods: can goal-oriented social modeling help?. the heavy reliance on the human factor in agile methods poses new challenges for organizations intent on adopting them. improper role assignment, neglected team dependencies, and overlooked required skills have all been reported as reasons for failures during the introduction of an agile method. current process modelling languages are not designed for describing or analyzing such human-related issues, and thus, provide little assistance to organizations in the process of adopting an agile method. this paper advocates the use of goal-oriented modeling techniques for depicting social aspects of agile methods. these social models can be used to identify the key factors that contribute to the success or failure of an agile method, thus providing guidance early during the introduction of the method in an organization. the approach is illustrated using the scrum process.
tools for modeling and generating safe interface interactions in web applications. modern web applications that embed sophisticated user interfaces and business logic have rendered the original interaction paradigm of the web obsolete. in previous work, we have advocated a paradigm shift from static content pages that are browsed by hyperlinks to a state-based model where back and forward navigation is replaced by a full-fledged interactive application paradigm, featuring undo and redo capabilities, with support for exception management policies and transactional properties. in this demonstration, we present an editor and code generator designed to build applications based on our approach.
synthesis of ocl pre-conditions for graph transformation rules. graph transformation (gt) is being increasingly used in model driven engineering (mde) to describe in-place transformations like animations and refactorings. for its practical use, rules are often complemented with ocl application conditions. the advancement of rule post-conditions into pre-conditions is a well-known problem in gt, but current techniques do not consider ocl. in this paper we provide an approach to advance post-conditions with arbitrary ocl expressions into pre-conditions. this presents benefits for the practical use of gt in mde, as it allows: (i) to automatically derive pre-conditions from the meta-model integrity constraints, ensuring rule correctness, (ii) to derive pre-conditions from graph constraints with ocl expressions and (iii) to check applicability of rule sequences with ocl conditions.
specifying aggregation functions in multidimensional models with ocl. multidimensional models are at the core of data warehouse systems, since they allow decision makers to early define the relevant information and queries that are required to satisfy their information needs. the use of aggregation functions is a cornerstone in the definition of these multidimensional queries. however, current proposals for multidimensional modeling lack the mechanisms to define aggregation functions at the conceptual level: multidimensional queries can only be defined once the rest of the system has already been implemented, which requires much effort and expertise. in this sense, the goal of this paper is to extend the object constraint language (ocl) with a predefined set of aggregation functions. our extension facilitates the definition of platform-independent queries as part of the specification of the conceptual multidimensional model of the data warehouse. these queries are automatically implemented with the rest of the data warehouse during the code-generation phase. the ocl extensions proposed in this paper have been validated by using the use tool.
situational evaluation of method fragments: an evidence-based goal-oriented approach. despite advances in situational method engineering, many software organizations continue to adopt an ad-hoc mix of method fragments from well-known development methods such as scrum or xp, based on their perceived suitability to project or organizational needs. with the increasing availability of empirical evidence on the success or failure of various software development methods and practices under different situational conditions, it now becomes feasible to make this evidence base systematically accessible to practitioners so that they can make informed decisions when creating situational methods for their organizations. this paper proposes a framework for evaluating the suitability of candidate method fragments prior to their adoption in software projects. the framework makes use of collected knowledge about how each method fragment can contribute to various project objectives, and what requisite conditions must be met for the fragment to be applicable. pre-constructed goal models for the selected fragments are retrieved from a repository, merged, customized with situational factors, and then evaluated using a qualitative evaluation procedure adapted from goal-oriented requirements engineering.
industrialization of research tools: the atl case. research groups develop plenty of tools aimed at solving real industrial problems. unfortunately, most of these tools remain as simple proof-of-concept tools that companies consider too risky to use due to their lack of proper user interface, documentation, completeness, support, etc that companies expect from commercial-quality level tools. based on our tool development experience in the atlanmod research team, specially regarding the evolution of our atl model transformation tool, we argue in this paper that the best solution for research teams aiming to create high-quality and widely-used tools is to industrialize their research prototypes through a partnership with a technology provider.
combining model-driven engineering and cloud computing. service-orientation and model-driven engineering are two of the most dominant software engineering paradigms nowadays. this position paper explores the synergies between them and show how they can benefit from each other. in particular, the paper introduces the notion of modeling as a service (maas) as a way to provide modeling and model-driven engineering services from the cloud.
model transformation chains in model-driven performance engineering: experiences and future research needs. null
towards incremental execution of atl transformations. up to now, the execution of atl transformations has always followed a two-step algorithm: 1) matching all rules, 2) applying all matched rules. this algorithm does not support incremental execution. for instance, if a source model is updated, the whole transformation must be executed again to get the updated target model. in this paper, we present an incremental execution algorithm for atl, as well as a prototype. with it, changes in a source model are immediately propagated to the target model. our approach leverages previous works of the community, notably on live transformations and incremental ocl. we achieve our goal on a subset of atl, without requiring modifications to the language.
operating systems and virtualization frameworks: from local to distributed similarities. virtualization technologies radically changed the way in which distributed architectures are exploited. with the contribution of vm capabilities and with the emergence of iaas platforms, more and more frameworks tend to manage vms across distributed architectures like operating systems handle processes on a single node. taking into account that most of these frameworks follow a centralized model – where roughly one node is in charge of the management of vms – and considering the growing size of infrastructures in terms of nodes and vms, new proposals relying on more autonomic and decentralized approaches should be submitted. designing and implementing such models is a tedious and complex task. however, as well as research studies on oses and hypervisors are complementary at the node level, we advocate that virtualization frameworks can benefit from lessons learnt from distributed operating system proposals. in this article, we motivate such a position by analyzing similarities between oses and virtualization frameworks. more precisely, we focus on the management of processes and vms, first at the node level and then on a cluster scale. from our point of view, such investigations can guide the community to design and implement new proposals in a more autonomic and distributed way.
charged-particle multiplicity density at midrapidity in central pb-pb collisions at sqrt(snn) = 2.76 tev. the first measurement of the charged-particle multiplicity density at mid-rapidity in pb-pb collisions at a centre-of-mass energy per nucleon pair sqrt(snn) = 2.76 tev is presented. for an event sample corresponding to the most central 5% of the hadronic cross section the pseudo-rapidity density of primary charged particles at mid-rapidity is 1584 +- 4 (stat) +- 76 (sys.), which corresponds to 8.3 +- 0.4 (sys.) per participating nucleon pair. this represents an increase of about a factor 1.9 relative to pp collisions at similar collision energies, and about a factor 2.2 to central au-au collisions at sqrt(snn) = 0.2 tev. this measurement provides the first experimental constraint for models of nucleus-nucleus collisions at lhc energies.
optimal condition-based resurfacing decisions for roads. we develop a condition-based maintenance optimization approach for the road cracking problem in order to derive an optimal action-planning policy from a markov decision process that minimizes the expected cost. our model deals with multiple imperfect actions that consist of different resurfacing thicknesses and that have varying effects on the future deterioration law. we model the deterioration after maintenance to be dependent upon both the state of the road before maintenance and the type of maintenance performed. another new aspect of our model is the possibility for a maintenance action to render the road to a state better than as-good-as-new. the application of new road layers is, however, constrained by a maximum total road thickness. some maintenance actions may then become infeasible due to the thickness of the road relative to the constraint. as the road is constrained by a maximum total thickness, the maintenance decision is made complex by the determination of not only how thick of a layer to apply, but also how much old road to remove. we model the road state by two deterioration variables: the longitudinal cracking percentage and its associated growth rate. our degradation model will take into account the changing road composition which is a function of the successive application of new and removal of old layers.
a condition-based imperfect maintenance model with action dependent deterioration. we propose a maintenance decision framework where the state of the system is based on a percentage of total degrada- tion and the rate of change of the degradation. at each decision interval, the degradation percentage may be observed perfectly as well as the rate of change with respect to the last observation. we consider ﬁve possible actions based on these two observable phenomenon: do nothing, three imperfect maintenance actions, and a complete renewal. prior imperfect maintenance models do not consider a scenario where imperfect maintenance not only restores the system to a state less than “as good as new” but also under a new deterioration law. in this case, it is necessary to consider that the rate of degradation after imperfect maintenance will be altered based on the action performed and the state of the system prior to the maintenance. we propose to develop an imperfect maintenance optimization model based on a markov decision process framework to determine the optimal maintenance actions given the state of the system at each decision interval and motivate the problem through road maintenance.
optimal highway maintenance policies under uncertainty. we develop an inspection and maintenance policy to minimize the cost of maintaining a given section of road or highway when there is a great deal of uncertainty in the degradation process. we propose to model the degradation of a section of road based on the proliferation and growth of cracks. we utilize a combination of a poisson and gamma process to account for the tremendous amount of uncertainty and difficulty in predicting the proliferation of cracks. our policy defines the optimal inspection interval as well as the minimum threshold at which to perform crack repairs. furthermore, our policy contains a safety constraint to prevent the probability of a "catastrophic" failure from exceeding a pre-determined reliability value. numerical calculations have shown that our model will extend the lifecycle of the road by performing preventive, conditioned-based maintenance to slow down the growth of cracks. classical preventive maintenance policies usually shorten the lifecycle by forcing earlier renewals.
multiperiodic planning and routing on a rolling horizon for field force optimization logistics. field force planning and optimization is a new challenge in logistics for the service sector. in the energy or water distribution areas, it consists in planning the visits of commercial or technical personnel on the field over a set of time periods (days) to visit industrial facilities or customers in order to satisfy requests for service. demands can be for planned maintenance of equipments, commercial activities or emergency maintenance activities. our application is relative to water distribution and field force optimization for planning the visits of personnel to clients over a multiperiod horizon and a specific geographic area. visits to customers can be planned through a call center upon customer demand or upon the company's request. in order to provide a tool for optimization on a rolling horizon in this context and test these hypotheses, we have proposed a reoptimization procedure. we have developed a demand generator and solution technique using the memetic algorithm and column generation methods previously developed for the fixed horizon planning problem. experiences on realistic data validate our approach and allow us to suggest some further researches for this problem.
multiperiod planning and routing on a rolling horizon for field force optimization logistics. we address the problem of the planning and scheduling of technicians visits to customers on the field, in the context of service logistics over a multiperiod horizon. we have developed a vrp type memetic algorithm and a column generation/branch and bound heuristic in order to optimize the plan over a static horizon. we have then adapted our procedures to cope with a rolling horizon context, when a new plan has to be determined after the execution of each first daily period of the previous plan. we have tested our procedures on realistic data from the water distribution context, and obtained good solutions in a reasonable amount of time. we show in particular the advantage of reutilization of partial solutions from the previous plan, for the optimization of each new plan. directions for further research are then indicated.
a tabu search algorithm for the integrated scheduling problem of container handling systems in a maritime terminal. the scheduling problem in a container terminal is characterized by high level of coordination of different types of equipment. in this paper we present an integrated model to schedule these equipment at the same time with the objective to minimize the makespan, or the time it takes to serve a given set of ships. the problem is formulated as a hybrid flow shop scheduling problem with precedence and blocking constraints (hfss-b). a tabu search algorithm is proposed to solve this problem. certain mechanisms are developed and introduced into the algorithm to assure the quality and efficiency of the algorithm. the performance of the tabu search algorithm is analyzed from the computational point of view.
analytic expressions for alpha-decay half-lives and potential barriers. from an adjustment on a recent selected data set of partial $\alpha$-decay half-lives of 344 ground state to ground state transitions, analytical formulas are proposed for $log_{10}t_{1/2}(s)$ depending or not on the angular momentum of the $\alpha$ particle. in particular, an expression allows to reproduce precisely the partial $\alpha$-decay half-lives of even-even heavy nuclei and, then, to predict accurately the partial $\alpha$-decay half-lives of other very heavy elements from the experimental or predicted $q_\alpha$. comparisons have been done with other empirical approaches. moreover, the potential barrier against $\alpha$-decay or $\alpha$-capture has been determined within a liquid drop model including a proximity energy term. simple expressions are provided to calculate the potential barrier radius and height.
elliptic flow of charged particles in pb-pb collisions at 2.76 tev. we report the first measurement of charged particle elliptic flow in pb-pb collisions at 2.76 tev with the alice detector at the cern large hadron collider. the measurement is performed in the central pseudorapidity region (|eta|&lt;0.8) and transverse momentum range 0.2&lt; p_t&lt; 5.0 gev/c. the elliptic flow signal v_2, measured using the 4-particle correlation method, averaged over transverse momentum and pseudorapidity is 0.087 +/- 0.002 (stat) +/- 0.004 (syst) in the 40-50% centrality class. the differential elliptic flow v_2(p_t) reaches a maximum of 0.2 near p_t = 3 gev/c. compared to rhic au-au collisions at 200 gev, the elliptic flow increases by about 30%. some hydrodynamic model predictions which include viscous corrections are in agreement with the observed increase.
prompt and non-prompt j/psi production in pp collisions at sqrt(s) = 7 tev. the production of j/psi mesons is studied in pp collisions at sqrt(s)=7 tev with the cms experiment at the lhc. the measurement is based on a dimuon sample corresponding to an integrated luminosity of 314 inverse nanobarns. the j/psi differential cross section is determined, as a function of the j/psi transverse momentum, in three rapidity ranges. a fit to the decay length distribution is used to separate the prompt from the non-prompt (b hadron to j/psi) component. integrated over j/psi transverse momentum from 6.5 to 30 gev/c and over rapidity in the range |y| &lt; 2.4, the measured cross sections, times the dimuon decay branching fraction, are 70.9 \pm 2.1 (stat.) \pm 3.0 (syst.) \pm 7.8(luminosity) nb for prompt j/psi mesons assuming unpolarized production and 26.0 \pm 1.4 (stat.) \pm 1.6 (syst.) \pm 2.9 (luminosity) nb for j/psi mesons from b-hadron decays.
cold nuclear matter effects on j/psi yields as a function of rapidity and nuclear geometry in deuteron-gold collisions at sqrt(s_nn) = 200 gev. we present measurements of j/psi yields in d+au collisions at sqrt(s_nn) = 200 gev recorded by the phenix experiment and compare with yields in p+p collisions at the same energy per nucleon-nucleon collision. the measurements cover a large kinematic range in j/psi rapidity (-2.2 &lt; y &lt; 2.4) with high statistical precision and are compared with two theoretical models: one with nuclear shadowing combined with final state breakup and one with coherent gluon saturation effects. to remove model dependent systematic uncertainties we also compare the data to a simple geometric model. we find that calculations where the nuclear modification is linear or exponential in the density weighted longitudinal thickness are difficult to reconcile with the forward rapidity data.
managing virtual resources: fly through the sky. virtualization technologies have been a key element in the adoption of infrastructure-as-a-service (iaas) cloud computing platforms as they radically changed the way in which distributed architectures are exploited. however, a closer look suggests that the way of managing virtual and physical resources still remains relatively static.
fiesta toolkit: model-driven software product lines in practice. model-driven software product lines (md-spls) are those product lines whose members are created by using models and model transformations as first-class artifacts. in this paper we present the fiesta toolkit, a set of tools to assist product line architects and products designers when creating md-spls. the main characteristic of our toolkit is that it facilitates the creation of fine-grained configurations of products, i.e. configurations at the level of each element in models that will be transformed into product line members. for that, the toolkit includes a set of tools for the creation of md-spl projects, feature models, constraint models, binding models, ocl-type expressions to validate binding models against constraint models, and decision models.
automated reasoning for derivation of model-driven spls. model-driven spl approaches use metamodels and transformation rules to obtain concrete software artifacts departing from models. most of such approaches use also feature models to express variability. because of the variability, to derive products, they have to adapt the transformation rules according to user choices captured in feature configurations. in this paper we propose an approach based on constraint programming to derive model-driven spls. our contribution is twofold. first, we assist product line architects when relating transformation rules and features in order to derive prod- ucts based on feature configurations; the novelty is that we facilitate the management of feature interactions to architects. second, current approaches to reason on feature models in spl engineering only deal with problems related to product configuration. we improve such approaches adding facilities for product derivation.
controlling contractors with monads for hybrid dynamical systems. physical systems with intelligent behaviors result from inter-actions of different fields: sensor networks, robotics, optimization, reasoning, etc. rooted in this philosophy of interdisciplinary, this paper makes a connexion between hybrid dynamical systems, interval-based constraint propagation and functional programming. it shows how to build a monadic program in haskell to control contractors (constraint propagators) for the state estimation of multi-model (hy- brid) dynamical systems, subject to partial and uncertain measurements. the example of system taken here is an elevator that can either be moving upward, downward or stopped. the altitude is measured directly and the estimation problem is simply to track its motion. the purpose of the haskell library is to offer both a high-level and flexible framework for building propagation strategies based on user knowledge or user requirements.
clusters in light nuclei. a great deal of research work has been undertaken in the alpha-clustering study since the pioneering discovery, half a century ago, of 12c+12c molecular resonances. our knowledge of the field of the physics of nuclear molecules has increased considerably and nuclear clustering remains one of the most fruitful domains of nuclear physics, facing some of the greatest challenges and opportunities in the years ahead. in this work, the occurence of "exotic" shapes in light n=z alpha-like nuclei is investigated. various approaches of superdeformed and hyperdeformed bands associated with quasimolecular resonant structures are presented. results on clustering aspects are also discussed for light neutron-rich oxygen isotopes.
dilated lmi characterizations for linear time-invariant singular systems. this article explores, based on projection lemma, dilated linear matrix inequality (lmi) characterisations for linear time-invariant singular systems. the deduced formulations cover not only the existing results reported in the literature, but also complete some missing lmi conditions. this article also lightens the mutual relations of these characterisations and clarifies the relation between the dilated lmis for the conventional state-space systems and those for singular systems. the well-known results for the state-space setting reported in the literature can be reviewed as special cases.
self-optimisation of the energy footprint in service-oriented architectures. the impact of it on the global energy consumption has frighteningly increased over the last years. one of the reasons for this is the demand for infrastructure to support the increasing number of online (24x7) services and data, followed by the popularisation of practices like cloud computing. from the infrastructure point of view, hardware throttling and server consolidation are techniques used to deal with energy efficiency. however, details about the application behavior are not visible from the infrastructure layer, which prevents a more complete energy-efficient treatment. this paper presents an approach for self-optimisation of the energy consumption at the application layer. we rely on service-oriented architectures, since they allow rapid and seamless service composition and eases the application adaptation. the energy efficiency properties of services are defined by means of quality of service criteria and a set of event-condition-actions is defined to enable the application to react to environmental changes and optimise its energy consumption. as a proof of concept, we present a prototype for energy-aware self-adaptation in soa-based applications as well as an example scenario that shows the practical usage of our approach.
modisco: a generic and extensible framework for model driven reverse engineering. nowadays, almost all companies, independently of their size and type of activity, are facing the problematic of having to manage, maintain or even replace their legacy systems. many times, the first problem they need to solve is to really understand what are the functionalities, architecture, data, etc of all these often huge legacy applications. as a consequence, reverse engineering still remains a major challenge for software engineering today. this paper introduces modisco, a generic and extensible open source reverse engineering solution. modisco intensively uses mde principles and techniques to improve existing approaches for reverse engineering.
model-driven interoperability of dependencies visualizations. software tools and corresponding knowledge tend to be collected and packaged into platforms like eclipse, mathlab or kde. their success and usefulness combined with their growing size and complexity rise issues about management of dependencies between their components and between the platform and other applications which rely on its plug-in system and/or provided functionalities. such problems imply need for dependencies management tools in which visualization is a core feature. as dependencies are also a concern in domains like object-oriented programming or operating system packaging, we may expect to reuse corresponding works in visualization. but each domain and its related dependencies problem have induced their own hard coded viewing and browsing tools. in this article we present how we have reuse existing visualization tools for our platform cartography together with our own displays using a model-driven interoperability approach to easily realize bindings between visualization tools.
how to deal with your it legacy? reverse engineering with modisco... the modisco (model discovery) project is an eclipse modeling project dedicated to reverse engineering. to this end, it provides a customizable model-driven reverse engineering (mdre) framework. legacy systems and corresponding data currently embrace a large number of heterogeneous technologies, making the design, development and maintenance of tools dealing with the reuse or evolution of such legacy a tedious and time consuming task. as reverse-engineering projects usually face with both the combination of many technologies and various different scenarios, model-driven approaches and related tools offer the required abstraction level to build up mature and flexible solutions. the modisco generic and extensible framework is dedicated to the resolution of these concrete problems by allowing: 1) the description of the information extracted out of the legacy as models; 2) the understanding of these models in order to take the most efficient decisions; 3) the transformation of these models into other exploitable artifacts (source code, documentation, metrics, etc). this framework has been designed to be applied on many different reverse-engineering use cases, such as those mentioned in this non-exhaustive list: 1) the migration/modernization of existing systems considering their architecture, used technologies or just available data; 2) the documentation of complex legacy in order to better understand their different aspects and specificities; 3) the evaluation of such legacy in terms of quality (computation of metrics, detection of anti-patterns, etc). this talk will present the overall status of the situation within the modisco project. it will start by briefly summarizing the main objectives of modisco and will introduce its general organization and actual development team. the focus will be then set on more precisely describing the various components now available from the provided mdre framework, emphasizing on different possible concrete applications of these tools and underlying approach. finally, the future of modisco will be discussed interactively with the audience considering as potential subjects the global project roadmap, the next components to be released, the possible evolution of the community, etc.
model driven tool interoperability in practice. model driven engineering (mde) advocates the use of models, metamodels and model transformations to revisit some of the classical operations in software engineering. mde has been mostly used with success in forward and reverse engineering (for software development and better maintenance, respectively). supporting system interoperability is a third important area of applicability for mde. the particular case of tool interoperability is currently receiving a lot of interest. in this paper, we describe some experiments in this area that have been performed in the context of open source modeling efforts. taking stock of these achievements, we propose a general framework where various tools are associated to implicit or explicit metamodels. one of the interesting properties of such an organization is that it allows designers starting some software engineering activity with an informal light-weight tool and carrying it out later on in a more complete or formal context. we analyze such situations and discuss the advantages of using mde to build a general tool interoperability framework.
towards model driven tool interoperability: bridging eclipse and microsoft modeling tools. successful application of model-driven engineering approaches requires interchanging a lot of relevant data among the tool ecosystem employed by an engineering team (e.g., requirements elicitation tools, several kinds of modeling tools, reverse engineering tools, development platforms and so on). unfortunately, this is not a trivial task. poor tool interoperability makes data interchange a challenge even among tools with a similar scope. this paper presents a model-based solution to overcome such interoperability issues. with our approach, the internal schema/s (i.e., metamodel/s) of each tool are explicited and used as basis for solving syntactic and semantic dierences between the tools. once the corresponding metamodels are aligned, model-to model transformations are (semi)automatically derived and executed to perform the actual data interchange. we illustrate our approach by bridging the eclipse and microsoft (dsl tools and sql server modeling) modeling tools.
inter-dsl coordination support by combining megamodeling and model weaving. model-driven engineering (mde) advocates the use of models at every step of the software development process. within this context, a team of engineers collectively and collaboratively contribute to a large set of interrelated models. even if the main focus can be on a single model (e.g. a class diagram model), related elements in other models (e.g. a requirement model) often have to be considered and/or accessed. moreover, all the involved models do not necessarily conform to the same metamodel and thus may have been built using different independent domain-specific languages (dsls). such a situation has already been frequently observed in many large-scale industrial deployments of mde. manually coordinating all the involved models, i.e. being able to both manage and use the links existing between them, can become a cumbersome and difficult task. as a proposal to solve this inter-dsl coordination issue, we introduce in this paper a generic and extensible inter-model traceability and navigation environment based on the complementary use of megamodeling and model weaving. we illustrate our solution with a concrete working example.
measurement of bottom versus charm as a function of transverse momentum with electron-hadron correlations in p+p collisions at sqrt(s)=200 gev. the momentum distribution of electrons from semi-leptonic decays of charm and bottom for mid-rapidity |y|&lt;0.35 in p+p collisions at sqrt(s)=200 gev is measured by the phenix experiment at the relativistic heavy ion collider (rhic) over the transverse momentum range 2 &lt; p_t &lt; 7 gev/c. the ratio of the yield of electrons from bottom to that from charm is presented. the ratio is determined using partial d/d^bar --&gt; e^{+/-} k^{-/+} x (k unidentified) reconstruction. it is found that the yield of electrons from bottom becomes significant above 4 gev/c in p_t. a fixed-order-plus-next-to-leading-log (fonll) perturbative quantum chromodynamics (pqcd) calculation agrees with the data within the theoretical and experimental uncertainties. the extracted total bottom production cross section at this energy is \sigma_{b\b^bar}= 3.2 ^{+1.2}_{-1.1}(stat) ^{+1.4}_{-1.3}(syst) micro b.
adaptation and evaluation of generic model matching strategies. model matching is gaining importance in model-driven engineering (mde). the goal of model matching is to identify correspondences between the elements of two metamodels or two models. one of the main application scenarios is the derivation of model transformations from metamodel correspondences. model correspondences, in turn, offer a potential to address other mde needs. manually finding of correspondences is labor intensive and error-prone when (meta)models are large. to automate the process, research community proposes matching strategies combining multiple heuristics. a problem is that the heuristics are limited to certain representation formalisms instead of being reusable. another problem is the difficulty to systematically evaluate the quality of matching strategies. this work contributes an approach to deal with the mentioned issues. to promote reusability, the approach consists of strategies whose heuristics are loosely coupled to a given formalism. to systematize model matching evaluation, the approach automatically extracts a large set of modeling test cases from model repositories, and uses megamodels to guide strategy execution. we have validated the approach by developing the aml domain specific language on top of the amma platform. by using aml, we have implemented a library of strategies and heuristics. to demonstrate that our approach goes beyond the modeling context, we have tested our strategies on ontology test cases as well. at last, we have contributed three use cases that show the applicability of (meta)model matching to interesting mde topics: model co-evolution, pivot metamodel evaluation, and model synchronization.
applying mde for the validation of correct eclipse plugin bundles. abstract. complex software systems are often constructed by assembling bundles from repositories. eclipse is one of these systems; build on top of a platform accepting different sets of bundles according to the user needs. this adaptability is one of the main benefits of this kind of systems but implies also several configuration problems. the consistency of eclipse plug‐in's bundles is one of them. this problem involves a need for the configuration validation. to adress this problem, this paper proposes an approach using model driven engineering. the presented solution combines different mde techniques such as global model management and model transformations to check the coherency of eclipse plug‐in's bundles.
multi-level tests for model driven web applications. null
improving higher-order transformations support in atl. null
search computing: a model-driven perspective. null
commissioning of the alice muon spectrometer trigger at lhc. alice (a large ion collider experiment) is the lhc experiment dedicated to the study of ultra-relativistic heavy ion collisions. the alice muon spectrometer covers a large range in pseudo-rapidity and is designed to study quarkonia and heavy flavours decaying into (di-)muons. the high particle multiplicities environment in such collisions require a specific, fast and efficient trigger system, the muon trigger. it consists of four planes of rpc detectors, covering an area of 36 m2 each, 21k front-end channels and a fast-decision electronics. the muon trigger is designed to reconstruct (muon) tracks and deliver a trigger signal each 25 ns (40 mhz) with a total latency of 800 ns. the hit position on the rpc is measured in two orthogonal directions with an accuracy of about 1 cm. the performance measured with the first p–p collisions at sqrt (s)= 900 gev carried out in december 2009 is reported.
double helicity dependence of jet properties from dihadrons in longitudinally polarized p+p collisions at sqrt(s) = 200 gev. it has been postulated that partonic orbital angular momentum can lead to a significant double-helicity dependence in the net transverse momentum of drell-yan dileptons produced in longitudinally polarized p+p collisions. analogous effects are also expected for dijet production. if confirmed by experiment, this hypothesis, which is based on semi-classical arguments, could lead to a new approach for studying the contributions of orbital angular momentum to the proton spin. we report the first measurement of the double-helicity dependence of the dijet transverse momentum in longitudinally polarized p+p collisions at sqrt(s) = 200 gev from data taken by the phenix experiment in 2005 and 2006. the analysis deduces the transverse momentum of the dijet from the widths of the near- and far-side peaks in the azimuthal correlation of the dihadrons. when averaged over the transverse momentum of the triggered particle, the difference of the root-mean-square of the dijet transverse momentum between like- and unlike-helicity collisions is found to be -37 +/- 88(stat) +/- 14(syst) mev/c.
a linear relaxation-based heuristic approach for logistics network design. we address the problem of designing and planning a multi-period, multi-echelon, multi-commodity logistics network with deterministic demands. this consists of making strategic and tactical decisions: opening, closing or expanding facilities, selecting suppliers and defining the product flows. we use a heuristic approach based on the linear relaxation of the original mixed integer linear problem (milp). the main idea is to solve a sequence of linear relaxations of the original milp, and to fix as many binary variables as possible at every iteration. this simple process is coupled with several rounding procedures for some key decision variables. the number of binary decision variables in the resulting milp is small enough for it to be solved with a solver. the main benefit of this approach is that it provides feasible solutions of good quality within an affordable computation time.
the time-consistent vehicle routing problem. null
a new time-consistent vehicle routing problem for the transportation of handicapped persons. null
the exposure of the hybrid detector of the pierre auger observatory. the pierre auger observatory is a detector for ultra-high energy cosmic rays. it consists of a surface array to measure secondary particles at ground level and a fluorescence detector to measure the development of air showers in the atmosphere above the array. the "hybrid" detection mode combines the information from the two subsystems. we describe the determination of the hybrid exposure for events observed by the fluorescence telescopes in coincidence with at least one water-cherenkov detector of the surface array. a detailed knowledge of the time dependence of the detection operations is crucial for an accurate evaluation of the exposure. we discuss the relevance of monitoring data collected during operations, such as the status of the fluorescence detector, background light and atmospheric conditions, that are used in both simulation and reconstruction.
suppression of away-side jet fragments with respect to the reaction plane in au+au collisions at sqrt(s_nn) = 200 gev. pair correlations between large transverse momentum neutral pion triggers (p_t=4--7 gev/c) and charged hadron partners (p_t=3--7 gev/c) in central (0--20%) and midcentral (20--60%) au+au collisions are presented as a function of trigger orientation with respect to the reaction plane. the particles are at larger momentum than where jet shape modifications have been observed, and the correlations are sensitive to the energy loss of partons traveling through hot dense matter. an out-of-plane trigger particle produces only 26+/-20% of the away-side pairs that are observed opposite of an in-plane trigger particle. in contrast, near-side jet fragments are consistent with no suppression or dependence on trigger orientation with respect to the reaction plane. these observations are qualitatively consistent with a picture of little near-side parton energy loss either due to surface bias or fluctuations and increased away-side parton energy loss due to a long path through the medium. the away-side suppression as a function of reaction-plane angle is shown to be sensitive to both the energy loss mechanism in and the space-time evolution of heavy-ion collisions.
proceedings of the first international workshop on multiple partonic interactions at the lhc (mpi08). the objective of this first workshop on multiple partonic interactions (mpi) at the lhc is to raise the profile of mpi studies, summarizing the legacy from the older phenomenology at hadronic colliders and favouring further specific contacts between the theory and experimental communities. the mpi are experiencing a growing popularity and are currently widely invoked to account for observations that would not be explained otherwise: the activity of the underlying event, the cross sections for multiple heavy flavour production, the survival probability of large rapidity gaps in hard diffraction, etc. at the same time, the implementation of the mpi effects in the monte carlo models is quickly proceeding through an increasing level of sophistication and complexity that in perspective achieves deep general implications for the lhc physics. the ultimate ambition of this workshop is to promote the mpi as unification concept between seemingly heterogeneous research lines and to profit of the complete experimental picture in order to constrain their implementation in the models, evaluating the spin offs on the lhc physics program.
cross section and double helicity asymmetry for eta mesons and their comparison to neutral pion production in p+p collisions at sqrt(s)=200 gev. measurements of double-helicity asymmetries for inclusive hadron production in polarized p+p collisions are sensitive to helicity--dependent parton distribution functions, in particular to the gluon helicity distribution, delta(g). this study focuses on the extraction of the double-helicity asymmetry in eta production: polarized p+p --&gt; eta + x, the eta cross section, and the eta/pi^0 cross section ratio. the cross section and ratio measurements provide essential input for the extraction of fragmentation functions that are needed to access the helicity-dependent parton distribution functions.
first pp results... from the aa physics perspective. null
comparison of the expressiveness of arc, place and transition time petri nets. null
scoping strategies for distributed aspects. dynamic deployment of aspects brings greater flexibility and reuse potential, but requires a proper means for scoping aspects. scoping issues are particularly crucial in a distributed context: adequate treatment of distributed scoping is necessary to enable the propagation of aspect instances across host boundaries and to avoid inconsistencies due to unintentional spreading of data and computations in a distributed system. we motivate the need for expressive scoping of dynamically-deployed distributed aspects by an analysis of the deficiencies of current approaches for distributed aspects. extending recent work on scoping strategies for non-distributed aspects, we then introduce a set of high-level strategies for specifying locality of aspect propagation and activation, and illustrate the corresponding gain in expressiveness. we present the operational semantics of our proposal using scheme interpreters, first introducing a model of distributed aspects that covers the range of current proposals, and then extending it with dynamic aspect deployment and scoping strategies. this work shows that, given some extensions to their original execution model, scoping strategies are directly applicable to the expressive scoping of distributed aspects.
update on the correlation of the highest energy cosmic rays with nearby extragalactic matter. data collected by the pierre auger observatory through 31 august 2007 showed evidence for anisotropy in the arrival directions of cosmic rays above the greisen-zatsepin-kuz'min energy threshold, \nobreak{$6\times 10^{19}$ev}. the anisotropy was measured by the fraction of arrival directions that are less than $3.1^\circ$ from the position of an active galactic nucleus within 75 mpc (using the véron-cetty and véron $12^{\rm th}$ catalog). an updated measurement of this fraction is reported here using the arrival directions of cosmic rays recorded above the same energy threshold through 31 december 2009. the number of arrival directions has increased from 27 to 69, allowing a more precise measurement. the correlating fraction is $(38^{+7}_{-6})%$, compared with $21%$ expected for isotropic cosmic rays. this is down from the early estimate of $(69^{+11}_{-13})%$. the enlarged set of arrival directions is examined also in relation to other populations of nearby extragalactic objects: galaxies in the 2 microns all sky survey and active galactic nuclei detected in hard x-rays by the swift burst alert telescope. a celestial region around the position of the radiogalaxy cen a has the largest excess of arrival directions relative to isotropic expectations. the 2-point autocorrelation function is shown for the enlarged set of arrival directions and compared to the isotropic expectation.
macro-microscopic mass formulae and nuclear mass predictions. different mass formulae derived from the liquid drop model and the pairing and shell energies of the thomas-fermi model have been studied and compared. they include or not the diffuseness correction to the coulomb energy, the charge exchange correction term, the curvature energy, different forms of the wigner term and powers of the relative neutron excess i = (n − z)/a. their coefficients have been determined by a least square fitting procedure to 2027 experimental atomic masses [1]. the coulomb diffuseness correction z2/a term or the charge exchange correction z4/3/a1/3 term plays the main role to improve the accuracy of the mass formula. the wigner term and the curvature energy can also be used separately but their coefficients are very unstable. the different fits lead to a surface energy coefficient of around 17-18 mev. a large equivalent rms radius (r0 = 1.22−1.24 fm) or a shorter central radius may be used. a rms deviation of 0.54 mev can be reached between the experimental and theoretical masses. the remaining differences come probably mainly from the determination of the shell and pairing energies. mass predictions of selected expressions have been compared to 161 new experimental masses and the correct agreement allows to provide extrapolations to masses of 656 selected exotic nuclei.
on solving inverse problems for electric fish like robots. this paper relates preliminary results concerning the solution of inverse problems arising in electric sense based navigation. this sense is used by electric fishes to move in dark waters using the electric current measurements perceived by the epidermal sensors as these are affected by the presence of obstacles. the latter change the resulting induced measures by instantaneously disturbing the fish self-produced electric field. the approach lies on a recently proposed graphical signature based classification methodology to overcome the computational burden associated to an explicit inversion of the mathematical equations. a preliminary validation of the proposed solution is obtained using a dedicated experimental setting.
elliptic flow of direct photons in au+au collisions at 200 gev. the rapidity dependence of the elliptic flow of direct photons in au+au collisions at √snn = 200 gev are predicted, based on a three-dimensional ideal hydrodynamic description of the hot and dense matter. the rapidity dependence of the elliptic flow v2(y) of direct photons (mainly thermal photons) is very sensitive to the initial energy density distribution along longitudinal direction, which provides a useful tool to extract the realistic initial condition from measurements.
event structure and double helicity asymmetry in jet production from polarized p+p collisions at sqrt(s) = 200 gev. we report on event structure and double helicity asymmetry ($a_ll$) of jet production in longitudinally polarized p+p collisions at $\sqrt{s}$=200 gev. photons and charged particles were measured at midrapidity $|\eta| &lt; 0.35$ with the requirement of a high-momentum ($&gt;2$ gev/$c$) photon in each event. measured event structure is compared with {\sc pythia} and {\sc geant} simulations. the shape of jets and the underlying event were well reproduced at this collision energy. for the measurement of jet $a_{ll}$, photons and charged particles were clustered with a seed-cone algorithm to obtain the cluster $p_t$ sum ($p_t^{\rm reco}$). the effect of detector response and the underlying events on $p_t^{\rm reco}$ was evaluated with the simulation. the production rate of reconstructed jets is satisfactorily reproduced with the nlo pqcd jet production cross section. for $4 &lt; p_t^{\rm reco} &lt; 12$ gev/$c$ with an average beam polarization of $&lt; p &gt; = 49%$ we measured $a_{ll} = -0.0014 \pm 0.0037^{\rm stat}$ at the lowest $p_t^{\rm reco}$ bin (4-5 gev/$c$) and $-0.0181 \pm 0.0282^{\rm stat}$ at the highest $p_t^{\rm reco}$ bin (10-12 gev/$c$) with a beam polarization scale error of 9.4% and a $\pt$ scale error of 10%. jets in the measured $p_t^{\rm reco}$ range arise primarily from hard-scattered gluons with momentum fraction $0.02 &lt; x &lt; 0.3$ according to {\sc pythia}. the measured $a_{ll}$ is compared with predictions that assume various $\delta g(x)$ distributions based on the grsv parameterization. the present result imposes the limit $-1.1 &lt; \int_{0.02}^{0.3}dx \delta g(x, \mu^2 = 1 {\rm gev}^2) &lt; 0.4$ at 95% confidence level or $\int_{0.02}^{0.3}dx \delta g(x, \mu^2 = 1 {\rm gev}^2) &lt; 0.5$ at 99% confidence level.
measurement of transverse single-spin asymmetries for j/psi production in polarized p+p collisions at sqrt(s) = 200 gev. we report the first measurement of transverse single-spin asymmetries in $j/\psi$ production from transversely polarized $p+p$ collisions at $\sqrt{s} = 200$ gev with data taken by the phenix experiment in 2006 and 2008. the measurement was performed over the rapidity ranges $1.2 &lt; |y| &lt; 2.2$ and $ |y| &lt; 0.35$ for transverse momenta up to 6 gev/$c$. $j/\psi$ production at rhic is dominated by processes involving initial-state gluons, and transverse single-spin asymmetries of the $j/\psi$ can provide access to gluon dynamics within the nucleon. such asymmetries may also shed light on the long-standing question in qcd of the $j/\psi$ production mechanism. asymmetries were obtained as a function of $j/\psi$ transverse momentum and feynman-$x$, with a value of $-0.086 \pm 0.026^{\rm stat} \pm 0.003^{\rm syst}$ in the forward region. this result suggests possible nonzero trigluon correlation functions in transversely polarized protons and, if well defined in this reaction, a nonzero gluon sivers distribution function.
a genetic local search algorithm for minimizing total weighted tardiness in the job-shop scheduling problem. this paper considers the job-shop problem with release dates and due dates, with the objective of minimizing the total weighted tardiness. a genetic algorithm is combined with an iterated local search that uses a longest path approach on a disjunctive graph model. a design of experiments approach is employed to calibrate the parameters and operators of the algorithm. previous studies on genetic algorithms for the job-shop problem point out that these algorithms are highly depended on the way the chromosomes are decoded. in this paper, we show that the efficiency of genetic algorithms does no longer depend on the schedule builder when an iterated local search is used. computational experiments carried out on instances of the literature show the efficiency of the proposed algorithm.
extensions of an integrated approach for multi-resource shop scheduling. in a previous work, we proposed an integrated approach for a rather general shop scheduling problem, with multiresource, flexibility, and nonlinear routings. in this paper, we want to overcome some of the limitations of the approach. in particular, an operation that needs several resources might not need all the resources during its entire processing time. our first extension allows a resource to be released before the end of the operation. the second extension considers the fact that, for a given operation, one might have to prevent a set of incompatible resources to be chosen.
computation of the robust preference relation combining a choquet integral and utility functions. null
spectroscopy of the unbound nucleus 18na. the unbound nucleus 18na, the intermediate nucleus in the two-proton radioactivity of 19mg, is studied through the resonant elastic scattering 17ne(p,17ne)p. the spectroscopic information obtained in this experiment is discussed and put in perspective with previous measurements and the structure of the mirror nucleus 18n.
a gac algorithm for a class of global counting constraints. this paper presents the constraint class seq bin(n;x;c;b) where n is an integer variable, x is a sequence of integer variables and c and b are two binary constraints. a constraint of the seq bin class enforces the two following conditions: (1) n is equal to the number of times that the constraint c is satised on two consecutive variables in x, and (2) b holds on any pair of consecutive variables in x. providing that b satises the particular property of neighborhood-substitutability, we come up with a ltering algorithm that achieves generalized arc-consistency (gac) for seq bin(n;x;c;b). this algorithm can be directly used for the constraints change, smooth, increasing nvalue, among and increasing among, in time linear in the sum of domain sizes. for all these constraints, this time complexity either improves the best known results, or equals those results.
measurement of the parity-violating longitudinal single-spin asymmetry for $w^{\pm}$ boson production in polarized proton-proton collisions at $\sqrt{s} = 500 $gev. we report the first measurement of the parity violating single-spin asymmetries for midrapidity decay positrons and electrons from $w^{+}$ and $w^{-}$ boson production in longitudinally polarized proton-proton collisions at $\sqrt{s}=500 $gev by the star experiment at rhic. the measured asymmetries, $a^{w^+}_{l}=-0.27\pm 0.10\;({\rm stat.})\pm 0.02\;({\rm syst.}) \pm 0.03\;({\rm norm.})$ and $a^{w^-}_{l}=0.14\pm 0.19\;({\rm stat.})\pm 0.02 \;({\rm syst.})\pm 0.01\;({\rm norm.})$, are consistent with theory predictions, which are large and of opposite sign. these predictions are based on polarized quark and antiquark distribution functions constrained by polarized dis measurements.
resonances as a possible observable of hot and dense nuclear matter. one of the most fundamental questions in the field of relativistic heavy ion physics is how to reach and explore densities which are needed to cross the chiral and/or the deconfinement phase transition. in this analysis we investigate the information we can gather by analyzing baryonic and mesonic resonances on the hot and dense phase in such nuclear reactions. the decay products of these resonances carry information on the resonances properties at the space time point of their decay. we especially investigate the percentage of reconstructable resonances as a function of density for heavy ion collisions in the energy range between $e_{lab}$ = 30 agev and $\sqrt{s}$ = 200 agev, the energy domain between the future fair facility and the present rhic collider.
operation and calibration of the silicon drift detectors of the alice experiment during the 2008 cosmic ray data taking period. the calibration and performance of the silicon drift detector of the alice experiment during the 2008 cosmic ray run will be presented. in particular the procedures to monitor the running parameters (baselines, noise, drift speed) are detailed. other relevant parameters (sop delay, time-zero, charge calibration) were also determined.
inconsistencies in the determination of a capacity. null
method for producing solutions to a concrete multicriteria optimisation problem. null
mcs - a new algorithm for multicriteria optimisation in constraint programming. in this paper we propose a new algorithm called mcs for the search for solutions to multicriteria combinatorial optimisation problems. to quickly produce a solution that offers a good trade-off between criteria, the mcs algorithm alternates several branch \&amp; bound searches following diversified search strategies. it is implemented in cp in a dedicated framework and can be specialised for either complete or partial search.
microbial corrosion of p235gh steel under geological conditions. the role of microbial activity on the alteration of steel containers used for nuclear waste disposal is increasingly discussed. in this work we isolated and identified sulphate-reducing bacteria (srb) in the callovo–oxfordian clay rock studied as a potential host rock formation for a repository for high-level and long-lived radioactive waste in france. then, the effect of the srb growth on the overpack steel corrosion was investigated. the corrosion rate of the steel coupons was high under biotic conditions (not, vert, similar30 μm/year) in comparison to blank sterilized runs (not, vert, similar14 μm/year). culture experiments in compacted conditions with clay–stone cores and steel coupons under 120 bar, simulating deep geological conditions, gave results similar to those obtained in batch experiments (e.g. production of h2s). this indicates the plausibility of srb growth during the construction and operational phases of the repository and their survival at least temporarily after the disposal closure if water is available, which may cause fast corrosion of the steel containers under disposal conditions.
high $p_t$ resonances as a possibility to explore hot and dense nuclear matter. one of the fundamental objectives of experiments with ultrarelativistic heavy ions is to explore strongly interacting matter at high density and high temperature. in this investigation we study in particular the information which can be obtained by analyzing baryonic and mesonic resonances. the decay products of these resonances carry information on the resonances properties at the space time point of their decay. we especially investigate the percentage of reconstructable resonances as a function of density for heavy ion collisions in the energy range between $e_{lab}$ = 30 agev and $\sqrt{s}$ = 200 agev, the energy domain between the future fair facility and the present rhic collider.
cross section and parity violating spin asymmetries of w^+/- boson production in polarized p+p collisions at sqrt(s)=500 gev. large parity violating longitudinal single spin asymmetries a^{e^-}_l= -0.86^{+0.14}_{-0.30} and a^{e^+}_l= 0.88^{+0.12}_{-0.71} are observed for inclusive high transverse momentum electrons and positrons in polarized pp collisions at a center of mass energy of \sqrt{s}=500\ gev with the phenix detector at rhic. these e^{+/-} come mainly from the decay of w^{+/-} and z^0 bosons, and the asymmetries directly demonstrate parity violation in the couplings of the w^{\pm} to the light quarks. the observed electron and positron yields were used to estimate w^\pm boson production cross sections equal to \sigma(pp --&gt; w^+ x) \times br(w^ --&gt; \nu_e)= 144.1+/-21.2(stat)^{+3.4}_{-10.3}(syst) +/- 15%(norm) pb, and \sigma(pp --&gt; w^{-}x) \times br(w^--&gt;e^-\bar{\nu_e}) = 31.7+/-12.1(stat)^{+10.1}_{-8.2}(syst)+/-15%(norm) pb.
wyfiwif: a haptic communication paradigm for collaborative motor skills learning. motor skills transfer is a challenging issue for many applications such as surgery, design and industry. in order to design virtual environments that support motor skills learning, a deep understanding of humans' haptic interactions is required. to ensure skills transfer, experts and novices need to collaborate. this requires the construction of the common frame of reference between the teacher and the learner in order to understand each other. in this paper, human-human haptic collaboration is investigated in order to understand how haptic information is exchanged. furthermore, wyfiwif (what you feel is what i feel), a haptic communication paradigm is introduced. this paradigm is based on a hand guidance metaphor. the paradigm helps operators to construct an efficient common frame of reference by allowing a direct haptic communication. a learning virtual environment is used to evaluate this haptic communication paradigm. hence, 60 volunteer students performed a needle insertion learning task. the results of this experiment show that, compared to conventional methods, the learning method based on haptic communication improves the novices' performance in such a task. we conclude that the wyfiwif paradigm facilitate expert-novice haptic collaboration to teach motor skills.
fics 2010. informal proceedings of the 7th workshop on fixed points in computer science (fics 2010), held in brno, 21-22 august 2010.
structured and flexible gray-box composition: application to task rescheduling for grid benchmarking. the evolution of complex distributed software systems often requires intricate composition operations in order to adapt or add functionalities, react to unanticipated changes to security policies, or do performance improvements, which cannot be modularized in terms of existing services or components. they often need controlled access to selected parts of the implementation, e.g., to manage exceptional situations and crosscutting within services and their compositions. however, existing composition techniques typically support only interface-level (black-box) composition or arbitrary access to the implementation (gray-box or white-box composition). in this paper, we present a more structured approach to the composition of complex software systems that require invasive accesses. concretely, we provide two contributions, we (i) present a small kernel composition language for structured gray-box composition with explicit control mechanisms and a corresponding aspect-based implementation; (ii) present and compare evolutions using this approach to gray-box composition in the context of two real-world software systems: benchmarking of grid algorithms with nasgrid and transactional replication with jboss cache.
sensitivity of isolated photon production at tev hadron colliders to the gluon distribution in the proton. we compare the single inclusive spectra of isolated photons measured at high transverse energy in proton-antiproton collisions at sqrt(s)=1.96 tev with next-to-leading order perturbative qcd predictions with various parametrizations of the parton distribution functions (pdfs). within the experimental and theoretical uncertainties, the tevatron data can be reproduced equally well by the recent cteq6.6, mstw08 and nnpdf1.2 pdf sets. we present also the predictions for isolated gamma spectra in proton-proton collisions at sqrt(s)=14tev for central (y=0) and forward (y=4) rapidities relevant for lhc experiments. different proton pdfs result in maximum variations of order 30% in the expected e_t-differential isolated gamma cross sections. the inclusion of the isolated photon data in global pdf fits will place extra independent constraints on the gluon density.
three-particle coincidence of the long range pseudorapidity correlation in high energy nucleus-nucleus collisions. we report the first three-particle coincidence measurement in pseudorapidity (δη) between a high transverse momentum (p⊥) trigger particle and two lower p⊥ associated particles within azimuth |δϕ|&lt;0.7 in √snn=200  gev d+au and au+au collisions. charge ordering properties are exploited to separate the jetlike component and the ridge (long range δη correlation). the results indicate that the correlation of ridge particles are uniform not only with respect to the trigger particle but also between themselves event by event in our measured δη. in addition, the production of the ridge appears to be uncorrelated to the presence of the narrow jetlike component.
an experimental exploration of the qcd phase diagram: the search for the critical point and the onset of de-confinement. the qcd phase diagram lies at the heart of what the rhic physics program is all about. while rhic has been operating very successfully at or close to its maximum energy for almost a decade, it has become clear that this collider can also be operated at lower energies down to 5 gev without extensive upgrades. an exploration of the full region of beam energies available at the rhic facility is imperative. the star detector, due to its large uniform acceptance and excellent particle identification capabilities, is uniquely positioned to carry out this program in depth and detail. the first exploratory beam energy scan (bes) run at rhic took place in 2010 (run 10), since several star upgrades, most importantly a full barrel time of flight detector, are now completed which add new capabilities important for the interesting physics at bes energies. in this document we discuss current proposed measurements, with estimations of the accuracy of the measurements given an assumed event count at each beam energy.
longitudinal scaling property of the charge balance function in au + au collisions at 200 gev. we present measurements of the charge balance function, from the charged particles, for diverse pseudorapidity and transverse momentum ranges in au + au collisions at 200 gev using the star detector at rhic. we observe that the balance function is boost-invariant within the pseudorapidity coverage [-1.3, 1.3]. the balance function properly scaled by the width of the observed pseudorapidity window does not depend on the position or size of the pseudorapidity window. this scaling property also holds for particles in different transverse momentum ranges. in addition, we find that the width of the balance function decreases monotonically with increasing transverse momentum for all centrality classes.
is the centrality dependence of the elliptic flow $v_2$ and of the average $$ in rhic experiments more than a core-corona effect?. recently we have shown that the centrality dependence of the multiplicity of different hadron species observed in rhic and sps experiments can be well understood in a simple model, dubbed core-corona model. there it is assumed that those incoming nucleons which scatter only once produce hadrons as in pp collisions whereas those which scatter more often form an equilibrated source which decays according to phase space. in this article we show that also kinematical variables like $v_2/\epsilon_{part} (n_{part})$ as well as $v_2^i/\epsilon_{part} (n_{part})$ and $$ of identified particles are well described in this model. the correlation of $$ between peripheral heavy ion collisions and pp collisions for different hadrons, reproduced in this model, questions whether hydrodynamical calculations are the proper tool to describe non-central heavy ion collision. the model explains as well the centrality dependence of $v_2/\epsilon_{part}$ of charged particles, considered up to now as an observable which allows to determine the viscosity of the quark gluon plasma. the observed dependence of $v_2^i/\epsilon_{part}(n_{part})$ on the particle species is a simple consequence of the different ratios of core to corona particles.
measurement of the bottom quark contribution to non-photonic electron production in $p+p$ collisions at $\sqrt{s} $=200 gev. the contribution of b meson decays to non-photonic electrons, which are mainly produced by the semi-leptonic decays of heavy flavor mesons, in p+p collisions at $\sqrt{s} =$ 200 gev has been measured using azimuthal correlations between non-photonic electrons and hadrons. the extracted b decay contribution is approximately 50% at a transverse momentum of $p_{t} \geq 5$ gev/c. these measurements constrain the nuclear modification factor for electrons from b and d meson decays. the result indicates that b meson production in heavy ion collisions is also suppressed at high pt even under the extreme case for the ratio of b to d contributions to non-photonic electrons.
alice emcal physics performance report. the alice detector at the lhc (a large ion collider experiment) will carry out comprehensive measurements of high energy nucleus-nucleus collisions, in order to study qcd matter under extreme conditions and the phase transtion between conã¯â¬âned matter and the quark-gluon plasma (qgp). this report presents our current state of understanding of the physics performance of the large acceptance electromagnetic calorimeter (emcal) in the alice central detector. the emcal enhances aliceã¢ââs capabilities for jet measurements. the emcal enables triggering and full reconstruction of high energy jets in alice, and augments existing alice capabilities to measure high momentum photons and electrons. combined with aliceã¢ââs excellent capabilities to track and identify particles from very low pt to high pt , the emcal enables a comprehensive study of jet interactions in the medium produced in heavy ion collisions at the lhc.
proving fixed points. we propose a method to characterize the fixed points described in tarski's theorem for complete lattices. the method is deductive: the least and greatest fixed points are "proved" in some inference system defined from deduction rules. we also apply the method to two other fixed point theorems, a generalization of tarski's theorem to chain-complete posets and bourbaki-witt's theorem. finally, we compare the method with the traditional iterative method resorting to ordinals and the original impredicative method used by tarski.
a new challenge for the energy efficiency evaluation community: energy savings and emissions reductions from urban transportation policies. the energy efficiency evaluation community has a large experience about programs for industries, residential and commercial sectors. but now the largest share of the energy consumption growth is due to the transportation sector. moreover, as the stakes related to the transport sector are considerable, relying on separate actions for technological energy efficiency improvements will not be sufficient. therefore transport policies now support the development of more integrated approaches. all together, this raises new evaluation issues. this paper first look at what makes or would make the transport specific and different from the other “usual” sectors, as far as the evaluation of energy savings and avoided co2 emissions is concerned. this is illustrated by a comparison between two simple action types, a car replacement for the transport sector and a boiler replacement for the building sector. then, taking into account the change toward more integrated action plans, the common evaluation methods used for energy efficiency programs and urban transport planning are discussed, to what extent they can be applied to evaluation of urban mobility plans. key differences between the building and transport sectors that have an influence on what evaluation methods can be used are: 1) the level of complexity for the definition of the service delivered, 2) the relative importance of variables having long term perspective (building stock) and short term perspective (mobility behaviors). the evaluation of integrated approaches raises additional issues, mainly interactions between the distinct policy measures implemented on a given territory, and causality between the measures implemented and the changes observed. two interesting tracks stand out: - developing new portfolio approaches starting from available bottom-up methods; - adapting methods used for transport infrastructure design or transport planning. this emphasizes progress that could be achieved by crossing experiences from both scientific communities, evaluation of energy efficiency programs and transport planning.
compatibility of the french white certificate program to fulfil the objective of energy savings claimed by the energy service directive. the commission has proposed a directive on the promotion of end-use efficiency and energy services (esd) to enhance the cost-effective and efficient end-use of energy in member states. according to the directive, the member states shall adopt and aim to achieve an overall national indicative energy savings target of 9% (or beyond) in 2016. this target is to be reached by way of energy services and other energy efficiency measures. the french national energy efficiency action plan to comply with the esd includes a white certificates scheme (or fwc) as one of the important measures to fulfil the target. as the accountings of energy savings in the fwc scheme and in the esd are different (e.g. lifetime-cumulated and discounted kwh for fwc and annual kwh for esd), an analysis of the compliance of both methodologies and a comparison of the assessed savings are necessary. in this paper, we evaluate the compliance with the esd requirements of two different end-use actions (insulation, heating boiler) included in the fwc scheme. this is done through the concrete case of certificates filed by edf. the main objective of this evaluation is to assess the contribution of the savings of these fwc actions to the target of the esd. finally, general conclusions are drawn about the use of a white certificates scheme as a monitoring and evaluation tool for the esd purpose.
can energy savings from operations promoting energy efficient behaviors in office buildings be accounted for?. when looking for solutions to mitigate the growth of energy consumption in the commercial buildings sector, research works often focus on the energy performance of buildings. indeed, many studies established how large the technical improvement potential was in this sector. but cost-effective energy savings can also be achieved in a complementary way by an improved energy management promoting energy efficient behaviors, because energy consumptions depend on both energy performance of buildings and equipments, and end-users behaviors. past experiences tend to show that if awareness operations were widely disseminated, a significant amount of energy savings could be realized. it is likely that more and more organizations engage such operations. unfortunately, their real impacts remain rather unknown and uncertain, mainly because they are not perceived as a serious option. consequently they are implemented in very heterogeneous ways. thus, their results may vary a lot too. this paper first reminds success factors analyzed in previous works, before presenting monitoring guidelines to ensure that energy savings can be accounted for. this methodological approach could be an entry to consider the inclusion of behavioral actions in schemes accounting for energy savings, such as white certificates. the option to include awareness operations in an energy management service appears to create good conditions ensuring the quality of the operations and therefore an accounting system reliable enough for certified energy savings. admitting this new kind of energy service in white certificates schemes would on the one side provide a clear recognition of behavioral actions, and on the other side promote quality standards ensuring more homogeneity and effectiveness among this kind of operation.
evaluation as a "learning-by-doing" tool for the implementation of local energy efficiency activities. with the "think global, act local" trends, local levels are taking an increasing role in the implementation of action plans, especially in the field of energy efficiency. an inventory of local energy efficiency operations in france confirmed a significant expansion of these activities, but also highlighted how rare their evaluation is, although a rich methodological evaluation material is available. the research question for this study was then how to fill the gap between theory and practice. this was addressed through studying the issue of evaluation use. the first step was to find in the evaluation literature the key components of evaluation use and the success factors to overcome the barriers to evaluation practice previously identified. this was used to adjust our evaluation methods and approach, and then to apply this to a particular case study. key success factors for evaluation use were highlighted, such as the constructive and regular contacts between evaluators and program partners, and presenting the evaluation as a win-win collaboration. finally, the main evaluation use was not to quantify the results of the operation, even if it was initially the most important stakeholder expectation, but to learn how to work together, how to supervise and use an evaluation, and how to improve the operation management and the operations themselves. this way, the evaluation really appears to be a learning-by-doing tool for all stakeholders involved in the implementation of local energy efficiency activities.
raising awareness for energy efficiency in the service sector: learning from success stories to disseminate good practices. energy efficiency in the service sector is a key issue because of the important growth of its energy consumption. the energy performance of buildings and equipment can be improved through technical investments, but this has to be linked with an efficient management and good practices in order to reach better energy efficiency levels in a cost-effective way. experience feedback concerning awareness activities in the service sector highlights the interesting opportunities of energy efficiency improvements they represent. this paper first draws a synthesis of the available feedback in this area to detect factors of success for this kind of activities. more than twenty operations from europe and north america were analyzed looking at items such as the stakeholders involved, the actions implemented, the communication means, and the evaluation performed. then a case study describes an edf pilot operation in south east of france. an awareness campaign was led in four particular edf buildings to inform the employees of the best practices and to involve them to apply these advice. different action packages were used to compare their efficiency. the evaluation emphasizes the success of the operation, with around 10% of energy savings (i.e. more than 270 mwh/a). more than 80% of the employees said they changed their energy behavior and other indicators show their commitment and satisfaction towards the campaign. finally, suggestions are made to disseminate good practices at a broader scale, especially out of the "initiated" circle. building up a know-how from the evaluation of past experiences makes easier the development of process such as networking, experience sharing, and including these activities in energy services offers and in white certificates systems.
a process to develop operational bottom-up evaluation methods – from reference guidebooks to a practical culture of evaluation. needs for evaluating energy efficiency (ee) activities are increasing, for the accounting of results and for understanding their success/failures. indeed evaluation results should be used for both reporting past activities and improving future operations. lack of easy to use methods is pointed out by local stakeholders as a major barrier to evaluation. another issue is the frequent negative perception of evaluation, experienced as a control and/or a waste of time. this paper presents a systematic process to develop bottom-up evaluation methods designed to fit to stakeholders needs: directly operational, easy to appropriate, providing useful conclusions to improve operations and to communicate about their results. our approach relies on the principle of experience capitalisation and on an organisation with two levels, central and on-field. it aims to create conditions for continuous improvement. moreover it should insure involved stakeholders do actually take part in and take advantage of the evaluation process. this methodology handles both impact and process evaluation. for the impacts, focus is on calculations transparency, data quality and reliability of the results. regarding operation process, main issues are analysing causality between actions and results, and detecting the success and failure factors. this work was first developed for the evaluation of local operations in france[1]. the resulting methodology was tested on two case studies from the eco energy plan, a local ee programme implemented in south-east of france. [1] within a partnership between armines, the wuppertal institute for climate environment and energy, and edf r&amp;d (electricité de france).
local energy efficiency and demand-side management activities in france. recent french political decisions have tended to increase responsibilities for local authorities regarding energy issues. the main frames, which should be the basis for local energy policies and energy efficiency (ee) activities, have been identified. however, past experience has shown that theoretical opportunities do not necessarily lead to actions. this paper compiles an inventory of the local ee activities in france in order to provide an overview of what has been done from 2000 to 2004. this study indicates firstly, what kinds of actions have been done and secondly, whether the local dimension mattered. the analysis of the local dimension was made using criteria defined from the analysis of the theoretical context. the inventory also enables the definition of a practical typology for ee actions, useful for benchmarking works, for the definition of specific objectives, and then for evaluation. difficulties encountered in collecting data show the need for information systems, which could help for structuring action plans and for making evaluations. this process could take advantage of the future “white certificates” framework, which will change the way actions are reported. moreover, this paper emphasises the importance of local authorities involved in 75% of the activities encountered. an evolution towards increasing involvement and transverse actions is observed, with twice as many actions taking into account the local context between 2002 and 2004. a good reason to look at the actual trends.
extended h2 output feedback control for continuous descriptor systems. this paper investigates the design of an “extended” h2 output feedback controller for continuous descriptor systems. in such problem, the controller, which is described within descriptor framework, has to satisfy simultaneously two conditions: 1-the closed-loop is admissible and has minimum h2 norm. 2-only the internal stability of a part of the generalized closed-loop is sought. in such case, the standard h2 output feedback control problem for descriptor systems cannot be solved. an explicit characterization of the optimal solution, based on two generalized algebraic riccati equations and two structured generalized sylvester-type equations, is given. a numerical example is given to illustrate the applicability of the proposed results.
extended stabilizing controllers for continuous-time descriptor systems. this paper investigates the so-called extended stabilization control problem for continuous-time descriptor systems. in such nonstandard stabilization problem, the generalized plant and its unstable and/or nonproper weights are all described within the descriptor framework. the extended stabilizing controller (esc) is found such that: -the closed-loop is admissible; - only the internal stability of a part of the generalized closed-loop is sought. first, the necessary and sufficient conditions for the existence of a solution are explicitly given in terms of two structured generalized sylvester equations. moreover, the parametrization of a class of controllers satisfying such stability is formulated. finally, numerical examples are presented to show the effectiveness of the proposed results.
in-target radioactive nuclei production rates with eurisol single-stage target configuration. null
a comparison of model migration tools. modelling languages and thus their metamodels are subject to change. when a metamodel evolves, existing models may no longer conform to the evolved metamodel. to avoid rebuilding them from scratch, existing models must be migrated to conform to the evolved metamodel. manually migrating existing models is tedious and errorprone. to alleviate this, several tools have been proposed to build a migration strategy that automates the migration of existing models. little is known about the advantages and disadvantages of the tools in different situations. in this paper, we thus compare a representative sample of migration tools - aml, cope, ecore2ecore and epsilon flock - using common migration examples. the criteria used in the comparison aim to support users in selecting the most appropriate tool for their situation.
transverse momentum spectra of charged particles in proton-proton collisions at $\sqrt{s} = 900$~gev with alice at the lhc. the inclusive charged particle transverse momentum distribution is measured in proton-proton collisions at $\sqrt{s} = 900$~gev at the lhc using the alice detector. the measurement is performed in the central pseudorapidity region $(|\eta|&lt;0.8)$ over the transverse momentum range $0.15.
two-pion bose-einstein correlations in pp collisions at sqrt(s)=900 gev. we report on the measurement of two-pion correlation functions from pp collisions at sqrt(s)=900 gev performed by the alice experiment at the large hadron collider. our analysis shows an increase of the hbt radius with increasing event multiplicity, in line with other measurements done in particle- and nuclear collisions. conversely, the strong decrease of the radius with increasing transverse momentum, as observed at rhic and at tevatron, is not manifest in our data.
trends in yield and azimuthal shape modification in dihadron correlations in relativistic heavy ion collisions. fast parton probes produced by hard scattering and embedded within collisions of large nuclei have shown that partons suffer large energy loss and that the produced medium may respond collectively to the lost energy. we present measurements of neutral pion trigger particles at transverse momenta p^t_t = 4-12 gev/c and associated charged hadrons (p^a_t = 0.5-7 gev/c) as a function of relative azimuthal angle delta phi at midrapidity in au+au and p+p collisions at sqrt(s_nn) = 200 gev. these data lead to two major observations. first, the relative angular distribution of low momentum hadrons, whose shape modification has been interpreted as a medium response to parton energy loss, is found to be modified only for p^t_t &lt; 7 gev/c. at higher p^t_t, the data are consistent with unmodified or very weakly modified shapes, even for the lowest measured p^a_t. this observation presents a quantitative challenge to medium response scenarios. second, the associated yield of hadrons opposite to the trigger particle in au+au relative to that in p+p (i_aa) is found to be suppressed at large momentum (iaa ~ 0.35-0.5), but less than the single particle nuclear modification factor (r_aa ~0.2).
crystal chemistry, rietveld refinements and raman spectroscopy studies of the new solid solution series: ba3−xsrx(vo4)2 (0 ≤ x ≤ 3). the new solid solution series ba3−xsrx(vo4)2 (0 ≤ x ≤3) has been synthesized and studied by a combination of x-ray powder diffraction and raman vibrational spectroscopy. this continuous solid solution crystallise in the hexagonal system with view the mathml source space group. the structure has been determined at room temperature from x-ray diffraction by the rietveld method analysis. it is formed by a 3d network of (ba/sr)(1)(vo4)24− layers linked into a crystal network by (ba/sr)2+(2) cations. the vibrational spectra of this crystalline orthovanadate solid solution series are interpreted by means of factor group analysis in terms of space group view the mathml source (view the mathml source). assignments of the v–o vibrational stretching and bending modes, as well as some of the external modes, have been made. while all the modes show a monotonous shift as a function of the composition x, a break in the curves of intensities, full width at half maximum and band areas as a function of x is observed and attributed to the statistical distribution of ba and sr ions in the same crystallographic sites.
behaviour of a glass ceramic in contact with various media and solvents. null
neutron correlations in 6he viewed through nuclear break-up reactions. null
midrapidity antiproton-to-proton ratio in pp collisions at $\sqrt{s} = 0.9$ and $7$~tev measured by the alice experiment. the ratio of the yields of antiprotons to protons in pp collisions has been measured by the alice experiment at $\sqrt{s} = 0.9$ and $7$~tev during the initial running periods of the large hadron collider(lhc). the measurement covers the transverse momentum interval $0.45 &lt; p_{\rm{t}} &lt; 1.05$~gev/$c$ and rapidity $|y| &lt; 0.5$. the ratio is measured to be $r_{|y| &lt; 0.5} = 0.957 \pm 0.006 (stat.) \pm 0.014 (syst.)$ at $0.9$~tev and $r_{|y| &lt; 0.5} = 0.991 \pm 0.005 (stat.) \pm 0.014 (syst.)$ at $7$~tev and it is independent of both rapidity and transverse momentum. the results are consistent with the conventional model of baryon-number transport and set stringent limits on any additional contributions to baryon-number transfer over very large rapidity intervals in pp collisions.
cosmic ray properties from the electric fields measured by the codalema experiment. this thesis is dedicated to the study of high energy cosmic rays that penetrating the atmosphere and dissipating their energy by producing a very large amount of secondary particles called an air shower. the codalema experiment is measuring the radio counterpart associated to these air showers by using antennas in the 1-100 mhz frequency domain. from the characteristics of the air shower some questions have been studied in order to determine the main effects giving rise to the radio signal. in a second part, we have develop a linear prediction method to identify transients associated to air shower in the noise environment of the experiment. finally, possible signatures of the radio emission mechanism have been searched for in the experimental signals. this last study is linking the two previous analysis.
on the liquid drop model mass formulas and alpha decay of the heaviest nuclei. the coefficients of different macro-microscopic liquid drop model mass formulas have been determined by a least square fitting procedure to 2027 experimental atomic masses. a rms deviation of 0.54 mev can be reached. the remaining differences come mainly from the determination of the shell and pairing energies. extrapolations are compared to 161 new experimental masses and to 656 mass evaluations. the different fits lead to a surface energy coefficient of around 17-18 mev. finally, alpha decay potential barriers are revisited and predictions of alpha decay half-lives of still unknown superheavy elements are given from previously proposed analytical formulas and from extrapolated qalpha values.
declarative events for object-oriented programming. in object-oriented designs inversion of control is achieved by an event-driven programming style based on imperatively triggered events. an alternative approach can be found in aspect-oriented programming, which defines events as declarative queries over implicitly available events. this helps to localize definition of events and avoid preplanning, but lacks a clean integration with object-oriented features and principles. the contribution of this work is a concept of object-oriented events that combines imperative, declarative and implicit events and provides their seamless integration with object-oriented features, preserving encapsulation and modular reasoning. we present an efficient and type-safe implementation of the concept as an extension to the scala language.
architecture for the next generation system management tools for distributed computing platforms. in order to get more results or greater accuracy, computational scientists execute mainly parallel or distributed applications, and try to scale these applications up. accordingly, they use more and more distributed resources, using local large-scale hpc systems, grids or even clouds. however, in most of cases, the use and management of such platforms is static. indeed generally, the application has to be adapted to the environment rather than adapting the environment to the applications' needs. in addition, platforms are managed through the concept of time and space partitioning mainly via the use of batch schedulers: time partitioning enables the execution of several applications on a same resources, and space partitioning enables the execution of applications across several distributed resources. this leads to some usage limitations, where applications can only be executed on a subset of the available resources. therefore, scientists have to manage technical details related to the execution of their applications on each target hpc platforms, which could result in application modifications, rather than focusing on the science. in this article, we advocate for a system management tool enabling the transparent configuration of the hpc platform and the customization of the execution environment for large-scale hpc systems (such as clusters or mpps), grids, and clouds. we propose a new approach to manage these systems in a more dynamic way, where the resources can be configured and reconfigured automatically and transparently. the proposed solution is not removing the benefit of resource management systems such as batch system (they still provide a well-known interface for job submission), but rather redefine the underlying system capabilities. our approach is based on a refinement of the concept of emulation and virtualization introduced by goldberg. furthermore, the proposed approach leads to the definition of a method that provides a unique interface to scientists for the deployment and management of their applications on hpc platforms. this method is based on two concepts: (i) the virtual system environment (vse), and (ii) the virtual platforms (vps).
self-duality and supersymmetry. we observe that the hamiltonian view the mathml source, where view the mathml source is the flat 4d dirac operator in a self-dual gauge background, is supersymmetric, admitting 4 different real supercharges. a generalization of this model to the motion on a curved conformally flat 4d manifold exists. for an abelian self-dual background, the corresponding lagrangian can be derived from known harmonic superspace expressions.
producing hard processes regarding the complete event: the epos event generator. jet cross sections can be in principle compared to simple pqcd calculations, based on the hypothesis of factorization. but often it is useful or even necessary to not only compute the production rate of the very high pt jets, but in addition the "rest of the event". the proposed talk is based on recent work, where we try to construct an event generator fully compatible with pqcd which allows to compute complete events, consisting of high pt jets plus all the other low pt particles produced at the same time. whereas in "generators of inclusive spectra" like pythia one may easily trigger on high pt phenomena, this is not so obvious for "generators of physical events", where in principle one has to generate a very large number of events in order to finally obtain rare events (like those with a very high pt jet). we recently developped an independnat block method which allow us ta have a direct access to dedicated variables 1. we will present latest results concerning this approach.
higher moments of net-proton multiplicity distributions at rhic. we report the first measurements of the kurtosis (\kappa), skewness (s) and variance (\sigma^2) of net-proton multiplicity (n_p - n_pbar) distributions at midrapidity for au+au collisions at \sqrt(s_nn) = 19.6, 62.4, and 200 gev corresponding to baryon chemical potentials (\mu_b) between 200 - 20 mev. our measurements of the products \kappa \sigma^2 and s \sigma, which can be related to theoretical calculations sensitive to baryon number susceptibilities and long range correlations, are constant as functions of collision centrality. we compare these products with results from lattice qcd and various models without a critical point and study the \sqrt(s_nn) dependence of \kappa \sigma^2. from the measurements at the three beam energies, we find no evidence for a critical point in the qcd phase diagram for \mu_b below 200 mev.
sqm with non-abelian self-dual fields: harmonic superspace description. we present a lagrangian formulation for n=4 supersymmetric quantum-mechanical systems describing the motion in external non-abelian self-dual gauge fields. for any such system, one can write a component supersymmetric lagrangian by introducing extra bosonic variables with topological chern-simons type interaction. for a special class of such system when the fields are expressed in the `t hooft ansatz form, it is possible to give a superfield description using harmonic superspace formalism. as a new explicit example, the n=4 mechanics with yang monopole is constructed.
neutron production in neutron-induced reactions at 96 mev on iron and lead. null
k*0 production in cu+cu and au+au collisions at \sqrt{s_nn} = 62.4 gev and 200 gev. we report on k*0 production at mid-rapidity in au+au and cu+cu collisions at \sqrt{s_{nn}} = 62.4 and 200 gev collected by the solenoid tracker at rhic (star) detector. the k*0 is reconstructed via the hadronic decays k*0 \to k+ pi- and \bar{k*0} \to k-pi+. transverse momentum, pt, spectra are measured over a range of pt extending from 0.2 gev/c to 5 gev/c. the center of mass energy and system size dependence of the rapidity density, dn/dy, and the average transverse momentum, , are presented. the measured n(k*0)/n(k) and n(\phi)/n(k*0) ratios favor the dominance of re-scattering of decay daughters of k*0 over the hadronic regeneration for the k*0 production. in the intermediate pt region (2.0 &lt; pt &lt; 4.0 gev/c), the elliptic flow parameter, v2, and the nuclear modification factor, rcp, agree with the expectations from the quark coalescence model of particle production.
emission patterns of neutral pions in 40 a mev ta+au reactions. differential cross sections of neutral pions emitted in 181ta + 197au collisions at a beam energy of 39.5a mev have been measured with the photon spectrometer taps. the kinetic energy and transverse momentum spectra of neutral pions cannot be properly described in the framework of the thermal model, nor when the reabsorption of pions is accounted for in a phenomenological model. however, high energy and high momentum tails of the pion spectra can be well fitted through thermal distributions with unexpectedly soft temperature parameters below 10 mev.
spectra of identified high-pt π± and p(p¯) in cu+cu collisions at √snn=200 gev. we report new results on identified (anti)proton and charged pion spectra at large transverse momenta (3.
adequacy between autosar os specification and real-time scheduling theory. null
schedulability analysis of osek/vdx applications. null
improvement in mechanical properties of aluminium polypropylene composite fiber. null
high p_t direct photon and pi^0 triggered azimuthal jet correlations in sqrt(s)=200 gev p+p collisions. correlations of charged hadrons of 1 &lt; pt &lt; 10 gev/c with high pt direct photons and pi^ 0 mesons in the range 5.
energy dependence of pion interferometry scales in ultra-relativistic heavy ion collisions. a study of energy behavior of the pion spectra and interferometry scales is carried out for the top sps, rhic and lhc energies within the hydrokinetic approach. the latter allows one to describe evolution of quark–gluon and hadron matter as well as continuous particle emission from the fluid in agreement with the underlying kinetic equations. the main mechanisms that lead to the paradoxical, at first sight, behavior of the interferometry scales, are exposed. in particular, a slow decrease and apparent saturation of rout/rside ratio with an energy growth happens due to a strengthening of positive correlations between space and time positions of pions emitted at the radial periphery of the system. such en effect is a consequence of the two factors: a developing of the pre-thermal collective transverse flows and an increase of the initial energy density in the fireball.
on the compared expressiveness of arc, place and transition time petri nets. in this paper, we consider safe time petri nets where time intervals (strict and large) are associated with places (tppn), arcs (tapn) or transitions (ttpn). we give the formal strong and weak semantics of these models in terms of timed transition systems. we compare the expressiveness of the six models w.r.t. (weak) timed bisimilarity (behavioral semantics). the main results of the paper are : (i) with strong semantics, tapn is strictly more expressive than tppn and ttpn ; (ii) with strong semantics tppn and ttpn are incomparable ; (iii) ttpn with strong semantics and ttpn with weak semantics are incomparable. moreover, we give a complete classification by a set of 9 relations explained in a figure.
inclusive pi^0, eta, and direct photon production at high transverse momentum in p+p and d+au collisions at sqrt(s_nn) = 200 gev. we report a measurement of high-p_t inclusive pi^0, eta, and direct photon production in p+p and d+au collisions at sqrt(s_nn) = 200 gev at midrapidity (0 &lt; eta &lt; 1). photons from the decay pi^0 -&gt; gamma gamma were detected in the barrel electromagnetic calorimeter of the star experiment at the relativistic heavy ion collider. the eta -&gt; gamma gamma decay was also observed and constituted the first eta measurement by star. the first direct photon cross section measurement by star is also presented, the signal was extracted statistically by subtracting the pi^0, eta, and omega(782) decay background from the inclusive photon distribution observed in the calorimeter. the analysis is described in detail, and the results are found to be in good agreement with earlier measurements and with next-to-leading order perturbative qcd calculations.
abnormal combustions in internal combustion engines: review on the latest patents eliminating knock conditions. abnormal combustions occurred since the invention of internal combustion engines. knock was intensively studied by researchers and engineers since this date. first patents were taken out on fuel quality (defined by octane index nowadays). after patents on sensors itself, engine settings were piloted to eliminate knock conditions in 1979-1980. many companies followed this pathway. latest patents on knock are reviewed. a highlight on a preventive protection (open loop control) is proposed for stationary engines fuelled natural gas, see le corre et al. [1]: this patent is a new concept in rupture with curative protection (closed loop control). preventive protection is based on a gas quality sensor.
towards expressive, well-founded and correct aspect-oriented programming. null
explicitly distributed aop using awed. distribution-related concerns, such as data replication, often crosscut the business code of a distributed application. currently such crosscutting concerns are frequently realized on top of distributed frameworks, such as ejbs, and initial ao support for the modularization of such crosscutting concerns, e.g., jboss aop and spring aop, has been proposed. based on an investigation of the implementation of replicated caches using jboss cache, we motivate that crosscutting concerns of distributed applications benefit from an aspect language for explicit distributed programming. we propose awed , a new aspect language with explicit distributed programming mechanisms, which provides three contributions. first, remote pointcut constructors which are more general than those of previous related approaches, in particular, supporting remote sequences. second, a notion of distributed advice with support for asynchronous and synchronous execution. third, a notion of distributed aspects including models for the deployment, instantiation and state sharing of aspects. we show several concrete examples how awed can be used to modularly implement and extend replicated cache implementations. finally, we present a prototype implementation of awed , which we have realized by extending jasco, a system providing dynamic aspects for java.
traceability for model driven, software product line engineering. null
a study on human-human interactions for common frame of reference development within collaborative virtual environments. collaborative virtual environments are 3d spaces that allow multiple users to work together on a common task. to design such environments to support human-human interactions, it is important to study how people develop a common frame of reference during collaboration. the concept of common frame of reference is central to all collaborative activities. it allows partners to understand each other through a continuous exchange of information (explicit and implicit). the ultimate goal of this research is to facilitate and enrich the construction of common frame of reference to accommodate specific collaborative virtual environments characteristics. indeed, the design elements related to the common frame of reference (i.e. communication modes, environment's construction and interactions) are essential for successful collaborative activity. two experimental studies were conducted using different collaborative virtual environments conditions. the first study shows that adding fixed landmarks can improve the development of common frame of reference within an objects manipulation task. the second study shows that haptic communication can improve the construction of the common frame of reference in a technical gesture learning task. these results are used to provide recommendations for collaborative virtual environments design. it represents a first step towards the development of a standardized collaborative virtual environments design methodology.
vpa-based aspects: better support for aop over protocols. null
measurement of neutral mesons in p+p collisions at sqrt(s) = 200 gev and scaling properties of hadron production. the phenix experiment at the relativistic heavy ion collider has measured the invariant differential cross section for production of k^0_s , \omega, \eta prime, and \phi mesons in p + p collisions at = 200 gev. measurements \omega and \phi production in different decay channels give consistent results. new results for the \phi are in agreement with previously published data and extend the measured pt coverage. the spectral shapes of all hadron transverse momentum distributions measured by phenix are well described by a tsallis distribution functional form with only two parameters, n and t, determining the high-pt and characterizing the low-pt regions of the spectra, respectively. the values of these parameters are very similar for all analyzed meson spectra, but with a lower parameter t extracted for protons. the integrated invariant cross sections calculated from the fitted distributions are found to be consistent with existing measurements and with statistical model predictions.
haptic communication to enhance collaboration in virtual environments. motivation – to study haptic communication in collaborative virtual environments. research approach – an experimental study was conducted, in which 60 students were asked to perform in dyads a shared manual task after a training period. findings/design – the results show that haptic communication can influence the common frame of reference development in a shared manual task. research limitations/implications – deeper verbalization analyses are needed to evaluate the common frame of reference development. originality/value – this study highlights haptic interactions importance when designing virtual environment that support shared manual tasks. take away message – haptic communication, combined with visual and verbal communication, enriches interactions in virtual environments.
an experimental study of knock in a natural gas fuelled spark ignition engine. null
a method to determine biogas composition for combustion control. this paper presents a methodology for a rapid determination of biogas composition using easily detectable physical properties. as biogas is mainly composed of three constituents, it is possible to determine its composition by measuring two physical properties and using specific ternary diagrams. the first part of the work deals with the selection of two physical properties, which are easy and inexpensive to measure, from a group comprising thermal conductivity, viscosity and speed of sound. then, in the second part, a model to express these properties in terms of ternary composition is presented. it is demonstrated that the composition of a ternary gas mixture can be determined with good precision using the above. the model is applied to specific situations such as the online determination of the lower heating value of biogas without any complicated apparatus like calorimeters or batch techniques (gas chromatographs). the error on the lower heating value and wobbe index of biogas is less than 1% even when taking into account other constituents not specified in the ternary diagram like oxygen. the effect of small errors in the measurement of physical properties has also been highlighted.
new filtering for the \it cumulative constraint in the context of non-overlapping rectangles. this article describes new filtering methods for the cumulative constraint. the first method introduces the so called longest closed hole and longest open hole problems. for these two problems it first provides bounds and exact methods and then shows how to use them in the context of the non-overlapping constraint. the second method introduces balancing knapsack constraints which relate the total height of the tasks that end at a specific time-point with the total height of the tasks that start at the same time-point. experiments on tight rectangle packing problems show that these methods drastically reduce both the time and the number of backtracks for finding all solutions as well as for finding the first solution. for example, we found without backtracking all solutions to 65 perfect square instances of order 22-25 and sizes ranging from 192×192 to 661×661.
safe dynamic reconfigurations of fractal architectures with fscript. null
a language for quality of service requirements specification in web services orchestrations. service oriented architectures industry aims to deliver agile service infrastructures. in this context, solutions to specify service compositions (mostly bpel language) and quality of service (qos) of individual services have emerged. however, architects still lack adapted means to specify and implement qos in service compositions. typically, they use ad-hoc technical solutions that significantly reduce flexibility and require cost-effective development. our approach aims to overcome this shortcoming by introducing both a new language and tool for qos specification and implementation in service compositions. more specifically, our language is a declarative domain-specific language that allows the architect to specify qos constraints and mechanisms in web service orchestrations. our tool is responsible for the qos constraints processing and for qos mechanisms injection into the orchestration. a key property of our approach is to preserve compatibility with existing languages and standards. in this paper, we present our language and tool, as well as an illustrative scenario dealing with multiple qos concerns.
a declarative approach for qos-aware web service compositions. while bpel language has emerged to allow the specification of web service compositions from a functional point of view, it is still left to the architects to find proper means to handle the quality of service (qos) concerns of their compositions. typically, they use ad-hoc technical solutions, at the message level, that significantly reduce flexibility and require costly developments. in this paper, we propose a policy-based language aiming to provide expressivity for qos behavioural logic specification in web service orchestrations, as well as a non-intrusive platform in charge of its execution both at pre-deployment time and at runtime.
a hierarchical control scheme based on prediction and preview: an application to the cruise control problem. null
global constraint catalog website. null
reliability of dynamic reconfigurations in component-based architectures. software engineering must cope with a more and more increasing need for evolutivity of software systems in order to make their maintenance and more generally their administration easier. however, evolution and especially dynamic evolution in a system must not be done at the expense of its reliability, that is to say its ability to deliver the expected functionalities. actually modifications in a given system may let it in an inconsistent state and so it can have an impact on its reliability. the aim of this thesis is to guarantee reliability of dynamic reconfigurations used to make systems evolve at runtime while preserving their availibility, i.e. their continuity of service. we are especially interested in component based and distributed systems. the system architecture can be used as a support for dynamic, non-anticipated (also called ad-hoc) and concurrent reconfigurations. we propose a definition of consistency for configurations and reconfigurations in the fractal component model with a model based on integrity constraints like for example structural invariants. reliability of reconfigurations is ensured thanks to a transactional approach which allows both to deal with error recovery and to manage reconfiguration concurrency in systems. finally, we propose a modular component-based architecture so as to implement transactional mechanisms adapted to dynamic reconfigurations in fractal applications.
summary of the third workshop on domain-specific aspect languages. null
fiesta: an approach for fine-grained scope definition, configuration and derivation of model-driven software product lines. we present an approach based on model-driven development ideas to create software product lines(spls). in model-driven spl approaches, the derivation of a product starts from a domain application model. this model is transformed through several stages reusing model transformation rules until a product is obtained. transformations rules are selected according to variants included in configurations created by product designers. configurations include variants from variation points, which are relevant characteristics representing the variability of a product line. our approach (1) provides mechanisms to improve the expression of variability of model-driven spls by allowing designers to create fine-grained configurations of products, and (2) integrates a product derivation process which uses decision models and aspect-oriented programming facilitating the reuse, adaptation and composition of model transformation rules. we introduce constraint models which make it possible for product line architects to capture the scope of product lines using the concepts of constraint, cardinality property and structural dependency property. to configure products, we create domain models and binding models, which are sets of bindings between model elements and variants and satisfy the constraint models. we define a decision model as a set of aspects. an aspect maintains information of what and when transformations rules that generate commonalities of products must be intercepted (joinpoints) and what transformation rules (advices) that generate variable structures must be executed instead. our strategy maintains uncoupled variants from model transformation rules. this solves problems related to modularization, coupling, flexibility and maintainability of transformations rules because they are completely separated from variants; thus, they can evolve independently.
distributed aspects: better separation of crosscutting concerns in distributed software systems. this thesis shows that abstractions provided by current mainstream object oriented (oo) languages are not enough to address the modularization of distributed and concurrent algo- rithms, protocols, or architectures. in particular, we show that code implementing concurrent and distributed algorithms is scattered and tangled in the main implementation of jboss cache, a real industrial middleware application. we also show that not only code is tangled, but also conceptual algorithms are hidden behind object-based structures (i.e., they are not visible in the code). additionally, we show that such code is resilient to modularization. thus, we show that several cycles of re-engineering (we study the evolution of three diﬀerent version of jboss cache) using the same set of oo abstractions do not improve on the modularization of distributed and concurrent code. from these ﬁndings we propose a novel aspect oriented programming language with explicit support for distribution and concurrency (awed). the language uses aspects as main abstractions and propose a model for distributed aspects and remote pointcuts, extending sequential approaches with support for regular sequences of distributed events. the language also proposes advanced support for the manipulation of groups of host, and the ﬁne-grained deterministic ordering of distributed events. to evaluate the proposal we perform several experiments in diﬀerent domains: refactoring and evolution of replicated caches, development of automatic toll systems, and debugging and testing of distributed applications. finally, using this general model for distribution we provide two additional contributions. first, we introduce invasive patterns, an extension to traditional communication patterns for distributed applications. invasive patterns present an aspect-based language to express protocols over distributed topologies considering diﬀerent coordination strategies (architec- tural programming). the implementation of this approach is leveraged by the distributed features of awed and is realized by means of a transformation into it. second, we add the deterministic manipulation of distributed messages to our model by means of causally ordered protocols.
designing open-ended languages: an historical perspective. null
graph partitioning constraints. combinatorial problems based on graph partitioning enable to represent many practical applications. examples based on phylogenetic supertree problem, mission planning, or the routing problems in logistic, perfectly illustrate such applications. nevertheless, these problems are not based on the same partitioning pattern : generally, patterns like cycles, paths, or trees are distinguished. moreover, the practical applications are not often limited to theoretical problems like hamiltonian path problem, or k-node disjoint paths problems. indeed, they usually combine the graph partitioning problem with several restrictions related to the topology of nodes and arcs. the diversity of implied constraints in real-life applications is a practical limit to the resolution of such problems by approaches considering the partitioning problem independently from each additional restriction. this thesis focuses on constraint satisfaction problems related to tree partitioning problems enriched by several additional constraints that restrict the possible partitions topology. on the one hand, our study focuses the structural properties of tree partitioning constraints. on the ohter hand, it is dedicated to the interactions between the tree partitioning problem and classical restrictions (such as precedence relations or incomparability relations between nodes) involved in practical applications. precisely, we show how to globally take into account several restrictions within one single tree partitioning constraint. another interesting aspect of this thesis is related to the implementation of such a constraint. in the context of graph-based global constraints, we show how a fully dynamic management of data structures makes the runtime of filtering algorithms independent of the graph density.
radio emission in a toy model with point-charge-like air showers. null
geometric model of a narrow tilting car using robotics formalism. the use of an electrical narrow tilting car instead of a large gasoline car should dramatically decrease traffic congestion, pollution and parking problem. the aim of this paper is to give a unique presentation of the geometric modeling issue of a new narrow tilting car. the modeling is based on the modified denavit hartenberg geometric description, which is commonly used in robotics. also, we describe the special kinematic of the vehicle and give a method to analyze the tilting mechanism of it. primarily experimental results on the validation of the geometrical model of a real tilting car are given.
a diesel engine thermal transient simulation : coupling between a combustion model and a thermal model. null
characterisation of a syngas diesel fueled ci engines. null
modeling of in-cylinder pressure oscillations under knocking conditions: introduction to pressure envelope curve. null
observation of pi^+pi^-pi^+pi^- photoproduction in ultra-peripheral heavy ion collisions at sqrt{s_nn} = 200 gev at the star detector. we present a measurement of pi^+pi^-pi^+pi^- photonuclear production in ultra-peripheral au-au collisions at sqrt(s_{nn}) = 200 gev from the star experiment. the pi^+pi^-pi^+pi^- final states are observed at low transverse momentum and are accompanied by mutual nuclear excitation of the beam particles. the strong enhancement of the production cross section at low transverse momentum is consistent with coherent photoproduction. the pi^+pi^-pi^+pi^- invariant mass spectrum of the coherent events exhibits a broad peak around 1540 pm 40 mev/c^2 with a width of 570 pm 60 mev/c^2, in agreement with the photoproduction data for the rho^0(1700). we do not observe a corresponding peak in the pi^+pi^- final state and measure an upper limit for the ratio of the branching fractions of the rho^0(1700) to pi^+pi^- and pi^+pi^-pi^+pi^- of 2.5 % at 90 % confidence level. the ratio of rho^0(1700) and rho^0(770) coherent production cross sections is measured to be 13.4 pm 0.8 (stat.) pm 4.4 (syst.) %.
upsilon cross section in p+p collisions at sqrt(s) = 200 gev. we report on a measurement of the upsilon(1s+2s+3s) -&gt; e+e- cross section at midrapidity in p+p collisions at sqrt(s)=200 gev. we find the cross section to be 114 +/- 38 (stat.) +23,-24 (syst.) pb. perturbative qcd calculations at next-to-leading order in the color evaporation model are in agreement with our measurement, while calculations in the color singlet model underestimate it by 2 sigma. our result is consistent with the trend seen in world data as a function of the center-of-mass energy of the collision and extends the availability of upsilon data to rhic energies. the dielectron continuum in the invariant mass range near the upsilon is also studied to obtain a combined cross section of drell-yan plus (b b-bar) -&gt; e+e-.
competition of heavy quark radiative and collisional energy loss in deconfined matter. we extend our recently advanced model on collisional energy loss of heavy quarks in a quark gluon plasma (qgp) by including radiative energy loss. we discuss the approach and present first preliminary results. we show that present data on nuclear modification factor of non photonic single electrons hardly permit to distinguish between those 2 energy loss mechanisms.
heavy quark production in p+p and energy loss and flow of heavy quarks in au+au collisions at sqrt(s_nn)=200 gev. transverse momentum (p^e_t) spectra of electrons from semileptonic weak decays of heavy flavor mesons in the range of 0.3 &lt; p^e_t &lt; 9.0 gev/c have been measured at mid-rapidity (|eta| &lt; 0.35) by the phenix experiment at the relativistic heavy ion collider in p+p and au+au collisions at sqrt(s_nn)=200 gev. the nuclear modification factor r_aa with respect to p+p collisions indicates substantial energy loss of heavy quarks in the produced medium. in addition, the azimuthal anisotropy parameter v_2 has been measured for 0.3 &lt; p^e_t &lt; 5.0 gev/c in au+au collisions. comparisons of r_aa and v_2 are made to various model calculations.
isospin effects on the energy of vanishing flow in heavy-ion collisions. using the isospin-dependent quantum molecular dynamics model we study the isospin effects on the disappearance of flow for the reactions of $^{58}ni$+$^{58}ni$ and $^{58}fe$+$^{58}fe$ as a function of impact parameter. we found good agreement between our calculations and experimentally measured energy of vanishing flow at all colliding geometries. our calculations reproduce the experimental data within 5\%(10\%) at central (peripheral) geometries.
combustion of syngas in internal combustion engines. the combustion of synthesis gas will play an important role in advanced power systems based on the gasification of fuel feedstocks and combined cycle power production. while the most commonly discussed option is to burn syngas in gas turbine engines, another possibility is to burn the syngas in stationary reciprocating engines. whether spark ignited or compression ignited, syngas could serve to power large bore stationary engines, such as those presently operated on natural gas. to date, however, there has been little published on the combustion of syngas in reciprocating engines. one area that has received attention is dual-fueled diesel combustion, using a combination of diesel pilot injection and syngas fumigation in the intake air. in this article, we survey some of the relevant published work on the use of synthesis gas in ic engines, highlighting recent work on dual-fuel (syngas + diesel) combustion.
atl: a model transformation tool. null
solving allocation problems of hard real-time systems with dynamic constraint programming. in this paper, we present an original approach (cprta for ”constraint programming for solving real-time allocation”) based on constraint programming to solve an allocation problem of hard real-time tasks in the context of fixed priority preemptive scheduling. cprta is built on dynamic constraint programming together with a learning method to find a feasible processor allocation under constraints. it is a new approach which produce in its current version as acceptable performances as classical algorithms do. some experimental results are given to show it. moreover, cprta shows very interesting properties. it is complete —i.e., if a problem has no solution, the algorithm is able to prove it—, and it is non-parametric —i.e., it does not require specific initializations—. thanks to its capacity to explain failures, it offers attractive perspectives for guiding the architectural design process.
an inventory pick-up and delivery problem in the reverse logistics context: optimization using a grasp and hybrid approach. we consider the multiperiodic planning and optimization of transport activities (direct and reverse), and inventory management in a two level distribution network. the goal is to satisfy the customers demands while minimizing the routing and storage costs. we use the grasp and a hybrid approach to solve this problem.
choco: an open source java constraint programming library. choco is a java library for constraint satisfaction problems (csp), constraint programming (cp) and explanation-based constraint solving (e-cp). it is built on a event-based propagation mechanism with backtrackable structures.
learning from the past to dynamically improve search: a case study on the mosp problem. this paper presents a study conducted on the minimum number of open stacks problem (mosp) which occurs in various production environments where an efficient simultaneous utilization of resources (stacks) is needed to achieve a set of tasks. we investigate through this problem how classical look-back reasonings based on explanations could be used to prune the search space and design a new solving technique. explanations have often been used to design intelligent backtracking mechanisms in constraint programming whereas their use in nogood recording schemes has been less investigated. in this paper, we introduce a generalized nogood (embedding explanation mechanisms) for the mosp that leads to a new solving technique and can provide explanations.
explanation-based algorithms in constraint programming. constraint programming is a search paradigm for solving combinatorial optimization pro- blems, that has been used to design generic solvers. numerous researches are conducted to deal with over-constrained and dynamic problems. one of those, is based on the concept of explanations. explanations provide a trace of the behavior of the solver and have been initially introduce to improve backtracking based algorithms. they have been used to design clever but costly ways of exploring the search space since that day. this phd thesis study explanation based algorithms on industrial as well as academical problems. we study the interest of explanation within generic decomposition techniques and imple- ment such an algorithm for a hard real time task allocation problem. this approach outlines the role of explanations within the cooperation of di®erent solving techniques. we also show that the explanation network is a relevant information to analyse the struc- tures of a problem and understand the relationships between its di®erent parts (variables and constraints). this information, used to improve the search heuristic, is another step toward generic search techniques. finally, explanations have been often used for look-back but are still under-exploited for look-ahead in cp. nogood recording techniques have never been successful contrary to what happended in the sat community. we implement in this thesis such a nogood recording in the case of the minimum open stack problem.
krivine realizability for compiler correctness. we propose a semantic type soundness result, formalized in the coq proof assistant, for a compiler from a simple functional language to secd machine code. our result is quite independent from the source language as it uses krivine's realizability to give a denotational semantics to secd machine code using only the type system of the source language. we use realizability to prove the correctness of both a call-by-name (cbn) and a call-by-value (cbv) compiler with the same notion of orthogonality. we abstract over the notion of observation (e.g. divergence or termination) and derive an operational correctness result that relates the reduction of a term with the execution of its compiled secd machine code.
on-line resource allocation for atm networks with rerouting. null
method for protecting a gas engine and associated device. the invention relates to a device for protecting (24) a gas engine (12), especially a stationary gas engine, for regulating at least one parameter (pj) of the engine (12), said device comprising means (26) for obtaining an index (imi; img) which represents the combustion behaviour of a gas to be potentially supplied to the engine (12). the invention is characterised in that the device also comprises means (28) for determining an optimum regulating value of the parameter in view of the index obtained (imi; img).
device for protecting a gas engine. null
stationary gas engine protecting device for stationary power generating facility of vehicle, has determining unit determining power value, and changing unit changing set power value such that supplied power is equal to pre-determined value. the device (24) has a measuring unit i.e. sensor (26), measuring a quantity i.e. methane index (im), representing combustion behavior of gas supplied to a stationary gas engine (12). a determining unit (28) determines an optimum electrical power value (peopt) to be achieved based on the measured quantity representing the combustion behavior of the measured combustion gas. a changing unit (30) changes a set power value (pc) such that supplied electrical power (pe) is equal to the pre-determined optimum electrical power value. independent claims are also included for the following: (1) a method of protecting a stationary gas engine (2) a method of generating a digital drift cartography.
method for determining at least one energetic property of a gas fuel mixture by measuring physical properties of the gas mixture. null
method for defining at least one power property of gas fuel mixture by measuring physical properties of a gas mixture. null
experiments with resolution search. null
tdc determination in ic engines based on the thermodynamic analysis of the temperature-entropy diagram. a thermodynamic methodology of tdc determination in ic engines based on a motoring pressure-time diagram is presented. this method consists in entropy calculation and temperature- entropy diagram analysis. when the tdc position is well calibrated, compression and expansion strokes under motoring conditions are symmetrical with respect to the peak temperature in the (t,s) diagram. moreover, in case of error on the tdc position, a loop appears which has no thermodynamic significance. hence an easy methodology has been conceived to obtain the actual position of tdc. this methodology is applied to motoring measurements in order to present its performance, which are compared to usual methods.
lower bounds computation for rcpsp. null
a new method to determine the start and end of combustion in an internal combustion engine using entropy changes. simulation studies indicated that the start and end of combustion in an internal combustion engine could be determined from the points of minimum and maximum entropy in the cycle. this method was further used to predict the beginning and end of the combustion process from experimentally obtained pressure crank angle data from a natural gas operated, single cylinder, spark ignition engine. the end of combustion always matched well with the point of maximum entropy. the start of combustion could be determined easily from the rate of change of entropy, which showed a sharp change at ignition. results closely agreed with those obtained from a heat release analysis.
thermoeconomic analysis based on energy structure for chp. this paper proposes a unified comparison method for the calculations of thermodynamic efficiencies applied to combined heat and power (chp) plants. two new dimensionless indices are introduced. they are used to estimate the influence of technical and economical features on the profitability of a chp plant. a case of a chp installation by internal combustion engine is treated as a practical application.
mathematical programming formulations and lower bounds for the rcpsp. null
determination of the combustion properties of natural gases by pseudo-constituents. this paper presents a methodology for a rapid determination of important natural gas combustion properties (lower heating value, wobbe index and the stoichiometric air-fuel ratio) using easily detectable physical properties. it is possible to determine natural gas composition by measuring two physical properties and using specific ternary diagrams (ch4-c2h6-c3h8 and ch4-c2h6-n2). the first part of the work deals with the selection of two physical properties from a group comprising thermal conductivity, refraction index, and speed of sound. then, in the second part, a sensor using the best couple of physical property is used to determine the ternary pseudo-constituents of the gas mixture. the model and the sensor are applied to specific situations such as the online determination of lhv. the error on the combustion properties of natural gas is less than 1% over the gases considered in the present study and over about 20 typical gases supplied over europe. the effect of small errors in the measurement of physical properties has also been highlighted.
resource-constrained project scheduling: models, algorithms, extensions and applications. this title presents a large variety of models and algorithms dedicated to the resource-constrained project scheduling problem (rcpsp), which aims at scheduling at minimal duration a set of activities subject to precedence constraints and limited resource availabilities. in the first part, the standard variant of rcpsp is presented and analyzed as a combinatorial optimization problem. constraint programming and integer linear programming formulations are given. relaxations based on these formulations and also on related scheduling problems are presented. exact methods and heuristics are surveyed. computational experiments, aiming at providing an empirical insight on the difficulty of the problem, are provided. the second part of the book focuses on several other variants of the rcpsp and on their solution methods. each variant takes account of real-life characteristics which are not considered in the standard version, such as possible interruptions of activities, production and consumption of resources, cost-based approaches and uncertainty considerations. the last part presents industrial case studies where the rcpsp plays a central part. applications are presented in various domains such as assembly shop and rolling ingots production scheduling, project management in information technology companies and instruction scheduling for vliw processor architectures.
thermodynamic analysis of tri-generation with absorption chilling machine. the paper presents a method for analysing tri-generation systems. the authors have focused on solutions of tri-generation plants based on gas turbine or internal combustion engine with absorption chilling machine. several technical criteria have been defined. a thermodynamic analysis has been performed for the case of tri-generation with an absorption chilling machine. from the thermodynamic point of view there have been established the limits for the best energetic performance of tri-generation. the dependence of different technical criteria on each other has also been analysed. a certain case of a tri-generation plant has been analysed using this method. the dependence of the energetic performance of tri-generation on different technical criteria has also been studied.
fuel savings and co2 emissions for tri-generation systems. the paper presents a method for analysing previous termtri-generation systemsnext term from the point of view of previous termfuel savingsnext term and environmental impact. the authors have focused on the solution of previous termtri-generationnext term plants based on gas turbine or internal combustion engine with an absorption chilling machine. criteria that characterise the previous termfuel savings and co2 emissions of tri-generationnext term have been defined. an analysis has been performed for the case of previous termtri-generationnext term with an absorption chilling machine. the mathematical dependence of previous termfuel savings and co2 emissionsnext term on different technical criteria has been analysed. for a certain case there has been plotted the variation of previous termfuel savingsnext term as a dependence of technical criteria. the graphs highlight the range of technical criteria with better previous termfuel savingsnext term for previous termtri-generationnext term than for separate production. there has been established the order of influence of different technical criteria on previous termfuel savings.next term.
potential physics measurement with alice electromagnetic calorimeters. we present the two electromagnetic calorimeters of the alice (a large ion collider experiment) experiment at lhc (large hadron collider). one is the high-resolution photon spectrometer (phos) made of lead tungsten crystals and the other is the electromagnetic calorimeter (emcal), a lead-scintillator sampling calorimeter. they are dedicated to the measurement and identification of direct photons, light neutral mesons such as π0, η and ω(782), and jets emitted in proton-proton and heavy-ion collisions at the lhc energies. the phos is capable of precisely detecting photons with momentum range between 0.1 gev/c and 100 gev/c and the emcal can extend the prompt photon and light neutral meson momentum measurement beyond 200 gev/c. the objective of the study is to explore the physics of strongly interacting qcd matter under extreme conditions of energy density.
online determination of natural gas properties. methane number and lower heating value (lhv) of natural gas are determined by the online measurement of thermal conductivity at two different temperatures. natural gas is first considered as a ternary mixture of the most important components. a pseudo-ternary composition can then be calculated, using the thermal conductivity formula for mixtures derived from kinetic theory. a non-linear system is solved numerically using the newton-raphson method. a sensor based on thermal conductivity measurement has been developed and tested successfully for many natural gas compositions.
thermal efficiency and environmental performances of a biogas-diesel stationary engine. municipal and agricultural waste, and sludge from wastewater treatment represent a large source of pollution. gaseous fuels can be produced from waste decomposition and then used to run internal combustion engines for power and heat generation. the present paper focuses on thermal efficiency and environmental performances of dual-fuel engines fuelled with biogas. experiments have been carried out on a lister-petter single cylinder diesel engine, modified for dual-fuel operation. natural gas was first used as the primary fuel. an empirical correlation was determined to predict the engine load for a given mass flow rate for the pilot fuel (diesel) and for the primary fuel (natural gas). that correlation has then been tested for three synthesized biogas compositions. computations were performed and the error was estimated to be less than 10%. additionally, nox and co2 contents were measured from exhaust gases. based on exhausts gas temperature, the activation energy and the pre-exponential factor of an arrhenius law were then proposed, resulting in a simpler mean to predict nox.
a model of energetic interactions between a car engine, the cabin heating system and the electrical system. reduction of internal combustion engines fuel consumption is permanently researched. it leads automotive companies towards global energetic simulation tools to describe the interactions between the engine and the energy consumer systems. valeo with the emn department of energetic, develop a vehicle energy management tool. it will be able to describe the interactions between: engine, the car cabin heating system, electrical systems and other energy consumers (additional heating system, air conditioning system) implied in the vehicle operation. the first results given by the simulation model have approached quite accurately, the coolant loop warm-up curve, measured during a vehicle test in wind tunnel. the model solves the energy balance on the oil and coolant loops and computes: the heat flux from engine to coolant, the distribution of coolant flows in branches, the thermal exchanges involved in the heater core, the cooling radiator and the oil cooler.
co-combustion of pulverised coal/biomass for steam production: emergy approach. emergy, also called embodied energy, has been firstly defined by odum. it is the energy required to make a service or product expressed in energy of one form (here solar energy). emergy per mass or transformity (expressed as solar energy joule per gram [sej/g]) varies from 0.88 109 sej/g for wood to a high of 1.1 1015 sej/g for coal. this concept of emergy is particularly relevant to compare on the same basis different fuels especially to analyse co-combustion of renewable and non-renewable combustibles. the cost of co-combustion, in term of emergy, makes it relevant if collection of biomass remains below the threshold distance of 400 km from the power plant. that threshold can be raised to 700 km when the calorific value of biomass feed stock is higher.
knock rating of gaseous fuels in a single cylinder spark ignition engine. this paper presents the determination of knock rating of gaseous fuels in a single cylinder engine. the first part of the work deals with an application of a standard method for the knock rating of gaseous fuels. the service methane number (smn) is compared with the standard methane number (mn) calculated from the standard avl software methane (which corresponds to the mn measured on a cooperative fuel research engine). then, in the second part, the 'mechanical' resistance to knock of our engine is highlighted by means of the methane number requirement (mnr). a single cylinder lister petter engine was modified to run as a spark ignition engine with a fixed compression ratio and an adjustable spark advance. effects of engine settings on the mnr are deduced from experimental data and compared extensively with previous studies. using the above, it is then possible to adapt the engine settings for optimal knock control and performances. the error on the smn and mnr stands beneath ±2 mn units over the gases and engine settings considered.
a combustionless determination method for combustion properties of natural gases. this paper presents a methodology for the determination and measurement of important natural gas combustion properties (higher heating value, wobbe index and stoichiometric air-fuel ratio). a pseudo-gas formulation is used to determine an equivalent gas composition to the real natural gas tested. the pseudo-composition, made up of the most influent constituents of the natural gas, is determined by solving a system of non-linear equations. the input parameters to the procedure are: the thermal conductivity of the natural gas at 333 k and 383 k, the speed of sound at 303 k and the concentration of co2. a combustion properties measurement sensor has been developed and tested for many natural gas compositions. the tested natural gases are chosen to represent typical european gases as well as to account for large variations of individual components (heavy hydrocarbons, inert gases). with the developed sensor, combustion properties measured at standard conditions agree with gas chromatographic analysis, to within 0.5%.
safe operating conditions determination for stationary si gas engines. knock is a major problem when running combined heat and power (chp) gas engines because of the variation in the network natural gas composition. a curative solution is widely applied, using an accelerometer to detect knock when it occurs. the engine load is then reduced until knock disappears. the present paper deals with a knock preventive device. it is based on the knock prediction following the engine operating conditions and the fuel gas methane number, and it acts on the engine load before knock happens. a state of the art about knock prediction models is carried out. the maximum of the knock criterion is selected as knock risk estimator, and a limit value above which knock may occur is defined. the estimator is calculated using a two-zone thermodynamic model. this model is specifically based on existing formulas for the calculation of the combustion progress, modified to integrate the effect of the methane number. a chemical kinetic model with 53 species and 325 equilibrium reactions is used to calculate unburned and burned gases composition. the different parameters of the model are fitted with a least squares method from an experimental data base. errors less than 8% are achieved. the knock risks predicted for various natural gases and operating conditions are in agreement with previous work. nevertheless, the knock risk estimator is overestimated for natural gases with high concentrations of inert gases such as nitrogen and carbon dioxide. the definition of a methane number limit based on the engine manufacturer's recommendation is then required to eliminate unwarranted alerts. safe operating conditions are thus calculated and gathered in the form of a map. this map, combined with the real time measurement of the fuel gas methane number, can be integrated to the control device of the chp engine in order to guarantee a safe running towards fuel gas quality variation.
core-corona separation in heavy ion collision at rhic and sps. null
nuclear surface effects in auau@rhic. null
light algorithms for maintaining max-rpc during search. this article presents two new algorithms whose purpose is to maintain the max-rpc domain filtering consistency during search with a minimal memory footprint and implementation effort. both are sub-optimal algorithms that make use of support residues, a backtrack-stable and highly efficient data structure which was successfully used to develop the state-of-the-art ac-3rm algorithm. the two proposed algorithms, max-rpcrm and l-max-rpcrm are competitive with best, optimal max-rpcalgorithms while being considerably simplier to implement. l-max-rpcrm computes an approximation of the max-rpc consistency, which is garanteed to be strictly stronger than ac with the same space complexity and better worst-case time complexity than max-rpcrm. in practice, the difference in filtering power between l-max-rpcrm and standard max-rpc is nearly indistinguisable on random problems. max-rpcrm and l-max-rpcrm are implemented into the choco constraint solver through a strong consistency global constraint. this work opens new perspective upon the development of strong consistency algorithms into constraint solvers.
preventive knock protection technique for stationary si engines fuelled by natural gas. in a combined heat and power (chp) plant, spark ignition engines must operate at their maximum power to reduce the pay back time. because of environmental and economic concerns, engines are set with high compression ratios. consequently, optimal operating conditions are generally very close to those of knock occurrence and heavy knock can severely damage the engine piston. there are two main protection techniques: the curative one commonly used by engine manufacturers and well documented in the literature and the preventive one based on a knock prediction according to the quality of the supplied gas. the indicator used to describe gas quality is the methane number (mn). the methane number requirement (mnr) of the engine is defined, for an engine set (spark advance, air-fuel ratio, and load), as the minimum value of mn above which knock free operation is ensured. to prevent knock occurrence, it is necessary to adapt the engine tuning according to variable gas composition. the objective of the present work is to validate the concept of knock preventive protection. first, a prediction of mnr according to engine settings (es) is computed through a combustion simulator composed of a thermodynamic 2-zone model. predicted mnr are compared to experimental results performed on a single-cylinder si gas engine and show good agreement with numerical results (uncertainty below 1 point). then, the combustion simulator is used to generate a protection mapping. at last, the knock preventive protection was successfully tested.
global propagation of practicability constraints. null
a generic scheme for integrating strong consistencies into constraint solvers. this article presents a generic scheme for adding strong local consistencies to the set of features of constraint solvers, which is notably applicable to event-based constraint solvers. we encapsulate a subset of constraints into a global constraint. this approach allows a solver to use diﬀerent levels of consistency for diﬀerent subsets of constraints in the same model. moreover, we show how strong consistencies can be applied with diﬀerent kinds of constraints, including user-deﬁned constraints. we experiment our technique with a coarse-grained algorithm for max-rpc, called max-rpcrm and a variant of it, l-max-rpcrm. experiments conﬁrm the interest of strong consistencies for constraint programming tools.
global propagation of side constraints for solving over-constrained problems. this article deals with the resolution of over-constrained problems using constraint programming, which often imposes to add to the constraint network new side constraints. these side constraints control how the initial constraints of the model should be satisfied or violated, to obtain solutions that have a practical interest. they are specific to each application. in our experiments, we show the superiority of a framework where side constraints are encoded by global constraints on new domain variables, which are directly included into the model. the case-study is a cumulative scheduling problem with over-loads. the objective is to minimize the total amount of over-loads. we augment the cumulative global constraint of the constraint programming solver choco with sweep and task interval violation-based algorithms. we provide a theoretical and experimental comparison of the two main approaches for encoding over-constrained problems with side constraints.
trends in constraint programming. constraint programming is a constantly evolving field, something which is explored at the annual international conference on principles and practice of constraint programming. this conference provides papers and workshops which produce new insights, concepts and results which those involved in this area can then use to develop their own work. this title provides an accessible overview of this by bringing together the best papers on a range of topics within this subject area, thus allowing those involved in constraint programming to benefit from the new innovations and results created as a result of the conference.
superconformal higher-dimensional field theories and the theory of everything. null
heavy quark energy loss in finite size qgp. null
performance and power management for cloud infrastructures. a key issue for cloud computing data-centers is to maximize their proﬁts by minimizing power consumption and sla violations of hosted applications. in this paper, we propose a resource management framework combining a utility-based dynamic virtual machine provisioning manager and a dynamic vm placement manager. both problems are modeled as constraint satisfaction problems. the vm provisioning process aims at maximizing a global utility capturing both the performance of the hosted applications with regard to their slas and the energy- related operational cost of the cloud computing infrastructure. we show several experiments how our system can be controlled through high level handles to make different trade-off between application performance and energy consumption or to arbitrate resource allocations in case of contention.
the search of pentaquark. null
the pentaquark case: state of the art. null
exotic particle search in star. null
the nuclear equation of state from k+. null
description of low energy heavy ion collision within the isospin quantum molecular dynamics model. null
nuclear waste package performance in european geological disposal concepts. null
experimental research on improved antineutrino spectra from reactors. null
global constraints: introduction and graph-based representation. second international summer school of the association for constraint programming. samos, greece. global constraints: introduction and graph-based representation.
graph-properties based filtering. sics technical report t2006-10. graph-properties based filtering.
combining tree partitioning, precedence, incomparability, and degree constraints, with an application to phylogenetic and ordered-path problems, technical report 2006-20 of uppsala university. null
undirected forest constraints. null
graph-based filtering. graph-based filtering.
bounds of parameters for global constraints. bounds of parameters for global constraints.
sweep synchronisation as a global propagation mechanism. sweep synchronisation as a global propagation mechanism.
a generic geometrical constraint kernel in space and time for handling polymorphic k-dimensional objects. sics technical report t2007-08. a generic geometrical constraint kernel in space and time for handling polymorphic k-dimensional objects.
necessary condition for path partitioning constraints. necessary condition for path partitioning constraints.
a continuous multi-resources cumulative constraint with positive-negative resource consumption-production. a continuous multi-resources cumulative constraint with positive-negative resource consumption-production.
a generic geometrical constraint kernel in space and time for handling polymorphic k-dimensional objects. a generic geometrical constraint kernel in space and time for handling polymorphic k-dimensional objects.
special issue on global constraints. special issue on global constraints.
filtering for a continuous multi-resources cumulative constraint with resource consumption and production. filtering for a continuous multi-resources cumulative constraint with resource consumption and production.
new filtering for the cumulative constraint in the context of non-overlapping rectangles. new filtering for the cumulative constraint in the context of non-overlapping rectangles.
a geometric constraint over k-dimensional objects and shapes subject to business rules. a geometric constraint over k-dimensional objects and shapes subject to business rules.
combining tree partitioning, precedence, and incomparability constraints. combining tree partitioning, precedence, and incomparability constraints.
six ways of integrating symmetries within non-overlapping constraints, sics technical report t2009-01. six ways of integrating symmetries within non-overlapping constraints.
compiling business rules in a geometric constraint over k-dimensional objects and shapes. sics technical report t2009-02. compiling business rules in a geometric constraint over k-dimensional objects and shapes.
six ways of integrating symmetries within non-overlapping constraints. six ways of integrating symmetries within non-overlapping constraints.
on the difference of bremsstrahlung in qed and qcd processes. null
challenges for transport theories to describe high density systems. null
bimodality a smoking gun signal for a first order phase transition. null
tomography of the quark gluon plasma by charmed mesons. null
transport theories in the fermi energy domain. null
the nuclear equation of state in nuclear and astrophysics. null
strange particles in hadron matter. null
improving inter-block backtracking with interval newton. inter-block backtracking (ibb) computes all the solutions of sparse systems of nonlinear equations over the reals. this algorithm, introduced by bliek et al. (1998) handles a system of equations previously decomposed into a set of (small) k × k sub-systems, called blocks. partial solutions are computed in the different blocks in a certain order and combined together to obtain the set of global solutions. when solutions inside blocks are computed with interval-based techniques, ibb can be viewed as a new interval-based algorithm for solving decomposed systems of non-linear equations. previous implementations used ilog solver and its ilcinterval library as a black box, which implied several strong limitations. new versions come from the integration of ibb with the interval-based library ibex. ibb is now reliable (no solution is lost) while still gaining at least one order of magnitude w.r.t. solving the entire system. on a sample of benchmarks, we have compared several variants of ibb that differ in the way the contraction/filtering is performed inside blocks and is shared between blocks. we have observed that the use of interval newton inside blocks has the most positive impact on the robustness and performance of ibb. this modifies the influence of other features, such as intelligent backtracking. also, an incremental variant of inter-block filtering makes this feature more often fruitful.
a generalized interval lu decomposition for the solution of interval linear systems. null
on the approximation of linear ae-solution sets. null
observation of charge-dependent azimuthal correlations and possible local strong parity violation in heavy ion collisions. parity-odd domains, corresponding to non-trivial topological solutions of the qcd vacuum, might be created during relativistic heavy-ion collisions. these domains are predicted to lead to charge separation of quarks along the orbital momentum of the system created in non-central collisions. to study this effect, we investigate a three particle mixed harmonics azimuthal correlator which is a \p-even observable, but directly sensitive to the charge separation effect. we report measurements of this observable using the star detector in au+au and cu+cu collisions at $\sqrt{s_{nn}}$=200 and 62~gev. the results are presented as a function of collision centrality, particle separation in rapidity, and particle transverse momentum. a signal consistent with several of the theoretical expectations is detected in all four data sets. we compare our results to the predictions of existing event generators, and discuss in detail possible contributions from other effects that are not related to parity violation.
detailed measurement of the e+e- pair continuum in p+p and au+au collisions at √snn=200 gev and implications for direct photon production. in 2002, an innovative neutron time-of-flight facility started operation at cern: n_tof. the main characteristics that make the new facility unique are the high instantaneous neutron flux, high resolution and wide energy range. combined with state-of-the-art detectors and data acquisition system, these features have allowed to collect high accuracy neutron cross-section data on a variety of isotopes, many of which radioactive, of interest for nuclear astrophysics and for applications to advanced reactor technologies. a review of the most important results on capture and fission reactions obtained so far at n_tof is presented, together with plans for new measurements related to nuclear industry.
self consistent investigation of structural transitions in neutron star crusts. null
heavy ion collisions in a wavelet representation. null
advance waste forms. null
study of various options for final disposal of htr coated particles. null
leaching behaviour of spent htr fuel kernels. null
new synthesis route and characterisation of siderite (feco3) and coprecipitation of tc99. null
non proliferation studies with the double chooz detector. null
j/psi analysis in the alice muon spectrometer. null
electroweak boson detection in the alice muon spectrometer. null
on qualitative properties of linear neutral type control systems. null
online management of jobs in clusters using virtual machines. resources management systems relying on a dynamic management of jobs can efficiently use resources in clusters. indeed they provide mechanisms to manipulate online the state of the jobs and their assignment on the nodes. in practice, these scheduling strategies are hard to deploy on clusters as they can not necessarily handle the manipulation of the jobs and may have specific scheduling constraints to consider. in this thesis, we try to ease the development of resources management systems relying on a dynamic management of jobs. we based our environment on the use of virtual machines to execute the jobs in their legacy environments while providing the mechanisms to manipulate them in a non-intrusive way. moreover, we propose an autonomous environment to continuously optimize the scheduling of jobs. scheduling strategies are implemented using constraints programming which aims to model and solve combinatory problems. we validate our approach with the development of our prototype entropy, which has been used to implement various scheduling strategies. the evaluation of these strategies show their capability to to solve present problems.
cluster-wide context switch of virtualized jobs. clusters are massively used through resource management systems with a static allocation of resources for a bounded amount of time. such an approach leads to a coarse-grain exploitation of the architecture and an increase of the job completion times since most of the scheduling policies rely on users estimates and do no consider the real needs of applications in terms of both resources and times. encapsulating jobs into vms enables to implement finer scheduling policies through cluster-wide context switches: a permutation between vms present in the cluster. it results a more flexible use of cluster resources and relieve end-users of the burden of dealing with time estimates. leveraging the entropy framework, this paper introduces a new infrastructure enabling cluster-wide context switches of virtualized jobs to improve resource management. as an example, we propose a scheduling policy to execute a maximum number of jobs simultaneously, and uses vm operations such as migrations, suspends and resumes to resolve underused and overloaded situations. we show through experiments that such an approach improves resource usage and reduces the overall duration of jobs. moreover, as the cost of each action and the dependencies between them is considered, entropy reduces, the duration of each cluster-wide context switch by performing a minimum number of actions, in the most efficient way.
charged-particle multiplicity measurement in proton-proton collisions at sqrt(s) = 7 tev with alice at lhc. the pseudorapidity density and multiplicity distribution of charged particles produced in proton-proton collisions at the lhc, at a centre-of-mass energy sqrt(s) = 7 tev, were measured in the central pseudorapidity region |eta| &lt; 1. comparisons are made with previous measurements at sqrt(s) = 0.9 tev and 2.36 tev. at sqrt(s) = 7 tev, for events with at least one charged particle in |eta| &lt; 1, we obtain dnch/deta = 6.01 +- 0.01 (stat.) +0.20 -0.12 (syst.). this corresponds to an increase of 57.6% +- 0.4% (stat.) +3.6 -1.8% (syst.) relative to collisions at 0.9 tev, significantly higher than calculations from commonly used models. the multiplicity distribution at 7 tev is described fairly well by the negative binomial distribution.
linear logic as a foundation for service-oriented computing. we present a calculus that provides formal and uniﬁed foundations to service- oriented computing. service-oriented computing allows network-based software applica- tions to be developed by resorting to services as primitive components. to date, there are two popular – and often antagonistic – models for service-oriented computing. on the ﬁrst hand, the computation-oriented model, illustrated by ws* web services, considers services as sets of operations. on the other hand, the resource-oriented model, illustrated by restful web services, considers services as interfaces to resources. the lack of uniﬁed models leads to adaptation, integration and coordination problems, three major concerns in this ﬁeld. our calculus restores unity to service-oriented computing, by reconciling both points of view. we give the operational semantics of the calculus using chemical solutions, and illustrate its expressive power not only as a query language over resources with sup- port for recursion and aggregation, but also as a concurrent process language. finally, we show that our calculus is also meaningful in logic programming since computation can be interpreted as proof search in focused linear logic with resource modalities: afﬁne, contractible and exponential.
reliable dynamic reconfiguration in a reflective component model. null
sla-aware virtual resource management for cloud infrastructures. cloud platforms host several independent applications on a shared resource pool with the ability to allocate computing power to applications on a per-demand basis. the use of server virtualization techniques for such platforms provide great flexibility with the ability to consolidate several virtual machines on the same physical server, to resize a virtual machine capacity and to migrate virtual machine across physical servers. a key challenge for cloud providers is to automate the management of virtual servers while taking into account both high-level qos requirements of hosted applications and resource management costs. this paper proposes an autonomic resource manager to control the virtualized environment which decouples the provisioning of resources from the dynamic placement of virtual machines. this manager aims to optimize a global utility function which integrates both the degree of sla fulfillment and the operating costs. we resort to a constraint programming approach to formulate and solve the optimization problem. results obtained through simulations validate our approach.
autonomic virtual resource management for service hosting platforms. cloud platforms host several independent applications on a shared resource pool with the ability to allocate com- puting power to applications on a per-demand basis. the use of server virtualization techniques for such platforms provide great ﬂexibility with the ability to consolidate sev- eral virtual machines on the same physical server, to resize a virtual machine capacity and to migrate virtual machine across physical servers. a key challenge for cloud providers is to automate the management of virtual servers while taking into account both high-level qos requirements of hosted applications and resource management costs. this paper proposes an autonomic resource manager to con- trol the virtualized environment which decouples the provi- sioning of resources from the dynamic placement of virtual machines. this manager aims to optimize a global utility function which integrates both the degree of sla fulﬁllment and the operating costs. we resort to a constraint pro- gramming approach to formulate and solve the optimization problem. results obtained through simulations validate our approach.
charged-particle multiplicity measurement in proton-proton collisions at sqrt(s) = 0.9 and 2.36 tev with alice at lhc. charged-particle production was studied in proton-proton collisions collected at the lhc with the alice detector at centre-of-mass energies 0.9 tev and 2.36 tev in the pseudorapidity range |eta| &lt; 1.4. in the central region (|eta| &lt; 0.5), at 0.9 tev, we measure charged-particle pseudorapidity density dnch/deta = 3.02 +- 0.01 (stat.) +0.08 -0.05 (syst.) for inelastic interactions, and dnch/deta = 3.58 +- 0.01 (stat.) +0.12 -0.12 (syst.) for non-single-diffractive interactions. at 2.36 tev, we find dnch/deta = 3.77 +- 0.01 (stat.) +0.25 -0.12 (syst.) for inelastic, and dnch/deta = 4.43 +- 0.01 (stat.) +0.17 -0.12 (syst.) for non-single-diffractive collisions. the relative increase in charged-particle multiplicity from the lower to higher energy is 24.7% +- 0.5% (stat.) +5.7% -2.8% (syst.) for inelastic and 23.7% +- 0.5% (stat.) +4.6% -1.1% (syst.) for non-single-diffractive interactions. this increase is consistent with that reported by the cms collaboration for non-single-diffractive events and larger than that found by a number of commonly used models. the multiplicity distribution was measured in different pseudorapidity intervals and studied in terms of kno variables at both energies. the results are compared to proton-antiproton data and to model predictions.
proceedings of the 5th int. symposium on software composition (sc'06). null
aop for systems software and middleware. null
a formal semantics of flexible and safe pointcut/advice bindings. null
towards correct evolution of components using vpa-based aspects. null
proc. of the 9th international symposium on distributed objects, middleware, and applications (doa'07). null
atoll: aspect-oriented toll system. null
on dissipativity of continuous-time singular systems. this paper addresses the dessipativity problem for linear time-invariant (lti) continuous-time singular systems. a new kalman-yakubovish-popov (kyp) lemma for dissipativity of singular systems is formulated in terms of a strict linear matrix inequality (lmi) condition. this lemma removes the equality constraints in the results recently reported in the literature. moreover, by means of the proposed lmi condition, feedback controller synthesis for rendering the closed-loop dissipative is also investigated.
extended lmi characterizations and multiobjective synthesis for discrete descriptor systems. this paper proposes new lmi characterizations for linear discrete-time descriptor systems. these formulations not only encompass the existing results for descriptor systems, but provide an alternative angle of view for reviewing the known extended lmis for conventional state-space systems as well. an application of these lmis with regard to the robust analysis and multiobjective synthesis is also investigated. numerical examples are given to illustrate the effectiveness of the proposed lmis.
necessary and sufficient condition for generalized estimation/control forms of continuous descriptor controllers. this paper deals with observer-based forms of arbitrary continuous-time descriptor controllers. the general form includes static control and estimation gains with or without an extra descriptor youla parameter that can be either static or dynamic. a necessary and sufficient condition for the derived controller to be input-output equivalent to the initial one, with a separated estimation-control structure, is given. the present method is consistent with the existing results in the conventional linear state-space case. some numerical examples illustrate the effectiveness of the proposed method.
ecoop 2006-object-oriented technology. workshop reader, workshops, nantes, france, july 3-7, 2006, final reports. null
acp4is'07: proceedings of the 6th workshop on aspects, components, and patterns for infrastructure software. null
strongaspectj: flexible and safe pointcut/advice bindings. null
proceedings of the int. workshop on aspects, dependencies and interactions (adi'07). null
proceedings of the 8th int. workshop on foundations of aspect-oriented languages (foal'09). null
elaboration of a common frame of reference in collaborative virtual environments. motivation – to design virtual environments that support collaborative activities. research approach – an experimental approach in which 44 students were asked to work in pairs to reconstruct five 3d figures. findings/design – the results show that including a contextual clue in virtual environments improves collaboration between operators. research limitations – further investigative work must be carried out to extract accurate female collaboration profiles. originality/value – the results enable three collaboration profiles to be identified. they also allow the extraction of some characteristics of a contextual clue which can be added to a virtual environment to improve collaboration. take away message – the contents of a collaborative virtual environment influences the way that users collaborate.
cross-document dependency analysis for system-of-system integration. null
aspect-oriented software development in practice: tales from aosd-europe. null
proceedings of the 4th workshop on domain-specific aspect languages (dsal 2009). it is our great pleasure to host the fourth edition of the domain-specific aspect languages workshop (dsal09), as part of the eight international conference on aspect-oriented software development (aosd09). the tendency to raise the abstraction level in programming languages towards a particular domain is also a major driving force in the research domain of aspect-oriented programming languages. the dsal workshop series aims to bring the research communities of domain-specific language engineering and domain-specific aspect design together. in the editions held at gpce06/oopsla06 and aosd07 we approached domain-specific aspect languages both from a design and a language implementation point of view. at aosd08 we explicitly invited contributions of work on adding domain-specific extensions (dsxs) to general-purpose aspect languages (gpals). we continue this trend for this edition as the focus on language embedding raises specific issues for language designers, such as proper symbiosis between, and composition of, dsxs.
proceedings of the 2008 aosd workshop on domain-specific aspect languages (dsal 2008). the workshop aims to bring the research communities of domain-specific language engineering and domain- specific aspect design together. in the previous successful editions held at gpce'06/oopsla'06 and aosd'07 we approached domain-specific aspect languages both from a design and a language implementation point of view. new for this edition is that we explicitly invited contributions of work on adding domain-specific extensions (dsxs) to general-purpose aspect languages (gpals).
proof of correctness of aspect transformations in the casb. null
towards a common aspect semantic base (casb), deliverable 54. null
lts-based semantics and property analysis of distributed aspects and invasive patterns. null
invasive patterns: aspect-based adaptation of distributed applications. null
relational aspects for context passing beyond stack inspection. null
technology-oriented optimization of the secondary design parameters of robots for high-speed machining applications. null
unifying runtime adaptation and design evolution. the increasing need for continuously available software systems has raised two key-issues: self-adaptation and design evolution. the former one requires software systems to monitor their execution platform and automatically adapt their configuration and/or architecture to adjust their quality of service (optimization, fault-handling). the later one requires new design decisions to be reflected on the fly on the running system to ensure the needed high availability (new requirements, corrective and preventive maintenance). however, design evolution and selfadaptation are not independent and reflecting a design evolution on a running self-adaptative system is not always safe. we propose to unify run-time adaptation and run-time evolution by monitoring both the run-time platform and the design models. thus, it becomes possible to correlate those heterogeneous events and to use pattern matching on events to elaborate a pertinent decision for run-time adaptation. a flood prediction system deployed along the ribble river (yorkshire, england) is used to illustrate how to unify design evolution and run-time adaptation and to safely perform runtime evolution on adaptive systems.
reliable dynamic reconfigurations in the fractal component model. this article is an analysis based on our experience with the fractal component model of the need of reliability for dynamic reconfigurations in component based systems. we make a proposal to ensure this reliability for concurrent and distributed fractal applications. we started from the definition of acid properties in the context of dynamic reconfigurations in component models and we propose to use integrity constraints to define system consistency and transactions for guaranteeing the respect of these constraints at runtime. moreover we manage concurrency between reconfigurations by avoiding potential conflicts between reconfiguration operations. finally, a recovery mechanism has been conceived to deal with failures.
fpath and fscript: language support for navigation and reliable reconfiguration of fractal architectures. component-based systems must support dynamic reconfigurations to adapt to their execution context, but not at the cost of reliability. fractal provides intrinsic support for dynamic reconfiguration, but its definition in terms of low-level apis makes it complex to write reconfigurations and to ensure their reliability. this article presents a language-based approach to solve these issues: direct and focused language support for architecture navigation and reconfiguration make it easier both to write the reconfigurations and to ensure their reliability. concretely, this article presents two languages: (1) fpath, a domain-specific language that provides a concise yet powerful notation to navigate inside and query fractal architectures, and (2) fscript, a scripting language that embeds fpath and supports the definition of complex reconfigurations. fscript ensures the reliability of these reconfigurations thanks to sophisticated run-time control, which provides transactional semantics (acid properties) to the reconfigurations.
qos policies for business processes in service oriented architectures. the advent of service oriented architectures tends to promote a new kind of software architecture where services, exposing features accessible through highly standardized protocols, are composed in a loose coupling way. in such a context, where services are likely to be replaced or used by a large number of clients, the notion of quality of service (qos), which focuses on the quality of the relationship between a service and its customers, becomes a key challenge. this paper aims to ease qos management in service compositions through a better separation of concerns. for this purpose, we designed qosl4bp, a domain-specific language which allows qos policies specification for business processes. more specifically, the qosl4bp language is designed to allow an architect to specify qos constraints and mechanisms over parts of bpel compositions. this language is executed by our orqos platform which cooperates in a non-intrusive way with orchestration engines. at pre-deployment time, orqos platform performs service planning depending on services qos offers and on the qos requirements in qosl4bp policies. at runtime, qosl4bp policies allow to react to qos variations and to enact qos management related mechanisms.
capacitated multi-item lot-sizing problems with time windows. this research concerns a new family of capacitated multi-item lot-sizing problems, namely, lot-sizing problems with time windows. two classes of the problem are analyzed and solved using different lagrangian heuristics. capacity constraints and a subset of time window constraints are relaxed resulting in particular single-item time window problems that are solved in polynomial time. other relaxations leading to the classical wagner-whitin problem are also tested. several smoothing heuristics are implemented and tested, and their results are compared. the gaps between lower and upper bounds for most problems are very small (less than 1%). moreover, the proposed algorithms are robust and do not seem to be too affected when different parameters of the problem are varied.
proceedings of the 2nd workshop on domain specific aspect languages. although the majority of work in the aosd community focuses on general-purpose aspect languages (e.g. aspectj), seminal work on aosd proposed a number of domainspecific aspect languages, such as cool for concurrency management and ridl for serialization, rg, aml, and others. a growing trend of research in the aosd community is returning to this seminal work, as witnessed by the high attendance rate at the dsal06 workshop, held as part of gpce06/oopsla06. the workshop aimed to bring the research communities of domain-specific language engineering and domainspecific aspect design together. in the previous successful edition we approached domain-specific aspect languages from a language implementation point of view, where advances in the field of domain-specific language engineering were investigated to answer the implementation challenges of aspect languages. in this second edition, we approached the design and implementation of new domain-specific aspect languages, as well as the composition at all levels (from design to implementation) of these languages or individual features.
proceedings of the first domain-specific aspect languages workshop - acm international conference on generative programming and component engineering (gpce 2006). although the majority of work in the aosd community focuses on general-purpose aspect languages (eg. aspectj), seminal work on aosd proposed a number of domain-specific aspect languages, such as cool for concurrency management and ridl for serialization. a growing trend of research in the aosd community is returning to this seminal work, motivated by the known advantages of domain-specific approaches, as argued by mitchell wand in his keynote at icfp 2003. this workshop is conceived for researchers who are further exploring the area of domain-specific aspect languages, including language design, enabling technologies and composition issues. this volume contains 6 papers and abstracts of invited talks. we hope the reader will find them useful for advancing their understanding of some issues in concerning the design of aspect languages.
a domain-specific language for coordinating concurrent aspects in java. aspect-oriented programming (aop) promises the modularisation of so-called cross-cutting functionality in large applications. currently, almost all approaches to aop provide means for the description of sequential aspects that are to be applied to a sequential base program. a recent approach, concurrent event-based aop (ceaop), has been introduced, which models the concurrent application of aspects to concurrent base programs. ceaop uses finite state processes (fsp) and their representation as labeled transition systems (lts) for modeling aspects, base programs and their concurrent composition, thus enabling the use of the labeled transition system analyzer (ltsa) for formal property verification. the initial work on ceaop does not provide an implementation of its concepts, restricting the study of concurrent aspects to the study of a model. the contribution of this paper is the provision of an implementation of ceaop as a small dsal (domain-specific aspect language), baton, which is very close to fsp, and can be compiled into java. as an intermediate layer, we have developed a java library which makes it possible to associate a java implementation to a finite state process. the compilation process consists of translating both the baton aspects and the java base program into java finite state processes. this translation relies on metaborg/sdf to extend java with baton and reflex to instrument the base program.
towards a model of concurrent aop. aspect-oriented programming (aop) is concerned with the modularization of crosscutting functionalities. such functionalities are problematic, in particular, for the development of concurrent applications, such as graphical user interfaces or multithreaded server applications. however, few approaches address this problem: there is, in particular, no general model for concurrent aop that enables coordination among concurrently-executing aspects as well as coordination of concurrent aspects and base applications. in this paper, we discuss general requirements for such models, briefly present a specific instance meeting these requirements, and propose a set of general composition operators for the construction of concurrent applications using concurrently executing advice.
a seamless extension of components with aspects using protocols. this paper shows how components and aspects can be seamlessly integrated using protocols. a simple component model equipped with protocols is extended with aspect compo- nents. the protocol of an aspect component observes the service requests and replies of plain components, and possibly internal component actions, and react to these actions (possibly preventing some base actions to happen as is standard with aop). a nice feature of the model is that an assembly of plain and aspect components can be transformed back into an assembly of components. all this is done without breaking the black-box nature of the components (dealing with internal actions requires to extend the component interface with an action interface).
declarative definition of contexts with polymorphic events. this paper introduces a new model of event handling combining explicitly triggered events with events intercepted with aspect-oriented features. the model supports event abstraction, polymorphic references to events, and declarative definition of events as expressions involving references to events from other objects. we show that this model makes it easy to define a declarative and compositional notion of event-based context. we illustrate these ideas with examples in ecaesarj, a language with concrete support for our model, and relate the events of ecaesarj to other event-handling and context-handling models.
concurrent aspects. aspect-oriented programming (aop) promises the modularization of so-called crosscutting functionalities in large applications. currently, almost all approaches to aop provide means for the description of sequential aspects that are to be applied to a sequential base program. in particular, there is no formally-defined concurrent approach to aop, with the result that coordination issues between aspects and base programs as well as between aspects cannot precisely be investigated. this paper presents concurrent event-based aop (ceaop), which addresses this issue. our contribution can be detailed as follows. first, we formally define a model for concurrent aspects which extends the sequential event-based aop approach. the definition is given as a translation into concurrent specifications using finite sequential processes (fsp), thus enabling use of the labelled transition system analyzer (ltsa) for formal property verification. further, we show how to compose concurrent aspects using a set of general composition operators and sketch a java prototype implementation for concurrent aspects we have realized.
invasive patterns for distributed programs. null
proceedings of the 9th int. conference on aspect-oriented software development. null
automatizing the evaluation of model matching systems. model-driven engineering (mde) and the semantic web are valuable paradigms. this paper sketches how to use a scientific and technical result around mde in the semantic web. the work proposes a mechanism to automatize the evaluation of model matching algorithms. the mechanism involves megamodeling and a domain specific language named aml (atlanmod matching language). aml allows to implement matching algorithms in a straightforward way. we present how to adapt the mechanism to the ontology context, for example, to the ontology alignment evaluation initiative (oaei).
a domain specific language for expressing model matching. a matching strategy computes mappings between two models by executing a set of heuristics. in this paper, we introduce the atlanmod matching language (aml), a domain specific language (dsl) for expressing matching strategies. aml is based on the model-driven paradigm, i.e., it implements model matching strategies as chains of model transformations. a matching model transformation takes a set of models as input, and yields a mapping model as output. we present a compiler that takes aml programs and generates atl (atlanmod transformation language) and apache ant code. the atl code instruments the matching model transformations, and the ant code orchestrates their execution. we evaluate this implementation on two strategies including robust matching transformations from the literature.
managing model adaptation by precise detection of metamodel changes. technological and business changes influence the evolution of software systems. when this happens, the software artifacts may need to be adapted to the changes. this need is rapidly increasing in systems built using the model-driven engineering (mde) paradigm. an mde system basically consists of metamodels, terminal models, and transformations. the evolution of a metamodel may render its related terminal models and transformations invalid. this paper proposes a three-step solution that automatically adapts terminal models to their evolving metamodels. the first step computes the equivalences and (simple and complex) changes between a given metamodel, and a former version of the same metamodel. the second step translates the equivalences and differences into an adaptation transformation. this transformation can then be executed in a third step to adapt to the new version any terminal model conforming to the former version. we validate our ideas by implementing a prototype based on the atlanmod model management architecture (amma) platform. we present the accuracy and performance that the prototype delivers on two concrete examples: a petri net metamodel from the research literature, and the netbeans java metamodel.
an improved upper bound for the railway infrastructure capacity problem on the pierrefitte-gonesse junction. null
multi-criteria optimization with the choquet integral in shortest path problems. the multi-criteria decision aid (mcda) research field focuses on building a preference model that helps the decision maker (dm) choosing a preferred solution among the set of efficient solutions. the multi-objective combinatorial optimization (moco) research field focuses on algorithms to find a set of efficient solutions. the straightforward approach to integrate preferences in a moco algorithm is first to compute a complete set of efficient solutions with a moco algorithm and then to use a mcda preference model. this approach allows to work on these two research fields independently but algorithms computing a complete set of efficient solutions have in general exponential execution times. the aim of our work is to use the preference model during the search of solutions to reduce the computation time.
common frame of reference in collaborative virtual environments and their impact on presence. virtual collaborative environment are 3d shared spaces in which people can work together. to collaborate through these systems, users must have a shared comprehension of the environment. the objective of this experimental study was to determine if visual stable landmarks improve the construction of a common representation of the virtual environment and thus facilitate collaboration. this seems to increase the awareness of the partner's presence.
the exact controllability property of neutral type systems by the moment problem approach revisited. in a recent paper authors gave an analysis of the exact controllability problem via the moment problem approach. namely, the steering conditions of controllable states are formulated as a vectorial moment problem using some riesz basis. one of the main difficulties was the choice of the basis as, in general, a basis of eigenvectors does not exist. in this contribution we use a change of control by a feedback law and modify the structure of the system in such a way that there exists a basis of eigenvectors which allows a simpler expression of the moment problem. hence, one obtains the result on exact controllability and on the time of exact controllability.
low momentum heavy-flavours measurements in the semi-muonic channel, and tracking efficiency of the alice's muon spectrometer. in accelerators as the one of lhc, collisions will help to reproduce the quark gluon plasma. with such a purpose, the alice detector is optimized to study the transition toward this hypothetic state of matter. the alice's muon spectrometer will measure the muon related probes (quarkonia, heavy flavours...). the first part of this thesis presents the method to calculate the tracking efficiency of this spectrometer. the results, obtained by simulation, are placed in the global efficiency context. they show the evolution of the efficiency as a function of likely electronics failures. they establish that a proper working of the detector involves less than 90% of channels failures and 85% of missing so-called manu cards. the second part presents the distance closest approach method which enhance the identification of muons from charm and beauty particle decays. for momentum as for pt &lt; 4 gev/c, the determination of heavy flavours contribution (charm in particular) in the single muon spectrum requires subtracting the decay part from pions and kaons. this discrimination is not possible track by track, an alternative method to subtract the light-hadrons has been developed. the dca method uses the distance between the extrapolated track in the primary vertex transverse plan and the vertex itself. the different shapes of the distributions in dca between the signal and the noise, arising from the different decay length path to the particle types, permit a better separation, and therefore a better estimation of the corresponding cross sections.
a theory of distributed aspects. over the last five years, several systems have been proposed to take distribution into account in aspect-oriented programming. while they appeared to be fruitful to develop or improve distributed component infrastructures or application servers, those systems are not underpinned with a formal semantics and so do not permit to establish properties on the code to be executed. this paper introduces the aspect join calculus -- an aspect-oriented and distributed language based on the join calculus, a member of the pi-calculus family of process calculi suitable as a programming language. it provides a first formal theory of distributed aop as well as a base language in which many features of previous distributed aop systems can be formalized. the semantics of the aspect join calculus is given by a (chemical) operational semantics and a type system is developed to ensure properties satisfied by aspects during the execution of a process. we also give a translation of the aspect join calculus into the core join calculus. the translation is proved to be correct by a bisimilarity argument. in this way, we provide a well-defined version of a weaving algorithm which constitutes the main step towards an implementation of the aspect join calculus directly in jocaml. we conclude this paper by showing that despite its minimal definition, the aspect join calculus is a convenient language in which existing distributed aop languages can be formalized. indeed, many features (such as remote pointcut, distributed advice, migration of aspects, asynchronous and synchronous aspects, re-routing of messages and distributed control flow) can be defined in this simple language.
radioelectric fields from cosmic-ray air showers at large impact parameters. we discuss electric fields generated by cosmic-ray air showers at large impact parameters b. an approximation relevant to this situation is given. the formulation makes explicit the relationship between the shower profile and the radio pulse shapes at large b, putting forward one important observational consequence, namely the decrease of the high-frequency cutoff νc∝1/b when the impact parameter increases. the approximation is also used to give a detailed comparison between two emission models, the geosynchrotron model and the transverse current model.
study of a treatment of siloxane by adsorption process into porous materials: treatment application to biogas. study of a treatment of siloxane by adsorption process into porous materials: treatment application to biogas biogases have strong content of methane used in the production of heat or electricity. they contain more or less important quantities of siloxanes, which are forbidden for numerous uses of biogases. the possibility of siloxanes elimination by adsorption process is studied. the study in batch reactors allows us to evaluate the adsorption capacities of different materials as activated carbons cloths and grains, zeolites and silica gel. the influence on the treatment capacities of siloxanes under the presence of ch4, co2, humidity, and other volatile organic compounds is studied. good adsorption capacities for some adsorbents were found. for the most disadvantageous conditions of adsorption, a reduction of 20 % on the adsorption capacities has been found. the adsorption capacity remains modest. the influence of the operational conditions of the process is studied in order to improve the treatment capacities and to reduce the quantity of the adsorbent used. the results allowed us to define, validate and design a reduced scale system in the treatment of siloxanes. the process consists of an adsorption into activated carbon cloth, alternating with thermal regeneration by joule effect. a pilot plant allowed us to accomplish and evaluate the aging of the process. all this work allows us to see the possibilities of industrial applications of the process, even if a trial stage on site in real conditions is still necessary.
generalized h2-preview control and its application to car lateral steering. . this paper is dedicated to studying the characteristics of the optimal preview control for lateral steering of a passenger vehicle. this synthesis of control law has proved through many applications its ability to guarantee improved performance. the success of this advanced control strategy lies mainly in its ability to include knowledge of a path to follow in the future on a finite horizon, and its ability to moderate the effect of potential delays in the control loop through the advanced information. h2-preview; lateral steering; optimal control.
flexible pointcut implementation: an interpreted approach. one of the main elements of an aspect-oriented programming (aop) language or framework is its pointcut language. a pointcut is a predicate which selects program execution points and determines at which points the execution should be affected by an aspect. experimenting with aspectj shows that two basic primitive pointcuts, call and execution, dealing with method invoca- tion from the caller and callee standpoints, respectively, lead to confusion. this is due to a subtle interplay between the use of static and dynamic types to select execution points, dynamic lookup, and the expectation to easily select the caller and callee execution points related to the same invocation. as a result, alternative semantics have been proposed but have remained paper design. in this article, we reconsider these various semantics in a practical way by implementing them using cali, our common aspect language interpreter. this framework reuses both java as a base language and aspectj as a way to select the program execution points of interest. an additional interpretation layer can then be used to prototype interesting aop variants in a full-blown environment. the paper illustrates the benefits of applying such a setting to the case of the call and execution pointcuts. we show that alternative semantics can be implemented very easily and exercised in the context of aspectj without resorting to complex compiler technology.
automatically discovering hidden transformation chaining constraints. model transformations operate on models conforming to precisely defined metamodels. consequently, it often seems relatively easy to chain them: the output of a transformation may be given as input to a second one if metamodels match. however, this simple rule has some obvious limitations. for instance, a transformation may only use a subset of a metamodel. therefore, chaining transformations appropriately requires more information. we present here an approach that automatically discovers more detailed information about actual chaining constraints by statically analyzing transformations. the objective is to provide developers who decide to chain transformations with more data on which to base their choices. this approach has been successfully applied to the case of a library of endogenous transformations. they all have the same source and target metamodel but have some hidden chaining constraints. in such a case, the simple metamodel matching rule given above does not provide any useful information.
how synchronization protects from noise. the functional role of synchronization has attracted much interest and debate: in particular, synchronization may allow distant sites in the brain to communicate and cooperate with each other, and therefore may play a role in temporal binding, in attention or in sensory-motor integration mechanisms. in this article, we study another role for synchronization: the so-called "collective enhancement of precision". we argue, in a full nonlinear dynamical context, that synchronization may help protect interconnected neurons from the influence of random perturbations-intrinsic neuronal noise-which affect all neurons in the nervous system. more precisely, our main contribution is a mathematical proof that, under specific, quantified conditions, the impact of noise on individual interconnected systems and on their spatial mean can essentially be cancelled through synchronization. this property then allows reliable computations to be carried out even in the presence of significant noise (as experimentally found e.g., in retinal ganglion cells in primates). this in turn is key to obtaining meaningful downstream signals, whether in terms of precisely-timed interaction (temporal coding), population coding, or frequency coding. similar concepts may be applicable to questions of noise and variability in systems biology.
correcting a buffer overflow vunerability at runtime with arachne. null
expression and composition of design patterns with aspectj. design patterns are well-known couples of problems-solutions for software engineer- ing. by nature, they often lack support from languages and this further complicates the study of their composition in the code. aspect-oriented languages provide new mechanisms for modula- rization, which can help to improve design patterns implementation. (hannemann et al., 2002) is the first extensive study of patterns aspectization with aspectj. we notice some aspectj idioms are needed in order to implement object relationships. we give a more reusable visitor pat- tern. we highlight a reusable composition of composite and visitor patterns and expressive interactions of the observer pattern with a tree structure. we thus show that modularization by aspects helps composition of design patterns.
understanding design patterns density with aspects: a case study in jhotdraw using aspectj. design patterns offer solutions to common engineering prob- lems in programs [1]. in particular, they shape the evolution of program elements. however, their implementations tend to vanish in the code: thus it is hard to spot them and to understand their impact. the prob- lem becomes even more difficult with a "high density of pattern": then the program becomes easy to evolve in the direction allowed by patterns but hard to change [2]. aspect languages offer new means to modular- ize elements. implementations of object-oriented design patterns with aspectj have been proposed [3]. we aim at testing the scalability of such solutions in the jhotdraw framework. we first explore the impact of density on pattern implementation. we show how aspectj helps to reduce this impact. this unveils the principles of aspects and aspectj to control pattern density.
a human-centred approach of steering control modelling. future driving assistances will be designed in such a way that their action blends into the perceptual and motor control loops of the driver without yielding negative interference. this objective cannot be reached without integrating into the design process a model that allows making valid predictions on the behaviour of the driver. in that context, this paper will deal with the understanding and modelling of the driver's steering behaviour, with an emphasis on the role of visual information and kinaesthetic feedback. this paper describes strategies of steering and speed control used by the driver and the combined cornering and braking condition is analyzed. for the driver speed control, the information of driver visual direction is used to determinate anticipatory speed control during curve negotiation.
radiolytic yield of uiv oxidation into uvi: a new mechanism for uv reactivity in acidic solution. the yields of the radiolytic oxidation of uiv and of the uvi formation, measured by spectrophotometry, are found to be the same (g(−uiv)n2o = g(uvi)n2o = 8.4 × 10−7 mol j−1) and almost double the h2 formation yield (g(h2) = 4.4 × 10−7 mol j−1) in the 60co γ radiolysis of n2o-aqueous solutions in the presence of 2 mol l−1 cl− at ph = 0 (hcl). according to the mechanism of uiv radiolytic oxidation, we show that under the conditions of our experiments the uv ions do not disproportionate, but undergo a stoichiometric oxidation into uvi by h+ with forming h2.
javacompext: extracting architectural elements from java source code. software architecture erosion is a general problem in legacy software. to fight this trend, component models and languages are designed to try to make explicit, and automatically enforceable, the architectural decisions in terms of components, interfaces, and allowed communication channels between component interfaces. to help maintainers work on existing object-oriented systems, we explore the possibility of extracting architectural elements (components, communications, services, ...) from the source code. we designed a tool based on some heuristics for extracting component information from java source code.
partial behavioral reflection: spatial and temporal selection of reification. behavioral reflection is a powerful approach for adapting the behavior of running applications. in this paper we present and motivate partial behavioral reflection, an approach to more efficient and flexible behavioral reflection. we expose the spatial and temporal dimensions of such reflection, and propose a model of partial behavioral reflection based on the notion of hooksets. in the context of java, we describe a reflective architecture offering appropriate interfaces for static and dynamic configuration of partial behavioral reflection at various levels, as well as reflex, an open reflective extension for java implementing this architecture. reflex is the first extension that fully supports partial behavioral reflection in a portable manner, and that seamlessly integrates load-time and runtime behavioral reflection. the paper shows preliminary benchmarks and examples supporting the approach. the examples, dealing with the observer pattern and asynchronous communication via transparent futures, also show the interest of partial behavioral reflection as a tool for open dynamic aspect-oriented programming.
web cache prefetching as an aspect{:} towards a dynamic-weaving based solution. given the high proportion of http traffic in the internet, web caches are crucial to reduce user access time, network latency, and bandwidth consumption. prefetching in a web cache can further enhance these benefits. nevertheless, to achieve the best performance, the prefetching policy must match user and web server characteristics. this implies that new prefetching policies must be loaded dynamically as needs change. most web caches are large c programs, and thus adding a single prefetching policy to an existing web cache is a daunting task. providing multiple policies is even more complex. the essential problem is that prefetching concerns crosscut the cache structure. aspect-oriented programming is a natural technique to address this issue. nevertheless, existing approaches do not provide dynamic weaving of aspects targeted toward c applications. in this paper, we present $\mu$dyner, which addresses these needs. $\mu$dyner also provides lower overhead for aspect invocation than other dynamic approaches, thus meeting the performance needs of web caches.
a model of components with non-regular protocols. behavioral specifications which are integrated into component interfaces are an important means for the correct construction of component-based systems. currently, such specifications are typically limited to finite-state protocols because more expressive notions of protocol do not support reasonable basic composition properties, such as compatibility and substitutability. in this paper, we present first results of the integration into component interfaces of a notion of non-regular protocols based on {``}non-regular process types{''} introduced by puntigam. more concretely, we present three contributions: (i) a motivation of the usefulness of non-regular protocols in the context of peer-to-peer applications, (ii) a language for non-regular protocols and an outline of a suitable formal definition, (iii) a discussion of basic composition properties and an analysis of how to adequately integrate protocol-modifying operators in the model.
analysing mailboxes of asynchronous communicating components. asynchronous communications are prominent in distributed and mobile systems. often concurrent systems consider an abstract point of view with synchronous communications. however it seems more realistic and finer to consider asynchronous communicating systems, since it provides a more primitive communication protocol and maximize the concurrency. several languages and models have been defined using this communication mode: agent, actor, mobile computation, and so on. here we reconsider a previous component model with full data types and synchronous communications with an asynchronous flavour. the dynamic behaviour of a component is represented as a structured symbolic transition system with mailboxes. we also present an algorithm devoted to an analysis of the dynamic behaviour of the system. this algorithm decides if the system has bound mailboxes and computes the reachable mailbox contents of the system. the component model and the algorithm are illustrated on a flight system reservation.
a framework for the gat temporal logic. null
on the automatic evolution of an os kernel using temporal logic and aop. automating software evolution requires both identifying precisely the affected program points and selecting the appropriate modification at each point. this task is particularly complicated when considering large pieces of software, even when the modifications appear to be systematic. we illustrate this situation in the context of evolving the linux kernel to support bossa, an event-based framework for process-scheduler development. to support bossa, events must be added at points scattered throughout the kernel. in each case, the choice of event depends on properties of one or a sequence of instructions. to describe precisely the choice of event, we propose to guide the event insertion by using a set of rules, amounting to an aspect, that describes the control-flow contexts in which each event should be generated. in this paper, we present our approach and describe the set of rules that allow proper event insertion. these rules use temporal logic to describe sequences of instructions that require events to be inserted. we also give an overview of an implementation that we have developed to automatically perform this evolution.
a translation of uml components into formal specifications. null
dynamic configuration of software product lines in archjava. this paper considers the use of a state-of-the-art, general-purpose, component-programming language, specifically archjava, to implement software product lines. component-programming languages provide a more straightforward mapping between components as assets and components as implementation artifacts. however, guaranteeing that the implementation conforms to the architecture raises new issues with respect to dynamic configuration. we show how this can be solved in archjava by making the components auto-configurable, which corresponds to replacing components by component generators. such a scheme can be implemented in various ways, in particular with a two-stage generator. this solution goes beyond the initial technical archjava issue and complements the standard static generative approach to software product line implementation.
the bossa framework for scheduler development. null
checking asynchronously communicating components using symbolic transition systems. null
tarantula{:} killing driver bugs before they hatch. null
on the design of a domain-specific language for os process-scheduling extensions. null
bossa nova{:} introducing modularity into the bossa domain-specific language. null
recovering binary class relationships: putting icing on the uml cake. null
a pragmatic study of binary class relationships. null
automating adaptive image generation for medical devices using aspect-oriented programming. null
applying the b formal method to the bossa domain-specific language. null
formal methods meet domain specific languages. null
an aspect-oriented approach for developing self-adaptive fractal components. nowadays, application developers have to deal with increasingly variable execution contexts, requiring the creation of applications able to adapt themselves autonomously to the evolutions of this context. in this paper, we show how an aspect-oriented approach enables the development of self-adaptive applications where the adaptation code is well modularized, both spatially and temporally. concretely, we propose safran, an extension of the fractal component model for the development of the adaptation aspect as reactive adaptation policies. these policies detect the evolutions of the execution context and adapt the base program by reconfiguring it. this way, safran allows the development of the adaptation aspect in a modular way and its dynamic weaving into applications.
a framework for simplifying the development of kernel schedulers: design and performance evaluation. null
language design for implementing process scheduling hierarchies. null
supporting dynamic crosscutting with partial behavioral reflection{:} a case study. null
a crosscut language for control-flow. null
wildcat: a generic framework for context-aware applications. we present wildcat, an extensible java framework to ease the creation of context-aware applications. wildcat provides a simple yet powerful dynamic model to represent an application's execution context. the context information can be accessed by application programmers through two complimentary interfaces: synchronous requests (pull mode) and asynchronous notifications (push mode). internally, wildcat is designed to support different kinds of extensions, from the simple configuration of the default generic implementation to completely new implementations tailored to specific needs. a given application can mix different implementations for different aspects of its context while only depending on wildcat simple and unified api.
towards a framework for self-adaptive component-based applications. given today's fast pace of technological evolutions and diversity of computing platforms, building applications which can work in such a wide range of systems is becoming more and more challenging. to deal with this situation, applications must be \emph{self-adaptive}, that is adapt themselves to their environment and its evolutions. the goal of our works is to enable systematic development of self-adaptive component-based applications using the separation of concerns principle: we consider \emph{adaptation to a specific execution context and its evolutions} as a concern which should be treated separately from the rest of an application. in this paper, we first present the general approach we propose and the corresponding development framework and tools we are developing to support it. then, in order to validate this approach, we show how a small component-based application can be made self-adaptive using our approach.
from (meta) objects to aspects : from java to aspectj. null
towards generative programming. null
supporting aop using reflection. null
development of strategies for htr fuel waste management. in an effort to reduce the volume of nuclear wastes and to allow the reuse of remaining fissile materials, a strategy for management of high temperature reactors (htr) fuel was developed in this study. the volume reduction passes through the separation of highly radioactive triso particles with fuel kernels from the slightly radioactive graphite matrix (both combined in a fuel assembly called "compact") while the total recycling option requires the separation of the valuable kernel from the particle coating as ultimate waste. the separation methods must preserve the integrity of triso to prevent the release of radionuclides. a thermal shock treatment between liquid nitrogen and hot water allows for a partial destruction of the compact but only few particles are separated. alternatively, graphite erosion by a high pressure water jet presents the risk of fracturing the particles. better is the total combustion of carbon which releases all the particles. the treatment of compacts by ultrasounds in water erodes the graphite as function of the intensity, distance and direction of attack, temperature and gas saturation, and provides clean particles. the acid attack of compacts by a mixture h2o2 + h2so4 causes the intercalation of graphite by acid, which inflates the structure and releases the intact particles. the triso on one hand and coatings on the other hand were then vitrified by sintering to achieve a high density, up to a rate of 25% vol. finally, the leaching of composites in ultrapure water at 90°c shows strong confinement properties.
measurement of the energy spectrum of cosmic rays above 10^18 ev using the pierre auger observatory. we report a measurement of the flux of cosmic rays with unprecedented precision and statistics using the pierre auger observatory. based on fluorescence observations in coincidence with at least one surface detector we derive a spectrum for energies above 10^18 ev. we also update the previously published energy spectrum obtained with the surface detector array. the two spectra are combined addressing the systematic uncertainties and, in particular, the influence of the energy resolution on the spectral shape. the spectrum can be described by a broken power law e^-gamma with index gamma=3.3 below the ankle which is measured at log10(e/ev) = 18.6. above the ankle the spectrum is described by a power law with index 2.6 followed by a flux suppression, above about log10(e/ev) = 19.5, detected with high statistical significance.
combined effects of the filling ratio and the vapour space thickness on the performance of a flat plate heat pipe. null
[bevacizumab therapy for poems syndrome]. null
measurement of the depth of maximum of extensive air showers above 10^18 ev. we describe the measurement of the depth of maximum, xmax, of the longitudinal development of air showers induced by cosmic rays. almost four thousand events above 10^18 ev observed by the fluorescence detector of the pierre auger observatory in coincidence with at least one surface detector station are selected for the analysis. the average shower maximum was found to evolve with energy at a rate of (106 +35/-21) g/cm^2/decade below 10^(18.24 +/- 0.05) ev and (24 +/- 3) g/cm^2/decade above this energy. the measured shower-to-shower fluctuations decrease from about 55 to 26 g/cm^2. the interpretation of these results in terms of the cosmic ray mass composition is briefly discussed.
l-asparaginase-based treatment of 15 western patients with extranodal nk/t-cell lymphoma and leukemia and a review of the literature. background: extranodal natural killer (nk)/t-cell lymphoma, nasal type, and aggressive nk-cell leukemia are highly aggressive diseases with a poor outcome. patients and methods: we report a multicentric french retrospective study of 15 patients with relapsed, refractory, or disseminated disease, treated with l-asparaginase-containing regimens in seven french centers. thirteen patients were in relapse and/or refractory and 10 patients were at stage iv. results: all but two of the patients had an objective response to l-asparaginase-based treatment. seven patients reached complete remission and only two relapsed. conclusion: these data, although retrospective, confirm the excellent activity of l-asparaginase-containing regimens in refractory extranodal nk/t-cell lymphoma and aggressive nk-cell leukemia. therefore, l-asparaginase-based regimen should be considered as a salvage treatment, especially for patients with disseminated disease. first-line l-asparaginase combination therapy for extranodal nk/t-cell lymphoma and aggressive nk-cell leukemia should be tested in prospective trials.
investigation of deep inelastic reactions in 238u + 238u at coulomb barrier energies. we investigated deep inelastic reactions in 238u+238u collisions at 6.09, 6.49, 6.91, 7.1 and 7.35×amev at the vamos spectrometer (ganil). a large transfer of neutrons and protons was observed at all beam energies. for a transfer of more than 10 nucleons the total kinetic energy of the detected fragments becomes independent of the beam energy and reaches values far below the coulomb barrier for spherical fragments. this points to the formation of a di-nuclear system in the entrance channel which develops an elongated shape and a strong neck. for such reactions we expect an enhanced lifetime of the di-nuclear system which is significantly longer than the time scale for elastic and quasi-elastic reactions. different theoretical approaches predict delay times of more than 5 × 10−21 s for a subset of our data.
aqueous corrosion of the gese4 chalcogenide glass: surface properties and corrosion mechanism. the aqueous corrosion behavior of the gese4 glass composition has been studied over time under various conditions (temperature and ph). the evolution of the surface topography by atomic force microscopy and properties such as surface hardness and reduced modulus, as well as the optical transmission in the 1-16 μm window, have been measured as a function of time spent in the corrosive solution. it was found that even if the glass reacts at room temperature, its optical transparency was barely affected. nevertheless, the durability of gese4 was found to be drastically affected by an increase of both temperature and ph. furthermore, pure selenium nanoparticles were formed during the corrosion process, and the nature of these nanoparticles--amorphous or crystallized (hexagonal phase)--depends on temperature. a reaction mechanism was proposed, and the activation energy of the reaction of corrosion in deionized water (47 kj/mol) was determined from an original technique that relies on the temporal optical loss variation of a gese4 optical fiber placed in water at different temperatures.
optimal technology-oriented design of parallel robots for high-speed machining applications. null
characterization of new injectable biomaterials containing bisphosphonates. null
labelling of peptides and antibodies with bi-213 for targeted alpha-radionuclide therapy. null
how relativistic effects control elemental astatine chemistry. null
determination of complexation constants between inorganic ligands and at(i) and at(iii) species present at ultra-trace concentrations. null
complexation of actinide cations by carboxylated calix[4]arenes. null
complexation of actinide and alkali metal cations by carboxylated calix[4]arenes. null
extraction of u(vi) and np(v) by a calix[4]arene bearing carboxylic groups. null
novel phosphate – phosphonate hybrid materials, applied to biology. null
sorption of se(iv) on compacted clay materials. null
migration of uranium in the presence of natural organic matter colloids in a sandy aquifer. null
complexation of eu(iii) onto polymaleic acid either free in solution or sorbed onto alumina. null
are the complexation properties of soluble humic substances and mineral -bound humic substances identical? an attempt of understanding using the model system alumina/polyacrylic acid/m(iii) (m=eu, cm). null
exploration of the metallic character of astatine in aqueous solution. alpha-radiotherapy is an innovative technique for the treatment of cancers complementary to current approaches. the principle is to use tumor-specific vectors labeled with alpha-radioisotopes. astatine 211 is a very promising candidate if one considers the energy of the α particles emitted as well as its physical half-life (7.2 h). one of the ways of labeling is to use astatine at a higher oxidation state as a “metal cation”. indeed, considering its location in the periodic table. astatine is supposed to present a more metallic character than the other halogens. this way of labeling has however never been explored. this can be explained by the fact that the astatine chemistry is nearly unknown. the purpose of this work was therefore to explore this property of astatine and to define its eh, ph diagram (or pourbaix diagram) in non complexing aqueous solution. to this end, both experimental and theoretical approaches were used. the experimental approach uses competition methods to identify the formed species and the thermodynamics constants of studied equilibria. the theoretical approach uses methods of computational chemistry and provides information at the molecular scale on the studied systems in order to predict thermodynamic data which are used as support and complement to the experimental approach. the important result of this work shows the presence of two stable cationic forms of astatine in aqueous solution, i.e., at+ and ato+.
spectroscopic study of the interaction of u(vi) with transferrin and albumin for speciation of u(vi) under blood serum conditions. the quantitative description of the interactions of uranium with blood serum components is of high relevance for a rational design of molecules suitable for in vivo chelation of uranium. we have determined the stability constants for the complexation of u(vi) with human serum transferrin and albumin by time-resolved laser-induced fluorescence spectroscopy and difference ultraviolet spectroscopy. both proteins interact strongly with u(vi), forming ternary complexes with carbonate acting as a synergistic anion. together with literature data describing the interaction of u(vi) with low molecular weight inorganic and organic serum components, the speciation of u(vi) in blood serum was calculated. in agreement with published experimental data, the model calculation shows that complexation with proteins and carbonate ion governs u(vi) speciation: 35% of u(vi) is bound to proteins and 65% to carbonate. among the protein pool, albumin is the main protein interacting with u(vi). in addition, the results show that ca(ii) must be considered in the model as a competitive metal ion with respect to u(vi) for binding to albumin surface sites. based on these findings several promising molecules for in vivo chelation of u-230 could be identified. (c) 2009 elsevier inc. all rights reserved.
porosities accessible to hto and iodide on water-saturated compacted clay materials and relation with the forms of water: a low field proton nmr study. the aim of the present work was to quantify accessible porosities for iodide and for a water tracer (hto) on water-saturated compacted clay samples (illite, montmorillonite and mx-80 bentonite) and to relate these macroscopic values to the forms of water in these porosities (surface/bulk water, external/internal water). low field proton nmr was used to characterize and quantify the forms of water. this enabled the three different populations (structural oh, external surface and internal surface water) to be differentiated on hydrated clays by considering the difference in proton mobility. an accurate description of the water forms within the different populations did not appear possible when water molecules of these populations were in contact because of the occurrence of rapid exchange reactions. for this reason, it was not possible to use the low resolution nmr method to quantify external surface and bulk water in fully water-saturated compacted clay media at room temperature. this latter information could however be estimated when analyzing the samples at -25°c. at this temperature, a distinction based on the difference in mobility could be made since surface water remained in a semi-liquid state whereas bulk water froze. in parallel, accessible porosities for anions and hto were determined by an isotopic dilution method using capillaries to confine the materials. hto was shown to probe the whole pore volume (i.e. the space made of surface and bulk water). when the surface water volume was mainly composed of interlayer water (case of montmorillonite and bentonite), iodide was shown to be located in the pore space made of bulk water. when the interlayer water was not present (case of illite), the results showed that iodide could access a small fraction of the surface water volume localized at the external surface of the clay particles.
investigation of para-sulfonatocalix[n]arenes [n = 6, 8] as potential chelates for 230u. literature reports of the efficacy of para-sulfonatocalix[6]- and calix[8]-arenes as u(vi) complexants indicated that they might be useful for in vivo chelation of the novel therapeutic alhpa-emitter 230u. we have studied the complexation of u(vi) with para-sulfonatocalix[6]arene and para-sulfonatocalix[8]arene by time resolved laser induced fluorescence spectroscopy and using competition methods with chelex resin and 4-(2-pyridylazo)resorcinol in simplified and in biological media. new thermodynamic parameters describing the stability of u(vi)-para-sulfonatocalix[n]arene [n = 6, 8] complexes were obtained. although the interactions are strong, the complexes do not exhibit sufficient stability to compete with carbonate ions and serum proteins for complexation of u(vi) under physiological conditions.
astatine standard redox potentials and speciation in acidic medium. a combined experimental and theoretical approach is used to define astatine (at) speciation in acidic aqueous solution and answer the two main questions raised from literature data: does at(0) exist in aqueous solution and what is the chemical form of at(iii), if it exists. the experimental approach considers that a given species is characterized by its distribution coefficient (d) experimentally determined in a biphasic system. the change in speciation arising from a change in experimental conditions is observed by a change in d value. the theoretical approach involves quasi-relativistic quantum chemistry calculations. the results show that at at the oxidation state 0 cannot exist in aqueous solution. the three oxidation states present in the range of water stability are at(-i), at(i) and at(iii) and exist as at-, at+ and ato+, respectively, in the 1 to 2 ph range. the standard redox potentials of the at+/at- and ato+/at+ couples have been determined, the respective values being 0.36 ± 0.01 and 0.74 ± 0.01 v vs. nhe.
multidisciplinary and multiobjective optimization: comparison of several methods. engineering design of complex systems is a decision making process that aims at choosing from among a set of options that implies an irrevocable allocation of resources. it is inherently a multidisciplinary and multi-objective process. the paper describes some classical multidisciplinary optimization (mdo) methods with their advantages and drawbacks. some new approaches combining genetic algorithms (moga) and collaborative optimization (co) are presented. they allow to: 1) increase the convergence rate when a design problem can be broken up regarding design variables, and 2) provide an optimal set of design variables in case of multi-level multi-objective design problem.
how to build web self-services by functional profiles?. companies, institutions, and local authorities seek to computerize services to be more profitable in terms of productivity, profitability, quality of service, traceability... the main issues are the identification of problems to solve and the choice of adapted solutions. to build these solutions, functional and technical profiles must communicate to understand precisely the domain, the problem, and the expected features... this paper provides a framework composed of three contributions, which aim to simplify the building of web self-services by functional profiles. the first part deals with a language to specify functional needs using the main concept of goal. this language must be as simple as possible but not limiting. the second part deals with the tool to use this language and, finally, the last section is a set of interpretations to simplify communication between all involved people. to automate a large part of recurrent tasks, all the proposed approach complies with model driven methodology.
combined air treatment: effect of composition of fibrous filters on toluene adsorption and particle filtration efficiency. null
detection and characterization of the radio emission from atmospheric showers induced by cosmic rays with energy above 10^16 ev by the codalema experiment. ultra high energy cosmic rays, extraterrestrial particles which nature and origin remain today uncertain, are ordinarily studied by using two major techniques of eas (extensive air shower) detection: ground particles detectors or fluorescence light telescopes. appeared for the first time in the 60', researches on eas radiodetection by measuring the electric field induced by shower's charged particles was first stopped because of technical difficulties. with the developpement of fast electronic, radiodetection technique became again potentially interesting for cosmic rays study. the codalema experiment, since 2002, use and improve the radiodetection method. these last years, the experimental setup was largely modified, original log-periodic antennas were replaced by active dipole dedicated to radiodetection and the trigger, realized by an array of 17 scintillators, allow now to estimate the primary cosmic ray energy. present objective of codalema is to characterize the electric signal induced by an eas, according to the physical parameters of the shower. in this thesis the main results obtain by codalema are presented. the evidences for a geomagnetic origin of eas radioelectric field is one of the more important. moreover a first study of electric field lateral distribution functions and the correlation between the primary particle energies with the amplitude of the eas electric field are also discussed.
views for aspectualizing component models. component based software development (cbsd) and aspect- oriented software development (aosd) are two complemen- tary approaches. however, existing proposals for integrating aspects into component models are direct transposition of object-oriented aosd techniques to components. in this article, we propose a new approach based on views. our proposal introduces crosscutting components quite naturally and can be integrated into different component models.
challenging explanations for global constraints. null
on-line resources allocation for atm networks with rerouting. null
guiding architectural design process of hard real-time systems with constraint programming. null
visualizing explanations to exhibit dynamic structure in constraint problems. null
peeking in solver strategies using explanations -- visualization of dynamic graphs for constraint programming. null
implementing explained global constraints. null
decomposition and learning for a real time task allocation problem. we present a cooperation technique using an accurate management of nogoods to solve a hard real-time problem which consists in assigning periodic tasks to processors in the context of fixed priorities preemptive scheduling. the problem is to be solved off-line and our solving strategy is related to the logic based benders decomposition. a master problem is solved using constraint programming whereas subproblems are solved with schedulability analysis techniques coupled with an ad hoc nogood computation algorithm. constraints and nogoods are learnt during the process and play a role close to benders cuts.
optimal control theory : a method for the design of wind instruments. it has been asserted previously by the author that optimal control theory can be a valuable framework for theoretical studies about the shape that a wind instrument should have in order to satisfy some optimization criterion, inside a fairly general class. the purpose of the present work is to develop this new approach with a look at a specific criterion to be optimized. in this setting, the webster horn equation is regarded as a controlled dynamical equation in the space variable. pressure is the state, the control being made of two parts~: one variable part, the inside diameter of the duct and one constant part, the weights of the elementary time-harmonic components of the velocity potential. then one looks for a control that optimizes a criterion related to the definition of an {oscillation regime} as the cooperation of several natural modes of vibration with the excitation, the {playing frequency} being the one that maximizes the total generation of energy, as exposed by a.h. benade, following h. bouasse. at the same time the relevance of this criterion is questionned with the simulation results.
a study of the effect of molecular and aerosol conditions in the atmosphere on air fluorescence measurements at the pierre auger observatory. the air fluorescence detector of the pierreaugerobservatory is designed to perform calorimetric measurements of extensive air showers created by cosmic rays of above 10^18 ev. to correct these measurements for the effects introduced by atmospheric fluctuations, the observatory contains a group of monitoring instruments to record atmospheric conditions across the detector site, an area exceeding 3,000 km2. the atmospheric data are used extensively in the reconstruction of air showers, and are particularly important for the correct determination of shower energies and the depths of shower maxima. this paper contains a summary of the molecular and aerosol conditions measured at the pierreaugerobservatory since the start of regular operations in 2004, and includes a discussion of the impact of these measurements on air shower reconstructions. between 10^18 and 10^20 ev, the systematic uncertainties due to all atmospheric effects increase from 4% to 8% in measurements of shower energy, and 4 g cm^-2 to 8 g cm^-2 in measurements of the shower maximum.
positive invariance of polyhedrons and comparison of markov reward models with different state spaces. in this paper, we discuss the comparison of expected rewards for discrete-time reward markov chains with different state spaces. necessary and sufficient conditions for such a comparison are derived. due to the special nature of the introduced binary relation, a criterion may be formulated in terms of an inclusion of polyhedral sets. then, algebraic and geometric forms are easily obtained from haar's lemma. our results allow us to discuss some earlier results on the stochastic comparison of functions of markov chains.
nucleate boiling in flat grooved heat pipes. null
k-relevant explanations for constraint programming. this paper presents diagnosis tools and interaction-based tools which could help the constraint programming user to interactively develop its applications. the implementation of these tools rely on explanations, and more precisely on k-relevant explanations. an example is given to illustrate k-relevant explanations and to provide concrete situations illustrating the functionalities of our interactive and diagnosis tools.
integrating benders decomposition within constraint programming. null
how to solve allocation problems with constraint programming. null
the tree constraint. null
specialization tools and techniques for systematic optimization of system software. null
a generic reification technique for object-oriented reflective languages. computational reflection is gaining interest in practical applications as witnessed by the use of reflection in the java programming environment and recent work on reflective middleware. reflective systems offer many different reflection programming interfaces, the so-called meta-object protocols (mops). their design is subject to a number of constraints relating to, among others, expressive power, efficiency and security properties. since these constraints are different from one application to another, we should be able to easily provide specially-tailored mops.
the korrigan environment. null
a model curriculum for aspect-oriented software development. as new software engineering techniques emerge, there's a cognitive shift in how developers approach a problem's analysis and how they design and implement its software-based solution. future software engineers must be appropriately and effectively trained in new techniques' fundamentals and applications. with techniques becoming more mature, such training moves beyond specialized industrial courses into postgraduate curricula (as advanced topics) and subsequently into undergraduate curricula. a model curriculum for aspect-oriented software development provides guidelines about fundamentals, a common framework, and a step toward developing a body of knowledge.
strategic research directions in object oriented programming. null
an operational approach to the semantics of classes: application to type checking. null
the gat approach to specify mixed systems. null
alignment of the alice inner tracking system with cosmic-ray tracks. alice (a large ion collider experiment) is the lhc (large hadron collider) experiment devoted to investigating the strongly interacting matter created in nucleus-nucleus collisions at the lhc energies. the alice its, inner tracking system, consists of six cylindrical layers of silicon detectors with three different technologies; in the outward direction: two layers of pixel detectors, two layers each of drift, and strip detectors. the number of parameters to be determined in the spatial alignment of the 2198 sensor modules of the its is about 13,000. the target alignment precision is well below 10 micron in some cases (pixels). the sources of alignment information include survey measurements, and the reconstructed tracks from cosmic rays and from proton-proton collisions. the main track-based alignment method uses the millepede global approach. an iterative local method was developed and used as well. we present the results obtained for the its alignment using about 10^5 charged tracks from cosmic rays that have been collected during summer 2008, with the alice solenoidal magnet switched off.
checking class schema usefulness. null
temporal logic verifications for uml: the vending machine example. null
a domain-specific language approach to programmable networks. null
computing a lower approximation of the compulsory part of a task with varying duration and varying resource consumption. null
extending chip in order to solve complex scheduling and placement problems. null
using pivot consistency to decompose and solve functional csps. many studies have been carried out in order to increase the search efficiency of constraint satisfaction problems; among them, some make use of {\em structural} properties of the constraint network; others take into account {\em semantic} properties of the constraints, generally assuming that {\em all} the constraints possess the given property. in this paper, we propose a new decomposition method benefiting from both semantic properties of {\em functional} constraints (not {\em bijective} constraints) and structural properties of the network; furthermore, not all the constraints need to be functional. we show that under some conditions, the existence of solutions can be guaranteed. we first characterize a particular subset of the variables, which we name a {\em root set}. we then introduce {\em pivot consistency}, a new local consistency which is a we ak form of path consistency and can be achieved in $o(n^2d^2)$ complexity (instead of $o(n^3d^3)$ for path consistency), and we present associated properti es; in particular, we show that any consistent instantiation of the root set can be linearly extended to a solution, which leads to the presentation of the aforementioned new method for solving by decomposing functional {\sc csp}s.
resource allocation in a mobile telephone network~: a constructive repair algorithm. null
bounds of graph properties. null
introducing global constraints in chip. null
an expressive aspect language for system applications with arachne. c applications, in particular those using operating system level services, frequently comprise multiple crosscutting con- cerns: network protocols and security are typical examples of such concerns. while these concerns can partially be addressed during design and implementation of an applica- tion, they frequently become an issue at runtime, e.g., to avoid server downtime. a deployed network protocol might not be sufficiently efficient and may thus need to be re- placed. buffer overflows might be discovered that imply critical breaches in the security model of an application. a prefetching strategy may be required to enhance perfor- mance. while aspect-oriented programming seems attractive in this context, none of the current aspect systems is expres- sive and efficient enough to address such concerns. this paper presents a new aspect system solving these problems. while efficiency considerations have played an important part in the design of the aspect language, the language al- lows aspects to be expressed more concisely than previous approaches. in particular, it allows aspect programmers to quantify over sequences of execution points as well as over alias accesses. we show how the former can be used to modularize the replacement of network protocols and the latter to prevent buffer overflows. we also present an im- plementation of the language as an extension of arachne, a dynamic weaver for c applications. finally, we show eval- uations proving that arachne is fast enough to extend high performance applications, such as the squid web cache.
dynamic adaptation of the squid web cache with arachne. null
generalized dynamic probes for the linux kernel and applications with arachne. finding the root cause of bugs and performance problems in large applications is a difficult task. the main reason of this difficulty is that the comprehension of such applications crosscuts the boundaries of a single process, indeed the concurrent nature of large applications requires insight of multiple threads and process and even sometimes of the kernel. in the meantime, most existing tools lacks support for simultaneous kernel and applications analysis. in this paper, we present arachne, a tool for runtime analysis of complex applications. while efficiency considerations have played an important role in the design of arachne, it allows safe and runtime injection of probes inside the linux kernel and user space applications on both function calls and variable access. it features an aspect- oriented language that allows to access context of execution and to compose primitive probes (for example sequence of function calls). we show how arachne allows to easily analyze problems such as race conditions which involves complex interactions between multiple process. and finally, we show arachne is fast enough to analyze high performance applications such as the squid web cache.
power management in grid computing with xen. while chip vendor still stick to moore's law, and the performance per dollar keeps going up, the performance per watt has been stagnant for the last few years. moreover energy prices continue to rise world-wide. this poses a major challenge to organisations running grids, indeed such architectures require cooling systems. indeed the one-year cost of a cooling system and of the power consumption may outfit the grid initial investement. we observe , however, that a grid does not constantly run at peak performance. in this paper, we propose a workload concentration strategy to reduce grid power consumption. using the xen virtual machine migration technology, our power management policy can dispatch transparently and dynamically any applications of the grid. our policy concentrates the workload to shutdown nodes that are unused with a neglectable impact on performance. we show through evaluations that this policy decreases the overall power consumption of the grid significantly.
the case for execution replay using a virtual machine. debugging grid systems is complex, mainly because of the probe effect and non reproducible execution. the probe effect arises when an attempt to monitor a system changes the behavior of that system. moreover, two executions of a distributed system with identical inputs may behave differ- ently due to non determinism. execution replay is a tech- nique developed to facilitate the debugging of distributed systems: a debugger first monitors the execution of a dis- tributed system and then replays it identically. existing approaches to execution replay only partially address the probe effect and irreproducibility problem. in this paper, we argue for execution replay of distributed sys- tems using a virtual machine approach. the vm approach addresses the irreproducibility problem, it does not com- pletely avoid the probe effect. nevertheless, we believe that the full control of the virtual hardware addresses the probe issue well enough to debug distributed system errors.
a reflexive extension to arachne's aspect language. aspect weaving at run time has proven to be an effective way of implementing software evolution. nevertheless, it is often hard to achieve adequate modularization and reusability in face of run time and implementation issues. arachne is an ao system that features a run time aspect weaver for c applications, and a language close to the c syntax. in this paper we present a reflexive extension of arachne's aspect language. we show through extracts of a deadlock detection aspect, how this extension improves the modularization of crosscutting concerns and the reusability of aspects.
server protection through dynamic patching. recently, hackers has been developing fast propagat- ing worms exploiting vulnerabilities that had just been dis- closed by security experts. those attacks particularly ex- pose servers: this class of applications is constantly con- nected to the internet and must meet uptime constraints. hence they often run unprotected until the next scheduled update. in this paper, we propose a just-in-time protection for servers based on runtime injection of pre-made patches. the runtime injection permits to deal with uptime con- straints and induces only a minimal overhead over the vul- nerable code and only when a vulnerability is known to ex- ist. the pre-made patches forbid exploitation of most com- mon vulnerabilities (45% of attacks reported by debian se- curity in 2005 affecting c softwares) and allows continuous servicing.
software security patches -- audit, deployment and hot update. due to its ever growing complexity, software is and will probably never be 100% bug-free and secure. therefore in most cases, software companies publish updates regularly. for the lack of time or care, or maybe because stopping an applica- tion is annoying, such updates are rarely, if ever, deployed on users' machines. we propose an integrated tool allowing system administrators to deploy critical security updates on the fly on applications running remotely and without the intervention of the end-user. our approach is based on arachne, an aspect weaving system that dynamically rewrites binary code. hence applications are still running while they are updated. our second tool minerve integrates arachne within the standard updating process: minerve takes a patch produced by diff, a tool that lists textual differences between two versions of a file, and eventually builds a dynamic patch that can later be woven to update the application on the fly. in addition, by translating patches into aspects and thus generating a more abstract presentation of the changes, minerve eases auditing tasks.
performance of prototypes for the alice electromagnetic calorimeter. the performance of prototypes for the alice electromagnetic sampling calorimeter has been studied in test beam measurements at fnal and cern. a $4\times4$ array of final design modules showed an energy resolution of about 11% /$\sqrt{e(\mathrm{gev})}$ $\oplus$ 1.7 % with a uniformity of the response to electrons of 1% and a good linearity in the energy range from 10 to 100 gev. the electromagnetic shower position resolution was found to be described by 1.5 mm $\oplus$ 5.3 mm /$\sqrt{e \mathrm{(gev)}}$. for an electron identification efficiency of 90% a hadron rejection factor of $&gt;600$ was obtained.
nonlinear effects in stiffness modeling of robotic manipulators. the paper focuses on the enhanced stiffness modeling of robotic manipulators by taking into account influence of the external force/torque acting upon the end point. it implements the virtual joint technique that describes the compliance of manipulator elements by a set of localized six-dimensional springs separated by rigid links and perfect joints. in contrast to the conventional formulation, which is valid for the unloaded mode and small displacements, the proposed approach implicitly assumes that the loading leads to the non-negligible changes of the manipulator posture and corresponding amendment of the jacobian. the developed numerical technique allows computing the static equilibrium and relevant force/torque reaction of the manipulator for any given displacement of the end-effector. this enables designer detecting essentially nonlinear effects in elastic behavior of manipulator, similar to the buckling of beam elements. it is also proposed the linearization procedure that is based on the inversion of the dedicated matrix composed of the stiffness parameters of the virtual springs and the jacobians/hessians of the active and passive joints. the developed technique is illustrated by an application example that deals with the stiffness analysis of a parallel manipulator of the orthoglide family.
geomagnetic effects observed by the codalema experiment. the codalema experiment is measuring transient radio emissions associated to extended air showers produced by high energy cosmic rays. the experimental setup installed at the nancay radio observatory in france has recently undergone hardware upgrades and an extension of the surfaces covered by both the antenna and the scintillator detector arrays. the experimental data allow to investigate the main features of these radio signals and the underlying electric field production mechanisms. some of the latest experimental results of codalema are presented. they have been analyzed assuming a linear dependence of the electric field with respect to v ^ b. within the codalema observation conditions at nancay, the detection eficiency, the arrival direction distribution and the polarity of the radio signals can be interpreted in terms of a geomagnetic effect. a r&amp;d effort is currently underway to develop the hardware elements for the deployment of a large detector array based on active antennas. the main features of the first prototype of the codalema autonomous station are briefly described.
first proton--proton collisions at the lhc as observed with the alice detector: measurement of the charged particle pseudorapidity density at $\ sqrt(s)$= 900 gev. on 23rd november 2009, during the early commissioning of the cern large hadron collider (lhc), two counter-rotating proton bunches were circulated for the first time concurrently in the machine, at the lhc injection energy of 450 gev per beam. although the proton intensity was very low, with only one pilot bunch per beam, and no systematic attempt was made to optimize the collision optics, all lhc experiments reported a number of collision candidates. in the alice experiment, the collision region was centred very well in both the longitudinal and transverse directions and 284 events were recorded in coincidence with the two passing proton bunches. the events were immediately reconstructed and analyzed both online and offline. we have used these events to measure the pseudorapidity density of charged primary particles in the central region. in the range |eta| &lt; 0.5, we obtain dnch/deta = 3.10 +- 0.13 (stat.) +- 0.22 (syst.) for all inelastic interactions, and dnch/deta = 3.51 +- 0.15 (stat.) +- 0.25 (syst.) for non-single diffractive interactions. these results are consistent with previous measurements in proton--antiproton interactions at the same centre-of-mass energy at the cern spps collider. they also illustrate the excellent functioning and rapid progress of the lhc accelerator, and of both the hardware and software of the alice experiment, in this early start-up phase.
sorption of eu(iii) on attapulgite studied by batch, xps and exafs techniques. the effects of ph, ionic strength and temperature on sorption of eu(iii) on attapulgite were investigated in the presence and absence of fulvic acid (fa) and humic acid (ha). the results indicated that the sorption of eu(iii) on attapulgite was strongly dependent on ph and ionic strength, and independent of temperature. in the presence of fa/ha, eu(iii) sorption was enhanced at ph &lt; 4, decreased at ph range of 4 - 6, and then increased again at ph &gt; 7. the x-ray photoelectron spectroscopy (xps) analysis suggested that the sorption of eu(iii) might be expressed as ≡x3eu0 ≡swoheu3+ and ≡soeu-ooc-/ha in the ternary eu/ha/attapulgite system. the extended x-ray absorption fine structure (exafs) analysis of eu-ha complexes indicated that the distances of d(eu-o) decreased from 2.451 to 2.360 å with increasing ph from 1.76 to 9.50, whereas the coordination number (n) decreased from ~9.94 to ~8.56. different complexation species were also found for the different addition sequences of ha and eu(iii) to attapulgite suspension. the results are important to understand the influence of humic substances on eu(iii) behavior in the natural environment.
proceedings of the workshop: hera and the lhc workshop series on the implications of hera for lhc physics. 2nd workshop on the implications of hera for lhc physics. working groups: parton density functions multi-jet final states and energy flows heavy quarks (charm and beauty) diffraction cosmic rays monte carlos and tools.
study of hard processes in proton-proton and nucleus-nucleus collisions. collisions of high energy particles are useful to probe the elementary structure of matter: partons (quarks and gluons). ultra relativistic heavy ion collisions produce in the same event a so-called quark-gluon plasma (qgp) as well as elementary parton-parton interaction. those interactions, called hard processes, originate from the very first moment of the collision, and produce high transverse momentum partons which then decay into observable hadrons (jets). before this “hadronization”, the partons may interact with the qgp, which modifies the hadron properties. studying hard processes will be an important issue at lhc. jet production cross sections can be computed within the pqcd framework. the epos formalism tends to compute full events which are directly comparable to experimental events, reproducing the soft part (qgp) as well as the hard part in a coherent model. in this manuscript, i will motivate the realization of hard processes in a complete event. the hard part should be compatible with the results from pqcd. doing so, as in experiment, production of rare processes needs large statistics. i will therefore introduce method allowing to trigger on high transverse momentum events, to easily produce rare event in the context of soft particles.
on the liquid drop model mass formulae and charge radii. an adjustment to 782 ground state nuclear charge radii for nuclei with n,z $\ge$ 8 leads to $r_0=1.2257~a^{1/3}$~fm and $\sigma =0.124$~fm for the charge radius. assuming such a coulomb energy $e_c=\frac {3}{5} {e^2z^2}/{1.2257~a^{\frac {1}{3}}}$, the coefficients of different possible mass formulae derived from the liquid drop model and including the shell and pairing energies have been determined from 2027 masses verifying n,z $\ge$ 8 and a mass uncertainty $\le $ 150 kev. these formulae take into account or not the diffuseness correction ($z^2/a$ term), the charge exchange correction term ($z^{4/3}/a^{1/3}$ term), the curvature energy, the wigner terms and different powers of $i=(n-z)/a$. the coulomb diffuseness correction or the charge exchange correction term plays the main role to improve the accuracy of the mass formulae. the different fits lead to a surface energy coefficient of around 17-18~mev. a possible more precise formula for the coulomb radius is $r_0=1.2332a^{1/3}+{2.8961}/{a^{2/3}}-0.18688a^{1/3}i$~fm with $\sigma =0.052$~fm.
changes in fire regime since the last glacial maximum: an assessment based on a global synthesis and analysis of charcoal data. null
reducing kernel development complexity in distributed environments. setting up generic and fully transparent distributed services for clusters implies complex and tedious kernel developments. more flexible approaches such as user-space libraries are usually preferred with the drawback of requiring application recompilation. a second approach consists in using specific kernel modules (such as fuse in gnu/linux system) to transfer kernel complexity into user space. in this paper, we present a new way to design and implement kernel distributed services for clusters by using a cluster wide consistent data management service. this system, entitled kddm for "kernel distributed data management", offers flexible kernel mechanisms to transparently manage remote accesses, cache and coherency. we show how kddm simplifies distributed kernel developments by presenting the design and the implementation of a service as complex as a fully symmetric distributed file system. the innovative approach of kddm has the potential to boost the development of distributed kernel services because it relieves the developers of the burden of dealing with distributed protocols and explicit data transfers. instead, it allows focusing on the implementation of services in a manner very similar to that of parallel programming on smp systems. more generally, the use of kddm could be exploited in almost all local kernel services to extend them to cluster scale. cluster wide ipc, distributed namespaces (such as /proc) or process migration are some potential examples.
the effect of high power ultrasound on an aqueous suspension of graphite. ultrasound treatment was used to study the decrease of the granulometry of graphite, due to the cavitation, which allows the erosion by separating grains. at a smaller scale, cavitation bubble implosion tears apart graphite sheets as shown by hrtem, while ho and h radicals produced from water sonolysis, generate oxidative and reductive reactions on these sheet fragments. such reactions form smaller species, e.g. dissolved organic matter. the methodology proposed is very sensitive to unambiguously identifying the in situ composition of organic compounds in water. the use of the atmospheric pressure chemical ionization (apci) fourier transform mass spectrometry (ftms) technique minimizes the perturbation of the organic composition and does not require chemical treatment for analysis. the structural features observed in the narrow range (m/z &lt; 300) were mainly aromatic compounds (phenol, benzene, toluene, xylene, benzenediazonium, etc.), c4-c6 alkenes and c2-c10 carboxylic acids. synthesis of small compounds from graphite sonication has never been reported and will probably be helpful to understand the mechanisms involved in high energy radical reactions.
special issue : "robotics and factory of the future, new trends and challenges in mechatronics" from incom 2006. null
trigger and aperture of the surface detector array of the pierre auger observatory. null
erratum to "atmospheric effects on extensive air showers observed with the surface detector of the pierre auger observatory" [astroparticle physics 32(2) (2009), 89-99]. null
aspect-based patterns for grid programming. the development of grid algorithms is frequently ham- pered by limited means to describe topologies and lack of support for the invasive composition of legacy components in order to pass data between them. in this paper we present a solution to overcome these limitations using the notion of invasive patterns for the construction of distributed algo- rithms, a recent extension of well-known computation and communication patterns. concretely, we present two con- tributions. first, based on a study of how patterns are in- stantiated in nas grid, a well-known benchmark used for evaluating performance of computational grids, we show how invasive patterns can be used for the declarative defini- tion of large-scale grid topologies and checkpointing algo- rithms. second, we qualitatively and quantitatively evaluate how our approach can be used to implement the checkpoint- ing on top of grid applications.
extension of hansen-bliek's method to right-quantified linear systems. null
on pole assignment and stabilizability of neutral type systems. in this note we present a systematic approach to the stabilizability problem of linear infinite-dimensional dynamical systems whose infinitesimal generator has an infinite number of instable eigenvalues. we are interested in strong non-exponential stabilizability by a linear feed-back control. the study is based on our recent results on the riesz basis property and a careful selection of the control laws which preserve this property. the investigation may be applied to wave equations and neutral type delay equations.
stability analysis of mixed retarded-neutral type systems. the strong stability property of mixed retarded-neutral type systems is studied. we combine two technics: the existence of a riesz basis of invariant infinite dimensional subspaces and the boundedness of the resolvent on some invariant subspace.
proceedings of the 38th international symposium on multiparticle dynamics (ismd08). proceedings of ismd08.
analytic expressions for alpha particle preformation in heavy nuclei. the experimental alpha decay energies and half-lives are investigated systematically to extract the alpha particle preformation in heavy nuclei. formulas for the preformation factors are proposed. they can be used to guide the microscopic studies on preformation factors and perform accurate calculations of the alpha decay half-lives. there is little evidence for the existence of an island of long stability of superheavy nuclei (shn).
dynamic aspectj. this paper considers the diﬃculties linked to the static scheduling strategy of aspectj and shows how to overcome them by turning to a more dynamic strategy, making it possible to order, cancel, and deploy aspects at runtime. we show that this more dynamic strategy can be obtained by a minor update of the semantics of aspectj introducing the notion of current aspect group, that is, the aspects scheduled for the current join point. we show how to reﬂect this change at the language level and present a prototype of the resulting aspectj variant, dynamic aspectj. this prototype reuses aspectj to perform a ﬁrst step of static weaving, which we complement by a second step of dynamic weaving, implemented through a thin interpretation layer. this can be seen as an interesting example of reconciling interpreters and compilers, the dynamic and the static world.
hull consistency under monotonicity. we prove that hull consistency for a system of equations or inequalities can be achieved in polynomial time providing that the underlying functions are monotone with respect to each variable. this result holds including when variables have multiple occurrences in the expressions of the functions, which is usually a pitfall for interval-based contractors. for a given constraint, an optimal contractor can thus be enforced quickly under monotonicity and the practical significance of this theoretical result is illustrated on a simple example.
a constraint on the number of distinct vectors with application to localization. this paper introduces a generalization of the "nvalue" constraint that bounds the number of distinct values taken by a set of variables.the generalized constraint (called "nvector") bounds the number of distinct (multi-dimensional) vectors. the first contribution of this paper is to show that this global constraint has a significant role to play with continuous domains, by taking the example of simultaneous localization and map building (slam). this type of problem arises in the context of mobile robotics. the second contribution is to prove that enforcing bound consistency on this constraint is np-complete. a simple contractor (or propagator) is proposed and applied on a real application.
contractor programming. this paper describes a solver programming method, called "contractor programming", that copes with two issues related to constraint processing over the reals. first, continuous constraints involve an inevitable step of solver design. existing softwares provide an insufficient answer by restricting users to choose among a list of fixed strategies. our first contribution is to give more freedom in solver design by introducing programming concepts where only configuration parameters were previously available. programming consists in applying operators (intersection, composition, etc.) on algorithms called "contractors" that are somehow similar to propagators. second, many problems with real variables cannot be cast as the search for vectors simultaneously satisfying the set of constraints, but a large variety of different outputs may be demanded from a set of constraints (e.g., a paving with boxes inside and outside of the solution set). these outputs can actually be viewed as the result of different "contractors" working concurrently on the same search space, with a bisection procedure intervening in case of deadlock. such algorithms (which are not strictly speaking solvers) will be made easy to build thanks to a new branch &amp; prune system, called "paver". thus, this paper gives a way to deal harmoniously with a larger set of problems while giving a fine control on the solving mechanisms. the contractor formalism and the paver system are the two contributions. the approach is motivated and justified through different cases of study. an implementation of this framework named quimper is also presented.
a priori error analysis and spring arithmetic. error analysis is defined by the following concern: bounding the output variation of a (nonlinear) function with respect to a given variation of the input variables. this paper investigates this issue in the framework of interval analysis. the classical way of analyzing the error is to linearize the function around the point corresponding to the actual input, but this method is local and not reliable. both drawbacks can be easily circumvented by a combined use of interval arithmetic and domain splitting. however, because of the underlying linearization, a standard interval algorithm leads to a pessimistic bound, and even simply fails (i.e., returns an infinite error) in case of singularity. we propose an original nonlinear approach where intervals are used in a more sophisticated way through the so-called "springs". this new structure allows to represent an (infinite) set of intervals constrained by their midpoints and their radius. the output error is then calculated with a spring arithmetic in the same way as the image of a function is calculated with interval arithmetic. our method is illustrated on two examples, including an application of geopositioning.
calibration of 3-d.o.f. translational parallel manipulators using leg observations. the paper proposes a novel approach for the geometrical model calibration of quasi-isotropic parallel kinematic mechanisms of the orthoglide family. it is based on the observations of the manipulator leg parallelism during motions between the specific test postures and employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. they are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the tcp along the cartesian axes. using the measured differences, the developed algorithm estimates the joint offsets and the leg lengths that are treated as the most essential parameters. validity of the proposed calibration technique is confirmed by the experimental results.
functional analysis approach for delay systems of neutral type. linear delay systems of neutral type are presented using the hilbert space analysis approach. we consider in particular the problem of asymptotic non-exponential stability and the corresponding property of stabilizability for controlled systems. the main tool is the riesz basis property of eigenspaces. this property is also used for the exact controllability via the moment problem approach.
on a vector moment problem appearing in the analysis of controllability of neutral type systems. we consider the solvability of a vector moment problem associating it to the analysis of controllability for a certain delayed system of neutral type. in this way we succeeded to determine exactly the minimal interval on which the moment problem is solvable.
stability, stabilizability and exact controllability of a class of linear neutral type systems. linear systems of neutral type are considered using the infinite dimensional approach. the main problems are asymptotic, non-exponential stability, exact controllability and regular asymptotic stabilizability. the main tools are the moment problem approach, the riesz basis of invariant subspaces and the riesz basis of family of exponentials.
an input interaction model for highly configurable multi-device interactive systems. today's applications and tools exclusively rely on mice and keyboards and use them a stereotyped way, and are still ignoring any other interaction paradigm. though importance of input adaptability has been widely recognized, its issues have been very little studied. we propose a new input model based on input configurations. in this model, input devices are freely connected to applications through adapters, and traditional event mechanisms have been replaced by a reactive data-flow paradigm. using this model, we developed the icon (input configurator) input toolkit, which allows to build interactive applications that are fully configurable and that can make use of alternative input device and techniques. with its visual editor, developers can rapidly create by direct manipulation configurations for impoverished or enriched input. power users can then customize those configurations to suit their specific needs.
dealing with constraints during a feature configuration process in a model-driven software product line. null
implementing an mda approach for managing variability in product line construction using the gmf and gme frameworks. null
components with symbolic transition systems: a java implementation of rendez-vous. component-based software engineering is becoming an important approach for system development. a crucial issue is to fill the gap between high-level models, needed for design and verification, and implementation. this paper introduces first a component model with explicit protocols based on symbolic transition systems. it then presents a java implementation for it that relies on a rendezvous mechanism to synchronize events between component protocols. this paper shows how to get a correct implementation of a complex rendezvous in presence of full data types, guarded transitions and, possibly, guarded receipts.
identification of replication-competent hsv-1 cgal+ strain signaling targets in human hepatoma cells by functional organelle proteomics. in the present work, we have attempted a comprehensive analysis of cytosolic and microsomal proteomes to elucidate the signaling pathways impaired in human hepatoma (huh7) cells upon herpes simplex virus type 1 (hsv-1; cgal(+)) infection. using a combination of differential in-gel electrophoresis and nano liquid chromatography/tandem mass spectrometry, 18 spots corresponding to 16 unique deregulated cellular proteins were unambiguously identified, which were involved in the regulation of essential processes such as apoptosis, mrna processing, cellular structure and integrity, signal transduction, and endoplasmic-reticulum-associated degradation pathway. based on our proteomic data and additional functional studies target proteins were identified indicating a late activation of apoptotic pathways in huh7 cells upon hsv-1 cgal(+) infection. additionally to changes on ruvb-like 2 and bif-1, down-regulation of erlin-2 suggests stimulation of ca(2+)-dependent apoptosis. moreover, activation of the mitochondrial apoptotic pathway results from a time-dependent multi-factorial impairment as inferred from the stepwise characterization of constitutive pro- and anti-apoptotic factors. activation of serine-threonine protein phosphatase 2a (pp2a) was also found in huh7 cells upon hsv-1 cgal(+) infection. in addition, pp2a activation paralleled dephosphorylation and inactivation of downstream mitogen-activated protein (map) kinase pathway (mek(1/2), erk(1/2)) critical to cell survival and activation of proapoptotic bad by dephosphorylation of ser-112. taken together, our results provide novel molecular information that contributes to define in detail the apoptotic mechanisms triggered by hsv-1 cgal(+) in the host cell and lead to the implication of pp2a in the transduction of cell death signals and cell survival pathway arrest.
identification of replication-competent hsv-1 cgal(+) strain targets in a mouse model of human hepatocarcinoma xenograft. recent studies based on animal models have shown the advantages and potential of oncolytic viral therapy using hsv-1 -based replication-competent vectors in the treatment of liver tumors, but little is known about the cellular targets that are modulated during viral infection. in the present work, we have studied the effects of intratumoral injections of hsv-1 cgal(+) strain in a murine model of human hepatoma xenografts. viral replication was assessed for more than 1month, leading to a significant reduction of tumor growth rate mediated, in part, by a cyclin b dependent cell proliferation arrest. early events resulting in this effect were analyzed using a proteomic approach. protein extracts from xenografted human hepatomas treated with saline or hsv-1 cgal(+) strain during 24h were compared by 2-d dige and differential spots were identified by nanolc-esi-ms/ms. alterations on glutathione s transferase 1 omega, and erp29 suggest novel hsv-1 cgal(+) targets in solid liver tumors. additionally, erp29 showed a complex differential isoform pattern upon hsv-1 cgal(+) infection, suggesting regulatory mechanisms based on post-translational modification events.
handling persistent states in process checkpoint/restart mechanisms for hpc systems. computer clusters are today the reference architecture for high-performance computing. the large number of nodes in these systems induces a high failure rate. this makes fault tolerance mechanisms, e.g. process checkpoint/restart, a required technology to eﬀectively exploit clusters. most of the process checkpoint/restart implementations only handle volatile states and do not take into account persistent states of applications, which can lead to incoherent application restarts. in this paper, we introduce an eﬃcient persistent state checkpoint/restoration approach that can be interconnected with a large number of ﬁle systems. to avoid the performance issues of a stable support relying on synchronous replication mechanisms, we present a failure resilience scheme optimized for such persistent state checkpointing techniques in a distributed environment. first evaluations of our implementation in the kdfs distributed ﬁle system show the negligible performance impact of our proposal.
a multi-stage approach for reliable dynamic reconfigurations of component-based systems. in this paper we present an end-to-end solution to define and execute reliable dynamic reconfigurations of open component-based systems while guaranteeing their continuity of service. it uses a multi-stage approach in order to deal with the different kinds of possible errors in the most appropriate way; in particular, the goal is to detect errors as early as possible to minimize their impact on the target system. reconfigurations are expressed in a restricted, domain-specific language in order to allow different levels of static and dynamic validation, thus detecting errors before executing the reconfiguration where possible. for errors that can not be detected early (including software and hardware faults), a runtime environment provides transactional semantics to the reconfigurations.
thermal properties of phi mesons and the qcd equation of state. null
turning on the charm. i argue that the strong jet quenching of heavy flavors observed in heavy-ion collisions is to a large extent due to binary scatterings in the quark-gluon plasma. it can be understood from first principles: the charm collision probability beyond logarithmic accuracy and markov evolution.
the micro pattern gas detector pim : a multi-modalities solution for novel investigations in functional imaging. null
a heavy water converter concept for spiral ii. null
nuclear reactor simulations for unveiling diversion scenarios : capabilities of the antineutrino probe. nuclear reactors emit a huge amount of electronic antineutrinos, arising from the fission product decay. reactor antineutrinos thus posess unique features that place them as a potential new safeguards tool for the international atomic energy agency (iaea). indeed, they carry outside the core the direct picture of its isotopic fission rates, thus opening the possibility of a remote, non-intrusive and tamperproof reactor monitoring. sophisticated simulations of reactors and their associated antineutrino flux and energy spectrum have been developed to predict the neutrino signature of the fuel burnup and of a diversion. the only user-defined inputs driving the time evolution of the isotopic composition of the core are the initial fuel composition, the refueling scheme, and the thermal power. the evolution of the antineutrino flux and energy spectrum with the fuel burnup, as well as the effect of neutron capture on various nuclei are taken into account. non-proliferation scenarios and burnup monitoring with antineutrinos have been studied using these tools for pwr and candu reactors. a full core simulation of an n4-pwr will be presented in a first part. gross unveiling diversion scenarios using a pwr have been simulated in order to test the ability of the antineutrino probe. a channel of a heavy water reactor (candu 600) loaded with natural uranium, has been simulated also in order to provide a first hint of what antineutrino detection would bring to the monitoring of such on-line refueled reactor which are maintained in a steady state through quasi-continuous refueling. very simple proliferation scenario studies with candu reactors, based on several channel calculations, made at various fuel dwell-times, will be shown in a second part. in both cases, the response of a nucifer-like detector placed at 25m from the core to these scenarios has been studied.
hydrodynamic evolution : on the role of initial conditions and freeze out (thermal or not). null
gribov-regge theorie, partons, strings – and the epos model for hadronic interactions. null
epos and other. null
hydrodynamics evolution in epos. null
macroscopic of radio emission based on shower simulations and a realistic index of refraction. null
energy losses of ultrarelativistic particles in hot plasmas revisited. null
quark condensate in strong magnetic fields. null
heavy flavour in alice. null
hadronic collisions at the lhc and qcd at high energy loss in a qgp. null
collisional energy loss of a fast parton in a qgp. null
results of the radio-detection experiment codalema. measurements of the radio transients associated to extensive air showers could provide a timely new effective method of detection of ultra high energy cosmic rays with interesting performances in term of techniques and of accessible observables. the codalema experiment at the radio observatory of nançay, france, explores this possibility using an array of broadband active dipole antennas triggered by an array of ground particle detectors. the implemented detection techniques, the methods of analysis developed and the principal results achieved during the 5 years of upgrading will be recalled. this cursory glance will provide an opportunity to point out some new issues and to suggest some future challenges in this domain.
objectives of the carbonwaste project “treatment and disposal of irradiated graphite and other carboneous waste. null
carbonwaste: new euratom project on treatment and disposal of irradiated graphite and other carboneus waste. null
an overview on the current plan of eu for the radioactive wastes management- the role of the french national evaluation commission, the public interactions and the european framework. null
nuclear waste glasses. null
description of heavy ion collisions within the isospin quantum molecular dynamics model. null
radio detection of high energy cosmic ray showers-a short review. null
tomography of the quark gluon plasma. null
interaction of heavy quark with the quark gluon plasma. null
on running coupling constants and infared regulators. null
critical phenomena in heavy ion reactions. null
collisional energy loss of heavy quarks in a quark gluon plasma. null
gas production and activation calculation in megapie. null
high pt and photon physics at lhc with alice. null
the qcd phase transition and the thermal properties of the phi mesons. null
radiodetection of cosmic air showers with autonomous radio detectors installed at the pierre auger observatory. null
generation of complete event containing very high pt jets. null
astrophysical sources of cosmic rays and related measurements with the pierre auger observatory. studies of the correlations of ultra-high energy cosmic ray directions with extra-galactic objects, of general anisotropy, of photons and neutrinos, and of other astrophysical effects, with the pierre auger observatory. contributions to the 31st icrc, lodz, poland, july 2009.
calibration and monitoring of the pierre auger observatory. reports on the atmospheric monitoring, calibration, and other operating systems of the pierre auger observatory. contributions to the 31st international cosmic ray conference, lodz, poland, july 2009.
operations of and future plans for the pierre auger observatory. technical reports on operations and features of the pierre auger observatory, including ongoing and planned enhancements and the status of the future northern hemisphere portion of the observatory. contributions to the 31st international cosmic ray conference, lodz, poland, july 2009.
expressive scoping of distributed aspects. dynamic deployment of aspects brings greater flexibility and reuse potential, but requires proper means for scoping aspects. scoping issues are particularly crucial in a distributed context: adequate treatment of distributed scoping is necessary to enable the propagation of aspect instances across host boundaries and to avoid inconsistencies due to unintentional spreading of data and computations in a distributed system. we motivate the need for expressive scoping of dynamically-deployed distributed aspects by an analysis of the deficiencies of current approaches for distributed aspects. extending recent work on deployment strategies for non-distributed aspects, we then introduce a set of high-level strategies for specifying locality of aspect propagation and activation, and illustrate the corresponding gain in expressiveness. we present the operational semantics of our proposal using scheme interpreters, first introducing a model of distributed aspects that covers the range of current proposals, and then extending it with dynamic aspect deployment. this work shows that, given some extensions to their original execution model, deployment strategies are directly applicable to the expressive scoping of distributed aspects.
studies of cosmic ray composition and air shower structure with the pierre auger observatory. studies of the composition of the highest energy cosmic rays with the pierre auger observatory, including examination of hadronic physics effects on the structure of extensive air showers. submissions to the 31st icrc, lodz, poland (july 2009).
the cosmic ray energy spectrum and related measurements with the pierre auger observatory. studies of the cosmic ray energy spectrum at the highest energies with the pierre auger observatory.
study of 2,9-dicarboxy-1, 10-phenanthroline as potential chelate for the synthesis of 230u-labeled radioconjugates. null
a simulation approach to describe bi-213 radiolabelling on chx-dtpa-igg conjugate. null
arronax, a high energy and high intensity cyclotron for nuclear medicine. null
xt-ads spallation target proof of feasibility. null
cyclotron arronax. null
preformation of clusters in heavy nuclei and cluster radioactivity. within the preformed cluster model approach, the values of the preformation factors have been deduced from the experimental cluster decay half-lives assuming that the decay constant of the heavy-ion emission is the product of the assault frequency, the preformation factor and the penetrability. the law according to which the preformation factors follow a simple dependence on the mass of the cluster was confirmed. then predictions for some most possible cluster decays are provided.
identified particle production, azimuthal anisotropy, and interferometry measurements in au+au collisions at $\sqrt{s_{nn}}$ = 9.2 gev. we present the first measurements of identified hadron production, azimuthal anisotropy, and pion interferometry from au+au collisions below the nominal injection energy at the relativistic heavy-ion collider (rhic) facility. the data were collected using the large acceptance star detector at $\sqrt{s_{nn}}$ = 9.2 gev from a test run of the collider in the year 2008. midrapidity results on multiplicity density (dn/dy) in rapidity (y), average transverse momentum (), particle ratios, elliptic flow, and hbt radii are consistent with the corresponding results at similar $\sqrt{s_{nn}}$ from fixed target experiments. directed flow measurements are presented for both midrapidity and forward rapidity regions. furthermore the collision centrality dependence of identified particle dn/dy, , and particle ratios are discussed. these results also demonstrate the readiness of the star detector to undertake the proposed qcd critical point search and the exploration of the qcd phase diagram at rhic.
refinement proposal of the goldberg's theory. virtual machines (vm) allow the execution of various operating systems and provide several functionalities which are nowadays strongly appreciated by developers and administrators (isolation between applications, flexibility of resource management, and so on). as a direct consequence, ``virtualization'' has become a buzz word and a lot of ``virtualization'' solutions have been proposed, each providing particular functionalities. goldberg proposed to classify virtualization techniques in two models (type-i and type-ii), which does not enable the classification of latest ``virtualizations'' technologies such abstraction, emulation, partitioning and so on. in this document, we propose an extension of the goldberg model in order to take into account and formaly define latest ``virtualization'' mechanisms. after giving general definitions, we show how our proposal enables to rigorously formalize the following terms: virtualization, emulation, abstraction, partitioning, and identity. we also demonstrate that a single virtualization solution is generally composed by several layers of virtualization capabilities, depending on the granularity of the analysis.
challenge of assessing long-term performance of nuclear matrices in repository near-field environment- insights from the nf-pro and micado projects. null
carbonwaste wp6 disposal behaviour of i-graphite and other carbonaceous waste. null
the backend of the fuel cycle of hrt/vhrt reactors. null
monte carlo simulations in the context of reactor monitoring with antineutrinos. null
nuclear reactor and spectra simulations for unveiling diversion scenarios. null
selected effects of the in-medium nn cross section on heavy-ion dynamics below 100 mev/u. null
bimodality - a general feature of heavy ion reactions. recently, is has been observed that events with the {\it same} total transverse energy of light charged particles (lcp) in the quasi target region, $e_{\perp 12}^{qt}$, show two quite distinct reaction scenarios in the projectile domain: multifragmentation and residue production. this phenomenon has been dubbed "bimodality". using quantum molecular dynamics calculations we demonstrate that this observation is very general. it appears in collisions of all symmetric systems larger than ca and at beam energies between 50 a.mev and 600 a.mev and is due to large fluctuations of the impact parameter for a given $e_{\perp 12}^{qt}$. investigating in detail the $e_{\perp 12}^{qt}$ bin in which both scenarios are present, we find that neither the average fragment momenta nor the average transverse and longitudinal energies of fragments show the behavior expected from a system in statistical equilibrium, in experiment as well as in qmd simulations. on the contrary, the experimental as well as the theoretical results point towards a fast process. this observation questions the conjecture that the observed bimodality is due to the coexistence of 2 phases at a given temperature in finite systems.
complementarity between virtualization and single system image technologies. nowadays, the use of clusters in research centers or industries is undeniable. since few years, the usage of virtual machines (vm) offers more advanced resource management capabilities, using features such as virtual machine live migration. because of the latest contributions in the domain, some may argue that single system image (ssi) technologies are now deprecated, without considering some complementarities between vms and ssi technologies are possible. after evaluating different configurations, we show that combining both approaches allows us to better address cluster challenges such as flexibility for the usage of available resources and simplicity of use. in other terms, the study shows that vms add a level of management flexibility between the hardware and the application, whereas, ssis give an abstraction of the distributed resources. the simultaneous usage of both technologies could improve the overall platform resources utilization, the cluster productivity and the efficiency of the running applications.
debugging and testing middleware with aspect-based control-flow and causal patterns. many tasks that involve the dynamic manipulation of mid- dleware and large-scale distributed applications, such as debugging and testing, require the monitoring of intricate relationships of execution events that trigger modiﬁcations to the executing system. furthermore, events often are of interest only if they occur as part of speciﬁc execu- tion traces and not all possible non-deterministic interleavings of events in these traces. current techniques and tools for the deﬁnition of such manipulations provide only very limited support for such event relation- ships and do not allow to concisely deﬁne restrictions on the interleaving of events. in this paper, we argue for the use of high-level programming abstractions for the deﬁnition of relationships between execution events of distributed systems and the control of non-deterministic interleavings of events. con- cretely, we provide the following contributions: we (i) motivate that such abstractions improve on current debugging and testing methods for mid- dleware, (ii) introduce corresponding language mechanisms as well as corresponding implementation support by extending an existing aspect- oriented system for the dynamic manipulation of distributed systems, and (iii) evaluate our approach in the context of the debugging and test- ing of jboss cache, a java-based middleware for replicated caching.
specialized aspect languages preserving classes of properties. null
aspect preserving properties. null
kinematic calibration of orthoglide-type mechanisms from observation of parallel leg motions. the paper proposes a new calibration method for parallel manipulators that allows efficient identification of the joint offsets using observations of the manipulator leg parallelism with respect to the base surface. the method employs a simple and low-cost measuring system, which evaluates deviation of the leg location during motions that are assumed to preserve the leg parallelism for the nominal values of the manipulator parameters. using the measured deviations, the developed algorithm estimates the joint offsets that are treated as the most essential parameters to be identified. the validity of the proposed calibration method and efficiency of the developed numerical algorithms are confirmed by experimental results. the sensitivity of the measurement methods and the calibration accuracy are also studied.
stiffness analysis of multi-chain parallel robotic systems. the paper presents a new stiffness modelling method for multi-chain parallel robotic manipulators with flexible links and compliant actuating joints. in contrast to other works, the method involves a fea-based link stiffness evaluation and employs a new solution strategy of the kinetostatic equations, which allows computing the stiffness matrix for singular postures and to take into account influence of the internal forces. the advantages of the developed technique are confirmed by application examples, which deal with stiffness analysis of the orthoglide manipulator.
final results of the tests on the resistive plate chambers for the alice muon arm. the trigger for the alice muon spectrometer will be issued by single-gap, low resistivity bakelite resistive plate chambers (rpcs). the trigger system consists of four 5.5x6.5 m2 rpc planes arranged in two stations, for a total of 72 detectors. one hundred and sixteen detectors have been assembled and tested in torino. the tests have been performed with the streamer mixture developed for heavy ion data-taking. the tests include: the detection of gas leaks and parasitic currents; the measurement of the efficiency with cosmic rays, with particular regard to the uniformity of the efficiency throughout the whole active surface, with a granularity of about 2x2 cm2; the measurement of the dark current and of the mean and localised noise rate. all the rpcs produced have been characterised. among them, the detectors to be finally installed in alice and some spare have been selected; 17% of all the produced detectors have been discarded. a short description of the test set-up is given. the results of the tests are presented, with particular regard to the performance of the selected detectors.
ample : supporting product line engineering through synthesis of aspect-oriented and model-driven development. null
event strictness for components with complex bindings. null
product derivation in a model-driven software product line using decision models. null
dealing with fine-grained configurations in model-driven spls. null
the stslib project: towards a formal component model based on sts. null
the ample project, traceability of software product line development: models and uncertainty. null
an event-based coordination model for context-aware applications. context-aware applications adapt their behavior depending on changes in their environment context. programming such applications in a modular way requires to modularize the global context into more speciﬁc contexts and attach speciﬁc behavior to these contexts. this is reminiscent of aspects and has led to the notion of context-aware aspects. this paper revisits this notion of context-aware aspects in the light of previous work on concurrent event-based aspect-oriented programming (ceaop). it shows how ceaop can be extended in a seamless way in order to deﬁne a model for the coordination of concurrent adaptation rules with explicit contexts. this makes it possible to reason about the compositions of such rules. the model is concretized into a prototypical modeling language.
exact controllability of linear neutral type systems by the moment problem approach. the problem of exact null-controllability is considered for a wide class of linear neutral type systems with distributed delay. the main tool of the analysis is the application of the moment problem approach and the theory of the basis property of exponential families. a complete characterization of this problem is given. the minimal time of controllability is specified. the results are based on the analysis of the riesz basis property of eigenspaces of the neutral type systems in hilbert space.
design optimization of parallel manipulators for high-speed precision machining applications. the paper proposes an integrated approach to the design optimization of parallel manipulators, which is based on the concept of the workspace grid and utilizes the goal-attainment formulation for the global optimization. to combine the non-homogenous design specification, the developed optimization technique transforms all constraints and objectives into similar performance indices related to the maximum size of the prescribed shape workspace. this transformation is based on the dedicated dynamic programming procedures that satisfy computational requirements of modern cad. efficiency of the developed technique is demonstrated via two case studies that deal with optimization of the kinematical and stiffness performances for parallel manipulators of the orthoglide family.
accuracy improvement for stiffness modeling of parallel manipulators. the paper focuses on the accuracy improvement of stiffness models for parallel manipulators, which are employed in high-speed precision machining. it is based on the integrated methodology that combines analytical and numerical techniques and deals with multidimensional lumped-parameter models of the links. the latter replace the link flexibility by localized 6-dof virtual springs describing both translational/rotational compliance and the coupling between them. there is presented detailed accuracy analysis of the stiffness identification procedures employed in the commercial cad systems (including statistical analysis of round-off errors, evaluating the confidence intervals for stiffness matrices). the efficiency of the developed technique is confirmed by application examples, which deal with stiffness analysis of translational parallel manipulators.
structured lft representation of digital lti filters/controllers implementation as a graphic tool. as a powerful method in modeling parametric uncertainties, the linear fractional transformation (lft) scheme in this paper is extended to digital implementation of lti filters/controllers. the convenient coordinate transformation and modification of digital operators can be easily represented under the proposed lft interpretation. moreover, a specialized descriptor system is deduced from this graphic representation, and a systematic procedure of implementation is presented. an example is also provided to illustrate this approach.
a practical strategy of an efficient and simple fwl implementation of lti filters. the problem of finite word length implementation is discussed in this paper. alternatively to the rhodfiit recently proposed by g. li et al., and leaning on the specialized implicit form for a unified analysis, a new effective and sparse structure, named rho-modal realization, is developed. this realization meets simultaneously accuracy (low sensitivity, round-off noise gain and overflow risk), few and flexible computational efforts with a good readability (owing to sparsity), and simplicity (no tricky optimization is involved) as well. two numerical examples are presented to confirm the theoretical results and illustrate the rho-modal realization interest.
k/$\pi$ fluctuations at relativistic energies. we report k/$\pi$ fluctuations from au+au collisions at $\sqrt{s_{nn}}$=19.6, 62.4, 130, and 200 gev using the star detector at the relativistic heavy ion collider. k/$\pi$ fluctuations in central collisions show little dependence on incident energy and are on the same order as those from na49 at the super proton synchrotron in central pb+pb collisions at $\sqrt{s_{nn}}$=12.3 and 17.3 gev. we report results for the collision centrality dependence of k/$\pi$ fluctuations and results for charge-separated fluctuations. we observe that the k/$\pi$ fluctuations scale with the charged particle multiplicity density.
using transformation-aspects in model-driven software product lines. model-driven software product lines (md-spl) are configured by using configuration models and problem space metamodels that capture product line scope. products are derived by means of successive model transformations, starting from problem space models and based on the configuration models. fine-variations of md-spls correspond to characteristics that afect particular elements of models involved in the model transformations. in this paper, we present an approach to create md-spl including fine-variations. we configure products creating fine-feature configurations. then, based on such configurations, we create md-spls using principles of aspects oriented development. thus, our approach allows to derive products including fine-grained details of configuration.
on the optimal design of parallel robots taking into account their deformations and natural frequencies. this paper discusses the utility of using simple stiffness and vibrations models, based on the jacobian matrix of a manipulator and only the rigidity of the actuators, whenever its geometry is optimised. in many works, these simplified models are used to propose optimal design of robots. however, the elasticity of the drive system is often negligible in comparison with the elasticity of the elements, especially in applications where high dynamic performances are needed. therefore, the use of such a simplified model may lead to the creation of robots with long legs, which will be submitted to large bending and twisting deformations. this paper presents an example of manipulator for which it is preferable to use a complete stiffness or vibration model to obtain the most suitable design and shows that the use of simplified models can lead to mechanisms with poorer rigidity.
the quest for the nuclear equation of state. the present status of the efforts to determine the nuclear equation of state from results of heavy ion reactions and from astrophysical observations is reviewed.
elliptic flow of thermal photons at midrapidity in au+au collisions at $\sqrt{s_{nn}}=200$ gev. the elliptic flow $v_{2}$ of thermal photons at midrapidity in au+au collisions at $\sqrt{s_{nn}}=200$gev is predicted, based on three-dimensional ideal hydrodynamics. because of the interplay between the asymmetry and the strength of the transverse flow, the thermal photon $v_{2}$ reaches a maximum at $\pt \sim $ 2gev/$c$ and the $\pt$-integrated $v_{2}$ reaches a maximum at about 50% centrality. the $\pt$-integrated $v_{2}$ is very sensitive to the lower limit of the integral but not sensitive to the upper limit due to the rapid decrease in the spectrum of the transverse momentum.
on the role of initial conditions and final state interactions in ultrarelativistic heavy ion collisions. we investigate the rapidity dependence of the elliptical flow in heavy ion collisions at 200 gev (cms), by employing a three-dimensional hydrodynamic evolution, based on different initial conditions, and different freeze-out scenarios. it will be shown that the form of pseudo-rapidity ($\eta$) dependence of the elliptical flow is almost identical to space-time-rapidity ($\eta_{s}$) dependence of the initial energy distribution, independent of the freeze-out prescriptions.
energy loss of heavy quarks in a qgp with a running coupling constant approach. we show that the effective running coupling constant, $\alpha_{\rm eff}$, and the effective regulator, $\kappa \tilde{m}_{d}^2$, which we used recently to calculate the energy loss, $\frac{de}{dx}$, and the elliptic flow, $v_2$, of heavy quarks in an expanding quark gluon plasma plasma (qgp) are compatible with lattice results and with recently advanced analytical pqcd calculation.
the fluorescence detector of the pierre auger observatory. the pierre auger observatory is a hybrid detector for ultra-high energy cosmic rays. it combines a surface array to measure secondary particles at ground level together with a fluorescence detector to measure the development of air showers in the atmosphere above the array. the fluorescence detector comprises 24 large telescopes specialized for measuring the nitrogen fluorescence caused by charged particles of cosmic ray air showers. in this paper we describe the components of the fluorescence detector including its optical system, the design of the camera, the electronics, and the systems for relative and absolute calibration. we also discuss the operation and the monitoring of the detector. finally, we evaluate the detector performance and precision of shower reconstructions.
a memetic algorithm for the multi-compartment vehicle routing problem with stochastic demands. null
alpha-decay half-lives of new superheavy elements through quasimolecular shapes. null
search for a long-lived di-nuclear system in u+u reactions near the coulomb barrier. null
measurements of phi meson production in relativistic heavy-ion collisions at the bnl relativistic heavy ion collider (rhic). we present results for the measurement of $\phi$ meson production via its charged kaon decay channel $\phi$ -&gt; k+k- in au+au collisions at $\sqrt{s_{nn}}$=62.4,130, and 200 gev, and in p+p and d+au collisions at $\sqrt{s_{nn}}$=200 gev from the star experiment at the bnl relativistic heavy ion collider (rhic). the midrapidity (|y|&lt;0.5)$\phi$ meson transverse momentum (pt) spectra in central au+au collisions are found to be well described by a single exponential distribution. on the other hand, the pt spectra from p+p, d+au, and peripheral au+au collisions show power-law tails at intermediate and high pt and are described better by levy distributions. the constant $\phi$/k- yield ratio vs beam species, collision centrality, and colliding energy is in contradiction with expectations from models having kaon coalescence as the dominant mechanism for $\phi$ production at rhic. the $\omega/\phi$ yield ratio as a function of pt is consistent with a model based on the recombination of thermal s quarks up to pt~4 gev/c, but disagrees at higher transverse momenta. the measured nuclear modification factor, rdau, for the $\phi$ meson increases above unity at intermediate pt, similar to that for pions and protons, while raa is suppressed due to the energy loss effect in central au+au collisions. number of constituent quark scaling of both rcp and v2 for the $\phi$ meson with respect to other hadrons in au+au collisions at $\sqrt{s_{nn}}$=200 gev at intermediate pt is observed. these observations support quark coalescence as being the dominant mechanism of hadronization in the intermediate pt region at rhic.
development of a radio detection array for the observation of showers induced by uhe tau neutrinos. development of a radio detection array for the observation of showers induced by uhe tau neutrinos.
lazy composition of representations in java. abstract. the separation of concerns has been a core idiom of software engineering for decades. in general, software can be decomposed properly only according to a single concern, other concerns crosscut the prevailing one. this problem is well known as “the tyranny of the dominant decomposition”. similarly, at the programming level, the choice of a representation drives the implementation of the algorithms. this article explores an alternative approach with no dominant representation. instead, each algorithm is developed in its “natural” representation and a representation is converted into another one only when it is required. to support this approach, we designed a laziness framework for java, that performs partial conversions and dynamic optimizations while preserving the execution soundness. performance evaluations over graph theory examples demonstrates this approach provides a practicable alternative to a naive one.
a new lower bound for bin packing problem with general conflicts graph. null
a model-driven engineering framework for constrained model search. this document describes a formalization, a solver-independant methodology and implementation alternatives for realizing constrained model search in a model-driven engineering framework. the proposed approach combines model-driven engineering tools ((meta)model transformations, models to text, text to models) and constraint programming techniques. based on previous research, motivations to model search are first introduced together with objectives and background context. a theory of model search is then presented, and a methodology is proposed that details the different involved tasks. concerning implementation, three constraint programming paradigms are envisionned and discussed. an open-source implementation based on the relationnal language alloy is described and available for download.
atmospheric effects on extensive air showers observed with the surface detector of the pierre auger observatory. atmospheric parameters, such as pressure (p), temperature (t) and density, affect the development of extensive air showers initiated by energetic cosmic rays. we have studied the impact of atmospheric variations on extensive air showers by means of the surface detector of the pierre auger observatory. the rate of events shows a ~10% seasonal modulation and ~2% diurnal one. we find that the observed behaviour is explained by a model including the effects associated with the variations of pressure and density. the former affects the longitudinal development of air showers while the latter influences the moliere radius and hence the lateral distribution of the shower particles. the model is validated with full simulations of extensive air showers using atmospheric profiles measured at the site of the pierre auger observatory.
comments on thermodynamics of supersymmetric matrix models. we present arguments that the structure of the spectrum of the supersymmetric matrix model with 16 real supercharges in the large n limit is rather nontrivial, involving besides the natural energy scale ~\lambda^{1/3} = (g^2 n)^{1/3} also a lower scale ~\lambda^{1/3}n^{-5/9}. this allows one to understand a nontrivial behaviour of the mean internal energy of the system, e proportional to t^{14/5}, predicted by ads duality arguments.
measurement of d* mesons in jets from p+p collisions at $\sqrt{s}$=200 gev. we report the measurement of charged d* mesons in inclusive jets produced in proton-proton collisions at a center-of-mass energy sqrt(s)=200 gev with the star experiment at the relativistic heavy ion collider. for d* mesons with fractional momenta 0.2.
aeden : an adaptive framework for dynamic distribution over mobile environments. mobile computing is a domain in great expansion. wireless networks (gsm, satellite, etc) and portable information appliances pias (laptops, pdas, cellular phones, etc) are developing very rapidly. more and more mobile users would like to execute their applications with the same quality of service as on their desktop station, whatever their needs in memory and computing power. using such applications in a mobile environment raises new challenges. some of these applications are extremely costly in system and network resources, whereas pias resources are poor and wireless networks offer a very variable quality of connection. in this paper, we propose an adaptive and dynamic distribution of applications on the local environment to overcome the poorness of available resources on pias, and to reduce and regulate variability effects. moreover, due to the variety of distribution policies, we propose a framework providing adaptive distribution policies.
sequencing and counting with the multicost-regular constraint. this paper introduces a global constraint encapsulating a regular constraint together with several cumulative costs. it is motivated in the context of personnel scheduling problems, where a schedule meets patterns and occurrence requirements which are intricately bound. the optimization problem underlying the multicost-regular constraint is np-hard but it admits an efficient lagrangian relaxation. hence, we propose a filtering based on this relaxation. the expressiveness and the efficiency of this new constraint is experimented on personnel scheduling benchmark instances with standard work regulations. the comparative empirical results show how multicost-regular can significantly outperform a decomposed model with regular and global-cardinality constraints.
the qcd equation of state with thermal properties of phi mesons. in this work a first attempt is made to extract the equation of state (eos) using experimental results of the meson produced in nuclear collisions at ags, sps and rhic energies. the data are confronted to simple thermodynamic expectations and lattice results. the experimental data indicate a first-order phase transition, with a mixed phase stretching energy density between ~1and3.2gevfm−3.
proton radioactivity within a generalized liquid drop model. the proton radioactivity half-lives of spherical proton emitters are investigated theoretically. the potential barriers preventing the emission of protons are determined in the quasimolecular shape path within a generalized liquid drop model (gldm) including the proximity effects between nuclei in a neck and the mass and charge asymmetry. the penetrability is calculated with the wkb approximation. the spectroscopic factor has been taken into account in half-life calculation, which is obtained by employing the relativistic mean field (rmf) theory combined with the bcs method with the force nl3. the half-lives within the gldm are compared with the experimental data and other theoretical values. the gldm works quite well for spherical proton emitters when the spectroscopic factors are considered, indicating the necessity of introducing the spectroscopic factor and the success of the gldm for proton emission. finally, we present two formulas for proton emission half-life calculation similar to the viola-seaborg formulas and royer's formulas of alpha decay.
heavy-quark energy loss observed via muon spectra in pb-pb collisions at root s(nn)=5.5 tev. based on conesa del valle et al (2008 phys. lett. b 663 202), we present the results for the nuclear modification factors raa and rcp of the high transverse momentum (5 &lt; pt &lt; 60 gev/c) distribution of muons in pb–pb collisions at lhc energies, in which two pseudo-rapidity ranges covered by the lhc experiments, |η| &lt; 2.5 and 2.5 &lt; η &lt; 4, are investigated. heavy-quark energy loss is taken into account by mass-dependent bdmps quenching weights. muons from semi-leptonic decays of heavy quarks (c and b) and from leptonic decays of weak gauge bosons (w and z) are the main contributions to the muon pt distribution above a few gev/c. the muons from w and z, that dominates the high pt, can be used as a medium-blind reference to observe the medium-induced suppression of beauty quarks.
tomography of the quark gluon plasma by heavy quarks. using the recently published model \cite{gossiaux:2008jv,goss2} for the collisional energy loss of heavy quarks in a quark gluon plasma (qgp), based on perturbative qcd (pqcd), we study the centrality dependence of $r_{aa}$ and $r_{aa}(p_t^{min})$, %= \frac{dn_{aa}/dp_t}{ dn_{pp}/dp_t}$ measured by the phenix collaboration, and compare our model with other approaches based on pqcd and on anti de sitter/ conformal field theory (ads/cft).
kaon interferometric probes of space-time evolution in au+au collisions at sqrt(s_nn) = 200 gev. bose-einstein correlations of charged kaons are measured for au+au collisions at sqrt(s_nn) = 200 gev and are compared to charged pion probes, which have a larger hadronic scattering cross section. three dimensional gaussian source radii are extracted, along with a one-dimensional kaon emission source function. the centrality dependences of the three gaussian radii are well described by a single linear function if n_part^1/3 with zero intercept. imaging analysis shows a deviation from a gaussian tail at r &gt;~ 10 fm, although the bulk emission at lower radius is well-described by a gaussian. the presence of a non-gaussian tail in the kaon source reaffirms that the particle emission region in a heavy ion collision is extended, and that similar measurements with pions are not solely due to the decay of long-lived resonances.
eu(iii) sorption to tio2 (anatase and rutile): batch, xps, and exafs studies. the sorption of eu(iii) on anatase and rutile was studied as a function of ionic strength, humic acid (ha, 7.5 mg/l), and electrolyte anions over a large range of ph (2−12). the presence of ha significantly affected eu(iii) sorption to anatase and rutile. the sorption of eu(iii) on anatase and rutile was independent of ionic strength. results of an x-ray photoelectron spectroscopy (xps) analysis showed that eu(iii) was chemically present within the near-surface of tio2 due to the formation of soeu and sohaeu complexes. an extended x-ray absorption fine structure (exafs) technique was applied to characterize the local structural environment of the adsorbed eu(iii), and the results indicated that eu(iii) was bound to about seven or eight o atoms at a distance of about 2.40 å. the functional groups of surface-bound ha were expected to be involved in the sorption process. the measured eu−ti distance confirmed the formation of inner-sphere sorption complexes on a tio2 surface.
kinematic control of a robot-positioner system for arc welding application. null
combined routing and staff scheduling model for home health care. the combined routing and scheduling problem has become a relevant issue in health care logistics as demand for cost effective and quality health services rises. the main challenge of this problem is to combine aspects of routing and staff scheduling. both composing problems are well known as combinatorial optimization problems and there is an extensive literature of exact and heuristic methods for each of them. although combined routing and scheduling problems arise in many applications, only few works have been developed in the field of home health care logistics. we consider a set of patients with different geographical locations who require different medical treatments composed by a list of medical procedures. for each patient, the duration, frequency and hours in which the medical treatment must be applied are known. each medical procedure must be performed by a medical staff who has the adequate qualifications. medical staff professionals are also located in different geographical points, and each staff member is assigned to work shifts while respecting work legal guidelines. the problem that arises in this system is scheduling the activities of each staff member and sequencing visits to patients such that overall logistic costs are minimized and a high quality service is performed. desired conditions that must be satisfied in the system include the accomplishment of customer medical treatments in terms of time windows, and precedence and synchronization constraints of medical procedures. in terms of medical staff, work shifts must be respected, geographical location must be considered, assignment of visits must respect required qualifications, and balanced work loads are expected. the overall objective of this work is to propose a formulation of the combined routing and scheduling problem in the context of the home health care services. we propose a mixed integer linear program and demonstrate the high complexity of the resulting problem through computational experiments. research and improvement opportunities in the field are also detailed.
a tabu search based heuristic for the transportation of mentally disabled people. transportation of mentally disabled persons is subject to many specific constraints due to medical or organisational reasons. we consider the problem of the daily transportation of persons from their home to specialized medical or social institutions. this concerns either mentally disabled children who attend specialised schools, or adults working in vocational rehabilitation centres. most of them are unable to travel on their own, so that a dedicated transportation system must be managed by the centres. daily inbound and return trips for dozens of persons are generally performed by costly taxis or minibuses. we model the underlying optimisation problem as a multi-periodic vehicle routing problem with time windows, heterogeneous fleet and additional constraints. the main characteristic of the model is to mix closed routes (that form loops from the centre) and open routes from the driver's home to the centre. we propose two objective functions aimed at minimising the transportation cost or finding a trade-off between the cost and the quality of service respectively. our approach for solving this problem efficiently and rapidly is a three phase heuristic. the first phase addresses a so called standard problem where every person is located at their main address with the most classical time window. initial routes are built through a best insertion procedure based on a regret calculation. the second phase is a tabu search that combines several classical moves of the literature: customer relocation, customer exchange, cross-exchange, string exchange etc. the third step considers the real daily demand of every person. this step integrates possible daily variations in the demand: possible absence, variations in the time windows, daily changes in the addresses (pick-up or delivery at the parents, home family, nurse, etc.). the algorithm has been embedded in a decision support system called marika. this decision support system seeks both technical efficiency and the satisfaction of human concerns. we present numerical results obtained from both test instances from the vrp literature and real instances. we show that the average of the individual travelling times as well as the total travelling time of the vehicles can be reduced by around 15% for real instances. we finally enumerate some research perspectives. a first improvement is to obtain consistent schedules for every person. the idea is that daily variations in the demand of some transported persons should have limited impact on the schedules of other persons. this issue is important when applied to a fragile population. a second objective is to look for a trade-off between the cost, the quality and the consistency of service through a multi-objective optimization. the last project research concerns the possibility for distinct centres to organize common transportation services in order to reduce the costs.
binary reaction decays from $^{24}$mg+$^{12}c. charged particle and gamma decays in 24mg* are investigated for excitation energies where quasimolecular resonances appear in 12c+12c collisions. various theoretical predictions for the occurence of superdeformed and hyperdeformed bands associated with resonance structures with low spin are discussed within the measured 24mg* excitation energy region. the inverse kinematics reaction 24mg+12c is studied at e_lab(24mg) = 130 mev, an energy which enables the population of 24mg states decaying into 12c+12c resonant break-up states. exclusive data were collected with the binary reaction spectrometer in coincidence with euroball iv installed at the vivitron tandem facility at strasbourg. specific structures with large deformation were selectively populated in binary reactions and their associated gamma decays studied. coincident events associated with inelastic and alpha-transfer channels have been selected by choosing the excitation energy or the entry point via the two-body q-values. the analysis of the binary reaction channels is presented with a particular emphasis on 24mg-gamma, 20ne-gamma and 16o-gamma coincidences. new information (spin and branching ratios) is deduced on high-energy states in 24mg and 16o, respectively.
neutron correlations in 6he viewed through nuclear break-up. the nuclear break-up of 6he on a 208pb target was studied at 20amev using a secondary beam of 6he produced by the spiral facility at ganil. α-particles were detected in coincidence with two neutrons with a large angular coverage and the reaction mechanism was identified. from the distribution of the relative angles between the two neutrons the correlation function was extracted. it shows a strong correlation at small relative angles attributed to the contribution of the di-neutron configuration of 6he.
electron energy loss spectroscopy of lixtip4, a possible negative electrode for lithium-ion batteries. null
eels determination of lithium insertion sites in a possible new negative electrode for li-ion battery. null
epos model and ultra high energy cosmic rays. interpretation of extensive air showers (eas) experiments results is strongly based on air shower simulations. the latter being based on hadronic interaction models, any new model can help for the understanding of the nature of cosmic rays. the epos model reproducing all major results of existing accelerator data (including detailed data of rhic experiments) has been introduced in air shower simulation programs corsika and conex few years ago. the new epos 1.99 has recently been updated taking into account the problem seen in eas development using epos 1.61. we will show in details the relationship between some epos hadronic properties and eas development, as well as the consequences on the model and finally on cosmic ray analysis.
systematic studies of elliptic flow measurements in au+au collisions at sqrt(s_nn) = 200 gev. we present inclusive charged hadron elliptic flow v_2 measured over the pseudorapidity range |\eta| &lt; 0.35 in au+au collisions at sqrt(s_nn) = 200 gev. results for v_2 are presented over a broad range of transverse momentum (p_t = 0.2-8.0 gev/c) and centrality (0-60%). in order to study non-flow effects that are not correlated with the reaction plane, as well as the fluctuations of v_2, we compare two different analysis methods: (1) event plane method from two independent sub-detectors at forward (|\eta| = 3.1-3.9) and beam (|\eta| &gt; 6.5) pseudorapidities and (2) two-particle cumulant method extracted using correlations between particles detected at midrapidity. the two event-plane results are consistent within systematic uncertainties over the measured p_t and in centrality 0-40%. there is at most 20% difference of the v_2 between the two event plane methods in peripheral (40-60%) collisions. the comparisons between the two-particle cumulant results and the standard event plane measurements are discussed.
determination of stability constants between complexing agents and at(i) and at(iii) species present at ultra-trace concentrations. a competition method is proposed to determine the complexation constants between at(i) and at(iii) species and complexing agents. the method, tested with an inorganic ligand, thiocyanate ion (scn−), and an organic macromolecule, thiacalix[4]arenetetrasulfonate (lh4) is based on solid/liquid separation or liquid/liquid extraction. for the solid/liquid separation, the cationic exchanger dowex 50x8 was used. the interaction of at(i) and at(iii) with the cationic exchanger is specific but could not be described by the expected cation exchange process. most probably, at(i, iii) interacts with a “strong” site (in weak amount) to form a surface complex at the surface of the resin organic skeleton. for the liquid/liquid separation, chloroform, toluene and hexane were used. all solvents extract astatine species with distribution coefficients varying between 0.7 and 120. the extraction process was shown to be independent of aqueous phase characteristics (ph, ionic strength) and was explained by the solvation of astatine species by the organic solvent. the effect of the addition of the thiacalix[4]arenetetrasulfonate on the solid/liquid or liquid/liquid distribution coefficients could be well described by the formation of a 1:1 complex with stability constants of log β1 = 4.5 ± 0.4 and 3.3 ± 0.3 for at(i) and at(iii), respectively. for the thiocyanate ion, the data measured in the presence of the organic solvents could be explained by the formation of both 1:1 and 1:2 at:scn complexes. in the case of the solid/liquid separation, data analysis was hampered by the probable formation of a ternary complex between at(i, iii), scn− and the functional groups of the resin. as for the calixarene, the interaction strength appeared slightly higher for at(i) (log β2 = 5.9 ± 0.3 and log β1 = 3.8 ± 0.2 for 1:2 and 1:1 complexed species, respectively) than for at(iii) (log β2 = 5.3 ± 0.2 and log β1 = 2.8 ± 0.2 for 1:2 and 1:1 complexed species, respectively).
extraction of astatine-211 in diisopropylether (dipe). the extraction mechanism of astatine-211 in diisopropylether (dipe) is studied by analyzing the effect of the nature of the aqueous solution and organic solvent on the distribution coefficients characterizing the ratio of organic and aqueous concentration of astatine (d). on the one hand, at a given ph value, d was shown to be independant of the nature and concentration of the counter-ion present in the aqueous solution (cl-, clo4-, no3-). on the other hand, the nature of the organic solvent had a strong effect. d increases as the polarity of the organic solvent increases. these experimental observations are explained by the solvation of astatine by the organic solvent. the back-extraction of astatine from the organic to the aqueous phase is efficient in basic conditions. this is explained by the formation of a hydrolyzed species of astatine presenting no affinity for dipe. hydrolysis constants of astatine are deduced from experimental data (logβ1=-3.0±0.1 and logβ2=-14.2±0.1).
making bound consistency as effective as arc consistency. we study under what conditions bound consistency (bc) and arc consistency (ac), two forms of propagation used in constraint solvers, are equivalent to each other. we show that they prune exactly the same values when the propagated constraint is connected row convex / closed under median and its complement is row convex. this characterization is exact for binary constraints. since row convexity depends on the order of the values in the domains, we give polynomial algorithms for computing orders under which bc and ac are equivalent, if any.
growth of long range forward-backward multiplicity correlations with centrality in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. forward-backward multiplicity correlation strengths have been measured for the first time with the star detector for au+au and $\textit{p+p}$ collisions at $\sqrt{s_{nn}}$ = 200 gev. strong short and long range correlations are seen in central (0-10%) au+au collisions. the magnitude of these correlations decrease with decreasing centrality until only short range correlations are observed in 40-50% au+au collisions. the results are in agreement with predictions from the dual parton and color glass condensate models.
in-target yields forradioactive ion beam (rib) production with eurisol. eurisol ds (europe isotope separation on-line design study) project is the european common effort in planning a next generation rib factory able to deliver secondary beams up to 1013 pps at energies up to 150 mev u-1. the proposed schematic layout of the facility is based on four target stations, three direct targets of 100 kw of beam power and one multi-mw target two stages assembly. being produced via spallation the ribs produced in the direct targets are mainly proton rich. while in the multi-mw target high intensity ribs of neutron rich isotopes are produced by fission in actinide targets placed in the fast neutron spectrum given by a liquid metal spallation source. the purpose of this paper is to summarize the work carried out within task 11 “beam intensity calculations” with special emphasis to the estimation of the in-target yield intensities produced in the various target configurations. benchmark studies were performed initially in order to verify the accurate description of the spallation models used by the mcnpx2.5.0 code and to choose the best options to be used for the present work requirements. the predictions of the code tested against measured data are presented. several calculations using mcnpx2.5.0 combined with the evolution code cinder'90 were carried out to assess the performance of the direct targets. a complex analysis was performed to study the in-target production rib intensities varying with various parameters: target dimensions, materials and incident proton beam energies. the optimized configurations for the targets together with the corresponding quantitative estimates of the production rates for all interested nuclei resulted from this investigation are discussed.
the nucifer experiment : reactor antineutrino detection for reactor monitoring. during the last decades, tremendous progresses have been achieved on the fundamental knowledge and detection of neutrinos which give new opportunities of applied neutrino physics. among them, antineutrinos could be exploited for two nuclear reactor monitoring applications: the thermal power measurement and the control of the isotopic composition of the reactor fuel. this application arouses the international atomic energy agency (iaea) interest as a potential new safeguard tool. the nucifer detector, under development in france, will be dedicated to applied neutrino physics. the design of the detector takes advantage of the technical improvements performed for fundamental neutrino experiments such as double chooz. nucifer will be tested within the next two years at the osiris (saclay-france) and the ill (grenoble-france) research reactors. after an brief overview on the worldwide effort in the field of reactor monitoring with antineutrinos, the nucifer experiment will be presented, as well as monte-carlo pwr and candu reactor simulations and the method to compute the antineutrino energy spectrum using nuclear databases. the expected response of the nucifer detector to diversion scenarios in pwr and candu reactors will be shown.
pion interferometry in au+au and cu+cu collisions at $\sqrt{s_{nn}}$ = 62.4 and 200gev. we present a systematic analysis of two-pion interferometry in au+au collisions at $\sqrt{s_{\rm{nn}}}$ = 62.4 gev and cu+cu collisions at $\sqrt{s_{\rm{nn}}}$ = 62.4 and 200 gev using the star detector at rhic. the multiplicity and transverse momentum dependences of the extracted femtoscopic radii are studied. the scaling of the apparent freeze-out volume with charged particle multiplicity is studied for the rhic energy domain. the multiplicity scaling of the measured radii is found to be independent of colliding system and collision energy.
j/psi production at high transverse momentum in p+p and cu+cu collisions at \snn=200gev. the star collaboration at rhic presents measurements of \jpsi$\to{e^+e^-}$ at mid-rapidity and high transverse momentum ($p_t&gt;5$ gev/$c$) in \pp and central \cucu collisions at \snn = 200 gev. the inclusive \jpsi production cross section for \cucu collisions is found to be consistent at high $p_t$ with the binary collision-scaled cross section for \pp collisions, in contrast to previous measurements at lower $p_t$, where a suppression of \jpsi production is observed relative to the expectation from binary scaling. azimuthal correlations of $j/\psi$ with charged hadrons in \pp collisions provide an estimate of the contribution of $b$-meson decays to \jpsi production of $13% \pm 5%$.
cut generation for an integrated employee timetabling and production scheduling problem. this paper investigates the integration of the employee timetabling and production scheduling problems. at the first level, we manage a classical employee timetabling problem. at the second level, we aim at supplying a feasible production schedule for a set of interruptible tasks with qualification requirements and time-windows. instead of hierarchically solving these two problems as in the current practice, we try here to integrate them and propose two exact methods to solve the resulting problem. the former is based on a benders decomposition while the latter relies on a specific decomposition and a cut generation process. the relevance of these different approaches is discussed here through experimental results.
upper limit on the cosmic-ray photon fraction at eev energies from the pierre auger observatory. from direct observations of the longitudinal development of ultra-high energy air showers performed with the pierre auger observatory, upper limits of 3.8%, 2.4%, 3.5% and 11.7% (at 95% c.l.) are obtained on the fraction of cosmic-ray photons above 2, 3, 5 and 10 eev (1 eev = 10^18 ev) respectively. these are the first experimental limits on ultra-high energy photons at energies below 10 eev. the results complement previous constraints on top-down models from array data and they reduce systematic uncertainties in the interpretation of shower data in terms of primary flux, nuclear composition and proton-air cross-section.
system size dependence of associated yields in hadron-triggered jets. we present results on the system size dependence of high transverse momentum di-hadron correlations at $\sqrt{s_{nn}}$ = 200 gev as measured by star at rhic. measurements in d+au, cu+cu and au+au collisions reveal similar jet-like correlation yields at small angular separation ($\delta\phi\sim0$, $\delta\eta\sim0$) for all systems and centralities. previous measurements have shown that the away-side yield is suppressed in heavy-ion collisions. we present measurements of the away-side suppression as a function of transverse momentum and centrality in cu+cu and au+au collisions. the suppression is found to be similar in cu+cu and au+au collisions at a similar number of participants. the results are compared to theoretical calculations based on the parton quenching model and the modified fragmentation model. the observed differences between data and theory indicate that the correlated yields presented here will provide important constraints on medium density profile and energy loss model parameters.
the fréchet contingency array problem is max-plus linear. in this paper we show that the so-called array fréchet problem in probability/statistics is (max, +)-linear. the upper bound of fréchet is obtained using simple arguments from residuation theory and lattice distributivity. the lower bound is obtained as a loop invariant of a greedy algorithm. the algorithm is based on the max-plus linearity of the fréchet problem and the monge property of bivariate distribution.
stiffness analysis of overconstrained parallel manipulators. the paper presents a new stiffness modeling method for overconstrained parallel manipulators with flexible links and compliant actuating joints. it is based on a multidimensional lumped-parameter model that replaces the link flexibility by localized 6-dof virtual springs that describe both translational/rotational compliance and the coupling between them. in contrast to other works, the method involves a fea-based link stiffness evaluation and employs a new solution strategy of the kinetostatic equations for the unloaded manipulator configuration, which allows computing the stiffness matrix for the overconstrained architectures, including singular manipulator postures. the advantages of the developed technique are confirmed by application examples, which deal with comparative stiffness analysis of two translational parallel manipulators of 3-puu and 3-prpar architectures. accuracy of the proposed approach was evaluated for a case study, which focuses on stiffness analysis of orthoglide parallel manipulator.
introduction to the experimental study of hadronic matter in heavy ion collisions. the quark gluon plasma. in the last 20 years, heavy ion collisions have been an unique way to study the hadronic matter in the laboratory. the phase diagram of hadronic matter remains unknown, although many experimental and theoretical studies have been done in the last decennia, aiming at studying its phase transitions. after a general introduction, two phases transition of the hadronic matter, liquid-gas and the transition to the quark gluon plasma, are addressed. a general view about the experimental methods to study these phase transitions is presented in chapter three. the most important results of the heavy ion program in the rhic collider at bnl (upton, ny, usa) are presented in chapter four. the last three chapters are devoted to the heavy ion program in the future lhc collider at cern (geneva, switzerland). in particular, the unique lhc experiment specially designed for heavy ion physics, alice and its muon spectrometer are presented.
interfacial tension for studying asphaltene flocculation under pressure. null
an experimental investigation of liquid-solid phase transition in crude oils under high pressure conditions. null
limit on the diffuse flux of ultra-high energy tau neutrinos with the surface detector of the pierre auger observatory. data collected at the pierre auger observatory are used to establish an upper limit on the diffuse flux of tau neutrinos in the cosmic radiation. earth-skimming ντ may interact in the earth's crust and produce a τ lepton by means of charged-current interactions. the τ lepton may emerge from the earth and decay in the atmosphere to produce a nearly horizontal shower with a typical signature, a persistent electromagnetic component even at very large atmospheric depths. the search procedure to select events induced by τ decays against the background of normal showers induced by cosmic rays is described. the method used to compute the exposure for a detector continuously growing with time is detailed. systematic uncertainties in the exposure from the detector, the analysis, and the involved physics are discussed. no τ neutrino candidates have been found. for neutrinos in the energy range 2×1017ev.
mobile fission and activation products in nuclear waste disposal. when disposing nuclear waste in clay formations it is expected that the most radiotoxic elements like pu, np or am move only a few centimetres to meters before they decay. only a few radionuclides are able to reach the biosphere and contribute to their long-term exposure risks, mainly anionic species like i129, cl36, se79 and in some cases c14 and tc99, whatever the scenario considered. the recent oecd/nea cosponsored international mofap workshop focussed on transport and chemical behaviour of these less toxic radionuclides. new research themes have been addressed, such as how to make use of molecular level information for the understanding of the problem of migration at large distances. diffusion studies need to face mineralogical heterogeneities over tens to hundreds of meters. diffusion rates are very low since the clay rock pores are so small (few nm) that electrostatic repulsion limits the space available for anion diffusion (anion exclusion). the large volume of traversed rock will provide so many retention sites that despite weak retention, even certain of these “mobile” nuclides may show significant retardation. however, the question how to measure reliably very low retention parameters has been posed. an important issue is whether redox states or organic/inorganic speciation change from their initial state at the moment of release from the waste during long term contact with surfaces, hydrogen saturated environments, etc.
interaction of selenite with mx-80 bentonite: effect of minor phases, ph, selenite loading, solution composition and compaction. we propose a simple model describing the retention of selenite, se(iv), by the mx-80 bentonite in a synthetic groundwater (sgw) in dispersed and compacted states. the model was calibrated on a pure montmorillonite from data obtained for 4 &lt; ph &lt; 10 and total selenite concentrations between 10−7 and 5 × 10−3 mol/l. furthermore, the matrix solution covers a wide range of conditions regarding the ionic strength and the nature of the background electrolyte. three selenite surface species had to be considered. below ph 5, sorption is governed by a ligand exchange reaction with h2seo3. between ph 5 and 7, the experimental data are well described considering the formation of a surface complex implying hseo3−. finally, at ph above 7, we propose ternary surface complexes involving ca2+ and mg2+. the model, which we consider as operational, appears in agreement with spectroscopic data available in the literature and predicts surprisingly well selenite sorption on mx-80 bentonite even in the compacted state at a dry density of 1100 kg/m3. based on the model, the solid phase montmorillonite is responsible for selenite retention. minor solid phases containing iron (pyrite, hematite) had not to be considered in the modelling. interestingly, calcite (an important phase in mx-80 bentonite) has an indirect effect via the release of calcium into solution and its subsequent contribution to se(iv) sorption through a ternary surface complex.
self-consistent dynamical mean-field investigation of exotic structures in isospin-asymetric nuclear matter. the exotic structures expected in the outermost layer of neutron stars are investigated in a new approach. it is based on the dynamical wavelets in nuclei (dywan) model of nuclear collisions. this microscopic dynamical approach is an extended time-dependent hartree-fock description based on a wavelet representation. the model addresses the dynamical exploration of complex nuclear structures, beyond the wigner-seitz (ws) approximation and without any assumption on their final shapes. the present study focuses on exotic phases of cold matter evidenced dynamically at sub-saturation densities, currently within a pure mean field framework, before tackling the effects of the multi-particle correlations in a forthcoming study. starting from inhomogeneous initial conditions provided by nuclei located on an initial crystalline lattice, the exotic structures result from a dynamical self-consistent treatment where, in principle, the nuclear system can freely self-organize, modify the lattice structure or even break the lattice and the initial matter distribution symmetries. in this work nuclei are initially slightly excited with low-lying collective modes. the system can then explore geometrical configurations with similar energies, without being trapped in the vicinity of a local minimum. in this quantum framework, different effects are analyzed, among them the sensitivity to the equation of state and to the proton fraction.
photoproduction of j/psi and of high mass e+e- in ultra-peripheral au+au collisions at sqrt(s_nn) = 200 gev. we present the first measurement of photoproduction of j/psi and of two-photon production of high-mass e+e- pairs in electromagnetic (or ultra-peripheral) nucleus-nucleus interactions, using au+au data at sqrt(s_nn) = 200 gev. the events are tagged with forward neutrons emitted following coulomb excitation of one or both au^{star} nuclei. the event sample consists of 28 events with m_{e+e-} &gt; 2 gev/c^2 with zero like-sign background. the measured cross sections at midrapidity of d\sigma / dy (j/psi + xn, y=0) = 76 +/- 33 (stat) +/- 11 (syst) micro b and d^2\sigma/dm dy (e^+e^- + xn, y=0) = 86 +/- 23 (stat) +/- 16 (syst) micro b/(gev/c^2) for m_{e+e-} \in [2.0,2.8] gev/c^2 are consistent with various theoretical predictions.
high spatial resolution in beta-imaging with a pim device. the autoradiography is an imaging technique able to locate in 2 dimensions molecules labeled with p radioactive emitters. the usual technique in use is based on solid device, mostly films or phosphor screens. we report on tests performed with pim (parallel ionization multiplier) on microscope slides demonstrating that mpgd incorporating micromegas micromeshes have to be considered for a next generation of beta-imager. the main advantages come from the high spatial resolution measured (25 gin fwhm in two dimensions) and from the event by event reconstruction position measurement resulting in "on- line" imaging availability in which no "after development" have to be performed.
energy and system size dependence of $\phi$ meson production in cu+cu and au+au collisions. we study the beam-energy and system-size dependence of $\phi$ meson production (using the hadronic decay mode $\phi$→k+k−) by comparing the new results from cu+cu collisions and previously reported au+au collisions at $\sqrt{s_{nn}}$ = 62.4 and 200 gev measured in the star experiment at rhic. data presented in this letter are from mid-rapidity (|y|&lt;0.5) for 0.4.
design and performances of a fully autonomous antenna for radio detection of extensive air showers. the use of the radio-detection technique in a wide area cosmic-ray detector requires autonomous antenna stations, in terms of power feeding, triggering and data transmission. a prototype has been tested at the nançay radio observatory (france). it uses the broadband (1-200 mhz) active dipoles installed on the codalema experiment (see other contributions in this conference), together with a solar power supply, an independent trigger electronics and a dedicated communication system. we present here the complete setup and the performances of this new kind of detector.
the alice electromagnetic calorimeter. null
pi0 measurement with the alice photon spectrometer in first p+p collisions at the lhc. null
generation of complete events containing very high pt jets. the study of very high transverse-momentum jets will be an important issue at the lhc, in particular since the corresponding cross sections will be considerably larger than at rhic energies. jets are expected to provide information on qgp formation, due to the energy loss of fast partons in the medium. jet cross sections can in principle be compared to simple pqcd calculations, based on the hypothesis of factorization. but often it is useful or even necessary to not only compute the production rate of the very high-p t jets, but in addition the “rest of the event”. the proposed talk is based on recent work, where we try to construct an event generator—fully compatible with pqcd—which allows one to compute complete events, consisting of high-p t jets plus all the other low p t particles produced at the same time. whereas in “generators of inclusive spectra” like pythia one may easily trigger on high-p t phenomena, this is not so obvious for “generators of physical events”, where in principle one has to generate a very large number of events in order to finally obtain rare events (like those with a very high-p t jet). we shall discuss how we overcome these difficulties in the framework of the epos model.
extensions of logical analysis of data for medical applications. logical analysis of data (lad) has already been applied on several biomedical applications [bonates and hammer, 2006]. in this talk we shall present two developments designed to cope with new requirements. for a problem of height-loss due to irradiation during childhood, we shall show how combinatorial regression - an extension of lad to numerical outputs (instead of the classical binary ones) - allowed to precisely characterize the influences of several variables for a problem of height-loss due to irradiation during childhood. for the diagnosis of growth hormone deficiency, there is a need for very simple (e.g., straightforwardly usable by a gp) and accurate classifiers. we shall show an the concept of functional patterns allowed to meet this requirement by aggregating many boolean patterns into one single (non-binary) condition.
transaction activation scheduling support for transactional memory. transactional memory (tm) is considered as one of the most promising paradigms for developing concurrent applications. tm has been shown to scale well on multiple cores when the data access pattern behaves “well,” i.e., when few conflicts are induced. in contrast, data patterns with frequent write sharing, with long transactions, or when many threads contend for a smaller number of cores, produce numerous aborts. these problems are traditionally addressed by application-level contention managers, but they suffer from a lack of precision and provide unpredictable benefits on many workloads. in this paper, we propose a system approach where the scheduler tries to avoid aborts by preventing conflicting transactions from running simultaneously. we use a combination of several techniques to help reduce the odds of conflicts, by (1) avoiding preempting threads running a transaction until the transaction completes, (2) keeping track of conflicts and delaying the restart of a transaction until conflicting transactions have committed, and (3) keeping track of conflicts and only allowing a thread with conflicts to run at low priority. our approach has been implemented in linux for software transactional memory (stm) using a shared memory segment to allow fast communication between the stm library and the scheduler. it only requires small and contained modifications to the operating system. experimental evaluation demonstrates that our approach significantly reduces the number of aborts while improving transaction throughput on various workloads.
efficient labeling and constraint relaxation for solving time tabling problems. null
a new lower bound for the open-shop problem. null
improving branch-and-bound algorithms for open-shop problems. null
combining ai/or techniques for solving open-shop problems. null
loading aircrafts for military operations. null
a column generation approach for a two-stage hybrid flow-shop problem. null
explanation-based repair techniques for solving dynamic scheduling problems. null
condition-based maintenance models for deteriorating systems in stressful environments. null
a predictive maintenance policy based on two explicative variables. null
integration of environmental stress factors in maintenance optimization. null
a predictive maintenance policy combining statistical process control and condition-based approaches. null
condition-based maintenance approaches for deteriorating system influenced by environmental conditions. null
comparison of health monitoring strategies for a gradually deteriorating system in a stressfull envirnment. null
maintenance policy for non-stationary deteriorating system. null
combining statistical process control and condition-based maintenance for gradually deteriorating systems subject to stress. null
solving dynamic rcpsp using explanation-based constraint programming. null
a simulated annealing method for the flow-shop with a new blocking constraint. null
vehicle routing optimization for the pick-up and delivery: application to the transport of personnel on oil platforms. null
building university timetables using constraint logic programming. null
a constraint programming approach to integrated routing and packing problems. null
two-dimensional pickup and delivery routing problem with loading constraints. null
single machine customer order scheduling to minimize total weighted late orders. null
customer order scheduling on a single machine with release dates. null
single machine scheduling of jobs with time window dependent processing times. null
two local search approaches for minimizing total weighted tardiness in a job shop with customer orders. null
reactive approaches. null
complexity of flowshop scheduling problems with a new blocking constraint. null
heuristics for the two-stages hybrid flowshop with a new blocking constraint. null
constraint programming for dynamic scheduling problems. null
a memetic algorithm for a pick-up and delivery problem by helicopter. null
a branch-and-price algorithm to minimize the maximum lateness on a batch processing machine. null
vehicle routing in a public utility: a decision support system with hybrid techniques. null
a genetic algorithm for the multi-compartment vehicle routing problem with stochastic demands. null
design and implementation of a decision support system in a public utility. null
construction heuristics for the multi-compartment vehicle routing problem with stochastic demands. null
jcw: an object-oriented framework for the rapid development of vehicle routing heuristics based on savings. null
integration and propagation of a multicriteria decision model in constraint programming. in this paper we propose a general integration scheme for a multi-criteria decision making model of the multi-attribute utility theory in constraint programming . we introduce the choquet integral as a general aggregation function for multi-criteria optimization problems and define the choquet global constraint that propagates this function during the search. finally the benefit of the propagation of the choquet constraint are evaluated on the examination timetabling problem.
new methods for htr fuel waste management. considering the need to reduce waste production and greenhouse emissions by still keeping high energy efficiency, various 4th generation nuclear energy systems have been proposed. as far as graphite moderated reactors are concerned, one of the key issues is the large volumes of irradiated graphite encountered (1770 m3 for fuel elements and 840 m3 for reflector elements during the lifetime (60 years) of a single reactor module [1]). with the objective to reduce volume of waste in the htr concept, it is very important to be able to separate the fuel from low level activity graphite. this requires to separate triso particles from the graphite matrix with the sine qua non condition to not break triso particles in case of future embedding of particles in a matrix for disposal. according to national regulatory systems, in case of limited graphite waste production or of short duration htr projects (e.g. in germany), direct disposal without separation is acceptable. nevertheless, in case of large scale deployment of htr technology, such approach is not economical and sustainable. previous attempts in graphite management (furnace, fluidised bed and laser incinerations and encapsulation matrices) dealt with graphite matrix only. these are the reasons why we studied the management of irradiated compact-type fuel element. we simulated the presence of fuel in the particles by using zro2 kernels. compacts with zro2 triso particles were manufactured by areva np. two original methods have been studied. first, we tested high pressure jet to erode graphite and clean triso particles. best erosion rate reached about 0.18 kg/h for a single nose ending. examination of treated graphite showed a mixture of undamaged triso particles, particles that have lost the outer pyrolytic carbon layer and zro2 kernels. secondly, we studied the thermal shock method by immerging successively graphite into liquid nitrogen and hot water to cause fracturing of the compact. this produced particles and graphite fragments with diameter ranging from several centimetres to less than 500 µm. this relatively simple and economic method may potentially be considered as a pre-treatment step and be coupled with other method(s) before reprocessing and recycling for example.
classical and new heuristics for the open-shop problem, a computational evaluation. null
using intelligent backtracking to improve branch and bound methods: an application to open-shop problems. null
new tools for solving dynamic timetabling problems. null
conflict-based repair techniques for solving dynamic scheduling problems. null
stable solutions for dynamic project scheduling problems. null
maintenance policy for deteriorating system evolving in a stressful environment. this paper deals with the maintenance optimization of a system subject to a stressful environment. the system deterioration behaviour can be modified by the environment; the degradation mode can change due to the random evolution of the stressful environment. reciprocally, the environment conditions can be influenced by the system state and as a consequence, a change in the environment can be an indicator of the system state. this paper describes a condition-based maintenance decision framework to tackle the potential variations in the system deterioration, and especially in the deterioration rate, and the new information on the system state given by the evolution of the environmental variables.
classical and new heuristics for the open-shop problem. null
using intelligent backtracking to improve branch-and-bound methods. null
loading aircraft for military operations. null
complexity of flow-shop scheduling problems with a new blocking constraint. null
predictive maintenance policy for a gradually deteriorating system subject to stress. this paper deals with a predictive maintenance policy for a continuously deteriorating system subject to stress. we consider a system with two failure mechanisms which are, respectively, due to an excessive deterioration level and a shock. to optimize the maintenance policy of the system, an approach combining statistical process control (spc) and condition-based maintenance (cbm) is proposed. cbm policy is used to inspect and replace the system according to the observed deterioration level. spc is used to monitor the stress covariate. in order to assess the performance of the proposed maintenance policy and to minimize the long-run expected maintenance cost per unit of time, a mathematical model for the maintained system cost is derived. analysis based on numerical results are conducted to highlight the properties of the proposed maintenance policy in respect to the different maintenance parameters.
adaptation of models to evolving metamodels. the problem of automatic or semi-automatic adaptation of models to their evolving metamodels is gaining importance in the model-driven community. recent approaches propose to adapt models using predefined information (i.e., a trace of changes). unfortunately, this information is not always available in practice. in many situations metamodels evolve without keeping track of the applied changes. we propose a more general two step solution. first step computes equivalences and differences between the metamodels and saves these into a ``weaving model''. this weaving model acts as a high-level specification of adaptation transformation. second step translates this model into an executable transformation. this technical report shows the results obtained in applying the approach on two concrete scenarios: a petri net metamodel, and the netbeans java metamodel.
development of a radio-detection method array for the observation of ultra-high energy neutrino induced showers. the recent demonstration by the codalema collaboration of the ability of the radio-detection technique for the characterization of uhe cosmic-rays calls for the use of this powerful method for the observation of uhe neutrinos. for this purpose, an adaptation of the existing 21cm array (china) is presently under achievment. in an exceptionally low electromagnetic noise level, 10160 log-periodic 50-200 mhz antennas sit along two high valleys, surrounded by mountain chains. this lay-out results in 30-60 km effective rock thicnesses for neutrino interactions with low incidence trajectories along the direction of two 4-6 km baselines. we will present first in-situ radio measurements demonstrating that this environment shows particularly favourable conditions for the observation of electromagnetic decay signals of taus originating from the interaction of 10^17-20 ev tau neutrinos.
branching ratios of $\alpha$-decay to excited states of even-even nuclei. branching ratios of $\alpha $-decay to members of the ground state rotational band and excited 0$^{+}$ states of even-even nuclei are calculated in the framework of the generalized liquid drop model (gldm) by taking into account the angular momentum of the $\alpha$-particle and the excitation probability of the daughter nucleus. the calculation covers isotopic chains from hg to fm in the mass regions $180&lt; a &lt;202$ and a$\geq 224$. the calculated branching ratios of the $\alpha $-transitions are in good agreement with the experimental data and some useful predictions are provided for future experiments.
a language-based approach for robust and efficient network application protocol implementations. null
selenide retention onto pyrite under reducing conditions. pyrite (fes2) is a mineral phase often present as inclusions in temperate soils. moreover, it turns out to be a sorption sink for certain radionuclides in deep geological disposals. the present study was thus initiated to determine the capacity of pyrite to immobilize selenide (se(-ii)). due to the fact that pyrite surface oxidizes readily, potentials were applied in order to minimise its surface evolution, and ensure the reducing conditions necessary for stabilizing se(-ii). the sorption experiments were carried out in nacl electrolyte and were amperometrically controlled. after only several minutes of reaction, at least 97% of se(-ii) initially present in solution was disappeared. the kd values vary from 7–65 l/g and the isotherm curve shows site saturation at higher initial selenide concentrations and no ph-dependence. by means of several spectroscopic techniques, the reaction mechanism was investigated. the xrd and in situ xanes results indicate the presence of se(0) on pyrite surface, which explain the rapid disappearance of se observed in the sorption experiments. moreover, xps results obtained from se-reacted pyrite particles reveal cleavage of s–s bonding which resulted in formation of s2− on pyrite surface. thus, we conclude that se(-ii) can be immobilized by pyrite via surface redox reaction: ≡fes2 + hse− ⇔ ≡fes + se(0) + hs−.
search for a long living giant system in $^{238}$u+$^{238}$u collisions near the coulomb barrier. we searched for a long-living component in the collision of 238u+238u between 6.09 a and 7.35 a mev. the experiment was performed at ganil using the spectrometer vamos, tuned for observing reactions with kinematics similar to quasi-fission events. theoretical calculations indicate that reactions with strong energy dissipation and a large number of transferred nucleons are correlated to a time delay in the decay of the giant system. we detected events of such type in the focal plane of vamos. these events present an excitation function increasing with bombarding energy.
alpha decay potential barriers and half-lives and analytical formula predictions for superheavy nuclei. the synthesis of superheavy elements has advanced strongly recently and their main observed decay mode is alpha emission. predictions of alpha decay half-lives of other possible superheavy nuclei are needed. the alpha decay potential barrier is often described using a finite square well for the one-body shapes plus an hyperbola for the coulomb repulsion between the alpha particle and its daughter. an arbitrary adjustment of the parameters allows to reproduce roughly the experimental data. here the alpha decay potential barriers are determined in the cluster-like shape path within a generalized liquid drop model including the proximity effects between the alpha particle and the daughter nucleus and adjusted to reproduce the experimental qalpha. the ? emission half-lives are deduced within the wkb penetration probability through these barriers.
ttomography of quark gluon plasma at energies available at the bnl relativistic heavy ion collider (rhic) and the cern large hadron collider (lhc). using the recently published model for the collisional energy loss of heavy quarks (q) in a quark gluon plasma (qgp), based on perturbative qcd (pqcd) with a running coupling constant, we study the interaction between heavy quarks and plasma particles in detail. we discuss correlations between the simultaneously produced $c$ and $\bar{c}$ quarks, study how central collisions can be experimentally selected, predict observable correlations and extend our model to the energy domain of the large hadron collider (lhc). we finally compare the predictions of our model with that of other approaches like ads/cft.
gamma-jet physics with emcal calorimeter in alice experiment at lhc. heavy ion collisions at lhc will produce a new state of matter : the quark-gluon plasma (qgp). photons are not sensible to the strong interaction which dominates the nuclear medium, and hence are a valuable tool to explore qgp. gamma-jets are rare hard processes : a photon and a parton are emitted back-to-back. the parton hadronises and produces a jet of particles. these jets are quenched due to the strong interaction of the parton with the qgp. this quenching, or more precisely the re-distribution of the energy in the jet, can be measured by the modification of the distribution of the particle energy in the jet, comparing p-p and pb-pb collisions (fragmentation functions or hump-backed plateau distributions). for this porpose, jet energy is needed, and can be provided precisely by gamma-jet measurement.&lt;br /&gt;our goal is to use emcal to detect a photon correlated with a jet reconstructed in alice tracking system. then, the jet energy distribution are compared for p-p an pb-pb collisions.&lt;br /&gt;gamma-jet physics is first addressed, the particle identification with emcal is introduced to isolate the direct photon, i.e. a photon and a jet emitted back-to-back. methods of jet identification and reconstruction are developed to determine hump-backed plateau distributions. finally, these methods are tested to evaluate alice and particularly emcal capabilities for gamma-jet study at lhc and to quantify the sensibility of this probe to explore the qgp.
beta neutrino correlation measurement with trapped radioactive ions. null
late quaternary climatic control on erosion and weathering in the eastern tibetan plateau and the mekong basin. null
a generative programming approach to developing dsl compilers. domain-specific languages (dsls) represent a proven approach to raising the abstraction level of programming. they offer high-level constructs and notations dedicated to a domain, structuring program design, easing program writing, masking the intricacies of underlying software layers, and guaranteeing critical properties. on the one hand, dsls facilitate a straightforward mapping between a conceptual model and a solution expressed in a specific programming language. on the other hand, dsls complicate the compilation process because of the gap in the abstraction level between the source and target language. the nature of dsls make their compilation very different from the compilation of common general-purpose languages (gpls). in fact, a dsl compiler generally produces code written in a gpl; low-level compilation is left to the compiler of the target gpl. in essence, a dsl compiler defines some mapping of the high-level information and features of a dsl into the target gpl and underlying layers (e.g., middleware, protocols, objects, ...). this paper presents a methodology to develop dsl compilers, centered around the use of generative programming tools. our approach enables to structure the development of a dsl compiler on facets that represent dimensions of compilation. each facet can then be implemented in a modular way, using aspects, annotations and specialization. because these tools are high level, they match the needs of a dsl, facilitating the development of the dsl compiler, and making it modular and re-targetable. we illustrate our approach with a dsl for telephony services. the structure of the dsl compiler is presented, as well as practical uses of generative tools for some compilation facets.
a language-based approach for improving the robustness of network application protocol implementations. the secure and robust functioning of a network relies on the defect-free implementation of network applications. as network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. in this paper, we present a domain-specific language, zebu, for describing protocol message formats and related processing constraints. from a zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. zebu is easy to use, as it builds on notations used in rfcs to describe protocol grammars. zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. finally, zebu-based applications are robust, as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. using a mutation analysis in the context of sip and rtsp, we show that zebu significantly improves application robustness.
erosional history of the eastern tibetan plateau since 190 kyr ago: clay mineralogical and geochemical investigations from the southwestern south china sea. null
clay mineral records of east asian monsoon evolution during late quaternary in the southern south china sea. null
observation of two-source interference in the photoproduction reaction $au au \to au au \rho^0$. in ultra-peripheral relativistic heavy-ion collisions, a photon from the electromagnetic field of one nucleus can fluctuate to a quark-antiquark pair and scatter from the other nucleus, emerging as a $\rho^0$. the $\rho^0$ production occurs in two well-separated (median impact parameters of 20 and 40 fermi for the cases considered here) nuclei, so the system forms a 2-source interferometer. at low transverse momenta, the two amplitudes interfere destructively, suppressing $\rho^0$ production. since the $\rho^0$ decay before the production amplitudes from the two sources can overlap, the two-pion system can only be described with an entangled non-local wave function, and is thus an example of the einstein-podolsky-rosen paradox. we observe this suppression in 200 gev per nucleon-pair gold-gold collisions. the interference is $87% \pm 5% {\rm (stat.)}\pm 8%$ (syst.) of the expected level. this translates into a limit on decoherence due to wave function collapse or other factors, of 23% at the 90% confidence level.
development and data analysis of a radiodetection of ultra high energy cosmic rays experiment. radiodetection of cosmic rays was first attempt in the early 60's. unfortunately, at that time, results suffered of poor reproducibility and the technique was abandoned in favour of direct particle and fluorescence detection. &lt;br /&gt;&lt;br /&gt;nowadays, the ultra high energy cosmic rays constitute an area of scientific interest of first plan and the extreme rarity of those particles implies development of huge detectors. in this context and taking advantage of recent technologic improvements, the radiodetection of ultra high cosmic rays is being reinvestigated in order to enhance the statistic of experiments besides already existing methods.&lt;br /&gt;&lt;br /&gt;in this document, we first remind to the reader the global problematic of cosmic rays. then, the several mechanisms involved in the emission of electric field associated with extensive air showers are discussed. the codalema experiment that aims to demonstrate the feasibility of cosmic rays radiodetection, is extensively described along with the obtained results. and finally, a radiodetection test experiment implanted at the giant detector pierre auger is presented. it should provide inputs to design the future detector using this technique at extreme energies.
on the compact formulation of the derivation of a transfer matrix with respect to another matrix. a new operator is considered, allowing compact formulae and proofs in the context of the derivation of a transfer matrix with respect to another matrix. the problem of the parametric sensitivity matrix calculation is chosen for illustration. it consists in deriving a multiple input multiple output transfer function with respect to a parametric matrix and is central in robust control theory. efficient algorithms may be straightforwardly got from the compact analytic formulae using the operator introduced.
condition-based maintenance policies for a deteriorating system subject to a stressful environment. one of the challenges of maintenance optimisation is the development of decision-making models combining performance at the strategic level and at the operational level. a classic hypothesis is to consider that the degradation level of the system can be modelled by a stochastic process characterized in stationary state without taking into account the effects of the operating environment on the system. this hypothesis can be seen as one of the factors leading to gaps between expected performance and measured performance of maintenance policies. however, many works have been developed in the reliability field for the integration of the impact of these environmental conditions. the main objective of this manuscript is to develop maintenance decision tools for gradually deteriorating systems evolving in a stressful environment. first, we propose different ways to model the environment and its impact since it can directly influence the system failure or the degradation process. we express mutual relationships between environment and the degradation processes. next, we propose and compare different adaptive maintenance policies which are based not only on the degradation level, but also on the stressful environment level. in addition, the proposed policies can be based either on an a priori knowledge of the system, or integrate the available online information on the environment. we will try throughout this manuscript to propose new maintenance approaches which combine theoretical expected performance on one side and operational reality and pragmatism on the other.
structural investigation of coprecipitation of technetium-99 with iron phases. technetium is a long-lived product of nuclear fission which commonly exhibits two oxidation states (iv and vii). siderite (feco3), suspected to be formed as a container corrosion product in geological radioactive waste repositories, may concentrate by coprecipitation more than 90% of technetium-99, present as tc(iv) in surrounding aqueous fluids. x-ray diffraction and transmission electron microscopy measurements indicate that technetium can be incorporated within the siderite structure, even if we note that technetium-bearing green rust phase may also be observed. these results suggest that siderite might play a beneficial role in limiting tc diffusion to the next environment of nuclear waste repositories.
thermodynamic interpretation of neptunium coprecipitation in uranophane for application to the yucca mountain repository. interpretation and modeling of recent experimental data [1] yield thermodynamic constants for the distribution of trace np(v) between aqueous solutions and uranophane. these data indicate that neptunyl is relatively excluded from the uranyl mineral structure, but the interpretation depends on uncertain aqueous speciation and thermodynamic properties as a function of temperature. despite np exclusion, the low calculated solubility of uranophane at 25 °c under conditions relevant to the proposed nuclear waste repository at yucca mountain, nevada, leads to np concentrations at equilibrium with a np-bearing uranophane solid solution that are low compared to concentrations invoked as solubility limits in yucca mountain performance assessments.
precision measurements in $\beta decay trapped and cooled radiocative ions. null
neutron correlations in $^{6}$he viewed through nuclear break-up reactions. null
service level in scheduling. null
simultaneous computation of the wilcoxon's and ansari-bradley's statistics for small samples. null
approximations for pci bias correlation factor under normality. null
exponential probability of defectives in sampling. null
an improvement of the weighted variance x control chart. null
extension of the scaled weighted variance method to the ewma-x control chart. null
simple control charts for data having a symmetrical leptokurtic distribution. null
an ewma control chart for monitoring the logarithm of the process sample variance. null
an ewma control chart for monitoring the process sample median. null
a new ewma control chart for monitoring the process standard deviation. null
toward the use of statistical analysis in positional tolerancing. null
monitoring capability indice cm using ewma. null
industrial-strength rule interoperability using model driven engineering. model driven engineering (mde) is rapidly maturing and is being deployed in several situations. we report here on an experiment conducted in the context of ilog, a leader in the development of business rule management systems (brms). brmss aim at enabling business users automating their business policies. there is a growing number of brms supporting different languages, but also a lack of tools for bridging them. in this paper, we present an approach based on mde techniques for bridging rule languages; the solution has been fully implemented and tested on different brms. the success of the experiment has led to the development of a significant number of model transformations. at the same time, this deployment has shown new problems arising from the management of a high number of artifacts. we discuss the positive assessment of mde in this field, but also the need to address the complexity generated.
effect of non-normality on residual-based methods. null
new process capability indices for two quality characteristics. null
introduction of a new selection parameter in genetic algorithm. null
monitoring of unequal length batch processes. null
bivariate process capability indices for non-normal distributions. null
introduction of a new selection parameter in genetic algorithm for constrained reliability problems with several failure modes. null
a comparison of the readability of graphs using node-link and matrix-based representations. null
zero degree cherenkov calorimeters for the alice experiment. the collision centrality in the alice experiment will be determined by the zero degree calorimeters (zdcs) that will measure the spectator nucleons energy in heavy ion collisions. the zdcs detect the cherenkov light produced by the fast particles in the shower that cross the quartz fibers, acting as the active material embedded in a dense absorber matrix. test beam results of the calorimeters are presented.
symbolic bounded analysis for component behavioural protocols. null
bounded analysis and decomposition for behavioural descriptions of components. explicit behavioural interfaces are now accepted as a mandatory feature of components to address architectural analysis. behavioural interface description languages should be able to deal with data types and with rich communication means. symbolic transition systems (sts) support the definition of component models which take into account control, concurrency, communication and data types. however, verification of components described with protocol modelled by sts, especially model-checking, is difficult since they possibly involve different sources of infinity. in this paper, we propose the notions of bounded analysis and bounded decomposition. they can be used to test boundedness of systems and to generate finite simulations for them so that standard model-checking techniques may be applied for verification purposes.
a formal architectural description language based on symbolic transition systems and modal logic. component based software engineering has now emerged as a discipline for system development. after years of battle between component platforms, the need for means to abstract away from specific implementation details is now recognized. this paves the way for model driven approaches (such as mde) but also for the more older architectural description language (adl) paradigm. in this paper we present kadl, an adl based on the korrigan formal language which supports the following features: integration of fully formal behaviours and data types, expressive component composition mechanisms through the use of modal logic, specification readability through graphical notations, and dedicated architectural analysis techniques. key words: architectural description language, component based software engineering, mixed formal specifications, symbolic transition systems, abstract data types, modal logic glue, graphical notations, verification.
a formal component model with explicit symbolic protocols and its implementation in java. null
java implementation of a component model with explicit symbolic protocols. null
forward neutral-pion transverse single-spin asymmetries in p+p collisions at $\sqrt{s}$=200 gev. we report precision measurements of the feynman x (xf) dependence, and first measurements of the transverse momentum (pt) dependence, of transverse single-spin asymmetries for the production of $\pi$0 mesons from polarized proton collisions at $\sqrt{s}$=200 gev. the xf dependence of the results is in fair agreement with perturbative qcd model calculations that identify orbital motion of quarks and gluons within the proton as the origin of the spin effects. results for the pt dependence at fixed xf are not consistent with these same perturbative qcd-based calculations.
control charts for data having a symmetrical distribution with a positive kurtosis. null
an optimized algorithms for computing wilcoxon's tn+ statistics when n is small. null
optimized algorithms for computing wilcoxon's tn, wilcoxon's wm,n and ansari-bradley's am,n statistics when m and n are small. null
evaluation of non normal process capability indices using burr's distributions. null
approximation of the normal sample median distribution using symmetrical johnson su distributions: application to quality control. null
single acceptance sampling by attributes with an increasing proportion of defective items. null
x control chart for skewed populations using a scaled weighted variance method. null
an (x/r)-ewma control chart for monitoring the process sample median. null
how to monitor capability index cm using ewma. null
consequences of lambda c/d enhancement on the non-photonic electron nuclear modification factor in central heavy ion collisions at rhic energies. null
climatic and tectonic controls on weathering in south china and indochina peninsula: clay mineralogical and geochemical investigations from the pearl, red, and mekong drainage basins. null
clay minerals in surface sediments of the pearl river drainage basin and their contribution to the south china sea. null
major element geochemistry of glass shards and minerals of the youngest toba tephra in the southwestern south china sea. null
a new contribution to the nuclear modification factor of non-photonic electrons in au+au collisions at sqrt(s) = 200 gev. we investigate the effect of the so-called anomalous baryon/meson enhancement to the nuclear modification factor of non-photonic electrons in au+au collisions at sqrt(s) = 200 gev. it is demonstrated that an enhancement of the charm baryon/meson ratio, as it is observed for non-strange and strange hadrons, can be responsible for part of the amplitude of the nuclear modification factor of non-photonic electrons. about half of the measured suppression of non-photonic electrons in the 2-4 pt range can be explained by a charm baryon/meson enhancement of 5. this contribution to the non-photonic electron nuclear modification factor has nothing to do with heavy quark energy loss.
consequences of a lambda c/d enhancement effect on the non-photonic electron nuclear modification factor in central heavy-ion collisions at rhic energy. null
quantitative description and local structures of trivalent metal ions eu(iii) and cm(iii) complexed with polyacrylic acid. the trivalent metal ion (m(iii) = cm, eu)/polyacrylic acid (paa) system was studied in the ph range between 3 and 5.5 for a molar paa-to-metal ratio above 1. the interaction was studied for a wide range of paa (0.05 mg l−1–50 g l−1) and metal ion concentrations (2×10−9–10−3 m). this work aimed at 3 goals (i) to determine the stoichiometry of m(iii)–paa complexes, (ii) to determine the number of complexed species and the local environment of the metal ion, and (iii) to quantify the reaction processes. asymmetric flow-field-flow fractionation (asflfff) coupled to icp-ms evidenced that size distributions of eu–paa complexes and paa were identical, suggesting that eu bound to only one paa chain. time-resolved laser fluorescence spectroscopy (trlfs) measurements performed with eu and cm showed a continuous shift of the spectra with increasing ph. the environment of complexed metal ions obviously changes with ph. most probably, spectral variations arose from conformational changes within the m(iii)–paa complex due to ph variation. complexation data describing the distribution of complexed and free metal ion were measured with cm by trlfs. they could be quantitatively described in the whole ph-range studied by considering the existence of only a single complexed species. this indicates that the slight changes in m(iii) speciation with ph observed at the molecular level do not significantly affect the intrinsic binding constant. the interaction constant obtained from the modelling must be considered as a mean interaction constant.
surface site density, silicic acid retention and transport properties of compacted magnetite powder. in france, within the framework of investigations of the feasibility of deep geological disposal of high-level radioactive waste, studies on corrosion products of steel over packs are ongoing. such studies concern silica and radionuclide retention. the objective of the present work is to study sorption of silicic acid on compacted magnetite in percolation cells to attempt to simulate confined site conditions. potentiometric titration of commercial magnetite was carried out with both dispersed and compacted magnetite. the titration of the magnetite suspension has been made with two different methods: a batch method (several suspensions) and a direct fast method (one suspension). the gran's function gave 1.7 (±0.4) and 2.4 (±0.5) sorption sites nm−2 with these respective methods but site densities as high as 20/nm2 could be obtained by modelling. the titration of magnetite compacted at 120 bars showed that the evolution of charge density on magnetite surfaces is similar for compacted and dispersed magnetite. silicic acid sorption onto dispersed and compacted magnetite was similar with sorption site densities ranging between 2.2 and 4.4/nm2.
inclusive cross section and double helicity asymmetry for pi^0 production in p+p collisions at sqrt(s) = 62.4 gev. the phenix experiment presents results from the rhic 2006 run with polarized proton collisions at sqrt(s) = 62.4 gev for inclusive pi^0 production at mid-rapidity. unpolarized cross section results are measured for transverse momenta p_t = 0.5 to 7 gev/c. next-to-leading order perturbative quantum chromodynamics calculations are compared with the data, and while the calculations are consistent with the measurements, next-to-leading logarithmic corrections improve the agreement. double helicity asymmetries a_ll are presented for p_t = 1 to 4 gev/c and probe the higher range of bjorken_x of the gluon (x_g) with better statistical precision than our previous measurements at sqrt(s)=200 gev. these measurements are sensitive to the gluon polarization in the proton for 0.06 &lt; x_g &lt; 0.4.
the gluon spin contribution to the proton spin from the double helicity asymmetry in inclusive pi^0 production in polarized p+p collisions at sqrt(s)=200 gev. the double helicity asymmetry in neutral pion production for p_t = 1 to 12 gev/c has been measured with the phenix experiment in order to access the gluon spin contribution, delta-g, to the proton spin. measured asymmetries are consistent with zero, and at a theory scale of \mu^2 = 4 gev^2 give delta-g^[0.02,0.3] = 0.1 to 0.2, with a constraint of -0.7 &lt; delta-g^[0.02,0.3] &lt; 0.5 at delta-chi^2 = 9 (~3 sigma) for our sampled gluon momentum fraction (x) range, 0.02 to 0.3. the results are obtained using predictions for our measured asymmetries generated from four representative fits to polarized deep inelastic scattering data. we also consider the dependence of the delta-g constraint on the choice of theoretical scale, a dominant uncertainty in these predictions.
reaction mechanisms in 24mg+12c and 32s+24mg. the occurence of "exotic'' shapes in light n=z alpha-like nuclei is investigated for 24mg+12c and 32s+24mg. various approaches of superdeformed and hyperdeformed bands associated with quasimolecular resonant structures with low spin are presented. for both reactions, exclusive data were collected with the binary reaction spectrometer in coincidence with euroball iv installed at the vivitron tandem facility of strasbourg. specific structures with large deformation were selectively populated in binary reactions and their associated $\gamma$-decays studied. the analysis of the binary and ternary reaction channels is discussed.
hadronic resonance production in d+au collisions at $\sqrt{s_{nn}}$=200 gev measured at the bnl relativistic heavy ion collider. we present the first measurements of the $\rho$(770)0,k*(892),$\delta$(1232)++,$\sigma$(1385), and $\lambda$(1520) resonances in d+au collisions at $\sqrt{s_{nn}}$=200 gev, reconstructed via their hadronic decay channels using the star detector (the solenoidal tracker at the bnl relativistic heavy ion collider). the masses and widths of these resonances are studied as a function of transverse momentum pt. we observe that the resonance spectra follow a generalized scaling law with the transverse mass mt. the pt of resonances in minimum bias collisions are compared with the pt of $\pi$,k, and $\bar{p}$. the $\rho$0/$\pi$-,k*/k-,$\delta$++/p,$\sigma$(1385)/$\lambda, and $\lambda$(1520)/$\lambda$ ratios in d+au collisions are compared with the measurements in minimum bias p+p interactions, where we observe that both measurements are comparable. the nuclear modification factors (rdau) of the $\rho$0,k*, and $\sigma$* scale with the number of binary collisions (nbin) for pt&gt; 1.2 gev/c.
fission decay of n = z nuclei at high angular momentum: $^{60}$zn. using a unique two-arm detector system for heavy ions (the brs, binary reaction spectrometer) coincident fission events have been measured from the decay of $^{60}$zn compound nuclei formed at 88mev excitation energy in the reactions with $^{36}$ar beams on a $^{24}$mg target at $e_{lab}(^{36}$ar) = 195 mev. the detectors consisted of two large area position sensitive (x,y) gas telescopes with bragg-ionization chambers. from the binary coincidences in the two detectors inclusive and exclusive cross sections for fission channels with differing losses of charge were obtained. narrow out-of-plane correlations corresponding to coplanar decay are observed for two fragments emitted in binary events, and in the data for ternary decay with missing charges from 4 up to 8. after subtraction of broad components these narrow correlations are interpreted as a ternary fission process at high angular momentum through an elongated shape. the lighter mass in the neck region consists dominantly of two or three-particles. differential cross sections for the different mass splits for binary and ternary fission are presented. the relative yields of the binary and ternary events are explained using the statistical model based on the extended hauser-feshbach formalism for compound nucleus decay. the ternary fission process can be described by the decay of hyper-deformed states with angular momentum around 45-52 $hbar$.
spacetime scales and initial conditions in relativistic a+a collisions. the hydro-kinetic model accounting for continuous decoupling of expanding thermal systems and deviations from local equilibrium is applied for a description of the pion spectra and interferometry radii in cental au+au collisions at the rhic energies. we use the initial conditions inspired by the color glass condensate (cgc) approach and equation of state which incorporates the results of lattice qcd and accounts for gradual decays of the resonances during the expansion of hadron gas. the initial transverse flows which are developed at the pre-thermal glasma stage are important for simultaneous description of the spectra and spacetime scales.
reduced mean model for controlling a three-dimensional eel-like robot. null
improved screening for growth hormone deficiency using logical analysis data. null
neutronic characterization of the megapie target. null
sedimentary responses to the pleistocene climatic variations recorded in the south china sea. null
consequences of a lambda_c/d enhancement effect on the non-photonic electron nuclear modification factor in central heavy ion collisions at rhic energy. the rhic experiments have measured the nuclear modification factor r_aa of non-photonic electrons in au+au collisions at sqrt(s_nn) = 200gev. this r_aa exhibits a large suppression for p_t&gt; 2gev/c which is commonly attributed to heavy-quark energy loss. it is expected that the heavy-quark radiative energy loss is smaller than the light quark one because of the so-called dead-cone effect. an enhancement of the charm baryon yield with respect to the charm meson yield, as it is observed for light and strange hadrons, can explain part of the suppression. this phenomenon has been put forward in a previous work. we present in this paper a more complete study based on a detailed simulation which includes electrons from charm and bottom decay, charm and bottom quark realistic energy loss as well as a more realistic modeling of the lambda_c/d enhancement. we show that a lambda_c/d ratio close to unity, as observed for light and strange quarks, could explain 20-25% of the suppression of non-photonic electrons in central au+au collisions. this effect remains significant at relatively high non-photonic electron transverse momenta of 8-9gev/c.
physics perspectives with alice emcal. null
toward an understanding of the single electron data measured at the bnl relativistic heavy ion collider (rhic). high transverse momentum (pt) single nonphotonic electrons which have been measured in the rhic experiments come dominantly from heavy meson decay. the ratio of their pt spectra in pp and aa collisions [raa(pt)] reveals the energy loss of heavy quarks in the environment created by aa collisions. using a fixed coupling constant and the debye mass (md[approximate]gt) as the infrared regulator, perturbative qcd (pqcd) calculations are not able to reproduce the data, neither the energy loss nor the azimuthal (v2) distribution. employing a running coupling constant and replacing the debye mass by a more realistic hard thermal loop (htl) calculation, we find a substantial increase in the collisional energy loss, which brings the v2(pt) distribution as well as raa(pt) to values close to the experimental ones without excluding a contribution from radiative energy loss.
global properties of nucleus-nucleus collisions. in this lecture note, we discuss the global properties of nucleus-nucleus collisions. after a brief introduction to heavy-ion collisions, we introduce useful kinematics and then discuss the bulk hadron production in a+a collisions. at the end we discuss the hadronization and hadronic freeze-out in a+a collisions. we have tried to cover the topic from very fundamental arguments especially for the beginners in the field. we also give very useful formulae frequently used by experimentalists, from a first principle derivation.
heavy ion collisions at the lhc - last call for predictions. this writeup is a compilation of the predictions for the forthcoming heavy ion program at the large hadron collider, as presented at the cern theory institute 'heavy ion collisions at the lhc - last call for predictions', held from may 14th to june 10th 2007.
the path-repair algorithm. in this paper, we introduce a new solving algorithm for constraint satisfaction problems: the path-repair algorithm. the two main points of that algorithm are: it makes use of a repair algorithm (local search) as a basis and it works on a partial instantiation in order to be able to use filtering techniques. different versions are presented and first experiments with both systematic and non systematic versions show promising results.
solving dynamic resource constraint project scheduling problems using new constraint programming tools. timetabling problems have been studied a lot over the last decade. due to the complexity and the variety of such problems, most work concern static problems in which activities to schedule and resources are known in advance, and constraints are fixed. however, every timetabling problem is subject to unexpected events (consider for example, for university timetabling problems, a missing teacher, or a slide projector breakdoawn). in such a situation, one has to quickly build a new solution which takes these events into account and which is preferably not too different form the current one. we introduce in this paper constraint-programming based tools for solving dynamic timetabling problems modelled as rcpsp (resource-constrained project scheduling problems). this approach uses explanation-based constraint programming and operational research techniques.
systematic measurements of identified particle spectra in pp, d+au and au+au collisions from star. identified charged particle spectra of $\pi^{\pm}$, $k^{\pm}$, $p$ and $\pbar$ at mid-rapidity ($|y|&lt;0.1$) measured by the $\dedx$ method in the star-tpc are reported for $pp$ and d+au collisions at $\snn = 200$ gev and for au+au collisions at 62.4 gev, 130 gev, and 200 gev. ... [shortened for arxiv list. full abstract in manuscript.].
alpha-cluster states populated in 24mg+12c. charged particle and gamma decays in light alpha-like nuclei are investigated for 24mg+12c. various theoretical predictions for the occurence of superdeformed and hyperdeformed bands associated with resonance structures with low spin are presented. the inverse kinematics reaction 24mg+12c is studied at elab(24mg) = 130 mev. exclusive data were collected with the binary reaction spectrometer in coincidence with euroball iv installed at the vivitron tandem facility at strasbourg. specific structures with large deformation were selectively populated in binary reactions and their associated gamma decays studied. coincident events from $\alpha$-transfer channels were selected by choosing the excitation energy or the entry point via the two-body q-values. the analysis of the binary reaction channels is presented with a particular emphasis on 20ne-gamma and 16o-gamma coincidences.
reproducibility of the uptake of u(vi) onto degraded cement pastes and calcium silicate hydrate phases. the u(vi) uptake in degraded cement pastes was undertaken in the laboratories of cea/l3mr and subatech in order to check the reproducibility of the study. two well hydrated cement pastes, cem i (ordinary portland cement, opc) and cem v (blast furnace slag (bfs) and fly ash added to opc) were degraded using similar protocols. equilibrium solutions and solid materials were characterised for three degradation states for each paste. all samples are free of portlandite and the ph of the equilibrated cement solutions vary in the range 9.8–12.2. three calcium silicate hydrate phases (c-s-h) were synthesised in order to compare the sorption properties of degraded cement pastes and of hydrate phases in similar ph conditions. in order to avoid precipitation processes, the operational solubility limit was evaluated before batch experiments. these solubility values vary significantly in the ph range [9–13] with a 2.4×10−7 mol/l minimum at ph close to 10.5. in batch sorption experiments, the distribution ratio rd values are high: 30000–150000 ml/g. the uptake of u(vi) increases when comparing the least and the most degraded cement pastes whereas the initial composition of cement has relatively insensitive effect. sorption isotherms, expressed as a log [u(vi)solid]/ log [u(vi)solution] plots are linear. a slope of 1 is calculated indicating the predominance of sorption processes. as sorption and desorption values are close, the uptake mechanism seems reversible. the rd values measured in c-s-h suspensions are in good agreement with rd values of degraded cement pastes, and c-s-h materials could be one of the cementitious phases which control u(vi) uptake in cement pastes.
the alice experiment at the cern lhc. alice (a large ion collider experiment) is a general-purpose, heavy-ion detector at the cern lhc which focuses on qcd, the strong-interaction sector of the standard model. it is designed to address the physics of strongly interacting matter and the quark-gluon plasma at extreme values of energy density and temperature in nucleus-nucleus collisions. besides running with pb ions, the physics programme includes collisions with lighter ions, lower energy running and dedicated proton-nucleus runs. alice will also take data with proton beams at the top lhc energy to collect reference data for the heavy-ion programme and to address several qcd topics for which alice is complementary to the other lhc detectors. the alice detector has been built by a collaboration including currently over 1000 physicists and engineers from 105 institutes in 30 countries. its overall dimensions are 161626 m3 with a total weight of approximately 10 000 t. the experiment consists of 18 different detector systems each with its own specific technology choice and design constraints, driven both by the physics requirements and the experimental conditions expected at lhc. the most stringent design constraint is to cope with the extreme particle multiplicity anticipated in central pb-pb collisions. the different subsystems were optimized to provide high-momentum resolution as well as excellent particle identification (pid) over a broad range in momentum, up to the highest multiplicities predicted for lhc. this will allow for comprehensive studies of hadrons, electrons, muons, and photons produced in the collision of heavy nuclei. most detector systems are scheduled to be installed and ready for data taking by mid-2008 when the lhc is scheduled to start operation, with the exception of parts of the photon spectrometer (phos), transition radiation detector (trd) and electro magnetic calorimeter (emcal). these detectors will be completed for the high-luminosity ion run expected in 2010. this paper describes in detail the detector components as installed for the first data taking in the summer of 2008.
natural organic matter nom: a challenge for assessing nuclear waste disposal safety in clay. null
prompt photon production in p-a collisions at lhc and the extraction of gluon shadowing. a report is given on the study of using prompt photon production at the lhc to probe the gluon nuclear density, and more specifically the shadowing ratio ga/gp that one could access in foreseen p-a runs.
scavenging of $e_s^-$ and oh° radicals in concentrated hcl and nacl aqueous solutions. null
comments on the dynamics of the pais-uhlenbeck oscillator. we discuss the quantum dynamics of the pais-uhlenbeck oscillator. the lagrangian of this higher-derivative model depends on two frequencies. when the frequencies are different, the free pu oscillator has a pure point spectrum that is dense everywhere. when the frequencies are equal, the spectrum is continuous. it is not bounded from below, running from minus to plus infinity, but this is not disastrous as the hamiltonian is still hermitian and the evolution operator is still unitary. generically, the inclusion of interaction terms break unitarity, but in some special cases unitarity is preserved. we discuss also the nonstandard realization of the pu oscillator suggested by bender and mannheim, where the spectrum of the free hamiltonian is positive definite, but wave functions grow exponentially for large real values of canonical coordinates. the free nonstandard pu oscillator is unitary when the frequencies are different, but unitarity is broken in the equal frequencies limit.
coinductive big-step operational semantics. using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. we formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. we then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. a methodological originality of this paper is that all results have been proved using the coq proof assistant. we explain the proof-theoretic presentation of coinductive definitions and proofs offered by coq, and show that it facilitates the discovery and the presentation of the results.
a dynamic model for facility location in the design of complex supply chains. this paper proposes a mixed integer linear program (milp) for the design and planning of a production-distribution system. this study aims to help strategic and tactical decisions: opening, closing or enlargement of facilities, supplier selection, flows along the supply chain. these decisions are dynamic, i.e. the value of the decision variables may change within the planning horizon. the model considers a multi-echelon, multi-commodity production-distribution network with deterministic demands. we present one application: how to plan the expansion of a company that has to face increasing demands. we report numerical experiments with an milp solver.
herodotus and amathus. null
study of processes involving selenite immobilization in a soil-plant-microorganisms system. null
studies of neutron-induced light-ion production with the medley facility. the growing interest in applications involving high-energy neutrons (e &gt; 20 mev) demands high-quality experimental data on neutron-induced reactions. such data have been measured with the medley setup at the the svedberg laboratory (tsl), uppsala, sweden. it has been used to measure differential cross sections for elastic nd scattering and double-differential cross sections for light-ion production (a ≤ 4) with targets ranging from c to u and at incident neutron energies around 96mev. we summarize the experimental results obtained so far and compared with theoretical reaction model calculations. a new method for correcting charged-particle spectra for thick target effects has been used for data obtained with the medley facility. the new quasi-monoenergetic neutron beam facility of tsl offers the possibility to extend these measurements up to neutron energies of 175mev. in january 2007, the neutron beam facility at tsl has been equipped with improved shielding and pre-collimator to reduce the background observed with medley during the first experimental campaigns at 175mev to an acceptable level. we present the current status of the medley facility after the shielding upgrade. we summarize also our ongoing projects including both measurements of light-ion production at 175mev from c to u targets and fission studies of u-238 in the energy region of 11 to 175mev.
beam-energy and system-size dependence of dynamical net charge fluctuations. we present measurements of net charge fluctuations in $au + au$ collisions at $\sqrt{s_{nn}} = $ 19.6, 62.4, 130, and 200 gev, $cu + cu$ collisions at $\sqrt{s_{nn}} = $ 62.4, 200 gev, and $p + p$ collisions at $\sqrt{s} = $ 200 gev using the dynamical net charge fluctuations measure $\nu_{+-{\rm,dyn}}$. we observe that the dynamical fluctuations are non-zero at all energies and exhibit a modest dependence on beam energy. a weak system size dependence is also observed. we examine the collision centrality dependence of the net charge fluctuations and find that dynamical net charge fluctuations violate $1/n_{ch}$ scaling, but display approximate $1/n_{part}$ scaling. we also study the azimuthal and rapidity dependence of the net charge correlation strength and observe strong dependence on the azimuthal angular range and pseudorapidity widths integrated to measure the correlation.
underwater robotic: localization with electrolocation for collision avoidance. this paper proposes and compares two observers designed to calculate the location of an obstacle. the two methods are bio-inspired with a sense used by electric fishes of equatorial forests: the electrolocation. firstly, this study presents the electrolocation and then develops two models of emitter-sensors inspired by the electrical sense. secondly, the two models are used in different observers for detection and localisation of wall obstacles. the estimation methods are based on an extended kalman filter algorithm. observers are tested on simulations in order to assess their potentials and to analyze observability.
heavy quarks thermalization in heavy-ion ultrarelativistic collisions: elastic or radiative?. we present a dynamical model of heavy quark evolution in the quark–gluon plasma (qgp) based on the fokker–planck equation. we then apply this model to the case of ultrarelativistic nucleus–nucleus collisions performed at rhic in order to investigate which experimental observables might help to discriminate the fundamental process leading to thermalization.
eas radio detection at large impact parameter: the inverse problem and the design of a giant array. extensive air shower radio electric fields can be evaluated at large impact parameter with analytical expressions. such a theoretical tool is most valuable in the present phase where the capabilities of the radio detection of extensive air shower are under investigations. it can help shaping strategies for the analysis of radio detection data. it can also be used to perform non trivial test of much more detailed numerical approaches which are currently under development. the approximation leading to such a formulation will be presented and two applications will be discussed: the "inverse" problem of how to go from a sampling of the radio electric field on a few antennas to the main characteristics of the extensive air shower, and the question of the antenna spacing of a giant array for ultra high energy cosmic rays.
detection of photons and electrons in emcal. null
error band in heavy quark thermalization: the viewpoint of theory. null
collisional energy loss at rhic and predictions for the lhc. null
toward an understanding of the rhic single electron data. null
open and hidden charm from rhic to fair. null
open and hidden heavy flavors, separately and together. null
a foundation for flow-based program matching using temporal logic and model checking. reasoning about program control-flow paths is an important functionality of a number of recent program matching languages and associated searching and transformation tools. temporal logic provides a well-defined means of expressing properties of control-flow paths in programs, and indeed an extension of the temporal logic ctl has been applied to the problem of specifying and verifying the transformations commonly performed by optimizing compilers. nevertheless, in developing the coccinelle program transformation tool for performing linux collateral evolutions in systems code, we have found that existing variants of ctl do not adequately support rules that transform subterms other than the ones matching an entire formula. being able to transform any of the subterms of a matched term seems essential in the domain targeted by coccinelle. in this paper, we propose an extension to ctl named ctl-vw (ctl with variables and witnesses) that is a suitable basis for the semantics and implementation of the coccinelle's program matching language. our extension to ctl includes existential quantification over program fragments, which allows metavariables in the program matching language to range over different values within different control-flow paths, and a notion of witnesses that record such existential bindings for use in the subsequent program transformation process. we formalize ctl-vw and describe its use in the context of coccinelle. we then assess the performance of the approach in practice, using a transformation rule that fixes several reference count bugs in linux code.
system-size independence of directed flow at the relativistic heavy-ion collider. we measure directed flow ($v_1$) for charged particles in au+au and cu+cu collisions at $\sqrt{s_{nn}} =$ 200 gev and 62.4 gev, as a function of pseudorapidity ($\eta$), transverse momentum ($p_t$) and collision centrality, based on data from the star experiment. we find that the directed flow depends on the incident energy but, contrary to all existing models, not on the size of the colliding system at a given centrality. we extend the validity of the limiting fragmentation concept to different collision systems, and investigate possible explanations for the observed sign change in $v_1(p_t)$.
wysiwib: a declarative approach to finding protocols and bugs in linux code. although a number of approaches to finding bugs in systems code have been proposed, bugs still remain to be found. current approaches have emphasized scalability more than usability, and as a result it is difficult to relate the results to particular patterns found in the source code and to control the tools to be able to find specific kinds of bugs. in this paper, we propose a declarative approach based on a control-flow based program search engine. our approach is wysiwib (what you see is where it bugs), since the programmer is able to express specifications for protocol and bug finding using a syntax that is close to that of ordinary c code. search specifications, called semantic matches, can be easily tailored so as to either eliminate false positives or catch more potential bugs. we introduce our approach by describing three case studies which have allowed us to find 395 bugs.
the versatility of using explanations within constraint programming. constraint programming is a research topic benefiting from many other areas: discrete mathematics, numerical analysis, artificial intelligence, operations research, and formal calculus. it has proven its interest and its efficiency in various domains: combinatorial optimization, scheduling, finance, simulation and synthesis, diagnosis, molecular biology, or geometrical problems. however, some limitations and difficulties remain: designing stable and generic algorithms, handling dynamic problems, opening constraint programming to non-specialists, etc.&lt;br /&gt;&lt;br /&gt;in this document, we advocate the use of explanations within constraint programming. our aim is two-fold: drawing the big picture about explanations (definition, generation, management and use) and showing that they can help address several issues in constraint programming. we also introduce a new general explanation-based search technique that has been successfully used to design new efficient algorithms. finally, current open issues and research topics in this field are presented.
constraint solving in uncertain and dynamic environments - a survey. this article follows a tutorial, given by the authors on dynamic constraint solving at cp 2003 (ninth international conference on principles and practice of constraint programming) in kinsale, ireland. it aims at offering an overview of the main approaches and techniques that have been proposed in the domain of constraint satisfaction to deal with uncertain and dynamic environments.
solving a real-time allocation problem with constraint programming. in this paper, we present an original approach (cprta for "constraint programming for solving real-time allocation") based on constraint programming to solve a static allocation problem of hard real-time tasks. this problem consists in assigning periodic tasks to distributed processors in the context of fixed priority preemptive scheduling. cprta is built on dynamic constraint programming together with a learning method to find a feasible processor allocation under constraints. two efficient new approaches are proposed and validated with experimental results. moreover, cprta exhibits very interesting properties. it is complete (if a problem has no solution, the algorithm is able to prove it); it is non-parametric (it does not require specific tuning) thus allowing a large diversity of models to be easily considered. finally, thanks to its capacity to explain failures, it offers attractive perspectives for guiding the architectural design process.
constraint relaxation for dynamic problems. constraint relaxation for dynamic problems.
constraint programming based column generation for employee timetabling. the employee timetabling problem (etp) is a general class of problems widely encountered in service organizations (such as call centers for instance). given a set of activities, a set of demand curves (specifying the demand in terms of employees for each activity for each time period) the problem consists of constructing a set of work shifts such that each activity is at all time covered by a sufficient number of employees. work shifts can cover many activities and must meet work regulations such as breaks, meals and maximum working time constraints. furthermore, it is often desired to optimize a global objective function such as minimizing labor costs or maximizing a quality of service measure. this paper presents variants of this problem which are modeled with the dantzig formulation. this approach consists of first generating all feasible work shifts and then selecting the optimal set. we propose to address the shift generation problem with constraint satisfaction techniques based on expressive and efficient global constraints such as gcc and regular. the selection problem, which is modeled with an integer linear program, is solved by a standard mip solver for smaller instances and addressed by column generation for larger ones. since a column generation procedure needs to generate only shifts of negative reduced cost, the optimization constraint cost-regular is introduced and described. preliminary experimental results are given on a typical etp.
hybrid constraint programming-integer linear programming approaches for the resource-constrained project scheduling problem. states-of-the-art of the rcpsp and of constraint programming-linear programming hybrid methods.&lt;br /&gt;hybrid approaches of cp and ilp for the rcpsp: lower bounds computation by cp-based lagrangian relaxation; cp-based column generation and cp-based cutting-plane generation.&lt;br /&gt;application of a generic, exact, and non-tree search method, resolution search, proposed by vlasek chvatal. links with dynamic backtracking and nogood learning approaches in cp.
a cost-regular based hybrid column generation approach. constraint programming (cp) offers a rich modeling language of constraints embedding efficient algorithms to handle complex and heterogeneous combinatorial problems. to solve hard combinatorial optimization problems using cp alone or hybrid cp-ilp decomposition methods, costs also have to be taken into account within the propagation process. optimization constraints, with their cost-based filtering algorithms, aim to apply inference based on optimality rather than feasibility. this paper introduces a new optimization constraint: cost-regular. its filtering algorithm is based on the computation of shortest and longest paths in a layered directed graph. the support information is also used to guide the search for solutions. we believe this constraint to be particularly useful in modeling and solving column generation subproblems and evaluate its behaviour on complex employee timetabling problems through a flexible cp-based column generation approach. computational results on generated benchmark sets and on a complex real-world instance are given.
influence of process operating parameters of fibrous filter media on end-use properties. null
influence of complex fibrous media composition on their performances for voc and particle removal. null
complexation study of trivalent metal ions with polyacrylic acid using mass spectrometry and fluorescence spectroscopy as speciation tools. null
antineutrinos and non-proliferation via the double-chooz experiment. null
residue production in 136xe + p spallation reaction. a research program on spallation reactions in inverse kinematics has been performed at gsi, darmstadt, taking advantage of the relativistic heavy-ions beams available from gsi accelerators and the high-resolution magnetic spectrometer, used to identify the reactions products in-flight and to determine their kinematical properties. in this paper, we report the results obtained up to now on the spallation reaction 136xe on protons, focusing on 500 and 200 amev energies.
towards reactor neutrino applied physics. nuclear power plants are intense sources of antineutrinos. their energy spectrum and emitted flux depend on the composition of the nuclear fuel and on the thermal power of the reactor. these properties led to potential applications of neutrino physics: they could be used to non intrusively monitor a nuclear reactor. to this purpose, a better knowledge of the antineutrino energy spectra arising from uranium and plutonium isotope fission is necessary. in these proceedings we relate about on-going simulation efforts aiming at reducing the errors associated to these spectra. the generic tools under development and presented below will allow to perform scenario studies on the feasibility of using antineutrinos to measure thermal power and to test to which precision the fuel composition can be deduced using these particles.
towards reactor monitoring with antineutrinos. null
characterization of natural organic matter from various origins (river, sea, soil, lagoon) and its interaction with metallic pollutants. null
interaction of eu(iii)/cm(iii) with polyacrylic acids : effect of ph. null
scandium-dota complexe for a new pet/3gamma camera for medical applications and radio labelling studies. null
towards better understanding of neutrino spectrum. null
modeling metal–particle interactions with an emphasis on natural organic matter. modeling the binding of metal ions and actinides to natural particles of various origins remains a challenging task. the use of models to understand the speciation and distribution of metals in the environment is essential for predicting metal bioavailability. in this article, we discuss the primary metal–particle interaction models and their limits, with an emphasis on natural organic matter (nom). these models have varying levels of complexity, but we discuss only those based on parameters that are independent of environmental conditions. we also discuss the application of such models to field predictions.
discrepancies in thorium oxide solubility values: a new experimental approach to improve understanding of oxide surface at solid/solution interface. the solubility of tho2(cr) was studied since many years but a large discrepancy in solubility values is noticed in the literature. the present work suggests that this discrepancy is related to differences in the surface properties of thorium oxide. to understand the role of surface properties on solubility values, we conducted experiments with tho2(cr) spheres with reproducable surface properties. batch dissolution experiments were conducted in 0.01 m nacl solution at ph = 3.0 and 4.0 for periods of time up to 270 days. the solutions were spiked with 229th to determine precipitation (sorption) rates of thorium, while dissolution rates were determined by measuring 232th released from tho2(cr) spheres. we assume that 229th atoms are exchanged only with active sites involved in th-dissolution. using 229th as local sensor of attachement and detachment processes at the tho2(cr) surface under close-to-equilibrium conditions, allows to assess surface reactivity of the solid during solubility experiments.
a neutron beam facility at spiral-2. the future spiral-2 facility is mainly composed of a high-power superconducting driver linac, delivering a high-intensity deuteron, proton and heavy ions beams. the first two beams are particularly well suited to the construction of a neutron beam and irradiation facility called neutrons for science (nfs). thick c and be target-converters with incident deuteron beam will produce an intense white neutron spectrum, while thin 7li target and incident proton beam allows generating quasi-monoenergetic neutrons. the 1–40mev neutron energy range will be covered and characterized by very intense fluxes available. the primary ion beam characteristics (energy, time resolution, intensity, etc.) are adequate to create a neutron time-of-flight facility. irradiation stations for neutron, proton and deuteron induced reactions could also be built in order to perform cross-sections measurements by activation techniques. in this paper we will discuss the potential of this new installation to investigate numerous topics, both in fundamental and applied physics. in particular, cross section measurements could be performed for different purposes like nuclear data evaluation, fission and fusion technology, accelerator driven systems, nuclear medicine, astrophysics, etc.
dm1 design: development of a detailed design of xt-ads and of a conceptual design of efit with heavy liquid metal cooling. the domain dm1_design of the fp6 integrated project eurotrans has the mission to launch the conceptual design of a lead-alloy-cooled european facility for industrial transmutation (efit) loaded with transmutation-dedicated minor actinide fuel to “prove” transmutation at the industrial level. efit would represent a modular unit of a large power system able to handle european high-level waste. in parallel to the efit design, dm1_design partners are developing a detailed design of the short-term xt-ads (experimental demonstration of transmutation in an accelerator driven system) that would serve the full-scale demonstration of the ads concept and be able to accept up to a full ma fuel assembly. xt-ads is also intended to serve as a test banc for some components of efit. the work plan of the domain dm1_design addresses the following main issues of efit and xt-ads: definition of the design parameters for the optimisation of the transmutation objectives in an economical way for efit and the short-term realisation for xt-ads including the most essential features of efit. development and assessment of the reference pb design of efit and the back-up option based on gas. in parallel and in a synergetic way xt-ads with only pb-bi reference design is developed in a more detailed manner. further development of the high-power proton accelerator (hppa) needed for both efit and xt-ads and, in particular, qualification of the beam reliability, the development of the beam transport line and the demonstration of the prototypical components of the chosen hppa. proof of feasibility windowless spallation targets making maximum use of the existing work for defining the needed complementary experimental work for xt-ads spallation targets. evaluation of the safety performance and licensability of the three designs developed in the domain dm1. cost estimates and planning issues for the deployment of the transmutation facilities. in this paper we give the progress accomplished in these different work packages and present the first results obtained after 18 months of activity.
design and supporting r&amp;d of the xt-ads spallation target. the xt-ads is an experimental accelerator-driven system (ads) that is being developed within the framework of the european fp6 eurotrans project that runs from 2005 to 2009. in this paper the current level of the design of the xt-ads spallation target and the status of corresponding r&amp;d topics with respect to lbe handling, thermal-hydraulics and spallation product confinement are discussed.
megapie spallation target: design implementation and preliminary tests of the first prototypical spallation target for future ads. the megapie target has been designed, manufactured, set-up and fitted with all the ancillary systems on an integral test stand in paul scherrer institute for off-beam tests dedicated to thermo-hydraulic and operability tests, carried out during the last months of 2005. it was then moved for final implementation to the sinq facility, with the ancillary systems, for irradiation that is foreseen to be carried out from july to december 2006. the results obtained during the integral tests have shown that the target was well designed for a safe operation and allowed to validate the main procedures related to fill and drain, steady-state operation, and transients due to beam trips. a start-up procedure has been developed, and the operating and control parameters have been defined. the already performed steps, conceptual and engineering design, manufacturing and assembly, safety and reliability assessment, integral off-beam tests, start-up of irradiation at sinq psi, then later decommissioning, post-irradiation experiments, and waste management will provide the ads community a uniquely relevant design and operational feedback.
arronax, a high intensity cyclotron in nantes. a cyclotron named arronax is being built in nantes (france). it is mainly devoted to radiochemistry and nuclear medicine research and will be operational the last quarter of 2008. this machine will accelerate both protons and α-particles at high energy (up to 70 mev) and high intensity (2 simultaneous proton beams with intensity up to 350 µa). in nuclear medicine, these characteristics will allow the cyclotron to produce a large variety of radionuclides on a regular schedule and in sufficient amount to perform clinical trials. a priority list of 12 radioisotopes, which contains isotopes for therapeutic use as well as for pet imaging, has been established by an international scientific committee. in radiochemistry, a vertical pulsed α-beam will allow fundamental studies of radiolysis in aqueous media, which is of great interest for radiobiology and for nuclear waste management.
volatile elements production rates in a proton-irradiated molten lead-bismuth target. the is419 experiment at the isolde facility at cern dedicated to the measurement of production and release rates of volatile elements from an irradiated pb/bi target by a proton beam of 1/1.4 gev has been completed. the release of he, ne, ar, br, kr, cd, i, xe, hg, po and at isotopes was investigated at different target temperatures, ranging from 250 °c to 600 ° c. three experimental methods were used for the mass-separated, ionized beams: i) implantation of short- and medium-lived isotopes in a tape and on-line detection with a hpge γ detector; ii) implantation of longer-lived isotopes in al foils and off-line detection with a hpge detector; iii) a faraday cup used mainly for stable nuclides. the results were compared with predictions from the fluka and mcnpx codes using different options for the intra-nuclear cascades and evaporation/fission models. results show good agreement with calculations for hg and for noble gases. for other elements such as iodine it is apparent that only a fraction of the produced isotopes is released. the results from fluka and mcnpx with the incl4/abla models are in general more satisfactory than those obtained using mcnpx with the standard bertini/dresner model combination. interestingly also significant yields of 204-210at isotopes were observed. at isotopes are produced either by (p, π-xn) charge exchange reactions on 209bi or by secondary reactions involving 3he and 4he. despite the non-release of polonium from pb/bi targets at typical operation temperatures, a smaller amount of highly radiotoxic po isotopes can actually be liberated indirectly as decay daughters of the released astatine.
spin alignment measurements of the $k^{*0}(892)$ and $\phi(1020)$ vector mesons in heavy ion collisions at $\sqrt{^{s}nn}$=200 gev. we present the first spin alignment measurements for the $k^{*0}(892)$ and $\phi(1020)$ vector mesons produced at mid-rapidity with transverse momenta up to 5 gev/c at $\sqrt{s_{nn}}$ = 200 gev at rhic. the diagonal spin density matrix elements with respect to the reaction plane in au+au collisions are $\rho_{00}$ = 0.32 $\pm$ 0.04 (stat) $\pm$ 0.09 (syst) for the $k^{*0}$ ($0.8.
complexation of trivalent metal ions with polyacrylic acid : effect of ph. null
complexation properties of natural organic matter. null
arronax, a high-energy and high-intensity cyclotron for nuclear medicine. purpose this study was aimed at establishing a list of radionuclides of interest for nuclear medicine that can be produced in a high-intensity and high-energy cyclotron. methods we have considered both therapeutic and positron emission tomography radionuclides that can be produced using a high-energy and a high-intensity cyclotron such as arronax, which will be operating in nantes (france) by the end of 2008. novel radionuclides or radionuclides of current limited availability have been selected according to the following criteria: emission of positrons, low-energy beta or alpha particles, stable or short half-life daughters, half-life between 3 h and 10 days or generator-produced, favourable dosimetry, production from stable isotopes with reasonable cross sections. results three radionuclides appear well suited to targeted radionuclide therapy using beta (67cu, 47sc) or alpha (211at) particles. positron emitters allowing dosimetry studies prior to radionuclide therapy (64cu, 124i, 44sc), or that can be generator-produced (82rb, 68ga) or providing the opportunity of a new imaging modality (44sc) are considered to have a great interest at short term whereas 86y, 52fe, 55co, 76br or 89zr are considered to have a potential interest at middle term. conclusions several radionuclides not currently used in routine nuclear medicine or not available in sufficient amount for clinical research have been selected for future production. high-energy, high-intensity cyclotrons are necessary to produce some of the selected radionuclides and make possible future clinical developments in nuclear medicine. associated with appropriate carriers, these radionuclides will respond to a maximum of unmet clinical needs.
design of a low noise, wide band, active dipole antenna for a cosmic ray radiodetection experiment. an active dipole antenna has been designed to measure transient electric field induced by ultra high energy cosmic rays for the codalema experiment. the main requirements for this detector, composed of a low noise preamplifier placed close to a dipole antenna, are a wide bandwidth ranging from 100 khz to 100 mhz and a good sensitivity on the whole spectrum.
crypto-hermitian quantum systems. null
crypto-hermiticity and crypto-supersymmetry of non-anticommuting hamiltonians. null
hd field theory as a theory of everything. null
transverse energy measurement in au+au collisions by the star experiment. transverse energy ($e_t$) has been measured with both of its components, namely hadronic ($e_t^{had}$) and electromagnetic ($e_t^{em}$) in a common phase space at mid-rapidity for 62.4 gev au+au collisions by the star experiment. $e_t$ production with centrality and $\sqrt{s_{nn}}$ is studied with similar measurements from sps to rhic and is compared with a final state gluon saturation model (ekrt). the most striking feature is the observation of a nearly constant value of $e_t/n_{ch} \sim 0.8$ gev from ags, sps to rhic. the initial energy density estimated by the boost-invariant bjorken hydrodynamic model, is well above the critical density for a deconfined matter of quarks and gluons predicted by lattice qcd calculations.
cryptogauge symmetry and cryptoghosts for crypto-hermitian hamiltonians. we discuss the hamiltonian h = p^2/2 - (ix)^{2n+1} and the mixed hamiltonian h = (p^2 + x^2)/2 - g(ix)^{2n+1}, which are crypto-hermitian in a sense that, in spite of apparent complexity of the potential, a quantum spectral problem can be formulated such that the spectrum is real. we note first that the corresponding classical hamiltonian system can be treated as a gauge system, with imaginary part of the hamiltonian playing the role of the first class constraint. we observe then that, on the basis of this classical problem, several different nontrivial quantum problems can be formulated. we formulate and solve some such problems. we find in particular that the spectrum of the mixed hamiltonian undergoes in certain cases rather amazing transformation when the coupling g is sent to zero. there is an infinite set of phase transitions in g when a couple of eigenstates of h coalesce and disappear from the spectrum. when quantization is done in the most natural way such that gauge constraints are imposed on quantum states, the spectrum should not be positive definite, but must involve the negative energy states (ghosts). we speculate that, in spite of the appearance of ghost states, unitarity might still be preserved.
development of a liquid xenon compton telescope dedicated to functional medical imaging. functional imaging is a technique used to locate in three dimensions the position of a radiotracer previously injected to a patient. the two main modalities used for a clinical application to detect tumors, the spect and the pet, use solid scintillators as a detection medium. &lt;br /&gt;the objective of this thesis was to investigate the possibility of using liquid xenon in order to benefit from the intrinsic properties of this medium in functional imaging. the feasibility study of such a device has been performed by taking into account the technical difficulties specific to the liquid xenon.&lt;br /&gt;first of all, simulations of a liquid xenon pet has been performed using monte-carlo methods. the results obtained with a large liquid xenon volume are promising: we can expect a reduction of the injected activity of radiotracer, an improvement of the spatial resolution of the image and a parallax free camera.&lt;br /&gt;the second part of the thesis was focused on the development of a new concept of medical imaging, the three gamma imaging, based on the use of a new emitter: the 44 scandium. associated to a classical pet camera, the compton telescope is used to infer the incoming direction of the third gamma ray by triangulation. therefore, it is possible to reconstruct the position of each emitter in three dimensions. &lt;br /&gt;this work convinced the scientific community to support the construction and characterization of a liquid xenon compton telescope. the first camera dedicated to small animal imaging should then be operational in 2009.
study of ion emission from a germanium crystal surface under impact of fast pb ions in channeling conditions. a thin germanium crystal has been irradiated at ganil by pb beams of 29 mev/a (charge state qin = 56 and 72) and of 5.6 mev/a (qin = 28). the induced ion emission from the sample entrance surface was studied, impact per impact, as a function of qin, velocity vin and energy loss de in the crystal. the pb ions transmitted through the crystal were analyzed in charge (qout) and energy using the speg spectrometer. the emitted ionized species were detected and analyzed in mass by a time of flight multianode detector (lag). channeling was used to select peculiar de in ge and hence peculiar pb ion trajectories close to the emitting surface. the experiment was performed in standard vacuum. no ge emission was found. the dominating emitted species are h+ and hydrocarbon ions originating from the contamination layer on top of the crystal. the mean value  of the number of detected species per incoming pb ion (multiplicity) varies as (qin/vin)^p, with p values in agreement with previous results. we have clearly observed an influence of the energy deposition de in ge on the emission from the top contamination layer. when selecting increasing values of de, we observed a rather slow increase of . on the contrary, the probabilities of high multiplicity values, that are essentially connected to fragmentation after emission, strongly increase with de.
multi-variable constrained control approach for a three-dimensional eel-like robot. in this paper, a multi-variable feedback design for the 3d movement of an eel-like robot is presented. such a robot is under construction in the context of a national french robotic project. the proposed feedback enables the tracking of a desired 3d position of the eel's head as well as the stabilization of the rolling angle. the control design is based on a recently developed reduced model that have been validated using a 3d complete continuous model. several scenarios are proposed to assess the efficiency of the proposed feedback law.
charged hadron multiplicity fluctuations in au+au and cu+cu collisions from sqrt(s_nn) = 22.5 to 200 gev. a comprehensive survey of event-by-event fluctuations of charged hadron multiplicity in relativistic heavy ions is presented. the survey covers au+au collisions at sqrt(s_nn) = 62.4 and 200 gev, and cu+cu collisions sqrt(s_nn) = 22.5, 62.4, and 200 gev. fluctuations are measured as a function of collision centrality, transverse momentum range, and charge sign. after correcting for non-dynamical fluctuations due to fluctuations in the collision geometry within a centrality bin, the remaining dynamical fluctuations expressed as the variance normalized by the mean tend to decrease with increasing centrality. the dynamical fluctuations are consistent with or below the expectation from a superposition of participant nucleon-nucleon collisions based upon p+p data, indicating that this dataset does not exhibit evidence of critical behavior in terms of the compressibility of the system. an analysis of negative binomial distribution fits to the multiplicity distributions demonstrates that the heavy ion data exhibit weak clustering properties.
probing gluon shadowing with forward photons at rhic. there is a major need to better constrain nuclear parton densities in order to provide reliable perturbative qcd predictions at the lhc as well as to probe possible non-linear evolution at small values of x. in these proceedings, we discuss how the production of prompt photons at large rapidity in p-p and d-au collisions at rhic (sqrt(s_nn)=200 gev) is sensitive to the nuclear modifications of gluon distributions at x~0.001 and at rather low scales, q^2~10 gev^2. the nuclear production ratio, r_dau=sigma(d+a-&gt;gamma+x)/(2a sigma(p+p-&gt;gamma x), is computed for isolated prompt photons at nlo using the ndsg nuclear parton densities, in order to assess the visibility of the signal. we also emphasise that the expected counting rates in a year of running at rhic are large, indicating that r_dau could be measured with a high statistical accuracy.
$\alpha$ particle preformation in heavy nuclei and penetration probability. the $\alpha$ particle preformation in the even-even nuclei from $^{108}$te to $^{294}$118 and the penetration probability have been studied. the isotopes from pb to u have been firstly investigated since the experimental data allow us to extract the microscopic features for each element. the assault frequency has been estimated using classical methods and the penetration probability from tunneling through the generalized liquid drop model (gldm) potential barrier. the preformation factor has been extracted from experimental $\alpha$ decay energies and half-lives. the shell closure effects play the key role in the $\alpha$ preformation. the more the nucleon number is close to the magic numbers, the more the formation of $\alpha$ cluster is difficult inside the mother nucleus. the penetration probabilities reflect that 126 is a neutron magic number. the penetration probability range is very large compared to that of the preformation factor. the penetration probability determines mainly the $\alpha$ decay half-life while the preformation factor allows us to obtain information on the nuclear structure. the study has been extended to the newly observed heaviest nuclei.
ternary cluster decay within the liquid drop model. longitudinal ternary and binary fission barriers of $^{36}$ar, $^{56}$ni and $^{252}$cf nuclei have been determined within a rotational liquid drop model taking into account the nuclear proximity energy. for the light nuclei the heights of the ternary fission barriers become competitive with the binary ones at high angular momenta since the maximum lies at an outer position and has a much higher moment of inertia.
coplanar ternary cluster decay of hyper-deformed $^{56}$ni and $^{60}$zn at high angular momentum. using a unique two-arm detector system for heavy ions (brs, binary reaction spectrometer) coincident fission events have been measured from the decay of 60zn and 56ni compound nuclei formed at 83–88 mev excitation energy. from the binary coincidences inclusive and exclusive cross sections for fission channels with differing losses of charge were obtained. narrow out-of-plane correlations corresponding to coplanar decay are observed for two fragments emitted in binary events, and in the data for ternary decay with missing charges from 4 up to 8. differential cross sections for the different binary and ternary fission channels are obtained. the ternary cluster fission can be explained by the statistical decay from equilibrated compound nuclei with hyper-deformed shapes with angular momenta around 45–52 h.
indications of conical emission of charged hadrons at rhic. three-particle azimuthal correlation measurements with a high transverse momentum trigger particle are reported for pp, d+au, and au+au collisions at 200 gev by the star experiment. the acoplanarities in pp and d+au indicate initial state kt broadening. larger acoplanarity is observed in au+au collisions. the central au+au data show an additional effect signaling conical emission of correlated charged hadrons.
enhanced production of direct photons in au+au collisions at sqrt(s_nn)=200 gev and implications for the initial temperature. the production of low mass e+e- pairs for m_{e+e-} &lt; 300 mev/c^2 and 1 &lt; p_t &lt;5 gev/c is measured in p+p and au+au collisions at sqrt(s_nn)=200 gev. enhanced e+e- pair yield above hadronic sources is observed in au+au collisions. treating the excess as internal conversion of direct photons, the invariant yield of direct photons is deduced. in central au+au collisions, the excess over p+p is exponential in p_t}, with inverse slope t = 221 +/- 23 (stat) +/- 18 (syst) mev. hydrodynamical models with initial temperatures t_init ~ 300-600 mev at times of 0.6 - 0.15 fm/c after the collision are in qualitative agreement with the data. lattice qcd predicts a phase transition at ~ 170 mev.
charmed hadron production at low transverse momentum in au+au collisions at rhic. we report measurements of charmed hadron production from hadronic ($d\to k\pi$) and semileptonic ($\mu$ and $e$) decays in 200 gev au+au collisions at rhic. analysis of the spectra indicates that charmed hadrons have a different radial flow pattern from light or multi-strange hadrons. charm cross sections at mid-rapidity are extracted by combining the three independent measurements, covering the transverse momentum range that contributes to $\sim$90% of the integrated cross section. the cross sections scale with number of binary collisions of the initial nucleons, a signature of charm production exclusively at the initial impact of colliding heavy ions. the implications for charm quark interaction and thermalization in the strongly interacting matter are discussed.
selected effects of the in-medium nucleon-nucleon cross section on heavy-ion dynamics below 100 mev/u. the role of the value of the in-medium nucleon-nucleon cross section on nuclear dynamics at intermediate energies is studied within the framework of a semiclassical transport model. particular attention is payed to the early reaction phase and the effects produced on the energy transformed into heat and compression as well as on the prompt dynamical emission.
neutron-induced light-ion production from fe, pb and u at 96 mev. double-differential cross sections for light-ion production (up to a=4) induced by 96 mev neutrons have been measured for $^{nat}$fe, $^{nat}$pb and $^{nat}$u. the experiments have been performed at the the svedberg laboratory in uppsala, using two independent devices, medley and scandal. the recorded data cover a wide angular range (20º - 160º) with low energy thresholds. the work was performed within the hindas collaboration studying three of the most important nuclei for incineration of nuclear waste with accelerator-driven systems (ads). the obtained cross section data are of particular interest for the understanding of the so-called pre-equilibrium stage in a nuclear reaction and are compared with model calculations performed with the gnash, talys and preeq codes.
radio-detection of uhecr by the codalema experiment. the principle of the codalema experiment is based on an original approach of the de-tection of radio transients associated with extensive air showers induced by ultra high-energy cosmic rays. since september 2006, codalema has been under operation with a new setup at the nançay radio observatory, france. it uses 16 broadband dipole antennas associated with 13 particle detectors generating the trigger and allowing the primary cosmic ray energy estimation. we will present evi-dence for the radio detection of cosmic rays above $10^{17}$ ev, based on an event-by-event analysis and we will discuss the radio characteristics of these showers.
fission and ternary cluster decay of hyper-deformed $^{56}$n. coincidences between two heavy fragments have been measured from fission of 56ni compoud nuclei, formed in the 32s + 24mg reaction at elab(32s) = 165.4 mev. a unique experimental set-up consistiong of two large area position sensitive (x, y) gas-detector telescopes has been used allowing the complete determination of the observed fragments, and their momentum vectors. in addition to binary fission events with subsequent particle evaporation, narrow out-of-plane correlations are observed for two fragments emitted in purely binary events and in events with a missing charge consisting of 2$\alpha$ and 3$\alpha$-particles(12c). these events are interpreted as ternary cluster decay from 56ni-nuclei at high angular momenta through hyper-deformed shapes.
effect of heavy-quark energy loss on the muon differential production cross section in pb-pb collisions at $\sqrt{s_{nn}}$=5.5 tev. we study the nuclear modification factors raa and rcp of the high transverse momentum 5.
the hadronic interaction model epos. null
measuring gluon shadowing with prompt photons at rhic and lhc. the possibility to observe the nuclear modification of the gluon distribution at small-x (gluon shadowing) using high-p_t prompt photon production at rhic and at lhc is discussed. the per-nucleon ratio, sigma(p+a -&gt; gamma+x) / a sigma(p+p -&gt; gamma+x), is computed for both inclusive and isolated prompt photons in perturbative qcd at nlo using different parametrizations of nuclear parton densities, in order to assess the visibility of the shadowing signal. the production of isolated photons turns out to be a promising channel which allows for a reliable extraction of the gluon density, g^a/g^p, and the structure function, f_2^a/f_2^p, in a nucleus over that in a proton. moreover, the production ratio of prompt photons at forward-over-backward rapidity in p-a collisions provides an estimate of g^a/g^p (at small x) over f_2^a/f_2^p (at large x), without the need of p-p reference data at the same energy.
on the coefficients of the liquid drop model mass formulae and nuclear radii. the coefficients of different mass formulae derived from the liquid drop model and including or not the curvature energy, the diffuseness correction to the coulomb energy, the charge exchange correction term, different forms of the wigner term and different powers of the relative neutron excess $i=(n-z)/a$ have been determined by a least square fitting procedure to 2027 experimental atomic masses. the coulomb diffuseness correction $z^2/a$ term or the charge exchange correction $z^{4/3}/a^{1/3}$ term plays the main role to improve the accuracy of the mass formula. the wigner term and the curvature energy can also be used separately for the same purpose. the introduction of an $|i|$ dependence in the surface and volume energies improves slightly the efficiency of the expansion and is more effective than an $i^4$ dependence. different expressions reproducing the experimental nuclear charge radius are provided. the different fits lead to a surface energy coefficient of around 17-18~mev and a relative equivalent rms charge radius r$_0$ of 1.22-1.23~fm.
200 and 300 mev/nucleon nuclear reactions responsible for single-event effects in microelectronics. an experimental study of nuclear reactions between 28si nuclei at 200 and 300 mev/nucleon and hydrogen or deuterium target nuclei was performed at the celsius storage ring in uppsala, sweden, to collect information about the reactions responsible for single-event effects in microelectronics. inclusive data on 28si fragmentation, as well as data on correlations between recoils and spectator protons or particles are compared to predictions from the dubna cascade model and the japan atomic energy research institute version of the quantum molecular dynamics model. the comparison shows satisfactory agreement for inclusive data except for he fragments where low-energy sub-barrier fragments and recoiling fragments with very large momenta are produced much more frequently than predicted. the yield of exclusive data are also severely underestimated by the models whereas the charge distributions of recoils in these correlations compare well. the observed enhancement in he emission, which may well be important for the description of single-event effects, is most likely to be attributed to clustering in 28si nuclei.
a macroscopic description of coherent geo-magnetic radiation from cosmic-ray air showers. we have developed a macroscopic description of coherent electromagnetic radiation from air showers initiated by ultra-high-energy cosmic rays due to the presence of the geo-magnetic field. this description offers a simple and direct insight in the relation between the properties of the air shower and the time structure of the radio pulse.
macroscopic treatment of radio emission from cosmic ray air showers based on shower simulations. we present a macroscopic calculation of coherent electro-magnetic radiation from air showers initiated by ultra-high energy cosmic rays, based on currents obtained from monte carlo simulations of air showers in a realistic geo-magnetic field. we can clearly relate the time signal to the time dependence of the currents. we find that the the most important contribution to the pulse is related to the time variation of the currents. for showers forming a sufficiently large angle with the magnetic field, the contribution due to the currents induced by the geo-magnetic field is dominant, but neither the charge excess nor the dipole contribution can be neglected. we find a characteristic bipolar signal. in our calculations, we take into account a realistic index of refraction, whose importance depends on the impact parameter and the inclination. also very important is the role of the positive ions.
$\rho^0$ photoproduction in ultraperipheral relativistic heavy ion collisions at $\sqrt{s_{nn}}$=200 gev. photoproduction reactions occur when the electromagnetic field of a relativistic heavy ion interacts with another heavy ion. the star collaboration presents a measurement of rho^0 and direct pi^+pi^- photoproduction in ultra-peripheral relativistic heavy ion collisions at sqrt(s_{nn})=200 gev. we observe both exclusive photoproduction and photoproduction accompanied by mutual coulomb excitation. we find a coherent cross-section of sigma(auau) -&gt; au^*au^*rho^0 = 530 pm 19 (stat.) pm 57 (syst.) mb, in accord with theoretical calculations based on a glauber approach, but considerably below the predictions of a color dipole model. the rho^0 transverse momentum spectrum (p_{t}^2 ) is fit by a double exponential curve including both coherent and incoherent coupling to the target nucleus; we find sigma_{inc}/sigma_{coh} = 0.29 pm 0.03 (stat.) pm 0.08 (syst.). the ratio of direct pi^+pi^- to rho^0 production is comparable to that observed in gamma p collisions at hera, and appears to be independent of photon energy. finally, the measured rho^0 spin helicity matrix elements agree within errors with the expected s-channel helicity conservation.
saturation of $e_t/n_{ch}$ and freeze-out criteria in heavy-ion collisions. the pseudorapidity densities of transverse energy, the charged particle multiplicity and their ratios, $e_t/n_{ch}$, are estimated at mid-rapidity, in a statistical-thermal model based on chemical freeze-out criteria, for a wide range of energies from gsi-ags-sps to rhic. it has been observed that in nucleus-nucleus collisions, $e_t/n_{ch}$ increases rapidly with beam energy and remains approximately constant at about a value of 800 mev for beam energies from sps to rhic. $e_t/n_{ch}$ has been observed to be almost independent of centrality at all measured energies. the statistical-thermal model describes the energy dependence as well as the centrality independence, qualitatively well. the values of $e_t/n_{ch}$ are related to the chemical freeze-out criterium, $e/n \approx 1 gev$ valid for primordial hadrons. we have studied the variation of the average mass $(), n_{decays}/n_{primordial}, n_{ch}/n_{decays}$ and $e_t/n_{ch}$ with $\sqrt{s_{nn}}$ for all freeze-out criteria discussed in literature. these observables show saturation around sps and higher $\sqrt{s_{nn}}$, like the chemical freeze-out temperature ($t_{ch}$).
supersymmetry vs ghosts. we consider the simplest nontrivial supersymmetric quantum mechanical system involving higher derivatives. we unravel the existence of additional bosonic and fermionic integrals of motion forming a nontrivial algebra. this allows one to obtain the exact solution both in the classical and quantum cases. the supercharges $q, \bar q$ are not anymore hermitially conjugate to each other, which allows for the presence of negative energies in the spectrum. we show that the spectrum of the hamiltonian is unbounded from below. it is discrete and infinitely degenerate in the free oscillator-like case and becomes continuous running from $-\infty$ to $\infty$ when interactions are added. notwithstanding the absence of the ground state, there is no collapse, which suggests that a unitary evolution operator may be defined.
consequences of a $\lambda_c/d$ enhancement effect on the non-photonic electron nuclear modification factor in central heavy ion collisions at rhic energy. the rhic experiments have measured the nuclear modification factor r_aa of non-photonic electrons in au+au collisions at sqrt(s) = 200 gev. this r_aa exhibits a large suppression for pt &gt; 2 gev/c which is commonly attributed to heavy-quark energy loss. in order to reproduce satisfactorily the data, energy-loss models assume a heavy-quark energy loss similar to that of light quarks. however, it is expected that the heavy-quark radiative energy loss is smaller than the light quark one because of the so-called dead-cone effect. an enhancement of the charm baryon yield with respect to the charm meson yield, as it is observed for light and strange hadrons, can explain part of the suppression. this phenomenon has been put forward in a previous work. we present in this paper a more complete study based on a detailed simulation which includes electrons from charm and bottom decay, charm and bottom quark realistic energy loss as well as a more realistic modeling of the lambda_c/d enhancement. we show that a lambda_c/d ratio close to unity, as observed for light and strange quarks, could explain 20-25% of the suppression of non-photonic electrons in central au+au collisions. this effect remains significant at relatively high non-photonic electron transverse momenta of 8-9 gev/c.
qcd running coupling and collisional jet quenching. i point out that the existing results for the qcd collisional energy loss of a fast parton in the quark–gluon plasma need redressing. relating the corrected result to the running coupling (at t = 0) and to the debye mass, i argue that collisional energy loss is pertinent for the understanding of jet quenching.
recent $\alpha$ decay half-lives and analytic expression predictions including superheavy nuclei. new recent experimental $\alpha$ decay half-lives have been compared with the results obtained from previously proposed formulas depending only on the mass and charge numbers of the $\alpha$ emitter and the q$\alpha$ value. for the heaviest nuclei they are also compared with calculations using the density-dependent m3y (ddm3y) effective interaction and the viola-seaborg-sobiczewski (vss) formulas. the correct agreement allows us to make predictions for the $\alpha$ decay half-lives of other still unknown superheavy nuclei from these analytic formulas using the extrapolated q$\alpha$ of g. audi, a. h. wapstra, and c. thibault [nucl. phys. a729, 337 (2003)].
some boolean successes in medical research. null
collisional energy loss of a fast heavy quark in a quark-gluon plasma. we discuss the average collisional energy loss de/dx of a heavy quark crossing a quark-gluon plasma, in the limit of high quark energy e &gt;&gt; m^2/t, where m is the quark mass and t &gt;&gt; m is the plasma temperature. in the fixed coupling approximation, at leading order de/dx \propto \alpha_s^2, with a coefficient which is logarithmically enhanced. the soft logarithm arising from t-channel scattering off thermal partons is well-known, but a collinear logarithm from u-channel exchange had previously been overlooked. we also determine the constant beyond those leading logarithms. we then generalize our calculation of de/dx to the case of running coupling. we estimate the remaining theoretical uncertainty of de/dx, which turns out to be quite large under rhic conditions. finally, we point out an approximate relation between de/dx and the qcd debye mass, from which we derive an upper bound to de/dx for all quark energies.
stiffness analysis of 3-d.o.f. overconstrained translational parallel manipulators. the paper presents a new stiffness modelling method for overconstrained parallel manipulators, which is applied to 3-d.o.f. translational mechanisms. it is based on a multidimensional lumped-parameter model that replaces the link flexibility by localized 6-d.o.f. virtual springs. in contrast to other works, the method includes a fea-based link stiffness evaluation and employs a new solution strategy of the kinetostatic equations, which allows computing the stiffness matrix for the overconstrained architectures and for the singular manipulator postures. the advantages of the developed technique are confirmed by application examples, which deal with comparative stiffness analysis of two translational parallel manipulators.
towards an understanding of the rhic single electron data. the present theoretical approaches for the single electron spectra from heavy meson decay, measured at rhic by the star and the phenix collaborations, which are based on perturbative qcd (pqcd) underpredict the collisional energy loss by a large factor and are not able to reproduce the azimuthal distribution. they suffer in addition from the fact that neither the value of the (fixed) coupling constant nor that for the infrared (ir) regulator are well determined. we investigate the consequences of a physical running coupling constant and of a ir regulator, determined by a hard thermal loop (htl) approach. both together increase the collisional energy loss substantially and bring the out-of-plane $(v_2)$ anisotropy as well as the quenching to values close to the experimental values without excluding a contribution from radiative energy loss.
effects of ionizing radiation on the hollandite structure-type: $ba_{0.85}cs_{0.26}al_{1.35}fe_{0.77}ti_{5.90}o_{16}$. the hollandite structure-type has received considerable attention as a nuclear waste form for the incorporation of radioactive 135cs and 137cs, both of which are important fission product radionuclides in the high-level nuclear waste generated by the reprocessing of used nuclear fuel. a critical concern has been the effects of high doses of ionizing radiation from incorporated cs on the long-term structural stability of the hollandite structure. optimization of the synthesis conditions has resulted in the hollandite stoichiometry of ba0.85cs0.26al1.35fe0.77ti5.90o16. to evaluate the effect of cs-beta-decay on this stoichiometry, we have simulated the ionizing radiation using 200 kv electron beam using transmission electron microscopy (tem) at 298 and 573 k. complete amorphization was achieved at doses of 1.1 x 1014 and 1.8 x 1014 gy at temperatures of 298 and 573 k, respectively. electron energyloss spectroscopy (eels) of the cs m-edge revealed the selective loss of cs at the maximum doses. hollandite irradiated using gamma rays, ~106 gy, which has defects associated with the formation of ti3+ and o2 – had a dissolution rate similar to that of the pristine hollandite, suggesting that the initial stage of defect formation does not influence chemical durability. because the accumulated dose in the hollandite with 5 wt% of radioactive 137cs2o is estimated to be ~2.0 x 1010 gy after 500 years, the hollandite structure should be stable under the conditions anticipated for geologic disposal.
elastic scattering of 96 mev neutrons from iron, yttrium, and lead. data on elastic scattering of 96 mev neutrons from 56fe, 89y, and 208pb in the angular interval 10-70° are reported. the previously published data on 208pb have been extended, as a new method has been developed to obtain more information from data, namely to increase the number of angular bins at the most forward angles. a study of the deviation of the zero-degree cross section from wick's limit has been performed. it was shown that the data on 208pb are in agreement with wick's limit while those on the lighter nuclei overshoot the limit significantly. the results are compared with modern optical model predictions, based on phenomenology and microscopic nuclear theory. the data on 56fe, 89y, and 208pb are in general in good agreement with the model predictions.
dilepton mass spectra in p+p collisions at $\sqrt{s}$= 200 gev and the contribution from open charm. the phenix experiement has measured the electron-positron pair mass spectrum from 0 to 8 gev/c^2 in p+p collisions at sqrt(s)=200 gev. the contributions from light meson decays to e^+e^- pairs have been determined based on measurements of hadron production cross sections by phenix. they account for nearly all e^+e^- pairs in the mass region below 1 gev/c^2. the e^+e^- pair yield remaining after subtracting these contributions is dominated by semileptonic decays of charmed hadrons correlated through flavor conservation. using the spectral shape predicted by pythia, we estimate the charm production cross section to be 544 +/- 39(stat) +/- 142(syst) +/- 200(model) \mu b, which is consistent with qcd calculations and measurements of single leptons by phenix.
pion freeze-out time in pb+pb collisions at 158 a gev/c studied via $\pi^-/\pi^+$ and $k^-/k^+$ ratios. the effect of the final state coulomb interaction on particles produced in pb+pb collisions at 158 a gev/c has been investigated in the wa98 experiment through the study of the pi-/pi+ and k-/k+ ratios measured as a function of transverse mass. while the ratio for kaons shows no significant transverse mass dependence, the pi-/pi+ ratio is enhanced at small transverse mass values with an enhancement that increases with centrality. a silicon pad detector located near the target is used to estimate the contribution of hyperon decays to the pi-/pi+ ratio. the comparison of results with predictions of the rqmd model in which the coulomb interaction has been incorporated allows to place constraints on the time of the pion freeze-out.
dihadron azimuthal correlations in au+au collisions at $\sqrt{s_{nn}}$=200 gev. azimuthal angle (delta phi) correlations are presented for a broad range of transverse momentum (0.4 &lt; pt &lt; 10 gev/c) and centrality (0-92%) selections for charged hadrons from di-jets in au+au collisions at sqrt(s_nn) = 200 gev. with increasing pt, the away-side delta phi distribution evolves from a broad and relatively flat shape to a concave shape, then to a convex shape. comparisons to p+p data suggest that the away-side distribution can be divided into a partially suppressed head region centered at delta phi ~ \pi, and an enhanced shoulder region centered at delta phi ~ \pi \pm 1:1. the pt spectrum for the associated hadrons in the head region softens toward central collisions. the spectral slope for the shoulder region is independent of centrality and trigger pt . the properties of the near-side distributions are also modified relative to those in p + p collisions, reflected by the broadening of the jet shape in delta phi and delta eta, and an enhancement of the per-trigger yield. however, these modifications seem to be limited to pt &lt; 4 gev/c, above which both the dihadron pair shape and per-trigger yield become similar to p + p collisions. these observations suggest that both the away- and near-side distributions contain a jet fragmentation component which dominates for pt \ge 5gev and a medium-induced component which is important for pt \le 4 gev/c. we also quantify the role of jets at intermediate and low pt through the yield of jet-induced pairs in comparison to binary scaled p + p pair yield. the yield of jet-induced pairs is suppressed at high pair proxy energy (sum of the pt magnitudes of the two hadrons) and is enhanced at low pair proxy energy. the former is consistent with jet quenching; the latter is consistent with the enhancement of soft hadron pairs due to transport of lost energy to lower pt.
energy dependence of $\pi^0$ production in cu+cu collisions at $\sqrt{s_{nn}}$ = 22.4, 62.4, and 200 gev. neutral pion transverse momentum (pt) spectra at mid-rapidity (|y| &lt; 0.35) were measured in cu+cu collisions at \sqrt s_nn = 22.4, 62.4, and 200 gev. relative to pi -zero yields in p+p collisions scaled by the number of inelastic nucleon-nucleon collisions (ncoll) at the respective energies, the pi-zero yields for pt \ge 2 gev/c in central cu+cu collisions at 62.4 and 200 gev are suppressed, whereas an enhancement is observed at 22.4 gev. a comparison with a jet quenching model suggests that final state parton energy loss dominates in central cu+cu collisions at 62.4 gev and 200 gev, while the enhancement at 22.4 gev is consistent with nuclear modifications in the initial state alone.
macroscopic dynamics of the fusion process. a macroscopic dynamical model has been used to calculate fusion cross sections for a wide number of systems ranging from 16o + 27al to 40ar + 165ho. this model takes into account the reorganization of the densities and the nucleon exchanges. the possible reaction mechanisms after capture of the system into the pocket of the interaction potential are discussed two processes contribute to fusion : compound nucleus formation and fast fission phenomenon. the calculated fusion cross sections are in overall agreement with the data.
theoretical analysis of singleton arc consistency and its extensions. null
solutions of the landau-vlasov equation in nuclear physics. the properties of vlasov equation solutions obtained by projection on coherent state basis are discussed. such solutions satisfy stationarity conditions and satisfactorily describe the average diffusivity of nuclear phase space and reproduce the bulk properties of nuclei. sampling methods and their effects on dynamics are discussed for the study of heavy ion reactions at intermediate energies. the non-local gogny force is easily computable on this basis which allows to use it for dynamical nuclear studies.
a study on the fusion reaction 139la + 12c at 50 mev/u with the vuu equation. recently bownan et al. found that in the reaction 139la + 12c at 50 mev/u a compound nucleus is formed. we simulate this reaction with a numerical solution of the vuu equation and indeed find that for a central collision the system fuses and equilibrates after 90 fm/c.
phase space dynamics of heavy ion nuclear collisions in the fermi energy domain. null
status of the muon spectrometer offline software. null
perspectives of the muon offline. null
w production at lhc in the alice experiment. null
presentation of the pwg3 of alice. null
experimental conditions for quarkonia measurements in alice. null
heavy flavor physics with alice. null
centrality dependence of charged hadron and strange hadron elliptic flow from $\sqrt{s_{nn}}$ = 200 gev au+au collisions. we present star results on the elliptic flow v_2 of charged hadrons, strange and multi-strange particles from sqrt(s_nn) = 200 gev au+au collisions at rhic. the detailed study of the centrality dependence of v_2 over a broad transverse momentum range is presented. comparison of different analysis methods are made in order to estimate systematic uncertainties. in order to discuss the non-flow effect, we have performed the first analysis of v_2 with the lee-yang zero method for k_s^0 and lambda. in the relatively low p_t region, p_t &lt;= 2 gev/c, a scaling with m_t - m is observed for identified hadrons in each centrality bin studied. we do not observe v_2(p_t) scaled by the participant eccentricity to be independent of centrality. at higher p_t, 2 gev/c &lt;= p_t &lt;= 6 gev/c, v_2 scales with quark number for all hadrons studied. for the multi-strange hadron omega, which does not suffer appreciable hadronic interactions, the values of v_2 are consistent with both m_t - m scaling at low p_t and number-of-quark scaling at intermediate p_t. as a function of collision centrality, an increase of p_t-integrated v_2 scaled by the participant eccentricity has been observed, indicating a stronger collective flow in more central au+au collisions.
suppression pattern of neutral pions at high transverse momentum in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev and constraints on medium transport coefficients. for au + au collisions at 200 gev we measure neutral pion production with good statistics for transverse momentum, p_t, up to 20 gev/c. a fivefold suppression is found, which is essentially constant for 5 &lt; p_t &lt; 20 gev/c. experimental uncertainties are small enough to constrain any model-dependent parameterization for the transport coefficient of the medium, e.g. \mean(q^hat) in the parton quenching model. the spectral shape is similar for all collision classes, and the suppression does not saturate in au+au collisions; instead, it increases proportional to the number of participating nucleons, as n_part^2/3.
a process to develop operational bottom up evaluation methods - from reference guidebooks to a practical culture of evaluation. null
a systematic study on direct photon production from central heavy ion collisions. we investigate the production of direct photons in central au-au collisions at the relativistic heavy-ion collider (rhic) at 200 gev per nucleon, considering all possible sources. we treat thermal photons emitted from a quark-gluon plasma and from a hadron gas, based on a realistic thermodynamic expansion. hard photons from elementary nucleon-nucleon scatterings are included: primordial elementary scatterings are certainly dominant at large transverse momenta, but also secondary photons from jet fragmentation and jet-photon conversion cannot be ignored. in both cases we study the effect of energy loss, and we also consider photons emitted from bremsstrahlung gluons via fragmentation.
crypto-hermiticity of nonanticommutative theories. we note that, though nonanticommutative deformations of minkowski supersymmetric theories do not respect the reality condition and seem to lead to non-hermitian hamiltonians h, the latter belong to the class of crypto-hermitian (or quasi-hermitian) hamiltonians having attracted recently a considerable attention. they can be made manifestly hermitian via the similarity transformation h -&gt; e^r h e^{-r} with a properly chosen r. the deformed model enjoys the same supersymmetry algebra as the undeformed one though it is difficult in some cases to write explicit expressions for a half of supercharges. the deformed sqm models make perfect sense. it is not clear whether it is also the case for nac minkowski field theories -- the conventionally defined s--matrix is not unitary there.
high-order sliding-mode controllers of an electropneumatic actuator: application to an aeronautic benchmark. this paper presents the control of an electropneumatic system used for moving steering mechanism. this aeronautic application needs a high-precision position control and high bandwidth. the structure of the experimental setup and the benchmark on which controllers are evaluated have been designed in order to precisely check the use of such actuator in aeronautics. two kinds of controllers are designed: a linear one based on gain scheduling feedback, and two high order sliding mode controllers ensuring finite-time convergence, high accuracy and robustness. experimental results display feasibility and high performance of each controller and a comparison study is done.
source breakup dynamics in au+au collisions at $sqrt{s_{nn}}$=200 gev via three-dimensional two-pion source imaging. a three-dimensional (3d) correlation function obtained from mid-rapidity, low pt pion pairs in central au+au collisions at sqrt(s_nn)=200 gev is studied. the extracted model-independent source function indicates a long range tail in the directions of the pion pair transverse momentum (out) and the beam (long). model comparisons to these distensions indicate a proper breakup time \tau_0 ~ 9 fm/c and a mean proper emission duration \delta\tau ~ 2 fm/c, leading to sizable emission time differences (&lt;|\delta \tau_lcm |&gt; ~ 12 fm/c), partly due to resonance decays. they also suggest an outside-in "burning" of the emission source reminiscent of many hydrodynamical models.
performances of the silicon strip detector (ssd) of the star experiment at rhic. the silicon strip detector (ssd) is the fourth layer of detector using a double-sided microstrip technology of the star experiment at rhic, thus completes its inner tracking device. the goal of star is to study heavy ions collisions in order to probe the existence of the qgp, a deconfined state of nuclear matter. strangeness enhancement, such as k0-short, λ, ξ, ω particles production, has been proposed to sign the formation of the qgp. then precise measurement of secondary vertices is needed.&lt;br /&gt;the ssd will also permit an attempt to use the inner tracking device to measure charm and beauty with direct topological identification. it was proposed to enhance the star tracking capabilities by providing a better connexion between reconstructed tracks in the main tracking device (tpc) and the initial vertex detector (svt).&lt;br /&gt;in this thesis, we will present the intrinsic performances of the ssd and its impact on the inner tracking system performances by studying cu-cu collisions occurred at rhic in 2005. study of simulated data will also permit a better comprehension of these results.
motion control of a three-dimensional eel-like robot without pectoral fins. in this paper, recent advances in the design of feedback laws for the 3d movement of an eel-like robot are presented. such a robot is under construction in the context of a national french robotic project. the proposed feedback enables the tracking of a desired 3d position of the eel head as well as the stabilization of the rolling angle without using pectoral fins. we build on a previous work in which we proposed a complete control scheme for robot's 3d movement using its pectoral fins. the controller is tested on a recently developed complete 3d model in order to assess its efficiency in tackling 3d manoeuvres.
performance of the alice muon spectrometer. &lt;br /&gt;weak boson production and measurement in heavy-ion collisions at lhc. lattice qcd predicts a transition from a hadronic phase to a quark gluon plasma phase, qgp, for temperatures above 10^{13} k. heavy-ion collisions are proposed to recreate it in the laboratory. with such a purpose, the lhc will provide pb-pb collisions at 5.5 tev/u, and the alice experiment will permit to explore them. in particular, the alice muon spectrometer will permit to investigate the muon related probes (quarkonia, open beauty,...). the expected apparatus performances to measure muons and dimuons are discussed. a factorization technique is employed to unravel the different contributions to the global efficiency. results indicate that the detector should be able to measure muons up to pt~100 gev/c with a resolution of about 10%. we show that weak bosons production could be measured for the first time in heavy-ion collisions. single muon pt and dimuons invariant mass distributions will probe w and z production. as mainly muons from b- and c-quarks decays will populate the intermediate-pt of 5-25 gev/c, heavy quark in-medium energy loss calculations indicate that the single muon spectra would be suppressed by a factor 2-4 in the most central 0-10% pb-pb collisions at 5.5 tev. however, for pt &gt; 35 gev/c the weak boson decays are predominant, and no suppression is expected. estimations indicate that the b- and w-muons crossing point shifts down in transverse momenta by 5 to 7 gev/c in the most central 0-10% pb-pb collisions at 5.5 tev.
exclusive electroproduction and off-diagonal parton distributions. off-diagonal parton distributions occur in several hard exclusive reactions. they extend the study of hadron structure beyond what can be learned from ordinary distributions and have a particularly rich spin structure. the hard scattering subprocesses in electroproduction of mesons and of real photons satisfy helicity selection rules, which provide powerful tools to test leading-twist dominance at a given value of the hard scale.
learning implied global constraints. finding a constraint network that will be efficiently solved by a constraint solver requires a strong expertise in constraint programming. hence, there is an increasing interest in automatic reformulation. this paper presents a general framework for learning implied global constraints in a constraint network assumed to be provided by a non-expert user. the learned global constraints can then be added to the network to improve the solving process. we apply our technique to global cardinality constraints. experiments show the significance of the approach.
dynamical fragment production in central collisions xe(50 a.mev)+sn. for central collisions xe(50 a.mev)+sn we compared experimental data from the indra detector with qmd simulations. theory as well as experiment show a clear binary character of the fragment emission even for very central collisions. from the time evolution of the reaction (qmd simulation) we could built up a scenario for the dynamical emission of fragments.
polarisation in deeply virtual meson production. we discuss two aspects of polarisation in hard exclusive meson production: the leading-twist selection rule for the meson helicity, and the different partial waves of a (pi pi)-pair which may or may not be due to the decay of a rho.
vacuum structure in supersymmetric yang-mills theories with any gauge group. we consider the pure supersymmetric yang--mills theories placed on a small 3-dimensional spatial torus with higher orthogonal and exceptional gauge groups. the problem of constructing the quantum vacuum states is reduced to a pure mathematical problem of classifying the flat connections on 3-torus. the latter problem is equivalent to the problem of classification of commuting triples of elements in a connected simply connected compact lie group which is solved in this paper. in particular, we show that for higher orthogonal so(n), n &gt; 6, and for all exceptional groups the moduli space of flat connections involves several distinct connected components. the total number of vacuumstates is given in all cases by the dual coxeter number of the group which agrees with the result obtained earlier with the instanton technique.
radio pulses from cosmic ray air showers - boosted coulomb and cherenkov fields. high-energy cosmic rays passing through the earth's atmosphere produce extensive showers whose charges emit radio frequency pulses. despite the low density of the earth's atmosphere, this emission should be affected by the air refractive index because the bulk of the shower particles move roughly at the speed of radio waves, so that the retarded altitude of emission, the relativistic boost and the emission pattern are modified. we consider in this paper the contribution of the boosted coulomb and the cherenkov fields and calculate analytically the spectrum using a very simplified model in order to highlight the main properties. we find that typically the lower half of the shower charge energy distribution produces a boosted coulomb field, of amplitude comparable to the levels measured and to those calculated previously for synchrotron emission. higher energy particles produce instead a cherenkov-like field, whose amplitude may be smaller because both the negative charge excess and the separation between charges of opposite signs are small at these energies.
perspectives for heavy flavour physics in the alice detector. null
the physics of alice. null
hard probes in experiments. null
the muon spectrometer of alice and quarkonia performances. null
enhanced strange particle yields - signal of a phase of massless particles ?. the yields of strange particles are calculated with the urqmd model for p,pb(158 agev)pb collisions and compared to experimental data. the yields are enhanced in central collisions if compared to proton induced or peripheral pb+pb collisions. the enhancement is due to secondary interactions. nevertheless, only a reduction of the quark masses or equivalently an increase of the string tension provides an adequate description of the large observed enhancement factors (wa97 and na49). furthermore, the yields of unstable strange resonances as the lambda_star(1520) resonance or the phi meson are considerably affected by hadronic rescattering of the decay products.
classical yang-mills vacua on $t^{3}$ : explicit constructions. flat connections for unitary gauge groups on a 3--torus with twisted boundary conditions as well as recently discovered periodic nontrivial flat connections with ``nondiagonalizable'' triples of holonomies for higher orthogonal and exceptional groups are constructed explicitly in terms of jacobi theta functions with rational characteristics. the (fractional) chern-simons numbers of these vacuum gauge field configurations are verified by direct computation.
kaon production at subthreshold energies - what do we learn about the nuclear medium ?. the isospin quantum molecular dynamics model is used to compare spectra and elliptic flow of kaons produced at subthreshold energies with data taken at the sis accelerator at gsi. we find that temperatures of the spectra are dominated by the rescattering of the kaons. the study of elliptic flow observables indicates the influence of rescattering as well as of the optical potential of the kaons with increasing dominance of the optical potential at lower incident energies.
influence of process operating conditions on nonwoven filter properties : application to voc and particles combined treatment. null
cold nuclear matter effects on $j/\psi$ production as constrained by deuteron-gold measurements at $\sqrt{s_{nn}}$ = 200 gev. we present a new analysis of j/psi production yields in deuteron-gold collisions at sqrt(s_nn) = 200 gev using data taken by the phenix experiment in 2003 and previously published in [s.s. adler et al., phys. rev. lett 96, 012304 (2006)]. the high statistics proton-proton j/psi data taken in 2005 is used to improve the baseline measurement and thus construct updated cold nuclear matter modification factors r_dau. a suppression of j/psi in cold nuclear matter is observed as one goes forward in rapidity (in the deuteron-going direction), corresponding to a region more sensitive to initial state low-x gluons in the gold nucleus. the measured nuclear modification factors are compared to theoretical calculations of nuclear shadowing to which a j/psi (or precursor) break-up cross-section is added. breakup cross sections of sigma_breakup = 2.8^[+1.7_-1.4] (2.2^[+1.6_-1.5]) mb are obtained by fitting these calculations to the data using two different models of nuclear shadowing. these breakup cross section values are consistent within large uncertainties with the 4.2 +/- 0.5 mb determined at lower collision energies. projecting this range of cold nuclear matter effects to copper-copper and gold-gold collisions reveals that the current constraints are not sufficient to firmly quantify the additional hot nuclear matter effect.
charge injectors of alice silicon drift detector. null
 : the search for flexibility in training : conceptions and practices of self-learning. the author first of all proposes a rapide look back to the past in order to clarify the major socio-economic challenges involved in the search for flexilility in training. she shows that this search for flexibility has close links with the increasing place occupied by self-learning in educational institutions. she goes on to demonstrate that the growth of individualisation in training and of media-based pedagogical environments has contributed to the wide acceptance of two dominant conceptions of self-learning and to practices still in use to day. finally, the author gives a new interpretational key to flexibility in training and its links to self-learning. training.
research and development of a gaseous detector pim (parallel ionization multiplier) dedicated to particle tracking under high hadron rates. pim (parallel ionization multiplier) is a multi-stage micropattern gaseous detector using micromeshes technology. this new device, based on micromegas (micro-mesh gaseous structure) detector principle of operation, offers good characteristics for minimum ionizing particle tracking. however, this kind of detectors placed in hadron environment suffers discharges which degrade sensibly the detection efficiency and account for hazard to the front-end electronics. in order to minimize these strong events, it is convenient to perform charges multiplication by several successive steps. &lt;br /&gt;within the framework of the european hadronphysics project (eu-i3hp-jra4), we have investigated the multi-stage pim detector for high hadrons flux application. &lt;br /&gt;for this purpose, a systematic study for many geometrical configurations of a two amplification stages separated with a transfer space operated with the gaseous mixture ne+10%co2 has been performed. beam tests realised with high energy hadrons at cern facility have given that discharges probability could be strongly reduced with a suitable pim device. a discharges rate lower to 10-9 by incident hadron and a spatial resolution of 51 µm have been measured at the beginning efficiency plateau (&gt;96 %) operating point.
how can we explore the onset of deconfinement by experiment?. there is little doubt that quantumchromodynamics (qcd) is the theory which describes strong interaction physics. lattice gauge simulations of qcd predict that in the $\mu,t$ plane there is a line where a transition from confined hadronic matter to deconfined quarks takes place. the transition is either a cross over (at low $\mu$) or of first order (at high $\mu$). it is the goal of the present and future heavy ion experiment at rhic and fair to study this phase transition at different locations in the $\mu,t$ plane and to explore the properties of the deconfined phase. it is the purpose of this contribution to discuss some of the observables which are considered as useful for this purpose.
measurement of single muons at forward rapidity in p+p collisions at $\sqrt{s}$ = 200 gev and implications for charm production. muon production at forward rapidity (1.5 &lt; |\eta| &lt; 1.8) has been measured by the phenix experiment over the transverse momentum range 1 &lt; p_t \le 3 gev/c in sqrt(s) = 200 gev p+p collisions at the relativistic heavy ion collider. after statistically subtracting contributions from light hadron decays an excess remains which is attributed to the semileptonic decays of hadrons carrying heavy flavor, i.e. charm quarks or, at high p_t, bottom quarks. the resulting muon spectrum from heavy flavor decays is compared to pythia and a next-to-leading order perturbative qcd calculation. pythia is used to determine the charm quark spectrum that would produce the observed muon excess. the corresponding differential cross section for charm quark production at forward rapidity is determined to be d\sigma_(c c^bar)/dy|_(y=1.6)=0.243 +/- 0.013 (stat.) +/- 0.105 (data syst.) ^(+0.049)_(-0.087) (pythia syst.) mb.
identification of photon-tagged jets in the alice experiment. the alice experiment at lhc will detect and identify prompt photons and light neutral-mesons with the phos detector and the additional emcal electromagnetic calorimeter. charged particles will be detected and identified by the central tracking system. in this article, the possibility of studying the interaction of jets with the nuclear medium, using prompt photons as a tool to tag jets, is investigated by simulations. new methods to identify prompt photon-jet events and to distinguish them from the jet-jet background are presented.
rapidity and species dependence of particle production at large transverse momentum for $d$+au collisions at $\sqrt{s_{nn}}$ = 200 gev. we determine rapidity asymmetry in the production of charged pions, protons and anti-protons for large transverse momentum (pt) for d+au collisions at \sqrt s_nn = 200 gev. the identified hadrons are measured in the rapidity regions |y| &lt; 0.5 and 0.5 &lt; |y| &lt; 1.0 for the pt range 2.5 &lt; pt &lt; 10 gev/c. we observe significant rapidity asymmetry for charged pion and proton+anti-proton production in both rapidity regions. the asymmetry is larger for 0.5 &lt; |y| &lt; 1.0 than for |y|&lt; 0.5 and is almost independent of particle type. the measurements are compared to various model predictions employing multiple scattering, energy loss, nuclear shadowing, saturation effects, and recombination, and also to a phenomenological parton model. we find that asymmetries are sensitive to model parameters and show model-preference. the rapidity dependence of \pi^{-}/\pi^{+} and \bar{p}/p ratios in peripheral d+au and forward neutron-tagged events are used to study the contributions of valence quarks and gluons to particle production at high pt. the results are compared to calculations based on nlo pqcd and other measurements of quark fragmentation functions.
disappearance of flow in heavy-ion collisions for system mass $\geq$ 175. using the qmd model, we study the mass dependence of the disappearance of transverse flow in heavier colliding nuclei. a power law mass dependence is obtained in all cases. our results are in excellent agreement with data. for the first time, we predict the balance energy of u+u around 37-39 mev/nucleon.
suppression of high-p_t neutral pion production in central pb+pb collisions at sqrt{s_nn} = 17.3 gev relative to p+c and p+pb collisions. neutral pion transverse momentum spectra were measured in p+c and p+pb collisions at sqrt{s_nn} = 17.4 gev at mid-rapidity (2.3.
centrality dependence of charged hadron production in deuteron+gold and nucleon+gold collisions at $\sqrt{s_{nn}}$=200 gev. we present transverse momentum (p_t) spectra of charged hadrons measured in deuteron-gold and nucleon-gold collisions at \sqrts = 200 gev for four centrality classes. nucleon-gold collisions were selected by tagging events in which a spectator nucleon was observed in one of two forward rapidity detectors. the spectra and yields were investigated as a function of the number of binary nucleon-nucleon collisions, \nu, suffered by deuteron nucleons. a comparison of charged particle yields to those in p+p collisions show that the yield per nucleon-nucleon collision saturates with \nu for high momentum particles. we also present the charged hadron to neutral pion ratios as a function of p_t.
physics of crypto-hermitian and/or cryptosupersymmetric field theories. we discuss non-hermitian field theories where the spectrum of the hamiltonian involves only real energies. we make three observations. (i) the theories obtained from supersymmetric theories by nonanticommutative deformations belong in many cases to this class. (ii) when the deformation parameter is small, the deformed theory enjoys the same supersymmetry algebra as the undeformed one. half of the supersymmetries are manifest and the existence of another half can be deduced from the structure of the spectrum. (iii) generically, the conventionally defined s--matrix is not unitary for such theories.
disappearance of transverse flow in central collisions for heavier nuclei. for the first time, mass dependence of balance energy only for heavier systems has been studied. our results are in excellent agreement with the data which allow us to predict the balance energy of u+u, for the first time, around 37-39 mev/nucleon. also our results indicate a hard equation of state along with nucleon-nucleon cross-section around 40 mb.
how sensitive are di-leptons from rho mesons to the high baryon density region?. we show that the measurement of di-leptons might provide only a restricted view into the most dense stages of heavy ion reactions. thus, possible studies of meson and baryon properties at high baryon densities, as e.g. done at gsi-hades and envisioned for fair-cbm, might observe weaker effects than currently expected in certain approaches. we argue that the strong absorption of resonances in the high baryon density region of the heavy ion collision masks information from the early hot and dense phase due to a strong increase of the total decay width because of collisional broadening. to obtain additional information, we also compare the currently used approaches to extract di-leptons from transport simulations - i.e. shining, only vector mesons from final baryon resonance decays and instant emission of di-leptons and find a strong sensitivity on the method employed in particular at fair and sps energies. it is shown explicitly that a restriction to rho meson (and therefore di-lepton) production only in final state baryon resonance decays provide a strong bias towards rather low baryon densities. the results presented are obtained from urqmd v2.3 calculations using the standard set-up.
longitudinal double-spin asymmetry for inclusive jet production in $\vec{p}+\vec{p}$ collisions at $\sqrt{s}$=200 gev. we report a new star measurement of the longitudinal double-spin asymmetry a_ll for inclusive jet production at mid-rapidity in polarized p+p collisions at a center-of-mass energy of sqrt(s) = 200 gev. the data, which cover jet transverse momenta 5 &lt; p_t &lt; 30 gev/c, are substantially more precise than previous measurements. they provide significant new constraints on the gluon spin contribution to the nucleon spin through the comparison to predictions derived from one global fit of polarized deep-inelastic scattering measurements.
on the mass formula and wigner and curvature energy terms. the efficiency of different mass formulas derived from the liquid drop model including or not the curvature energy, the wigner term and different powers of the relative neutron excess $i$ has been determined by a least square fitting procedure to the experimental atomic masses assuming a constant r$_{0,charge}$/a$^{1/3}$ ratio. the wigner term and the curvature energy can be used independently to improve the accuracy of the mass formula. the different fits lead to a surface energy coefficient of around 17-18 mev, a relative sharp charge radius r$_0$ of 1.22-1.23 fm and a proton form-factor correction to the coulomb energy of around 0.9 mev.
detailed study of high-pt neutral pion suppression and azimuthal anisotropy in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. measurements of neutral pion production at midrapidity in sqrt(s_nn) = 200 gev au+au collisions as a function of transverse momentum, p_t, collision centrality, and angle with respect to reaction plane are presented. the data represent the final pi^0 results from the phenix experiment for the first rhic au+au run at design center-of-mass-energy. they include additional data obtained using the phenix level-2 trigger with more than a factor of three increase in statistics over previously published results for p_t &gt; 6 gev/c. we evaluate the suppression in the yield of high-p_t pi^0's relative to point-like scaling expectations using the nuclear modification factor r_aa. we present the p_t dependence of r_aa for nine bins in collision centrality. we separately integrate r_aa over larger p_t bins to show more precisely the centrality dependence of the high-p_t suppression. we then evaluate the dependence of the high-p_t suppression on the emission angle \delta\phi of the pions with respect to event reaction plane for 7 bins in collision centrality. we show that the yields of high-p_t pi^0's vary strongly with \delta\phi, consistent with prior measurements. we show that this variation persists in the most peripheral bin accessible in this analysis. for the peripheral bins we observe no suppression for neutral pions produced aligned with the reaction plane while the yield of pi^0's produced perpendicular to the reaction plane is suppressed by more than a factor of 2. we analyze the combined centrality and \delta\phi dependence of the pi^0 suppression in different p_t bins using different possible descriptions of parton energy loss dependence on jet path-length averages to determine whether a single geometric picture can explain the observed suppression pattern.
theoretical and experimental $\alpha$ decay half-lives of the heaviest odd-z elements and general predictions. theoretical decay half-lives of the heaviest odd-z nuclei are calculated using the experimental q value. the barriers in the quasimolecular shape path are determined within a generalized liquid drop model (gldm) and the wkb approximation is used. the results are compared with calculations using the density-dependent m3y (ddm3y) effective interaction and the viola-seaborg-sobiczewski (vss) formulas. the calculations provide consistent estimates for the half-lives of the decay chains of these superheavy elements. the experimental data stand between the gldm calculations and vss ones in the most time. predictions are provided for the decay half-lives of other superheavy nuclei within the gldm and vss approaches using the recent extrapolated q of audi, wapstra, and thibault [nucl. phys. a729, 337 (2003)], which may be used for future experimental assignment and identification.
collisional energy loss of a fast muon in a hot qed plasma. we calculate the collisional energy loss of a muon of high energy $e$ in a hot qed plasma beyond logarithmic accuracy, i.e., we determine the constant terms of order o(1) in $-de/dx \propto \ln{e}+ o(1)$. considering first the $t$-channel contribution to $-de/dx$, we show that the terms $\sim o(1)$ are sensitive to the full kinematic region for the momentum exchange $q$ in elastic scattering, including large values $q \sim o(e)$. we thus redress a previous calculation by braaten and thoma, which assumed $q &lt;&lt; e$ and could not find the correct constant (in the large $e$ limit). the relevance of 'very hard' momentum transfers then requires, for consistency, that $s$ and $u$-channel contributions from compton scattering must be included, bringing a second modification to the braaten-thoma result. most importantly, compton scattering yields an additional large logarithm in $-de/dx$. our results might have implications in the qcd case of parton collisional energy loss in a quark gluon plasma.
dynamics of $k^+$ production in heavy ion collisions close to threshold. in this article the production of $k^+$ at energies close to the threshold is studied in detail. the production mechanisms, the influence of in-medium effects, cross sections, the nuclear equation of state and the dynamics of the nucleons on the kaon dynamics are discussed. a special regard will be taken on the collision of au+au at 1.5 gev, a reaction that has recently been analyzed in detail by experiments performed by the kaos and fopi collaborations at the sis accelerator at gsi.
uhecr observed by the radiodetection experiment codalema. the radiodetection experiment codalema allows to study, on an event-by-event basis, cosmic ray air showers through the detection of the radiated electric field. 2 major upgrades have been made in september 2006: - a new antenna array made up of 16 active dipole antennas, arranged in a cross shape. - 13 new particule detector providing more accurate informations on air showers: primary cosmic ray energy estimation, core shower position... results from 6 months of measurements: radiodetection efficiency versus energy, arrival direction distribution or showers lateral electric field dependence, will be discussed. full analysis of a high energy event will be presented.
coplanar ternary cluster decay of hyper-deformed $^{56}$ni. coincidences between two heavy fragments have been measured from fission of 56ni compound nuclei, formed in the 32s + 24mg reaction at elab(32s) = 163.5 mev. a unique experimental set-up consisting of two large area position sensitive (x, y) gas-detector telescopes has been used allowing the complete determination of the observed fragments, and their momentum vectors. in addition to binary fission events with subsequent particle evaporation, narrow out-of-plane correlations are observed for two fragments emitted in purely binary events and in events with a missing mass consisting of 2$\alpha$ and 3$\alpha$ particles(12c). these events are interpreted as ternary cluster decay from 56ni-nuclei at high angular momenta through hyper-deformed shapes.
bimodality: a sign of critical behavior in nuclear reactions. the recently discovered coexistence of multifragmentation and residue production for the same total transverse energy of light charged particles can be well reproduced in numerical simulations of the heavy ion reactions. a detailed analysis shows that fluctuations (introduced by elementary nucleon-nucleon collisions) determine which of the exit states is realized. thus we observe for the first time nonlinear dynamics in heavy ion reactions. also the scaling of the coexistence region with beam energy is well reproduced in these results from the qmd simulation program.
probing the nuclear matter isospin asymmetry by nucleon-induced reactions at fermi energies. despite the fact that valuable experimental measures are still lacking, available nuclear data on nucleon-induced reactions open new opportunities to address either reaction mechanisms or nuclear interaction characteristics. in this work single and double differential cross sections of emitted particles are analyzed and compared with the experiment. it will be evidenced that these cross sections follow a precise hierarchy. the preequilibrium components of the spectra are built up by the dynamics of the reaction as well as by the properties of the nuclear interaction.
mass, quark-number, and $\sqrt{s_{nn}}$ dependence of the second and fourth flow harmonics in ultrarelativistic nucleus-nucleus collisions. we present star measurements of the azimuthal anisotropy parameter v(2) for pions, kaons, protons, lambda,(lambda) over bar,xi+ (xi) over bar, and omega+ (omega) over bar, along with v(4) for pions, kaons, protons, and lambda+(lambda) over bar at midrapidity for au+au collisions at root s(nn)=62.4 and 200 gev. the v(2)(p(t)) values for all hadron species at 62.4 gev are similar to those observed in 130 and 200 gev collisions. for observed kinematic ranges, v(2) values at 62.4, 130, and 200 gev are as little as 10-15% larger than those in pb+pb collisions at root s(nn)=17.3 gev. at intermediate transverse momentum (p(t) from 1.5-5 gev/c), the 62.4 gev v(2)(p(t)) and v(4)(p(t)) values are consistent with the quark-number scaling first observed at 200 gev. a four-particle cumulant analysis is used to assess the nonflow contributions to pions and protons and some indications are found for a smaller nonflow contribution to protons than pions. baryon v(2) is larger than antibaryon v(2) at 62.4 and 200 gev, perhaps indicating either that the initial spatial net-baryon distribution is anisotropic, that the mechanism leading to transport of baryon number from beam- to midrapidity enhances v(2) or that antibaryon and baryon annihilation is larger in the in-plane direction.
alignment experience in star. the star experiment at rhic uses four layers of silicon strip and silicon drift detectors for secondary vertex reconstruction. an attempt for a direct charm meson measurement put stringent requirements on alignment and calibration. we report on recent alignment and drift velocity calibration work performed on the inner silicon tracking system.
ex-post evaluation of local energy efficiency and demand-side management operations - state of the art, bottom-up methods, applied examples and approach for the development of a practical culture of evaluation. energy end-use efficiency (ee) is a priority for energy policies to face resources exhaustion and to reduce pollutant emissions.&lt;br /&gt;at the same time, in france, local level is increasingly involved into the implementation of ee activities, whose frame is changing (energy market liberalisation, new policy instruments). needs for ex-post evaluation of the local ee activities are thus increasing, for regulation requirements and to support a necessary change of scale. our thesis focuses on the original issue of the ex-post evaluation of local ee operations in france.&lt;br /&gt;the state of the art, through the analysis of the american and european experiences and of the reference guidebooks, gives a substantial methodological material and emphasises the key evaluation issues. concurrently, local ee operations in france are characterized by an analysis of their environment and a work on their segmentation criteria. the combination of these criteria with the key evaluation issues provides an analysis framework used as the basis for the composition of evaluation methods.&lt;br /&gt;this also highlights the specific evaluation needs for local operations.&lt;br /&gt;a methodology is then developed to complete and adapt the existing material to design evaluation methods for local operations, so that stakeholders can easily appropriate. evaluation results thus feed a know-how building process with experience feedback. these methods are to meet two main goals: to determine the operation results, and to detect the success/failure factors.&lt;br /&gt;the methodology was validated on concrete cases, where these objectives were reached.
extended air shower simulations based on epos. we discuss air shower simulations based on the epos hadronic interaction model. a remarkable feature is the fact that the number of produced muons is considerably larger compared to other interaction models. we show that this is due to an improved treatment of baryon-antibaryon production.
kinematic and stiffness analysis of the orthoglide, a pkm with simple, regular workspace and homogeneous performances. the orthoglide is a delta-type pkm dedicated to 3-axis rapid machining applications that was originally developed at irccyn in 2000-2001 to meet the advantages of both serial 3-axis machines (regular workspace and homogeneous performances) and parallel kinematic architectures (good dynamic performances and stiffness). this machine has three fixed parallel linear joints that are mounted orthogonally. the geometric parameters of the orthoglide were defined as function of the size of a prescribed cubic cartesian workspace that is free of singularities and internal collision. the interesting features of the orthoglide are a regular cartesian workspace shape, uniform performances in all directions and good compactness. in this paper, a new method is proposed to analyze the stiffness of overconstrained delta-type manipulators, such as the orthoglide. the orthoglide is then benchmarked according to geometric, kinematic and stiffness criteria: workspace to footprint ratio, velocity and force transmission factors, sensitivity to geometric errors, torsional stiffness and translational stiffness.
inclusive cross section and double helicity asymmetry for \pi^0 production in p+p collisions at sqrt(s)=200 gev: implications for the polarized gluon distribution in the proton. the phenix experiment presents results from the rhic 2005 run with polarized proton collisions at sqrt(s)=200 gev, for inclusive \pi^0 production at mid-rapidity. unpolarized cross section results are given for transverse momenta p_t=0.5 to 20 gev/c, extending the range of published data to both lower and higher p_t. the cross section is described well for p_t &lt; 1 gev/c by an exponential in p_t, and, for p_t &gt; 2 gev/c, by perturbative qcd. double helicity asymmetries a_ll are presented based on a factor of five improvement in uncertainties as compared to previously published results, due to both an improved beam polarization of 50%, and to higher integrated luminosity. these measurements are sensitive to the gluon polarization in the proton, and exclude maximal values for the gluon polarization.
analysis of emission mechanisms in nucleon-induced reactions around the fermi energy. neutron-induced reactions around the fermi energy are investigated in the framework of the microscopic dywan model. comparisons with experimental data in particle spectra are performed. special attention has been devoted to pre-equilibrium emissions. the theoretical results show that the emission processes are sensitive to the nucleon–nucleon cross-section and to the characteristics of the self-consistent nuclear mean field, as the isospin dependence and the equation of state. accordingly, nucleon-induced reactions provide a precious probe of the nuclear interaction in the concerned energy domain.
(n,xn) measurements at 96 mev. double differential cross section for neutron production were measured in 96mev neutrons induced reactions at the tsl laboratory in uppsala (sweden). measurements for fe and pb targets were performed using simultaneously two independent setups: decoi-demon and clodia-scandal. the double differential cross section were measured for an angular range between 15 and 100 degrees and with low-energy thresholds (1–2 mev). elastic distribution, angular distribution, energy distribution and total inelastic cross section were derived from measured double differential cross section. results are compared with predictions given by several simulation codes and with other experimental data.
measurement of density correlations in pseudorapidity via charged particle multiplicity fluctuations in au+au collisions at $\sqrt{s_{nn}}$=200 gev. longitudinal density correlations of produced matter in au+au collisions at sqrt(s_nn)=200 gev have been measured from the inclusive charged particle distributions as a function of pseudorapidity window sizes. the extracted \alpha \xi parameter, related to the susceptibility of the density fluctuations in the long wavelength limit, exhibits a non-monotonic behavior as a function of the number of participant nucleons, n_part. a local maximum is seen at n_part ~ 90, with corresponding energy density based on the bjorken picture of \epsilon_bj \tau ~ 2.4 gev/(fm^2 c) with a transverse area size of 60 fm^2. this behavior may suggest a critical phase boundary based on the ginzburg-landau framework.
energy dependence of $\pi^{\pm}$, $p$ and $\bar{p}$ transverse momentum spectra for au+au collisions at $\sqrt{s_{nn}}$=62.4 and 200 gev. we study the energy dependence of the transverse momentum $(p_t)$ spectra for charged pions, protons and anti-protons for au+au collisions at $\sqrt{s_{nn}$=62.4 and 200 gev. data are presented at mid-rapidity (|y|&lt;0.5) for $0.2\le p_t\le12 $ gev/c . in the intermediate $p_t$ region $(0.2\le p_t\le 6 gev/c$ ), the nuclear modification factor is higher at 62.4 gev than at 200 gev, while at higher $p_t (p_t\ge 7 gev/c)$ the modification is similar for both energies. the p/$\pi+$ and $\bar{p} \pi^-$ ratios for central collisions at $\sqrt{s_{nn}}$ =62.4 gev/c peak at $p_t \appeq 2 gev/c$ . in the $p_t$ range where recombination is expected to dominate, the p/$\pi + ratios at 62.4 gev are larger than at 200 gev, while the $\bar{p}$/$\pi _-} ratios are smaller. for $p_t$, the $\bar{p} /\pi_-$ ratios at the two beam energies are independent of $p_t$ and centrality indicating that the dependence of the $\bar{p}:\pi_-$ ratio on $p_t$ does not change between 62.4 and 200 gev. these findings challenge various models incorporating jet quenching and/or constituent quark coalescence.
effect of aqueous acetic, oxalic and carbonic acids on the adsorption of uranium(vi) onto alpha-alumina. the prediction of the migration for radionuclides in geologic media requires a quantitative knowledge of retardation phenomena. for this purpose, the sorption of u(vi) onto a model mineral-–-alumina—is studied here, including the effects of groundwater chemistry: ph and concentrations of small organic ligands (acetate, oxalate and carbonate anions). this work presents experimental evidence for the synergic sorption of uranium(vi) and the small organic ligands, namely sorption of cationic complexes onto alumina. conversely, since its neutral and anionic complexes were not sorbed, u(vi) cation could also be desorbed as a result of the formation of neutral or anionic complexes in the aqueous phase. by using the ion-exchange theory, and a corresponding restricted set of parameters—exchange capacities and thermodynamic equilibrium constants—the whole set of sorption experiments of u(vi) cationic species onto the -alumina was modelled under various chemical conditions.
analysis of the dilepton invariant mass spectrum in c + c collisions at 2a and 1a gev. recently the hades collaboration has published the invariant mass spectrum of $e^+e^-$ pairs, $dn/dm_{e^+e^-}$, produced in c + c collisions at 2a gev. using electromagnetic probes, one hopes to get information from this experiment on hadron properties at high density and temperature. simulations show that firm conclusions on possible in-medium modifications of meson properties will only be possible when the elementary meson production cross sections, especially in the pn channel, as well as production cross sections of baryonic resonances are better known. presently one can conclude that (i) simulations overpredict by far the cross section at $me^+e^-\simm_{\omega}^0$ if free production cross sections are used and that (ii) the upper limit of the $\eta$ decay into $e^+e^-$ is smaller than the present upper limit of the particle data group. this is the result of simulations using the isospin quantum molecular dynamics approach.
scaling properties of azimuthal anisotropy in au+au and cu+cu collisions at $\sqrt{s_{nn}}$ = 200 gev. detailed differential measurements of the elliptic flow for particles produced in au+au and cu+cu collisions at sqrt(s_nn) = 200 gev are presented. predictions from perfect fluid hydrodynamics for the scaling of the elliptic flow coefficient v_2 with eccentricity, system size and transverse energy are tested and validated. for transverse kinetic energies ke_t ~ m_t-m up to ~1 gev, scaling compatible with the hydrodynamic expansion of a thermalized fluid is observed for all produced particles. for large values of ke_t, the mesons and baryons scale separately. a universal scaling for the flow of both mesons and baryons is observed for the full transverse kinetic energy range of the data when quark number scaling is employed. in both cases the scaling is more pronounced in terms of ke_t rather than transverse momentum.
forward $\lambda$ production and nuclear stopping power in d + au collisions at $\sqrt{s_{nn}}$ = 200 gev. we report the measurement of lamda and anti-lamda yields and inverse slope parameters in d + au collisions at sqrt(s_nn) = 200 gev at forward and backward rapidities (y = +- 2.75), using data from the star forward time projection chambers. the contributions of different processes to baryon transport and particle production are probed exploiting the inherent asymmetry of the d + au system. comparisons to model calculations show that the baryon transport on the deuteron side is consistent with multiple collisions of the deuteron nucleons with gold participants. on the gold side hijing based models do not describe the measured particle yields while models with initial state nuclear effects and/or hadronic rescattering do. the multi-chain model can provide a good description of the net baryon density in d + au collisions at rhic, and the derived parameters of the model agree with those from nuclear collisions at lower energies.
transverse momentum and centrality dependence of dihadron correlations in au+au collisions at $\sqrt{s_{nn}}$=200 gev: jet-quenching and the response of partonic matter. azimuthal angle \delta\phi correlations are presented for charged hadrons from dijets for 0.4 &lt; p_t &lt; 10 gev/c in au+au collisions at sqrt(s_nn) = 200 gev. with increasing p_t, the away-side distribution evolves from a broad to a concave shape, then to a convex shape. comparisons to p+p data suggest that the away-side can be divided into a partially suppressed "head" region centered at delta\phi ~ \pi, and an enhanced "shoulder" region centered at delta\phi ~ \pi +/- 1.1. the p_t spectrum for the "head" region softens toward central collisions, consistent with the onset of jet quenching. the spectral slope for the "shoulder" region is independent of centrality and trigger p_t, which offers constraints on energy transport mechanisms and suggests that the "shoulder" region contains the medium response to energetic jets.
enhanced strange baryon production in au+au collisions compared to p+p at $\sqrt{s_{nn}}$ = 200 gev. we report on the observed differences in production rates of strange and multi-strange baryons in au+au collisions at sqrts = 200 gev compared to p+p interactions at the same energy. the yields in au+au collisions, when scaled by the number of participants, are larger than those measured in the p+p data. the magnitudes of the differences grow with the strangeness of the baryon and with increasing centrality. the enhancements of the au+au yields are close to those measured in sqrts = 17.3 gev collisions. further, when the binary scaled p+p pt spectra are compared to those of au+au the heavy-ion yields exceed binary scaling in the pt range 1 &lt; pt&lt; 4 gev/c.
measurement of transverse single-spin asymmetries for di-jet production in proton-proton collisions at $\sqrt{s} = 200$ gev. we report the first measurement of the opening angle distribution between pairs of jets produced in high-energy collisions of transversely polarized protons. the measurement probes (sivers) correlations between the transverse spin orientation of a proton and the transverse momentum directions of its partons. with both beams polarized, the wide pseudorapidity ($-1 \leq \eta \leq +2$) coverage for jets permits separation of sivers functions for the valence and sea regions. the resulting asymmetries are all consistent with zero and considerably smaller than sivers effects observed in semi-inclusive deep inelastic scattering (sidis). we discuss theoretical attempts to reconcile the new results with the sizable transverse spin effects seen in sidis and forward hadron production in pp collisions.
cryptoreality of nonanticommutative hamiltonians. we note that, though nonanticommutative (nac) deformations of minkowski supersymmetric theories do not respect the reality condition and seem to lead to non-hermitian hamiltonians h, the latter belong to the class of "cryptoreal'' hamiltonians considered recently by bender and collaborators. they can be made manifestly hermitian via the similarity transformation h -&gt; exp{r} h exp{-r} with a properly chosen r. the deformed model enjoys the same supersymmetry algebra as the undeformed one, though being realized differently on the involved canonical variables. besides quantum-mechanical models, we treat, along similar lines, some nac deformed field models in 4d minkowski space.
energy loss of a heavy quark produced in a finite-size quark-gluon plasma. we study the energy loss of an energetic heavy quark produced in a high temperature quark-gluon plasma and travelling a finite distance before emerging in the vacuum. while the retardation time of purely collisional energy loss is found to be of the order of the debye screening length, we find that the contributions from transition radiation and the ter-mikayelian effect do not compensate, leading to a reduction of the zeroth order (in an opacity expansion) energy loss.
correlations of neutral and charged particles in $^{40}$ar- $^{58}$ni reaction at 77 mev/u. the measurement of the two-particle correlation function for different particle species allows to obtain information about the development of the particle emission process: the space-time properties of emitting sources and the emission time sequence of different particles. the single-particle characteristics and two-particle correlation functions for neutral and charged particles registered in forward direction are used to determine that the heavy fragments (deuterons and tritons) are emitted in the first stage of the reaction (pre-equilibrium source) while the majority of neutrons and protons originates from the long-lived quasi-projectile. the emission time sequence of protons, neutrons and deuterons has been obtained from the analysis of non-identical particle correlation functions.
evidence for a long-range component in the pion emission source in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. emission source functions are extracted from correlation functions constructed from charged pions produced at mid-rapidity in au+au collisions at sqrt(s_nn)=200 gev. the source parameters extracted from these functions at low k_t, give first indications of a long tail for the pion emission source. the source extension cannot be explained solely by simple kinematic considerations. the possible role of a halo of secondary pions from resonance emissions is explored.
oxidation and dissolution rates of $uo_2$(s) in carbonate-rich solutions under external alpha irradiation and initially reducing conditions. this paper describes the effects of aqueous carbonate concentrations on the uo2 oxidation and dissolution rates under an alpha beam irradiation in a cyclotron. as solid samples, uo2 colloids were synthesized by nano-particle precipitation. the specific surface area obtained is 85.3 m2/g. all aqueous solutions were buffered in the ph range 8.
correlation between x-ray chemical shift and partial charge in tc(iv) complexes: determination of tc partial charge in $tc_no_y^{(4n-2y)+}$. the chemical shift of the x-ray absorption k-edge of tc(iv) complexes determined by x-ray absorption spectroscopy (xas) measurement was studied. a correlation between edge position (δe) and partial charge on tc atoms (δtc) was determined. this correlation was used to determinate δtc in tcnoy(4n-2y)+. furthermore, a theoretical relation between δtc and the charge of tcnoy(4n-2y)+, was also established and the overall charge of this complex was estimated.
coprecipitation of thorium and lanthanum with $uo_{2+x}$(s) as host phase. coprecipitation may be a significant process in controlling radionuclide release during spent fuel dissolution/reprecipitation in a breached geological disposal site by water. the objective of this work was to study the coprecipitation of thorium and lanthanum with uo2+x(s) as host phase under reducing conditions (pure n2 and eh=−300 mv/she) as a function of ph and [th]/[u+th] or [la]/[u+la] initial concentration ratios. at ph values between 3 to 9, the final concentrations of dissolved th and la were found to be considerably lower than expected if pure phases, th(oh)4(s) and la(oh)3(s) would have been formed. coprecipitation phenomena and formation of solid solutions thyu1-yo2+x'(s) and (u,la)o2+x'(s) are suggested to be responsible for the lower solubility.
nuclear medical imaging using $\beta+\gamma$ coincidences from $^{44}$sc radio-nuclide with liquid xenon as detection medium. we report on a new nuclear medical imaging technique based on the measurement of the emitter location in the three dimensions with a few mm spatial resolution using β+γ emitters. such measurement could be realized thanks to a new kind of radio-nuclides which emit a γ-ray quasi-simultaneously with the β+ decay. the most interesting radio-nuclide candidate, namely 44sc, will be potentially produced at the nantes cyclotron arronax. the principle is to reconstruct the intersection of the classical line of response (obtained with a standard pet camera) with the direction cone defined by the third γ-ray. the emission angle measurement of this additional γ-ray involves the use of a compton telescope for which a new generation of camera based on a liquid xenon (lxe) time projection chamber is considered. geant3 simulations of a large acceptance lxe compton telescope combined with a commercial micro-pet (lso crystals) have been performed and the obtained results will be presented. they demonstrate that a good image can be obtained from the accumulation of each three-dimensional measured position. a spatial resolution of 2.3 mm has been reached with an injected activity of 0.5 mbq for a 44sc point source emitter.
calibration of quasi-isotropic parallel kinematic machines: orthoglide. the paper proposes a novel approach for the geometrical model calibration of quasi-isotropic parallel kinematic mechanisms of the orthoglide family. it is based on the observations of the manipulator leg parallelism during motions between the specific test postures and employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. they are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the tcp along the cartesian axes. using the measured differences, the developed algorithm estimates the joint offsets and the leg lengths that are treated as the most essential parameters. validity of the proposed calibration technique is confirmed by the experimental results.
residual correlations between decay products of $\pi^0\pi^0$ and $p\sigma^0$ systems. residual correlations between decay products due to a combination of both correlations between parents at small relative velocities and small decay momenta are discussed. residual correlations between photons from pion decays are considered as a new possible source of information on direct photon fraction. residual correlations in $p\gamma$ and $p\lambda$ systems due to $p\sigma^0$ interaction in final state are predicted based on the $p\sigma^0$ low energy scattering parameters deduced from the spin-flavour su$_6$ model by fujiwara et al. including effective meson exchange potentials and explicit flavour symmetry breaking to reproduce the properties of the two-nucleon system and the low-energy hyperon-nucleon cross section data. the $p\gamma_{\sigma^0}$ residual correlation is concentrated at $k^* \approx 70$ mev/$c$ and its shape and intensity appears to be sensitive to the scattering parameters and space-time dimensions of the source. the $p\lambda_{\sigma^0}$ residual correlation recovers the negative parent $p\sigma^0$ correlation for $k^* &gt; 70$ mev/$c$. the neglect of this negative residual correlation would lead to the underestimation of the parent $p\lambda$ correlation effect and to an overestimation of the source size.
hadron production in proton-proton scattering in nexus3. using the recently introduced model nexus 3, we calculate for proton-proton, neutron-proton and antiproton-proton collisions the excitation function of particle yields and of average transverse momenta of different particle species as well as rapidity, x_{f} and transverse momentum distributions. our results are compared with available data in between sqrt{s} = 5 gev and 65 gev. we find for all observables quite nice agreement with data what make this model to a useful tool to study particle production in elementary hadronic reactions.
coins: a constraint-based interactive solving system. this paper describes the coins (constraint-based interactive solving) system: a conflict-based constraint solver. it helps understanding inconsistencies, simulates constraint additions and/or retractions (without any propagation), determines if a given constraint belongs to a conflict and provides diagnosis tools (e.g. why variable v cannot take value val). coins also uses user-friendly representation of conflicts and explanations.
prompt photon identification in the alice experiment: the isolation cut method. the alice experiment at lhc will detect and identify prompt photons and light neutral mesons with the phos and emcal detectors. charged particles will be detected and identified by the central tracking system. in this paper, a method to identify prompt photons and to separate them from the background of hadrons and decay photons in phos with the help of isolation cuts is presented.
alice: physics performance report, volume ii. alice is a general-purpose heavy-ion experiment designed to study the physics of strongly interacting matter and the quark–gluon plasma in nucleus–nucleus collisions at the lhc. it currently involves more than 900 physicists and senior engineers, from both the nuclear and high-energy physics sectors, from over 90 institutions in about 30 countries. the alice detector is designed to cope with the highest particle multiplicities above those anticipated for pb–pb collisions (dnch/dy up to 8000) and it will be operational at the start-up of the lhc. in addition to heavy systems, the alice collaboration will study collisions of lower-mass ions, which are a means of varying the energy density, and protons (both pp and pa), which primarily provide reference data for the nucleus–nucleus collisions. in addition, the pp data will allow for a number of genuine pp physics studies. the detailed design of the different detector systems has been laid down in a number of technical design reports issued between mid-1998 and the end of 2004. the experiment is currently under construction and will be ready for data taking with both proton and heavy-ion beams at the start-up of the lhc. since the comprehensive information on detector and physics performance was last published in the alice technical proposal in 1996, the detector, as well as simulation, reconstruction and analysis software have undergone significant development. the physics performance report (ppr) provides an updated and comprehensive summary of the performance of the various alice subsystems, including updates to the technical design reports, as appropriate. the ppr is divided into two volumes. volume i, published in 2004 (cern/lhcc 2003-049, alice collaboration 2004 j. phys. g: nucl. part. phys. 30 1517–1763), contains in four chapters a short theoretical overview and an extensive reference list concerning the physics topics of interest to alice, the experimental conditions at the lhc, a short summary and update of the subsystem designs, and a description of the offline framework and monte carlo event generators. the present volume, volume ii, contains the majority of the information relevant to the physics performance in proton–proton, proton–nucleus, and nucleus–nucleus collisions. following an introductory overview, chapter 5 describes the combined detector performance and the event reconstruction procedures, based on detailed simulations of the individual subsystems. chapter 6 describes the analysis and physics reach for a representative sample of physics observables, from global event characteristics to hard processes.
partonic flow and $\phi$-meson production in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. we present first measurements of the $\phi$-meson elliptic flow ($v_{2}(p_{t})$) and high statistics $p_{t}$ distributions for different centralities from $\sqrt{s_{nn}}$ = 200 gev au+au collisions at rhic. in minimum bias collisions the $v_{2}$ of the $\phi$ meson is consistent with the trend observed for mesons. the ratio of the yields of the $\omega$ to those of the $\phi$ as a function of transverse momentum is consistent with a model based on the recombination of thermal $s$ quarks up to $p_{t}\sim 4$ gev/$c$, but disagrees at higher momenta. the nuclear modification factor ($r_{cp}$) of $\phi$ follows the trend observed in the $k^{0}_{s}$ mesons rather than in $\lambda$ baryons, supporting baryon-meson scaling. since $\phi$-mesons are made via coalescence of seemingly thermalized $s$ quarks in central au+au collisions, the observations imply hot and dense matter with partonic collectivity has been formed at rhic.
extraction of the high transverse momentum photons in proton+proton collisions at 200 gev in the phenix experiment at rhic. ultra-relativistic heavy ions collisions allow to reach an hot and dense matter. this new state, called quarks and gluons plasma (qgp), would exist at the first moment of our universe according to the big bang theory. the phenix experiment, one of the interaction point of the rhic collider at brookhaven national laboratory (usa), aims to study the qgp's signatures. photons don't interact strongly with the matter and so are an accurate tool to explore the phase of qgp. moreover photons are emitted during all the phases of the nuclear collision: from the initial state to the final hadronization. we will present a direct photon, produced by hard scattering process in the beginning of the collision, identification method (sica, spectroscopic isolation cut analysis) applied on p+p collisions at 200 gev. this method allows for a better discrimination between direct photons and the other contribution (mainly the electromagnetic decay of the neutral pion). one could find in this thesis the direct photon rate production obtained by sica and compared to other analysis. with the p+p collisions we have an important reference for the more heavier collisions (au+au) where we assume the qgp formation.
immobilization of inert triso-coated fuel in glass for geological disposal. vitrification of triso-coated gas reactor fuel particles was achieved via two methods: glass melting and sintering. inert triso-coated fuel particles and a borosilicate glass were used. with glass melting at 1200–1300 °c floatation and decomposition of carbon and silicon carbide occurred. thermal pre-treatment of the particles for oxidation of pyrocarbon did not improve the coating properties of the glass. during cooling most of the particles floated and sorbed on the crucible or mold walls. the sintered glass at 700 °c showed better coating properties of the triso-coated fuel particles despite higher porosity compared to glass made by melting. aqueous leaching properties of glass with particles are similar regardless the mode of fabrication, indicating the good chemical durability of the sintered glass. sintered glasses may constitute a good technique for triso-coated fuel particles immobilization for an eventual deep geological disposal.
the energy dependence of p$_t$ angular correlations inferred from mean-pt fluctuation scale dependence in heavy ion collisions at the sps and rhic. we present the first study of the energy dependence of p$_t$ angular correlations inferred from event-wise mean transverse momentum (p$_$t) fluctuations in heavy ion collisions. we compare our large-acceptance measurements at cm energies $\sqrt{^{s}nn}$=19.6, 62.4, 130 and 200 gev to sps measurements at 12.3 and 17.3 gev. p$_t$ angular correlation structure suggests that the principal source of pt correlations and fluctuations is minijets (minimum-bias parton fragments). we observe a dramatic increase in correlations and fluctuations from sps to rhic energies, increasing linearly with ln$\sqrt{^s}nn}$ from the onset of observable jet-related p$_t$ fluctuations near 10 gev.
charged particle distributions and nuclear modification at high rapidities in d+au collisions at $\sqrt{s_{nn}}$ = 200 gev. the measurements of the centrality dependence of $dn/d\eta$ and transverse momentum spectra from mid- to forward rapidity in d+au collisions at $\sqrt{s_{_{\rm nn}}} =$ 200 gev are reported. they provide a sensitive tool for understanding the dynamics of multi-particle production in the high parton-density regime. in particular, we observe strong suppression of the nuclear modification factor $r_{cp}$ at forward rapidities (d-side, $\eta$ = 3.1) and enhancement at backward rapidity ($\eta$ = $-$3.1). an empirical scaling is obtained for multiplicity and $r_{cp}$ when a shift of the center-of-mass in the asymmetric d+au collisions with respect to the nucleon-nucleon system is applied.
scaling properties of hyperon production in au+au collisions at $\sqrt{s_{nn}}$=200 gev. we present the scaling properties of lambda, xi, and omega in midrapidity au+au collisions at the brookhaven national laboratory relativistic heavy ion collider at root s(nn)=200 gev. the yield of multistrange baryons per participant nucleon increases from peripheral to central collisions more rapidly than that of lambda, indicating an increase of the strange-quark density of the matter produced. the strange phase-space occupancy factor gamma(s) approaches unity for the most central collisions. moreover, the nuclear modification factors of p, lambda, and xi are consistent with each other for 2 &lt; p(t) &lt; 5 gev/c in agreement with a scenario of hadron formation from constituent quark degrees of freedom.
upgrade of the codalema eas radio-detection experiment. in order to improve the performances of the eas radio-measurements, the codalema has been upgraded. the detection array has been widen and new types of antennas and particle detectors are used. first results and new possibilities given by this new configuration will be presented.
multiple-humped fission and fusion barriers of actinide and superheavy elements. the energy of a deformed nucleus has been determined within a generalized liquid drop model taking into account the proximity energy, the microscopic corrections and quasi-molecular shapes. in the potential barrier a third peak exists for actinides when one fragment is close to a magic spherical nucleus while the other one varies from oblate to prolate shapes. the barrier heights and half-lives agree with the experimental data. the different entrance channels leading possibly to superheavy elements are studied as well as their $\alpha$-decay.
binary fission and coplanar cluster decay of $^{60}$zn compound nuclei at high angular momentum. fission decays with two heavy fragments in coincidence have been measured from $^{60}$ni compound nuclei, formed in the $^{36}$ar + $^{24}$mg reaction at $e_{lab}(^{36}ar=195 mev$ (5.4 mev/a). the experiment was performed with a unique kinematic coincidence set-up consisting of two large area position sensitive (x,y) gas detector telescopes with bragg-ionisation chambers. very narrow out-of-plane correlations are observed for two heavy fragments emitted in either purely binary events or in events with a missing mass consisting of 2, 3 and 4 $\alpha$-particles. the broad out-of-plane distributions correspond to binary fission with the evaporation of $\alpha$-particles or nucleons from excited fragments. the narrow correlations are interpreted as ternary coplanar cluster decay from compound nuclei at high angular momenta through elongated shapes with large moments of inertia, and the lighter mass remains with very low momentum in the centre of mass frame. it is shown in calculations of shapes, that the large moments of inertia of the hyper-deformed configurations expected in these nuclei, will lower the ternary fission barrier so as to make a competition with binary fission possible.
role of the experimental filter in obtaining the arrhenius plot in multifragmentation reactions. recently it has been argued that the linear relation between the transverse energy and the apparent probability to emit a fragment proves that the total system is in thermal equilibrium. it is shown, for a specific reaction xe+sn at 50 a.mev, that the same behavior is obtained in the context of quantum molecular dynamical without invoking the idea of equilibrium. the linear dependance is shown to be a detector effect.
measurement of direct photon production in p + p collisions at $\sqrt{s}$ = 200 gev. cross sections for mid-rapidity production of direct photons in p+p collisions at the relativistic heavy ion collider (rhic) are reported for 3 &lt; p_t &lt; 16 gev/c. next-to-leading order (nlo) perturbative qcd (pqcd) describes the data well for p_t &gt; 5 gev/c, where the uncertainties of the measurement and theory are comparable. we also report on the effect of requiring the photons to be isolated from parton jet energy. the observed fraction of isolated photons is well described by pqcd for p_t &gt; 7 gev/c.
measurement of the mid-rapidity transverse energy distribution from $\sqrt{s_{nn}}=130$ gev au+au collisions at rhic. the first measurement of energy produced transverse to the beam direction at rhic is presented. the mid-rapidity transverse energy density per participating nucleon rises steadily with the number of participants, closely paralleling the rise in charged-particle density, such that e_t / n_ch remains relatively constant as a function of centrality. the energy density calculated via bjorken's prescription for the 2% most central au+au collisions at sqrt(s_nn)=130 gev is at least epsilon_bj = 4.6 gev/fm^3 which is a factor of 1.6 larger than found at sqrt(s_nn)=17.2 gev (pb+pb at cern).
alice potential for heavy flavour physics. heavy quarks will be abundantly produced in heavy ion collisions at lhc energies. both, the production of open heavy flavoured mesons and quarkonia will probe the strongly interacting medium created in these reactions. in particular, the alice detector will be able to measure heavy flavour production down to low transverse momentum, combining leptonic and hadronic channels, covering a large rapidity range $|eta|&lt;0.9$ and $-4&lt;-2.5$. in this talk we will present the main physics motivations for the study of heavy flavour production at lhc energies and some examples of physics analyses developed by the heavy flavour working group of alice.
heavy-ion physics at lhc. the new conditions which will be reached when lhc will collide lead ions are discussed together with the probes which will become available for studying the properties of the hottest matter ever formed in the laboratory. the experimental requirements and how the lhc experiments, in particular alice, will face the challenges are presented.
proton-proton, anti-proton-anti-proton, proton-anti-proton correlations in au + au collisions measured by star at rhic. the analysis of two-particle correlations provides a powerful tool to study the properties of hot and dense matter created in heavy-ion collisions at ultra-relativistic energies. applied to identical and non-identical hadron pairs, it makes the study of space-time evolution of the source in femtoscopic scale possible. baryon femtoscopy allows extraction of the radii of produced sources which can be compared to those deduced from identical pion studies, providing complete information about the source characteristics. in this paper we present the correlation functions obtained for identical and non-identical baryon pairs of protons and anti-protons. the data were collected recently in au+au collisions at $\sqrt{s_{nn}}$ =62 gev and $\sqrt{s_{nn}}$ =200 gev by the star detector at the rhic accelerator. we introduce corrections to the baryon-baryon correlations taking into account: residual correlations from weak decays, particle identification probability and the fraction of primary baryons. finally we compare our results to theoretical predictions.
baryon - baryon correlations in au+au collisions at $\sqrt{s_{nn}}$= 62 gev and $\sqrt{s_nn}}$= 200 gev, measured in the star experiment at rhic. particle correlations at small relative velocities can be used to study the space-time evolution of hot and expanding system created in heavy ion collisions. baryon and antibaryon source sizes extracted from baryon-baryon correlations complement the information deduced from the correlation studies of identical pions. correlations of non-identical particles are sensitive also to the space-time asymmetry of their emission. high statistics data set of star experiment allows us to present the results of baryon-baryon correlation measurements at various centralities and energies, as well as to take carefully into account the particle identification probability and the fraction of primary baryons and antibaryons. preliminary results show significant contribution of annihilation channel in baryon-antibaryon correlations.
l'imagerie médicale 3 gammas - nuclear imagery 3 gammas. null
which radionuclides will nuclear oncology need tomorrow?. null
relativistic transport theory for systems containing bound states. using a lagrangian which contains quarks as elementary degrees of freedom and mesons as bound states, a transport formalism is developed, which allows for a dynamical transition from a quark plasma to a state, where quarks are bound into hadrons. simultaneous transport equations for both particle species are derived in a systematic and consistent fashion. for the mesons a formalism is used which introduces off-shell corrections to the off-diagonal green functions. it is shown that these off-shell corrections lead to the appearance of elastic quark scattering processes in the collision integral. the interference of the processes $q\bar q\to\pi$ and $q\bar q\to\pi\to q\bar q$ leads to a modification of the $s$-channel amplitude of quark-antiquark scattering.
semihard interactions in nuclear collisions based on a unified approach to high energy scattering. our ultimate goal is the construction of a model for interactions of two nuclei in the energy range between several tens of gev up to several tev per nucleon in the centre-of-mass system. such nuclear collisions are very complex, being composed of many components, and therefore some strategy is needed to construct a reliable model. the central point of our approach is the hypothesis, that the behavior of high energy interactions is universal (universality hypothesis). so, for example, the hadronization of partons in nuclear interactions follows the same rules as the one in electron-positron annihilation; the radiation of off-shell partons in nuclear collisions is based on the same principles as the one in deep inelastic scattering. we construct a model for nuclear interactions in a modular fashion. the individual modules, based on the universality hypothesis, are identified as building blocks for more elementary interactions (like e^+ e^-, lepton-proton), and can therefore be studied in a much simpler context. with these building blocks under control, we can provide a quite reliable model for nucleus-nucleus scattering, providing in particular very useful tests for the complicated numerical procedures using monte carlo techniques.
strangeness production incorporating chiral symmetry. chiral symmetry is known to be decisive for an understanding of the low energy sector of strong interactions. it is thus important for a model of relativistic heavy ion collisions to incorporate the dynamical breaking and restoration of chiral symmetry. thus we study an expansion scenario for a quark-meson plasma using the nambu--jona-lasinio (njl) model in its three flavor version. the equations of motion for light and strange quarks as well as for pions, kaons and etas are solved using a qmd type algorithm, which is based on a parametrization of the wigner function. the scattering processes incorporated into this calculation are of the types qq &lt;--&gt; qq, q\bar q &lt;--&gt; q\bar q, q\bar q &lt;--&gt; mm and m &lt;--&gt; q\bar q.
transverse momentum and centrality dependence of high-pt non-photonic electron suppression in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. the star collaboration at rhic reports measurements of the inclusive yield of non-photonic electrons, which arise dominantly from semi-leptonic decays of heavy flavor mesons, over a broad range of transverse momenta ($1.2 &lt; \pt &lt; 10$ \gevc) in \pp, \dau, and \auau collisions at \sqrtsnn = 200 gev. the non-photonic electron yield exhibits unexpectedly large suppression in central \auau collisions at high \pt, suggesting substantial heavy quark energy loss in hot qcd matter. the centrality and \pt dependences of the suppression provide stringent constraints on theoretical models of suppression.
measurement of high-$p_t$ single electrons from heavy-flavor decays in p+p collisions at $\sqrt{s}$ = 200 gev. the momentum distribution of electrons from decays of heavy flavor (charm and beauty) for midrapidity |y| &lt; 0.35 in p+p collisions at sqrt(s) = 200 gev has been measured by the phenix experiment at the relativistic heavy ion collider (rhic) over the transverse momentum range 0.3 &lt; p_t &lt; 9 gev/c. two independent methods have been used to determine the heavy flavor yields, and the results are in good agreement with each other. a fixed-order-plus-next-to-leading-log pqcd calculation agrees with the data within the theoretical and experimental uncertainties, with the data/theory ratio of 1.72 +/- 0.02^stat +/- 0.19^sys for 0.3 &lt; p_t &lt; 9 gev/c. the total charm production cross section at this energy has also been deduced to be sigma_(c c^bar) = 567 +/- 57^stat +/- 224^sys micro barns.
longitudinal double-spin asymmetry and cross section for inclusive jet production in polarized proton collisions at $\sqrt{s}$ =200 gev. we report a measurement of the longitudinal double-spin asymmetry a(ll) and the differential cross section for inclusive midrapidity jet production in polarized proton collisions at s=200 gev. the cross section data cover transverse momenta 5 &lt; p(t)&lt; 50 gev/c and agree with next-to-leading order perturbative qcd evaluations. the a(ll) data cover 5 &lt; p(t)&lt; 17 gev/c and disfavor at 98% c.l. maximal positive gluon polarization in the polarized nucleon.
recent astrophysical and accelerator based results on the hadronic equation of state. in astrophysics as well as in hadron physics progress has recently been made on the determination of the hadronic equation of state (eos) of compressed matter. the results are contradictory, however. simulations of heavy ion reactions are now sufficiently robust to predict the stiffness of the (eos) from (i) the energy dependence of the ratio of $k^+$ from au+au and c+c collisions and (ii) the centrality dependence of the $k^+$ multiplicities. the data are best described with a compressibility coefficient at normal nuclear matter density $\kappa$ around 200 mev, a value which is usually called ``soft'' the recent observation of a neutron star with a mass of twice the solar mass is only compatible with theoretical predictions if the eos is stiff. we review the present situation.
modeling the complexation properties of mineral-bound organic polyelectrolyte: an attempt at comprehension using the model system alumina/polyacrylic acid/m (m = eu, cm, gd). this paper contributes to the comprehension of kinetic and equilibrium phenomena governing metal ion sorption on organic-matter-coated mineral particles. sorption and desorption experiments were carried out with eu ion and polyacrylic acid (paa)-coated alumina colloids at ph 5 in 0.1 m naclo4 as a function of the metal ion loading. under these conditions, m interaction with the solid is governed by sorbed paa (paaads). the results were compared with spectroscopic data obtained by time-resolved laser-induced fluorescence spectroscopy (trlfs) with cm and gd. the interaction between m and paaads was characterized by a kinetically controlled process: after rapid metal adsorption within less than 1 min, the speciation of complexed m changed at the particle surface till an equilibrium was reached after about 4 days. at equilibrium, one part of complexed m was shown to be not exchangeable. this process was strongly dependent on the ligand-to-metal ratio. two models were tested to explain the data. in model 1, the kinetically controlled process was described through successive kinetically controlled reactions that follow the rapid metal ion adsorption. in model 2, the organic layer was considered as a porous medium: the kinetic process was explained by the diffusion of m from the surface into the organic layer. model 1 allowed a very good description of equilibrium and kinetic experimental data. model 2 could describe the data at equilibrium but could not explain the kinetic data accurately. in spite of this disagreement, model 2 appeared more realistic considering the results of the trlfs measurements.
analysis of dilepton invariant mass spectrum in c+c at 2 and 1 agev. recently the hades collaboration has published the invariant mass spectrum of $e^+e^-$ pairs, dn/dm$_{e^+e^-}$, produced in c+c collisions at 2 agev. using electromagnetic probes, one hopes to get in this experiment information on hadron properties at high density and temperature. simulations show that firm conclusions on possible in-medium modifications of meson properties will only be possible when the elementary meson production cross sections, especially in the pn channel, as well as production cross sections of baryonic resonances are better known. presently one can conclude that a) simulations overpredict by far the cross section at $m_{e^+e^-} \approx m_{\omega}{^0}$ if free production cross sections are used and that b) the upper limit of the $\eta$ decay into $e^+e^-$ is smaller than the present upper limit of the particle data group. this is the result of simulations using the isospin quantum molecular dynamics (iqmd) approach.
nuclear surface effects in heavy-ion collisions at rhic and sps. one may divide the collision zone in nuclear collisions into a central part ('core') with expected high energy densities, and a peripheral part ('corona') with smaller energy densities more like in pp or pa collisions. we discuss the consequences of the core–corona separation within the epos approach. the complicated centrality dependence of hadron yields observed at rhic and sps becomes almost trivial after subtracting the corona background.
electroweak boson detection in the alice muon spectrometer: w and z studies in hadron–hadron collisions at lhc. the alice capabilities for w and z detection at lhc are discussed. the contributions to single muon transverse momentum distribution are evaluated. the expected performance of the muon spectrometer for detecting w and z bosons via their muonic decay is presented. possible application for the study of in-medium effects in ultrarelativistic heavy ion collisions is discussed.
muon production in extended air shower simulations. whereas air shower simulations are very valuable tools for interpreting cosmic ray data, there is a long standing problem: is seems to be impossible to accommodate at the same time the longitudinal development of air showers and the number of muons measured at ground. using a new hadronic interaction model (epos) in air shower simulations produces considerably more muons, in agreement with results from the hires-mia experiment. we find that this is mainly due to a better description of baryon-antibaryon production in hadronic interactions. this is a new aspect of air shower physics which has never been considered so far.
multiple-humped fission and fusion barriers of the heaviest elements and ellipsoidal deformations. the deformation energy in the fusionlike deformation path has been determined from a generalized liquid drop model taking into account both the proximity energy, the asymmetry and the microscopic corrections. multiple-humped potential barriers appear in the exit and entrance channels of the heaviest elements. in the fission path of actinides, the second maximum corresponds to the transition from compact and creviced one-body shapes to two touching ellipsoids. a third peak appears in certain asymmetric exit channels where one fragment is almost a double magic nucleus with a quasi-spherical shape while the other one evolves from oblate to prolate shapes. the heights of the double and triple-humped fission barriers and the predicted half-lives of actinides follow the experimental results. in the fusion path leading possibly to superheavy elements, double-humped potential barriers appear for cold fusion but not for warm fusion. the $\alpha$ decay half-lives can be reproduced using the experimental $q\alpha$ value.
tight lp bounds for resource constrained project scheduling. null
towards documenting and automating collateral evolutions in linux device drivers. collateral evolutions are a pervasive problem in linux device driver development, due to the frequent evolution of linux driver support libraries and apis. such evolutions are needed when an evolution in a driver support library affects the library's interface, entailing modifications in all dependent device-specific code. currently, collateral evolutions in linux are done nearly manually. the large number of linux drivers, however, implies that this approach is time-consuming and unreliable, leading to subtle errors when modifications are not done consistently. in this paper, we describe the development of a language-based infrastructure, coccinelle, with the goal of documenting and automating the kinds of collateral evolutions that occur in device driver code. because linux programmers are accustomed to manipulating program modifications in terms of patch files, we base our language on the patch syntax, extending patches to semantic patches.
non identical strange particle correlations in au-au collisions at $\sqrt{s_{nn}}=$ 200 gev from the star experiment. information about the space-time evolution of colliding nuclei can be extracted correlating particles emitted from nuclear collisions. the high density of particles produced in the star experiment allows the measurement of non-identical strange particle correlations. due to the absence of coulomb interaction, $p-\lambda$ and $\bar{p}-\lambda$ systems are more sensitive to the source size than $p-p$ pairs. strong interaction potential has been studied using $p-\lambda$, and for the first time, $\bar{p}-\lambda$ pairs. the experimental correlation functions have been described in the frame of a model based on the $p-n$ interaction. the first preliminary measurement of $\pi$ - $\xi$ correlations has been performed, allowing to extract information about the freeze-out time and the space-time asymmetries in particle emission closely related to the transverse radial expansion and decay of resonances.
on the efficiency of fermi acceleration at relativistic shocks. it is shown that fermi acceleration at an ultra-relativistic shock wave cannot operate on a particle for more than 1 1/2 fermi cycle (i.e., u -&gt; d -&gt; u -&gt; d) if the particle larmor radius is much smaller than the coherence length of the magnetic field on both sides of the shock, as is usually assumed. this conclusion is shown to be in excellent agreement with recent numerical simulations. we thus argue that efficient fermi acceleration at ultra-relativistic shock waves requires significant non-linear processing of the far upstream magnetic field with strong amplification of the small scale magnetic power. the streaming or transverse weibel instabilities are likely to play a key role in this respect.
jet properties from dihadron correlations in p+p collisions at $\sqrt{s}$=200 gev. the properties of jets produced in p+p collisions at sqrt(s)=200 gev are measured using the method of two particle correlations. the trigger particle is a leading particle from a large transverse momentum jet while the associated particle comes from either the same jet or the away-side jet. analysis of the angular width of the near-side peak in the correlation function determines the jet fragmentation transverse momentum j_t . the extracted value, sqrt()= 585 +/- 6(stat) +/- 15(sys) mev/c, is constant with respect to the trigger particle transverse momentum, and comparable to the previous lower sqrt(s) measurements. the width of the away-side peak is shown to be a convolution of j_t with the fragmentation variable, z, and the partonic transverse momentum, k_t . the is determined through a combined analysis of the measured pi^0 inclusive and associated spectra using jet fragmentation functions measured in e^+e^-. collisions. the final extracted values of k_t are then determined to also be independent of the trigger particle transverse momentum, over the range measured, with value of sqrt() = 2.68 +/- 0.07(stat) +/- 0.34(sys) gev/c.
an active dipole for cosmic ray radiodetection with codalema. the codalema experiment detects the electromagnetic pulses radiated during the development of extensive air showers (eas). since 2005, in addition to spiral log-periodic antennas, ultra broad bandwidth active dipoles have been designed to detect the full electric pulse shape of these signals. a few performances of these new detectors are presented.
search for a long lived component in the reaction u+u near the coulomb barrier. we performed an experiment to search for a signature of a long living component in the collision of $^{238}$u + $^{238}$u between 6.09 and 7.35a mev. the experiment was performed at ganil using the spectrometer vamos, tuned for observing reactions with kinematics similar to fusion-fission events. theoretical calculations indicate that if a long living component would exist for this reaction, the most probable fission channel of such a giant system would be via the emissionof quasi-lead nuclei. we detected events of such a category in the focal plane of vamos. these events present an excitation function growing as a function of the bombarding energy.
energy loss and flow of heavy quarks in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. the phenix experiment at the relativistic heavy ion collider (rhic) has measured electrons from heavy flavor (charm and bottom) decays for 0.3 &lt; p_t &lt; 9 gev/c at midrapidity (|y| &lt; 0.35) in au+au collisions at sqrt(s_nn) = 200 gev. the nuclear modification factor raa relative to p+p collisions shows a strong suppression in central au+au collisions, indicating substantial energy loss of heavy quarks in the medium produced at rhic. a large azimuthal anisotropy, v_2, with respect to the reaction plane is observed for 0.5 &lt; p_t &lt; 5 gev/c indicating non-zero heavy flavor elliptic flow. a simultaneous description of raa(p_t) and v2(p_t) constrains the existing models of heavy-quark rescattering in strongly interacting matter and provides information on the transport properties of the produced medium. in particular, a viscosity to entropy density ratio close to the conjectured quantum lower bound, i.e. near a perfect fluid, is suggested.
correlated production of p and $\bar{p}$ in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. correlations between p and pbar's at transverse momenta typical of enhanced baryon production in au+au collisions are reported. the phenix experiment measures same and opposite sign baryon pairs in au+au collisions at sqrt(s_nn) = 200 gev. correlated production of p and p^bar with the trigger particle from the range 2.5 &lt; p_t &lt; 4.0 gev/c and the associated particle with 1.8 &lt; p_t &lt; 2.5 gev/c is observed to be nearly independent of the centrality of the collisions. same sign pairs show no correlation at any centrality. the conditional yield of mesons triggered by baryons (and anti-baryons) and mesons in the same pt range rises with increasing centrality, except for the most central collisions, where baryons show a significantly smaller number of associated mesons. these data are consistent with a picture in which hard scattered partons produce correlated p and p^bar in the p_t region of the baryon excess.
$j/\psi$ production vs transverse momentum and rapidity in p+p collisions at $\sqrt{s}$ = 200 gev. j/psi production in p+p collisions at sqrt(s) = 200 gev has been measured in the phenix experiment at the relativistic heavy ion collider (rhic) over a rapidity range of -2.2 &lt; y &lt; 2.2 and a transverse momentum range of 0 &lt; pt &lt; 9 gev/c. the statistics available allow a detailed measurement of both the pt and rapidity distributions and are sufficient to constrain production models. the total cross section times branching ratio determined for j/psi production is b_{ll} sigma_pp^j/psi = 178 +/- 3(stat) +/- 53(syst) +/- 18(norm) nb.
characteristics of radioelectric fields from air showers induced by uhecr measured with codalema. the detection of radio transients associated with extensive air showers eas induced by ultra high energy cosmic rays with the apparatus codalema allows for the first time the characterisation and sampling of electric field features . core location together with the electric field profile are determined on an event-by-event basis.the possibility to discriminate an eas event using a self triggering network of antennas opens the possible deployment of complementary set-ups to existing large surface detectors in order to study the longitudinal development of eas as well as a contribution to the energy determination and nature of uhecr.
sorption of cs, ni, pb, eu(iii), am(iii), cm, ac(iii), tc(iv), th, zr, and u(iv) on mx 80 bentonite: an experimental approach to assess model uncertainty. a multi-site surface complexation/ion exchange model for dispersed mx 80 bentonite has been calibrated, considering the dissolution properties of the constituting mineral assemblage, for sorption of a large number of radionuclides, using experimental data from the present study together with well constrained literature data. emphasis was on tri- and tetravalents actinides and fission products and reducing groundwater compositions.
system size and energy dependence of jet-induced hadron pair correlation shapes in cu+cu and au+au collisions at $\sqrt{s_{nn}}$ = 200 and 62.4 gev. we present azimuthal angle correlations of intermediate transverse momentum (1-4 gev/c) hadrons from {dijets} in cu+cu and au+au collisions at sqrt(s_nn) = 62.4 and 200 gev. the away-side dijet induced azimuthal correlation is broadened, non-gaussian, and peaked away from \delta\phi=\pi in central and semi-central collisions in all the systems. the broadening and peak location are found to depend upon the number of participants in the collision, but not on the collision energy or beam nuclei. these results are consistent with sound or shock wave models, but pose challenges to cherenkov gluon radiation models.
production of $\omega$ mesons at large transverse momenta in p+p and d+au collisions at $\sqrt{s_{nn}}$ = 200 gev. the phenix experiment at rhic has measured the invariant cross section for omega-meson production at mid-rapidity in the transverse momentum range 2.5 &lt; p_t &lt; 9.25 gev/c in p+p and d+au collisions at sqrt(s_nn) = 200 gev. measurements in two decay channels (omega --&gt; pi^0 pi^+ pi^- and omega --&gt; pi^0 gamma) yield consistent results, and the reconstructed omega mass agrees with the accepted value within the p_t range of the measurements. the omega/pi^0 ratio is found to be 0.85 +/- 0.05(stat) +/- 0.09(sys) and 0.94 +/- 0.08(stat) +/- 0.12(sys) in p+p and d+au collisions respectively, independent of p_t . the nuclear modification factor r_da is 1.03 +/- 0.12(stat) +/- 0.21(sys) and 0.83 +/- 0.21(stat) +/- 0.17(sys) in minimum bias and central (0-20%) d+au collisions, respectively.
microscopic calculations of double and triple giant resonance excitation in heavy ion collisions. we perform microscopic calculations of the inelastic cross sections for the double and triple excitation of giant resonances induced by heavy ion probes within a semicalssical coupled channels formalism. the channels are defined as eigenstates of a bosonic quartic hamiltonian constructed in terms of collective rpa phonons. therefore, they are superpositions of several multiphonon states, also with different numbers of phonons and the spectrum is anharmonic. the inclusion of (n+1) phonon configurations affects the states whose main component is a n-phonon one and leads to an appreacible lowering of their energies. we check the effects of such further anharmonicities on the previous published results for the cross section for the double excitation of giant resonances. we find that the only effect is a shift of the peaks towards lower energies, the double gr cross section being not modified by the explicity inclusion of the three-phonon channels in the dynamical calculations. the latters give an important contribution to the cross section in the triple gr energy region which however is still smaller than the experimental available data. the inclusion of four phonon configurations in the structure calculations does not modify the results.
radioelectric field features of extensive air showers observed with codalema. based on a new approach to the detection of radio transients associated with extensive air showers induced by ultra high energy cosmic rays, the experimental apparatus codalema is in operation, measuring about 1 event per day corresponding to an energy threshold ~ 5. 10^16 ev. its performance makes possible for the first time the study of radio-signal features on an event-by-event basis. the sampling of the magnitude of the electric field along a 600 meters axis is analyzed. it shows that the electric field lateral spread is around 250 m (fwhm). the possibility to determine with radio both arrival directions and shower core positions is discussed.
forward neutral pion production in p+p and d+au collisions at $\sqrt{^{s}nn}$=200 gev. measurements of the production of forward pi(0) mesons from p+p and d+au collisions at root s(nn) = 200 gev are reported. the p+p yield generally agrees with next-to-leading order perturbative qcd calculations. the d+au yield per binary collision is suppressed as eta increases, decreasing to similar to 30% of the p+p yield at =4.00, well below shadowing expectations. exploratory measurements of azimuthal correlations of the forward pi(0) with charged hadrons at eta approximate to 0 show a recoil peak in p+p that is suppressed in d+au at low pion energy. these observations are qualitatively consistent with a saturation picture of the low-x gluon structure of heavy nuclei.
identified baryon and meson distributions at large transverse momenta from au plus au collisions at $\sqrt{^{s}nn}$=200 gev. transverse momentum spectra of pi(+/-), p, and (p) over bar p up to 12 gev/c at midrapidity in centrality selected au + au collisions at root s(nn) = 200 gev are presented. in central au + au collisions, both pi(+/-) and p((p) over bar) show significant suppression with respect to binary scaling at p(t) greater than or similar to 4 gev/c. protons and antiprotons are less suppressed than pi(+/-), in the range 1.5 less than or similar to p(t) less than or similar to 6 gev/c. the pi(-)/pi(+) and (p) over bar /p ratios show at most a weak pt dependence and no significant centrality dependence. the p/pi ratios in central au + au collisions approach the values in p + p and d + au collisions at p(t) greater than or similar to 5 gev/c. the results at high p(t) indicate that the partonic sources of pi(+/-), p, and (p) over bar have similar energy loss when traversing the nuclear medium.
physics process of cosmogenics isotopes production on muons interactions with carbon target in liquid scintillator. in the present paper we will focuse on the problematic of comogenics isotopes, in particular li-9 and he-8, which are produced by muons interactions with the carbon target contained in the liquid scintillator. the study of this kind of isotopes has a major role in reactor neutrino experiments because their ability to mimic the detection reaction. we will discuss the physics porcess which dominate the production of such isotopes, after we will show via comparisons with availiable data the reliability of this point of view.
one-dimensional hybrid approach to extensive air shower simulation. an efficient scheme for one-dimensional extensive air shower simulation and its implementation in the program conex are presented. explicit monte carlo simulation of the high-energy part of hadronic and electromagnetic cascades in the atmosphere is combined with a numeric solution of cascade equations for smaller energy sub-showers to obtain accurate shower predictions. the developed scheme allows us to calculate not only observables related to the number of particles (shower size) but also ionization energy deposit profiles which are needed for the interpretation of data of experiments employing the fluorescence light technique. we discuss in detail the basic algorithms developed and illustrate the power of the method. it is shown that monte carlo, numerical, and hybrid air shower calculations give consistent results which agree very well with those obtained within the corsika program.
chemical basis properties of 211-astatine, a potential radionuclide in alpha-immunotherapy. null
study of the interaction of $seo_3^{2-}$, $i^-$, $cs^+$ and $ni^{2+}$ on consolidated bentonite. null
contribution to the understanding of the transfer of eu (iii) between bacteria and humic substances either free in solution or adsorbed onto a mineral phase. null
evaluation of a new experimental design to investigate the concentration dependent diffusion of radionuclides in compacted bentonite. null
complexation of eu(iiii) by polyacrylic acid either free in solution or sorbed onto alumina. null
experimental investigations of colloids and eu(iii)/ni(ii) adsorption properties of the cavallova-oxfordian claystone from bure (france). null
radioelement solubility, speciation and solid-solution partitioning in clay systems. null
comparison of complexed species of eu in alumina-bound and free polyacrylic acid: a spectroscopic study. the speciation of eu complexed with polyacrylic acid (paa) and alumina-bound paa (paaads) was studied at ph 5 in 0.1 m naclo4. structural parameters were obtained from 7f0 → 5d0 excitation spectra measured by laser-induced fluorescence spectroscopy as well as from eu liii-edge extended x-ray absorption fine structure (exafs) spectra. the coordination mode was also investigated by infrared spectroscopy. to elucidate the nature of the complexed species, eu–acetate complexes were used as references. the spectroscopic techniques show that two carboxylate groups with 2–3 (eupaa) and 4–5 (eupaaads) water molecules are coordinated to eu in the first coordination sphere. for eupaaads, the coordination between carboxylate groups and eu appears to be bidendate. a similar coordination is probable for eupaa but the exafs data indicate a slightly distorted coordination. the results show that the degree of freedom of carboxylate groups is not the same for free or adsorbed paa. for paa, the degree of freedom is constrained by the flexibility of the methylene chain. when paa is adsorbed on alumina, the polymer chains cannot any more be treated as independent chains. one may rather assume formation of aggregates that form an organic layer at the mineral surface presenting a complex arrangement of carboxylate groups.
interaction of eu with alumina-bound polyacrylic acid. sorption, desorption and spectroscopic studies. null
are the complexation properties of soluble humic substances and mineral -bound humic substances identical? an attempt of understanding using the model system alumina/polyacrylic acid/m(iii) (m=eu, cm). the ternary system composed of metal ions (m), humic substances (hs), and mineral phases is actually not understood. among the various equilibria considered, the most difficult to understand is the interaction of m with the mineral-bound hs. it appears actually established that an additive model based on data deduced from binary systems is not applicable. two main reasons are suggested to explain this: the first one invokes the formation of a ternary surface complex where the metal ion is coordinated to both mineral and organic matter functional groups. the second reason raises the problem of using an electrostatic term derived from a model developed for pure mineral oxides for a surface coated by an organic layer. all studies generally consider that sorbed hs and soluble hs behave similarly with respect to m (same complexed species, same mechanistic interaction equilibria). the goal of the work is to check this point using a model ternary system composed of g-alumina, trivalent metal ions m(iii) (m=eu and cm) and polycarboxylic acid (paa). paa is adsorbed on al2o3 prior to m adsorption to study selectively m / sorbed paa (paaads) interaction. in order to neglect eu-al2o3 interaction, the work is performed under conditions where m interaction is dominated by the organic matter (ph=5, maximum organic loading). the characterization of the interaction mechanisms between m and free and sorbed paa is based on sorption/desorption data as well as spectroscopic measurements (trlfs, exafs). the results clearly show a difference for the m sorption on paa and paaads since m speciation and mechanisms appear different in both systems. the origin of this discrepancy may be explained by the difference in paa “structure” in the two systems. when paa is free in solution, in the range of paa concentrations studied, the polymers are independent linear chains. when paa is adsorbed on alumina, the polymer chains cannot any more be treated as independent chains. one may rather assume formation of aggregates that form an organic layer at the mineral surface which can be viewed as a penetrable, gel-like structure.
experimental evidence for the influence of the excitation wavelength on the value of the equilibrium constant as determined by time-resolved emission spectroscopy (tres). in the case of a rapid photochemical process, a new theoretical result relating time-resolved emission spectroscopy data to three physical parameters of the chemical system has been recently proposed. this previous work, based on a simulation study, is experimentally evidenced in the present paper, using europium/acetate as a model system. the comparison of the emission spectra obtained upon direct excitation of europium (394 nm) and by use of the “antenna effect” (266 nm) evidences the occurring of a back-dissociation of excited europium complexes to form solvated excited free europium ions.
energy loss of a heavy quark produced in a finite size medium. we study the medium-induced energy loss $-\delta e_0(l_p)$ suffered by a heavy quark produced at initial time in a quark-gluon plasma, and escaping the plasma after travelling the distance $l_p$. the heavy quark is treated classically, and within the same framework $-\delta e_0(l_p)$ consistently includes: the loss from standard collisional processes, initial bremsstrahlung due to the sudden acceleration of the quark, and transition radiation. the radiative loss induced by rescatterings $-\delta e_{rad}(l_p)$ is not included in our study. for a ultrarelativistic heavy quark with momentum $p \gsim 10 \ {\rm gev}$, and for a finite plasma with $l_p \lsim 10 \ {\rm fm}$, the loss $-\delta e_0(l_p)$ is strongly suppressed compared to the standard stationary regime $-\delta e_0(l_p) \simeq -\delta e_{coll}(l_p) \propto l_p$ valid when $l_p$ becomes large. this suppression is a consequence of the retardation affecting collisional processes - the latter cannot start at initial time when the heavy quark has not built its asymptotic proper field yet. our results indicate that $-\delta e_{rad}$ should be the dominant contribution to the total heavy quark energy loss, $-\delta e_{tot} = -\delta e_0 -\delta e_{rad} \simeq -\delta e_{rad}$, as indeed assumed in most of jet-quenching analyses. however they might raise some question concerning the rhic data on large $p_{\perp}$ electron spectra.
perturbative gauge theory in a background. motivated by the gluon condensate in qcd we study the perturbative expansion of a gauge theory in the presence of gauge bosons of vanishing momentum, in the specific case of an abelian theory. the background is characterised by a dimensionful parameter $\lambda$ affecting only the on-shell prescription of the free (abelian) gluon propagator. when summed to all orders in $g\lambda$ the modification is equivalent to evaluating standard green functions in a pure gauge field with an imaginary gauge parameter $\propto \lambda$. we show how to calculate the corresponding dressed green functions, which are poincaré and gauge covariant. we evaluate the expressions for the dressed quark and $q \bar q$ propagators, imposing as boundary condition that they approach the standard perturbative form in the short-distance limit ($|p^2|\to\infty$). the on-shell ($p^2=m^2$) pole of the free quark propagator is removed for any $\lambda &gt; 0$, and replaced by a discontinuity which vanishes exponentially with $p^2$. the dressing introduces an effective interaction between quarks and antiquarks which is enhanced at low relative 3-momentum. further study should allow to identify the (bound) eigenstates of propagation and determine whether they define a unitary $s$-matrix.
nuclear effects on hadron production in d+au and p+p collisions at $\sqrt{s_{nn}}$=200 gev. phenix has measured the centrality dependence of mid-rapidity pion, kaon and proton transverse momentum distributions in d+au and p+p collisions at sqrt(s_nn) = 200 gev. the p+p data provide a reference for nuclear effects in d+au and previously measured au+au collisions. hadron production is enhanced in d+au, relative to independent nucleon-nucleon scattering, as was observed in lower energy collisions. the nuclear modification factor for (anti) protons is larger than that for pions. the difference increases with centrality, but is not sufficient to account for the abundance of baryon production observed in central au+au collisions at rhic. the centrality dependence in d+au shows that the nuclear modification factor increases gradually with the number of collisions suffered by each participant nucleon. we also present comparisons with lower energy data as well as with parton recombination and other theoretical models of nuclear effects on particle production.
study of the interaction of ni$^{2+}$ and cs$^+$ on mx-80 bentonite; effect of compaction using the "capillary method". the goal of the paper is to assess the applicability of sorption models to describe the retention of contaminants on clay materials, both in dispersed and compacted states. a batch method is used to characterize the sorption equilibria between cs, ni, and mx-80 bentonite for solid-to-liquid ratios varying from 0.5 to 4200 kg/m3. for compacted bentonite (dry density of 1100 kg/m3), a new method is presented where the material compaction is performed in peek capillaries. sorption edges and isotherms were measured in the presence of a synthetic groundwater. a model considering cation exchange reactions with interlayer cations and surface complexation reactions with edge sites was used for the dispersed state. montmorillonite was shown to be the dominant interacting phase in mx-80 bentonite. the applicability of the model to compacted bentonite was tested. the results indicate that under conditions where the cation exchange mechanism is dominant, there is no difference between the dispersed and compacted states. for the degree of compaction studied, all exchange sites are available for sorption. for ni, when surface complexation is the dominant sorption mechanism, a decrease of the kd values by a factor of about 3 was observed (ph 7-8, trace concentrations). this could be explained quantitatively by a diminution of the conditional interaction constant between ni and the edge surface site in the compacted state. one consequence of this decrease is that the contribution of the organic matter content of mx-80 bentonite to the total sorption becomes significant.
a universal primer set for pcr amplification of nuclear histone h4 genes from all animal species. to control the quality of genomic dna of samples from a wide variety of animals, a heminested pcr assay specifically targeting a nuclear gene has been developed. the histone h4 gene family comprises a small number of genes considered among the most conserved genes in living organisms. tissue samples from necropsies and from cells belonging to 43 different species were studied, eight samples from invertebrates and 35 samples from vertebrates covering all classes. ancient dna samples from three siberian woolly mammoths (mammuthus primigenius) dating between 40,000 and 49,000 years before present were also tested for pcr amplification. performance of hist2h4 amplification were also compared with those of previously published universal pcrs (28s rrna, 18s rrna, and cytochrome b). overall, 95% of species studied yielded an amplification product, including some old samples from gorilla and chimpanzees. the data indicate that the hist2h4 amplimers are, thus, suitable for both dna quality testing as well as species identification in the animal kingdom.
novel phosphate-phosphonate hybrid nanomaterials applied to biology. null
the multiplicity dependence of inclusive $p_t$ spectra from p-p collisions at $\sqrt{s}$ = 200 gev. we report measurements of transverse momentum $p_t$ spectra for ten event multiplicity classes of p-p collisions at $\sqrt{s} = 200$ gev. by analyzing the multiplicity dependence we find that the spectrum shape can be decomposed into a part with amplitude proportional to multiplicity and described by a lévy distribution on transverse mass $m_t$, and a part with amplitude proportional to multiplicity squared and described by a gaussian distribution on transverse rapidity $y_t$. the functional forms of the two parts are nearly independent of event multiplicity. the two parts can be identified with the soft and hard components of a two-component model of p-p collisions. this analysis then provides the first isolation of the hard component of the $p_t$ spectrum as a distribution of simple form on $y_t$.
azimuthal angle correlations for rapidity separated hadron pairs in d+au collisions at $\sqrt{s_{nn}}$=200 gev. deuteron-gold (d+au) collisions at the relativistic heavy ion collider provide ideal platforms for testing qcd theories in dense nuclear matter at high energy. in particular, models suggesting strong saturation effects for partons carrying small nucleon momentum fraction (x) predict modifications to jet production at forward rapidity (deuteron-going direction) in d+au collisions. we report on two-particle azimuthal angle correlations between charged hadrons at forward/backward (deuteron/gold going direction) rapidity and charged hadrons at midrapidity in d+au and p+p collisions at sqrt(s[sub nn])=200 gev. jet structures observed in the correlations are quantified in terms of the conditional yield and angular width of away-side partners. the kinematic region studied here samples partons in the gold nucleus with x~0.1 to ~0.01. within this range, we find no x dependence of the jet structure in d+au collisions.
water diffusion in the simulated french nuclear waste glass son 68 contacting silica rich solutions: experimental and modeling. to understand the role of water diffusion on the long-term nuclear waste glass alteration, dynamic experiments were conducted with the borosilicate son 68 glass in synthetic solutions enriched in si, na and b at 50 and 90 °c. the water entering the glass exists to 80% in the form of molecular h2o and to 20% in the form of sioh. the ratio of h/na was 2.6 ± 0.3, indicating a complex mechanism including water diffusion and ionic-exchange. it was in agreement with model calculations based on glass structural units such as reedmergnerite and b2o3. water diffusion coefficients in the glass, determined by modeling of the experimental data, were between 2 × 10−21 and 6 × 10−23 m2 s−1. finally, under hlw disposal conditions, where interaction of nuclear glass with groundwater is expected to maintain saturation conditions, it is likely that water diffusion will contribute to the control of the glass alteration and the release of radionuclides.
constraints on the time-scale of nuclear breakup from thermal hard-photon emission. measured hard photon multiplicities from second-chance nucleon-nucleon collisions are used in combination with a kinetic thermal model, to estimate the break-up times of excited nuclear systems produced in nucleus-nucleus reactions at intermediate energies. the obtained nuclear break-up time for the $^{129}${xe} + $^{nat}${sn} reaction at 50{\it a} mev is $\delta$$\tau$ $\approx$ 100 -- 300 fm/$c$ for all reaction centralities. the lifetime of the radiating sources produced in seven other different heavy-ion reactions studied by the taps experiment are consistent with $\delta$$\tau$ $\approx$ 100 fm/$c$, such relatively long thermal photon emission times do not support the interpretation of nuclear breakup as due to a fast spinodal process for the heavy nuclear systems studied.
strontium binding by calcium silicate hydrates. in the present study the binding of strontium with pure calcium silicate hydrates (c-s-h) has been investigated using batch-type experiments. synthetic c-s-h phases with varying cao:sio2 (c:s) mol ratios, relevant to non-degraded and degraded hardened cement paste, were prepared in the absence of alkalis (na(i), k(i)) and in an alkali-rich artificial cement pore water (acw). two types of experimental approaches have been employed, investigating sorption and co-precipitation processes, respectively. the sr(ii) sorption kinetics were determined as well as sorption isotherms, the effect of the solid to liquid ratio and the composition (c:s ratio) of the c-s-h phases. in addition, the reversibility of the sr(ii) sorption was tested. it was shown that both the sorption and co-precipitation tests resulted in sr(ii) distribution ratios which were similar in value, indicating that the same sites are involved in sr(ii) binding. in alkali-free solutions, the sr(ii) uptake by c-s-h phases was described in terms of a sr2+–ca2+ ion exchange model. the selectivity coefficient for the sr2+–ca2+ exchange was determined to be 1.2±0.3.
measurement and modeling of the surface potential evolution of hydrated cement pastes as a function of degradation. hydrated cement pastes (hcp) have a high affinity with a lot of (radio)toxic products and can be used as waste confining materials. in cementitious media, elements are removed from solution via (co)precipitation reactions or via sorption/diffusion mechanisms as surface complexation equilibria. in this study, to improve the knowledge of the surface charge evolution vs the degradation of the hcp particles, two cements have been studied: cem-i (ordinary portland cement, opc) and cem-v (blast furnace slag and fly ash added to opc). zeta potential measurements showed that two isoelectric points exist vs hcp leaching, i.e., ph. zeta potential increases from −17 to +20 mv for ph 13.3 to ph 12.65 (fresh hcp states) and decreases from 20 to −8 mv for ph 12.65 to 11 (degraded hcp states). the use of a simple surface complexation model of c-s-h, limited in comparison with the structural modeling of c-s-h in literature, allows a good prediction of the surface potential evolution of both hcp. using this operational modeling, the surface charge is controlled by the deprotonation of surface sites (&gt;so−) and by the sorption of calcium (&gt;soca+), which brings in addition a positive charge. the calcium concentration is controlled by portlandite or calcium silicate hydrate (c-s-h) solubilities.
coefficients and terms of the liquid drop model and mass formula. the coefficients of different combinations of terms of the liquid drop model have been determined by a least square fitting procedure to the experimental atomic masses. the nuclear masses can also be reproduced using a coulomb radius taking into account the increase of the ratio $r_0/a^{1/3}$ with increasing mass, the fitted surface energy coefficient remaining around 18 mev.
automatic verification of bossa scheduler properties. bossa is a development environment for operating-system process schedulers that provides numerous safety guarantees. in this paper, we show how to automate the checking of safety properties of a scheduling policy developed in this environment. we find that most of the relevant properties can be considered as invariant or refinement properties. in order to automate the related proof obligations, we use the ws1s logic for which a decision procedure is implemented by mona. the proof techniques are implemented using the fmona tool.
femtoscopy in hydrodynamics-inspired models with resonances. effects of the choice of the freeze-out hypersurface and resonance decays on the hbt interferometry in relativistic heavy-ion collisions are studied in detail within a class of models with single freeze-out. the monte-carlo method, as implemented in therminator, is used to generate hadronic events describing production of particles from a thermalized and expanding source. all well-established hadronic resonances are included in the analysis as their role is crucial at large freeze-out temperatures. we use the two-particle method to extract the correlation functions, which allows us to study the coulomb effects. we find that the pion hbt data from rhic are fully compatible with the single freeze-out scenario, pointing at the shape of the freeze-out hypersurface where the transverse radius is decreasing with time. results for the single-particle spectra for this situation are also presented. finally, we present predictions for the kaon femtoscopy.
contribution to the analysis and the structured control design of large-scale systems. nowadays, the control designer has to deal with more and more complex systems, with a high number of dynamics, but also of actuators and captors, or even spatial structural constraints on the controller. this comes from the increasing complexity of industrial processes, the use of more and more efficient communication systems, and increasing performances requirements (optimization as global as possible).&lt;br /&gt;&lt;br /&gt;even if decentralized control theory has benefit from numerous studies since the 70's, many problems are still opened. at the same time, efficient tools for analysis and synthesis have been developed, especially in the h2 and the h¥ context. however, application of such tools in the case of large-scale systems (composed most of the time of interconnected subsystems) may be difficult, even more if structural constraints are applied to the controller.&lt;br /&gt;&lt;br /&gt;methodology is essential when the control problem of complex system is considered. to this end, this document focuses on the analysis of the interconnections of subsystems in order to choose the most appropriated structure for the controller. the choice of the criterion, and the global optimization of a h2 controller in a sequential manner are also considered. these methodological considerations are applied on an experimental web transport system.
virial corrections to simulations of heavy ion reactions. within qmd simulations we demonstrate the effect of virial corrections on heavy ion reactions. unlike in standard codes, the binary collisions are treated as non-local so that the contribution of the collision flux to the reaction dynamics is covered. a comparison with standard qmd simulations shows that the virial corrections lead to a broader proton distribution bringing theoretical spectra closer towards experimental values. complementary buu simulations reveal that the non-locality enhances the collision rate in the early stage of the reaction. it suggests that the broader distribution appears due to an enhanced pre-equilibrium emission of particles.
alpha decay half-lives of new superheavy nuclei within a generalized liquid drop model. the alpha decay half-lives of the recently produced isotopes of the 112, 114, 116 and 118 nuclei and decay products have been calculated in the quasi-molecular shape path using the experimental qalpha value and a generalized liquid drop model including the proximity effects between nucleons in the neck or the gap between the nascent fragments. reasonable estimates are obtained for the observed alpha decay half-lives. the results are compared with calculations using the density-dependent m3y effective interaction and the viola-seaborg-sobiczewski formulae. generalized liquid drop model predictions are provided for the alpha decay half-lives of other superheavy nuclei using the finite range droplet model qalpha and compared with the values derived from the vss formulae.
screening corrections in simulating heavy ion collisions. one year ago, we presented a new approach to treat hadronic interactions for the initial stage of nuclear collisions. it is an effective theory based on the gribov-regge formalism, where the internal structure of the pomerons at high energies is governed by perturbative parton evolution, therefore the name "parton-based gribov-regge theory". the main improvement compared to models used so-far is the appropriate treatment of the energy sharing between the different elementary interactions in case of multiple scattering. it is clear that the above formalism is not yet complete. at high energies (rhic, lhc), the multiple elementary interactions (pomerons) can not be purely parallel, they interact. so we introduce multiple pomeron vertices into the theory.
double chooz, a search for the neutrino mixing angle $\theta$13. the double chooz reactor neutrino experiment in france plans to quickly measure the neutrino mixing angle theta-13, or limit it to sin^2 2-theta_13 less than 0.025. the physics reach, experimental site, detector structures, scintillator, photodetection, electronics, calibration and simulations are described. the possibility of using double chooz to explore the possible use of a antineutrino detector for non-proliferation goals is also presented.
correctness of constraint retraction algorithms. in this paper, we present a general scheme for incremental constraint retraction algorithms that encompasses all existing algorithms. moreover, we introduce some necessary conditions to ensure the correctness of any incremental constraint retraction algorithms. this rather theoretical work is based on the notion of explanation for constraint programming and is exemplified within the palm system: a constraint solver allowing dynamic constraint retractions.
hard scattering cross sections at lhc in the glauber approach: from pp to pa and aa collisions. the scaling rules of the invariant yields and cross sections for hard scattering processes in proton-nucleus ($pa$) and nucleus-nucleus ($ab$) reactions at lhc energies relative to those of nucleon-nucleon $nn$ (isospin averaged $pp$) collisions are reviewed within the glauber geometrical formalism. the number of binary inelastic collisions for different centrality classes in p+pb and pb+pb collisions at $\sqrt{s_{nn}}$ = 8.8 tev and 5.5 tev respectively, as obtained from a glauber monte carlo, are also given.
hard probes at heavy-ion collider energies: results from phenix. hard processes in nucleus-nucleus interactions at relativistic energies are reviewed with emphasis on recent phenix results from the first run of the relativistic heavy-ion collider at bnl. the observed suppression of moderately high $p_t$ hadrons ($p_t$ = 2 - 5 gev/c) in $\sqrt{s_{nn}}$ = 130 gev $au+au$ central collisions with respect to the scaled $pp$ data, is discussed in terms of conventional nuclear and ``quark-gluon-plasma'' effects. the meson and baryon composition at high $p_t$, as well as the implications for open charm of the measured single electron spectrum are also presented.
on dynamics of 5d superconformal theories. 5d superconformal theories involve vacuum valleys characterized in the simplest case by the vacuum expectation value of a real scalar field. if it is nonzero, conformal invariance is spontaneously broken and the theory is not renormalizable. in the conformally invariant sector with zero scalar v.e.v., the theory is intrinsically nonperturbative. we study classical and quantum dynamics of this theory in the limit when field dependence of the spatial coordinates is disregarded. the classical trajectories ``fall'' on the singularity at the origin of scalar moduli space. the quantum spectrum involves ghost states with unbounded from below negative energies, but such states fail to form complete 16-plets as is dictated by the presence of four complex supercharges and should be rejected by that reason. physical excited states come in supermultiplets and have all positive energies. we conjecture that the spectrum of the complete field theory hamiltonian is nontrivial and has a similar nontrivial ghost-free structure and also speculate that the ghosts in higher-derivative supersymmetric field theories are exterminated by a similar mechanism.
chiral anomalies in higher-derivative supersymmetric 6d theories. we show that the recently constructed higher-derivative 6d sym theory involves an internal chiral anomaly breaking gauge invariance. the anomaly may be cancelled if adding to the theory five adjoint matter hypermultiplets.
tenacious domain walls in supersymmetric qcd. we study the structure of the tenacious (existing for all values of masses of the matter fields) bps domain walls interpolating between different chirally asymmetric vacua in supersymmetric qcd in the limit of large masses. we show that the wall consists in this case of three layers: two outer layers form a ``coat'' with the characteristic size determined by \lambda_{sym} and there is also the core with width of order of inverse mass. the core always carries a significant fraction of the total wall energy. this fraction depends on $n_f$ and on the ``windings'' of the matter fields.
identified hadron spectra at large transverse momentum in p+p and d+au collisions at $\sqrt{{^s}nn}$ = 200 gev. we present the transverse momentum (p$_t$) spectra for identified charged pions, protons and anti-protons from p+p and d+au collisions at $\sqrt{^s}nn}$ . the spectra are measured around midrapidity (|y|&lt;0.5) over the range of 0.3.
star results on strangeness production at rhic energies. strangeness measurement at rhic energies constitutes one of the favorite theme of the star collaboration. besides the fact that strangeness enhancement has been proposed as a quark gluon plasma signature, its production provides various and relevant information about the collision evolution and especially on the hadronization process. the investigation of short-lived strange particles as well as their anti-particles and resonances allows an attempt of characterization of the matter created in rhic heavy ion collisions.
aspects of chiral symmetry. we give a pedagogical review of implications of chiral symmetry in qcd. first, we briefly discuss classical textbook subjects such as the axial anomaly, spontaneous breaking of the flavor-nonsinglet chiral symmetry, formation of light pseudo-goldstone particles, and their effective interactions. then we proceed to other issues. we explain in some detail a recent discovery how to circumvent the nielsen--ninomiya's theorem and implement chirally symmetric fermions on the lattice. we touch upon such classical issues as the vafa-witten's theorem and 't hooft's anomaly matching conditions. we derive a set of exact theorems concerning the dynamics of the theory in a finite euclidean volume and the behavior of the dirac spectral density. finally, we discuss an imaginary world with a nonzero value of the vacuum angle theta.
identification of particles and hard processes with the spectrometer phos of the alice experiment. heavy-ion collisions are a unique tool to study nuclear matter under extreme conditions of density and temperature like the ones present a few moments after the big bang, where nuclear matter may be composed of a plasma of almost free quarks and gluons. its production is the main interest of the alice experiment at the lhc collider, where pb-pb collisions at 5.5a tev will be made. photons are an interesting signature produced in these collisions as they can give unperturbed information on their creation conditions. the phos detector in the alice experiment is devoted to their measurement. for this thesis a phos performance study and the development of particle identification algorithms have been done. high-energy photons will be produced with a recoiling jet which can suffer modifications due to the nuclear medium produced, an identification method of such processes is described and its sensitivity to jet modifications induced by the nuclear medium is studied.
alice, the heavy ion experiment at lhc. alice, a large ion collider experiment, is the future large hadron collider (lhc) experiment at cern devoted to the physics of quantum chromo-thermo-dynamics. relativistic heavy-ion collisions (hic) at lhc aim at the production of a plasma of quarks and gluons (qgp). this plasma is expected to be much hotter, bigger and longer than in previous hic experiments at lower center-of-mass energies. in the alice experiment, the ephemeral qgp created during the first stages of the hic will be studied by the concomitant detection of most of the probes of high temperature strongly interacting matter.
transition from participant to spectator fragmentation in au+au reactions between 60a and 150a mev. using the quantum molecular dynamics approach, we analyze the results of the recent indra au+au experiments at gsi in the energy range between 60 amev and 150 amev. it turns out that in this energy region the transition toward a participant-spectator scenario takes place. the large au+au system displays in the simulations as in the experiment simultaneously dynamical and statistical behavior which we analyze in detail: the composition of fragments close to midrapidity follows statistical laws and the system shows bi-modality, i.e. a sudden transition between different fragmentation pattern as a function of the centrality as expected for a phase transition. the fragment spectra at small and large rapidities, on the other hand, are determined by dynamics and the system as a whole does not come to equilibrium, an observation which is confirmed by fopi experiments for the same system.
further results on the accessibility of a satellite with two reaction wheels. null
re injecting the structure in nmpc schemes. application to the constrained stabilization of a snake board. in this paper, a constrained nonlinear predictive control scheme is proposed for a class of under-actuated nonholonomic systems. the scheme is based on fast generation of steering trajectories that inherently fulfill the contraints while showing a ""{\em translatability}"" property which is generally needed to derive stability results in receding-horizon schemes. the corresponding open-loop optimization problem can be solved very efficiently with a fixed and deterministic upper bound on the execution time making possible a real-time implementation on fast systems. the whole framework is shown to hold for the well known challenging problem of a snake board constrained stabilization. illustrative simulations are proposed to assess the efficiency of the proposed solution.
vehicle and personnel routing optimization in the service sector: application to water distribution and treatment. this research, financed by a research contract with générale des eaux,&lt;br /&gt;concerns multiperiod vehicle routing problemwith time windows and a limited fleet.&lt;br /&gt;we propose several solution approaches. the main approaches are a metaheuristic, an optimal method, and an optimal-based heuristic method. the metaheuristic is based on evolution strategies, and memetic&lt;br /&gt;algorithms. the optimal method is based on column generation. the very low rate of time windows makes the subproblem very hard to solve. this implies to find new ways to solve this subproblem, which lead us to design the optimal-based heuristic method.&lt;br /&gt;in a second stage, applying these methods to real-life inspired scenarios helps us providing decision aid methodology and information.
behaviour of spent htr fuel elements in aquatic phases of repository host rock formations. one back-end option for spent htr fuel elements proposed for future htr fuel cycles in the ec is an open fuel cycle with direct disposal of conditioned or non-conditioned fuel elements. this option has already been chosen in germany due to the political decision to terminate the use of htr technology. first integral leaching investigations at research centre juelich on the behaviour of spent htr fuel in salt brines, typical of accident scenarios in a repository in salt, proved that the main part of the radionuclide inventory cannot be mobilised as long as the coated particles do not fail. however, such experiments will not lead to a useful model for performance assessment calculations, because a failure of the coatings by corrosion will not occur during experimental times of a few years. in order to get a robust and realistic model for the long-term behaviour in aqueous phases of host rock systems, it is necessary to understand the barrier function of the different parts of an htr fuel element, i.e. the matrix graphite, the different coating materials, and the fuel kernel. therefore, our attention is focused on understanding and modelling the barrier performance of the different parts of an htr fuel element with respect to their barrier function, and on the development of an overall model for performance assessment. in order to understand this behaviour, it is necessary to start with investigations of unirradiated material, and to proceed with experiments with external gamma irradiation to determine the effects of oxidising radiolysis species. further experiments with irradiated material have to be performed to investigate the influence of the irradiation damage, and finally an investigation has to be made of the irradiated material plus additional gamma irradiation. experimental data are now available for the diffusive transport of radionuclides in the water-saturated graphite pore system, the corrosion rates of unirradiated graphite with and without external gamma irradiation and unirradiated and irradiated silicon carbide, and for the dissolution rates of uo2 and (th,u)o2 fuel kernels with and without external gamma irradiation. all investigations were performed in aquatic phases from salt, granite, and clay host rock.
local energy efficiency and demand-side managment activities in france. null
chemical thermodynamics of zirconium. null
differential algebraic equations : a new look at the index. null
modeling railway control systems using graph grammars: a case study. in this report, we develop a railway control system. we are mainly concerned with the software architecture of the control system and its dynamic evolution ; we do not discuss here the implementation details of the components forming the control system. the software architecture is characterized by a hierarchy of controllers whose leaves are local controllers connected in a network that mimics the underlying railway topology. using a particular graph grammar, we formally define a class of software architectures for the railway control system ensuring several desirable properties by construction. the dynamic evolution of the architecture is modelled by a set of coordination rules which define graph transformations. particular emphasis is placed on the verification of these rules with respect to desirable properties encoded in the grammars. using the graph grammar and the coordination rules as a formal specification of the railway control system, we derive an implementation in concoord, an environment for concurrent coordinated programming whose coordination language permits us to define the software architecture of the control system and its dynamic evolution abstracting away from the implementation details of its components.
a software toolbox to carry-out virtual experiments on human motion. we present a simulation toolbox to carry-out virtual experiments on human motion. 3d visualization, automatic code generation and generic control design patterns bring dynamic simulation tools into the hands of biomechanicians and doctors.
detection and resolution of aspect interactions. aspect-oriented programming (aop) promises separation of concerns at the implementation level. however, aspects are not always orthogonal and aspect interaction is an important problem. currently there is almost no support for the detection and resolution of such interactions. the programmer is responsible for identifying interactions between conflicting aspects and implementing conflict resolution code. in this paper, we propose a solution to this problem based on a generic framework for aop. the contributions are threefold: we present a formal and expressive crosscut language, two static conflict analyses and some linguistic support for conflict resolution.
towards a concurrent model of event-based aspect-oriented programming. the event-based aspect-oriented programming model (eaop) makes it possible to define pointcuts in terms of sequences of events emitted by the base program. the current formalization of the model relies on a monolithic entity, the monitor, which observes the execution of the base program and executes the actions associated to the matching pointcut. this model is not intrinsically sequential but its current formalization favors a sequential point of view. in this paper, we present a new formalization of eaop as finite state processes. this new formalization paves the way to reasoning about aspects in a concurrent setting and to the definition and implementation of concurrent eaop languages.
condensation mechanisms of tetravalent technetium in chloride media. the condensation mechanisms of tetravalent technetium in chloride media were studied in the ph range 0–1.5. a new dimer complex of tc(iv) has thus been discovered, tc2ocl104−. spectroscopic and kinetics studies showed that the formation of this compound resulted from the condensation of tccl5(h2o)−. an exafs study indicates that the dimer displays a [tc-o-tc]6+ structure. as the ph increases, uv-visible measurements showed a cyclization of [tc-o-tc]6+ into [tc(μo)2tc]4+ leading, in fine, to the precipitation of tco2·x h2o. the aquation constant (kaq) of tccl62− into tccl5(h2o)− and the dimerisation constant (logkdim) of tccl5(h2o)− into tc2ocl104− were determined to be 2.20±0.26 and 4.68±0.09, respectively.
speciation of technetium and rhenium complexes by in situ xas-electrochemistry. a spectro-electrochemical cell was developed in order to study the speciation of radio-elements in thermodynamic unstable redox states using in situ xas spectroscopy. this cell was used for the speciation of re and tc complexes in chloride media. experiments on re were carried out with the aim to validate the functionality of the experimental set-up. during electro-reduction of re(vii) in hcl media, exafs and xanes studies were performed in order to reveal the formation of chloro-oxygenated compounds of re(iv). the speciation of technetium in aqueous solutions of deep geological deposits for radioactive waste is important to predict its mobility under reducing conditions. xanes spectra showed that electro-reduction of tc(vii) in chloride media leads to a position of k-edge absorption which agrees with a tc(iv)/tc(iii) mixture.
photochemical behaviour of tc$_2$ocl$_{10}^{4–}$ and tc$_no_y^{4n–2y+}$ in chloride media. the stabilities of the technetium polymers tc2ocl104− and tcnoy4n−2y+ have been studied under light irradiation in 3 m chloride media with a ph range from 0 to 1.3. it has been shown that under irradiation, tc2ocl104− is not stable and undergoes dissociation to tccl5(h2o)- at ph = 0 and ph = 0.3. at ph = 1, irradiation of tc2ocl104− leads to a stationary state involving tccl5(h2o)- and tc2ocl104−. at ph = 1.3, tcnoy4n−2 y+ remains stable under irradiation. under light irradiation, the predominance diagram of tc(iv) species obtained from tc2ocl104− aquation in a ph range from 0 to 1 is drawn. the chemical behaviour of tc2ocl104− and the influence of the light on the condensation of tc(iv) and solubility of tco2· x h2o are discussed.
leaching behaviour of unirradiated high temperature reactor (htr) uo$_2$-tho$_2$ mixed oxide fuel particles. null
effect of alpha radiolysis on doped uo$_2$ dissolution under reducing conditions. the aim of the present work is to quantify the influence of alpha radioactivity of 225ac doped uo2 on the dissolution rate under reducing conditions at ph 6. the doped or undoped material was prepared by precipitation and the size of particles was about 3 nm. the total alpha activity of the doped material was about 2000 mbq g-1 uo2, about 4 times higher than that of 15 years old spent fuel. the solution was kept under reducing conditions during the experiment by permanent electrochemical reduction under inert atmosphere. the results showed that the dissolution rate of doped material was a function of alpha activity and thus a function of the dose.
directed flow in au+au collisions at $\sqrt{{^s}nn}$ =62.4 gev. we present the directed flow (v1) measured in au+au collisions at $\sqrt((^s)nn)$=62.4 gev in the midpseudorapidity region |\eta|&lt;1.3 and in the forward pseudorapidity region 2.5&lt;|\eta|&lt;4.0. the results are obtained using the three-particle cumulant method, the event plane method with mixed harmonics, and for the first time at the relativistic heavy ion collider, the standard method with the event plane reconstructed from spectator neutrons. results from all three methods are in good agreement. over the pseudorapidity range studied, charged particle directed flow is in the direction opposite to that of fragmentation neutrons.
multiplicity and pseudorapidity distributions of charged particles and photons at forward pseudorapidity in au+au collisons at $\sqrt{{^s}nn}$=62.4 gev. we present the centrality-dependent measurement of multiplicity and pseudorapidity distributions of charged particles and photons in au+au collisions at $\sqrt((^s)nn)$=62.4 gev. the charged particles and photons are measured in the pseudorapidity region $2.9 \leq \eta \leq 3.9$ and 2.3 \leq \eta \leq 3.7$, respectively. we have studied the scaling of particle production with the number of participating nucleons and the number of binary collisions. the photon and charged particle production in the measured pseudorapidity range has been shown to be consistent with energy-independent limiting fragmentation behavior. photons are observed to follow a centrality-independent limiting fragmentation behavior, while for charged particles it is centrality dependent. we have carried out a comparative study of the pseudorapidity distributions of positively charged hadrons, negatively charged hadrons, photons, pions, and net protons in nucleus-nucleus collisions and pseudorapidity distributions from p+p collisions. from these comparisons, we conclude that baryons in the inclusive charged particle distribution are responsible for the observed centrality dependence of limiting fragmentation. the mesons are found to follow an energy-independent behavior of limiting fragmentation, whereas the behavior of baryons is energy dependent.
speciation of tc(iv) in chloride solutions by capillary electrophoresis. browse search my profile activate help home &gt; list of issues &gt; table of contents &gt; abstract speciation of tc(iv) in chloride solutions by capillary electrophoresis author(s): xiaolan liu | frédéric poineau | massoud fattahi | bernd grambow | l. vichot doi: 10.1524/ract.93.5.305.64276 view pdf article (245 k) view table of contents email this link add to my alerts what is rss? trouble viewing articles as pdf? radiochimica acta print issn: 0033-8230 volume: 93 | issue: 5/2005 cover date: 20050501 page(s): 305-309 abstract text a method for speciation of tc(iv) species (tccl62− and tccl5(h2o)−) in chloride solutions, using capillary electrophoresis (ce) technique was developed. the proposed method has overcome the difficulties of unstable oxidation states analysis by shortening their travel time in the capillary. tccl62− and tccl5(h2o)− were thus separated without being hydrolyzed and polymerized, and their uv/vis spectra were recorded. with a 1 m hcl/nacl buffer solution (ph=1), the electrophoretic mobilities were determined as 5.47×10−4 cm2/vs for tccl62− and 2.13×10−4 cm2/vs for tccl5(h2o)− at 25 °c. the total analysis time for one run is 12 minutes.
microbial reduction of $^{99}$tc in organic matter-rich soils. for safety assessment purposes, it is necessary to study the mobility of long-lived radionuclides in the geosphere and the biosphere. within this framework, we studied the behaviour of 99tc in biologically active organic matter-rich soils. to simulate the redox conditions in soils, we stimulated the growth of aerobic and facultative denitrifying and anaerobic sulphate-reducing bacteria (srb). in the presence of either a pure culture of denitrifiers (pseudomonas aeruginosa) or a consortium of soil denitrifiers, the solubility of tco4− was not affected. the nonsorption of tco4− onto bacteria was confirmed in biosorption experiments with washed cells of p. aeruginosa regardless of the ph. at the end of denitrification with indigenous denitrifiers in soil/water batch experiments, the redox potential (eh) dropped and this was accompanied by an increase of fe concentration in solution as a result of reduction of less soluble fe(iii) to fe(ii) from the soil particles. it is suggested that this is due to the growth of a consortium of anaerobic bacteria (e.g., fe-reducing bacteria). the drop in eh was accompanied by a strong decrease in tc concentration as a result of tc(vii) reduction to tc(iv). thermodynamic calculations suggested the precipitation of tco2. the stimulation of the growth of indigenous sulphate-reducing bacteria in soil/water systems led to even lower eh with final tc concentration of 10-8 m. experiments with glass columns filled with soil reproduced the results obtained with batch cultures. sequential chemical extraction of precipitated tc in soils showed that this radionuclide is strongly immobilised within soil particles under anaerobic conditions. more than 90% of tc is released together with organic matter (60–66%) and fe-oxyhydroxides (23–31%). the present work shows that ubiquitous indigenous anaerobic bacteria in soils play a major role in tc immobilisation. in addition, organic matter plays a key role in the stability of the reduced tc.
spent fuel evolution under disposal conditions. synthesis of the results from the eu spent fuel stability (sfs) project. null
the effect of dissolved hydrogen on the dissolution of 233u doped uo2(s), high burn-up spent fuel and mox fuel. null
effect of alpha irradiation field on long-term corrosion rates of spent fuel. null
spent fuel performance under repository conditions: a model for use in sr-can. null
the role of water diffusion in the corrosion of the french nuclear waste glass son 68 under solution saturation conditions. null
effect of gamma and alpha irradiation on the corrosion of the french borosilicate glass son 68. null
effect of alpha radiolysis on uo2 dissolution under reducing conditions. null
a proposal for a high performance $\gamma$-camera based on liquid xenon converter and gaseous photomultiplier for pet. null
hydrodynamic source with continuous emission in au+au collisions at $\sqrt{s}=200$ gev. we analyze single particle momentum spectra and interferometry radii in central au+au collisions at rhic within a hydro-inspired parametrization accounting for continuous hadron emission through the whole lifetime of hydrodynamically expanding fireball. we found that a satisfactory description of the data is achieved for a physically reasonable set of parameters when the emission from non space-like sectors of the enclosed freeze-out hypersurface is fairly long: $ 9$ fm/c. this protracted surface emission is compensated in outward interferometry radii by positive $r_{out} - t$ correlations that are the result of an intensive transverse expansion. the main features of the experimental data are reproduced: in particular, the obtained ratio of the outward to sideward interferometry radii is less than unity and decreases with increasing transverse momenta of pion pairs. the extracted value of the temperature of emission from the surface of hydrodynamic tube approximately coincides with one found at chemical freeze-out in rhic au+au collisions. a significant contribution of the surface emission to the spectra and to the correlation functions at relatively large transverse momenta should be taken into account in advanced hydrodynamic models of ultrarelativistic nucleus-nucleus collisions.
complexation studies of eu(iii) with alumina-bound polymaleic acid: effect of organic polymer loading and metal ion concentration. to contribute to the comprehension of the metal ion sorption properties in mixed mineral-organic matter systems, interaction studies between eu(iii) and polymaleic acid (pma)-coated alumina colloids were carried out at ph 5 in 0.1 m naclo4. the studied parameters were the metal ion concentration (1 × 10-10 to 1 × 10-4 m) and pma loading on alumina ( = 10-70 mg/g). the data were described by a surface complexation model. the constant capacitance model was used to account for electrostatic interactions. the results showed that two sites were necessary to explain the sorption data. at high eu loading, eu is surrounded by one carboxylate group and one aluminol group. the existence of this ternary surface site was in agreement with time-resolved laser-induced fluorescence spectroscopy measurements. at low metal ion concentra tions, a surface site corresponding to a 1:1 eu/coo- stoichiometry was deduced from modeling. spectroscopic data did not corroborate the existence of this latter site. this discrepancy was explained by postadsorption kinetic phenomenon: a migration of the metal ion within the adsorbed organic layer.
xas study of technetium(iv) polymer formation in mixed sulphate/chloride media. x-ray absorption spectroscopy has been used to establish polymer formation of tc(iv) in aqueous solutions of na+so42- and na+cl-/so42-. as the molybdenum chemistry show similarities to that of technetium, we used moo2 as a reference to model our technetium species. fitting of tco2·xh2o with this model led to a good correlation with the literature data: (tc-tc=2.53 å, tc-o=1.87-1.98 å). in aqueous solution, some polymers are formed regardless to the nature of the media composition: tc-tc=2.50±0.02 å. the general structure is in agreement with a first coordination shell containing 6 o. the modelling shows that, in the first coordination shell, there is no chloride ligand. the observed geometries are close to those found for tco2·xh2o, hence the unknown aqueous species must be considered as a precursor of the solid technetium dioxide. combination of these results with xanes led to attribute tcnivop(4n-2p)+(h2o)q with n&gt;2 to the species.
supersymmetric beta function with ultraviolet chop-off. null
born--oppenheimer corrections to the effective zero-mode hamiltonian in sym theory. we calculate the subleading terms in the born--oppenheimer expansion for the effective zero-mode hamiltonian of n = 1, d=4 supersymmetric yang--mills theory with any gauge group. the hamiltonian depends on 3r abelian gauge potentials a_i, lying in the cartan subalgebra, and their superpartners (r being the rank of the group). the hamiltonian belongs to the class of n = 2 supersymmetric qm hamiltonia constructed earlier by ivanov and i. its bosonic part describes the motion over the 3r--dimensional manifold with a special metric. the corrections explode when the root forms \alpha_j(a_i) vanish and the born--oppenheimer approximation breaks down.
quasiclassical expansion for $tr{(-1)^f ^{-\beta h}}$. we start with some methodic remarks referring to purely bosonic quantum systems and then explain how corrections to the leading--order quasiclassical result for the fermion--graded partition function tr{(-1)^f exp(-\beta h)} can be calculated at small \beta. we perform such calculation for certain supersymmetric quantum mechanical systems where such corrections are expected to appear. we consider in particular supersymmetric yang-mills theory reduced to (0+1) dimensions and were surprised to find that the correction of order \beta^2 vanishes in this case. we discuss also a nonstandard n =2 supersymmetric sigma model defined on s^3 and other 3--dimensional conformally flat manifolds and show that the quasiclassical expansion breaks down for this system.
complexation studies of eu(iii) with polyacrylic acid either free in solution or adsorbed onto alumina. the complexation of eu(iii) with polyacrylic acid (paa) and alumina-bound polyacrylic acid (paa-al2o3) was studied at ph 5 in 0.1 m naclo4. the experiments were carried out for two polyelectrolytes at 5000 da and 50000 da. the quantitative description of the interaction of eu and paa showed the formation of a 1:1 eu:paa complex and the existence of similar binding modes ("sites") of eu with both polymers. interaction studies of eu(iii) with alumina-bound paa particles were carried out in conditions where their overall surface charge were equal to 0. the description of the data indicated the existence of a similar site in binary and ternary systems for high eu loading. for low eu loading, the existence of a new site on paa-al2o3 particles resulting from the paa-al2o3 interaction was suggested.
scandal - a facility for elastic neutron scattering studies in the 50-130 mev range. null
measurement of the $\lambda$ and $\bar\lambda$ particles in au+au collisions at $\sqrt{s_{nn}}$=130 gev. we present results on the measurement of lambda and lambda^bar production in au+au collisions at sqrt(s_nn)=130 gev with the phenix detector at rhic. the transverse momentum spectra were measured for minimum bias and for the 5% most central events. the lambda^bar/lambda ratios are constant as a function of p_t and the number of participants. the measured net lambda density is significantly larger than predicted by models based on hadronic strings (e.g. hijing) but in approximate agreement with models which include the gluon junction mechanism.
flow measurements via two-particle azimuthal correlations in au + au collisions at $\sqrt{s_{nn}}$ = 130 gev. two particle azimuthal correlation functions are presented for charged hadrons produced in au + au collisions at rhic sqrt(s_nn) = 130 gev. the measurements permit determination of elliptic flow without event-by-event estimation of the reaction plane. the extracted elliptic flow values v_2 show significant sensitivity to both the collision centrality and the transverse momenta of emitted hadrons, suggesting rapid thermalization and relatively strong velocity fields. when scaled by the eccentricity of the collision zone, epsilon, the scaled elliptic flow shows little or no dependence on centrality for charged hadrons with relatively low p_t. a breakdown of this epsilon scaling is observed for charged hadrons with p_t &gt; 1.0 gev/c for the most central collisions.
an insulating grid spacer for large-area micromegas chambers. we present an original design for large area gaseous detectors based on the micromegas technology. this technology incorporates an insulating grid, sandwiched between the micro-mesh and the anode-pad plane, which provides an uniform 200 $\mu$m amplification gap. the uniformity of the amplification gap thickness has been verified under several experimental conditions. the gain performances of the detector are presented and compared to the values obtained with detectors using cylindrical micro spacers. the new design presents several technical and financial advantages.
one-, two- and three-particle distributions from 158 a gev/c central pb+pb collisions. several hadronic observables have been studied in central 158 a gev pb+pb collisions using data measured by the wa98 experiment at cern: single negative pion and kaon production, as well as two- and three-pion interferometry. the wiedemann-heinz hydrodynamical model has been fitted to the pion spectrum, giving an estimate of the temperature and transverse flow velocity. bose-einstein correlations between two identified negative pions have been analysed as a function of kt, using two different parameterizations. the results indicate that the source does not have a strictly boost invariant expansion or spend time in a long-lived intermediate phase. a comparison between data and a hydrodynamical based simulation shows very good agreement for the radii parameters as a function of kt. the pion phase-space density at freeze-out has been measured and agrees well with the tomasik-heinz model. a large pion chemical potential close to the condensation limit of the pion mass seems to be excluded. the three-pion bose-einstein interferometry shows a substantial contribution of the genuine three-pion correlation, but not quite as large as expected for a fully chaotic and symmetric source.
effective lagrangian for 3d n=4 sym theories for any gauge group and monopole moduli spaces. we construct low energy effective lagrangians for 3d n=4 supersymmetric yang-mills theory with any gauge group. they represent supersymmetric sigma models at hyper-kahlerian manifolds of dimension 4r (r is the rang of the group). in the asymptotic region, perturbatively exact explicit expression for the metric are written. we establish the relationship of this metric with the taub-nut metric describing the perturbatively exact effective lagrangians for unitary groups and monopole moduli spaces: the former is obtained out of the latter by a proper hyper-kahlerian reduction. we describe in details the reduction procedure for so/sp/g_2 gauge groups, where it can also be given a natural interpretation in d-brane language. we conjecture that the exact nonperturbative metrics can be obtained by a similar hyper-kahlerian reduction from the corresponding multidimensional atiyah-hitchin metrics.
quantum gravity as escher's dragon. the main obstacle in attempts to construct a consistent quantum gravity is the absence of independent flat time. this can in principle be cured by going out to higher dimensions. the modern paradigm assumes that the fundamental theory of everything is some form of string theory living in space of more than four dimensions. we advocate another possibility that the fundamental theory is a form of d=4 higher-derivative gravity. this class of theories has a nice feature of renormalizability so that perturbative calculations are feasible. there are also finite n=4 supersymmetric conformal supergravity theories. this possibility is particularly attractive. einstein's gravity is obtained in a natural way as an effective low-energy theory. the n=1 supersymmetric version of the theory has a natural higher-dimensional interpretation due to ogievetsky and sokatchev, which involves embedding of our curved minkowsky space-time manifold into flat 8-dimensional space. assuming that a variant of the finite n=4 theory also admit a similar interpretation, this may eventually allow one to construct consistent quantum theory of gravity. we argue, however, that even though future gravity theory will probably use higher dimensions as construction scaffolds, its physical content and meaning should refer to 4 dimensions where observer lives.
effective lagrangians for (0+1) and (1+1) dimensionally reduced versions of d=4 n=2 sym theory. we consider dimensionally reduced versions of n=2 four- dimensional supersymmetric yang-mills theory and determine the one-loop effective lagrangians associated with the motion over the corresponding moduli spaces. in the (0+1) case, the effective lagrangian describes an n=4 supersymmetric quantum mechanics of the diaconescu--entin type. in (1+1) dimensions, the effective lagrangian represents a twisted n=4 supesymmetric sigma model due to gates, hull, and rocek. we discuss the genetic relationship between these two models and present the explicit results for all gauge groups.
abelian matrix models in two loops. we perform a two-loop calculation of the effective lagrangian for the low--energy modes of the quantum mechanical system obtained by dimensional reduction from 4d, n = 1 supersymmetric qed. the bosonic part of the lagrangian describes the motion over moduli space of vector potentials a_i endowed with a nontrivial conformally flat metric. we determined the coefficient of the two-loop correction to the metric, which is proportional to 1/a^6. for the matrix model obtained from abelian 4d, n = 2 theory, this correction vanishes, as it should.
novel phosphate–phosphonate hybrid nanomaterials applied to biology. a new process for preparing oligonucleotide arrays is described that uses surface grafting chemistry which is fundamentally different from the electrostatic adsorption and organic covalent binding methods normally employed. solid supports are modified with a mixed organic/inorganic zirconium phosphonate monolayer film providing a stable, well-defined interface. oligonucleotide probes terminated with phosphate are spotted directly to the zirconated surface forming a covalent linkage. specific binding of terminal phosphate groups with minimal binding of the internal phosphate diesters has been demonstrated. on the other hand, the reaction of a bisphosphonate bone resorption inhibitor (zoledronate) with calcium deficient apatites (cdas) was studied as a potential route to local drug delivery systems active against bone resorption disorders. a simple mathematical model of the zoledronate/cda interaction was designed that correctly described the adsorption of zoledronate onto cdas. the resulting zoledronate-loaded materials were found to release the drug in different phosphate-containing media, with a satisfactory agreement between experimental data and the values predicted from the model.
a new experimental design to investigate the concentration dependent diffusion of eu(iii) in compacted bentonite. the 'lsquoin-diffusionrsquo method was used to study the diffusion behavior of eu(iii) in compacted bentonite. the results (k d, apparent and effective diffusion coefficients) derived from the capillary method are in good agreement with the literature data for similar bentonite dry densities and similar radionuclide concentrations, and fits the fick's second law very well. the method was used to study the effect of solution concentration (10-7-10-3 mol/l) on the diffusion and sorption behavior in compacted bentonite. the experiments were carried out in synthetic groundwater, at room temperature. the results suggested that the diffusion of eu(iii) in the bentonite was independent on the density of bentonite, but dependent on the solution concentration. in agreement to the literature, the k d values from the capillary experiments are in most cases lower than those from batch experiments, they are about one-half to one-third the value to those from batch experiments. the difference between the k dvalues from capillary and batch experiments are a strong function of the bulk density of the bentonite. the results suggest that the content of interlaminary space plays a very important role to the transport of eu(iii) in compacted bentonite.
study of the reversibility of the interaction between eu(iii) and polyacrylic acids. humic substances are known to be a potential vector for the transport of trivalent metal ions in the environment due to non-reversible interactions. the goal of the present paper is to understand the origin of this non-reversibility using polyacrylic acid, paa, as model substance and eu(iii). the reversibilty of the interaction between eu(iii) and two polyacrylic acids (5000 and 50000 da) is investigated by comparing the interaction constants deduced from complexation and dissociation experiments in the presence of the chelex resin. the work is done at ph=5 in 0.1 mol l–1 naclo4. fluorescence spectroscopy is used as a speciation method to characterize the mode of interaction between eu(iii) and paa functional groups. the results show that eu(iii), in a first step, interacts rapidly with polyacrylic acid functional groups, probably by electrostatic forces. this process is followed by a rearrangement of the polymer chain to form an inner sphere site where eu(iii) is probably coordinated with three carboxylate groups. the interaction between eu(iii) and polyacrylic acid is shown to be reversible: the parameters deduced from complexation and dissociation experiments are identical. this result differs from that previously reported for the model system composed of synthetic polycarboxylic acids and th(iv). this discrepancy is explained by the difference in the nature of the metal ion studied and/or by the difference in the range of ligand-to-metal ratios investigated. the difference in complexation behavior of eu(iii) with natural and synthetic polyelectrolytes is ascribed to their different structures.
study of the interaction between europium (iii) and bacillus subtilis: fixation sites, biosorption modeling and reversibility. in order to elucidate the underlying mechanisms involved in the biosorption of metal ions, potentiometric titrations, complexation studies, and time-resolved laser-induced fluorescence spectroscopy (trlfs) measurements were used to characterize the interaction between eu(iii) and bacillus subtilis. the reversibility of the interaction between eu(iii) and bacillus subtilis was studied by a cation-exchange technique using the chelex resin. for complexation studies in the presence of 0.15 mol/l of nacl, the metal ion, the biomass, concentrations and the ph were varied. the adsorption data were quantified by a surface complexation model without electrostatic term. the data on the eu(iii)/b.subtilis system at ph 5 were satisfactorily described by one site at which eu(iii) was bound through one carboxylic function of the bacteria. with increasing ph, another site should be considered, involving a phosphate-bound environment. this was partially confirmed by time-resolved laser-induced fluorescence spectroscopy. in addition to this, it was evidenced that the site availability was dependent on the nature of the cation, i.e., a proton or eu(iii). finally, it was shown that, at ph 5, the eu(iii)/bacillus subtilis equilibrium was reversible.
alien—alice environment on the grid. alien (http://alien.cern.ch) (alice environment) is a grid framework built on top of the latest internet standards for information exchange and authentication (soap, pki) and common open source components. alien provides a virtual file catalogue that allows transparent access to distributed datasets and a number of collaborating web services which implement the authentication, job execution, file transport, performance monitor and event logging. in the paper we will present the architecture and components of the system.
background field calculations and nonrenormalization theorems in 4d supersymmetric gauge theories and their low-dimensional descendants. we analyze the structure of multiloop supergraphs contributing to the effective lagrangians in 4d supersymmetric gauge theories and in the models obtained from them by dimensional reduction. when d=4, this gives the renormalization of the effective charge. for d &lt; 4, the low-energy effective lagrangian describes the metric on the moduli space of classical vacua. these two problems turn out to be closely related. in particular, we establish the relationship between the 4d nonrenormalization theorems (in minimal and extended supersymmetric theories) and their low--dimensional counterparts.
low-dimensional sisters of seiberg-witten effective theory. we consider the theories obtained by dimensional reduction to d=1,2,3 of 4d supersymmetric yang--mills theories and calculate there the effective low-energy lagrangia describing moduli space dynamics -- the low-dimensional analogs of the seiberg--witten effective lagrangian. the effective theories thus obtained are rather beautiful and interesting from mathematical viewpoint. in addition, their study allows one to understand better some essential features of 4d supersymmetric theories, in particular -- the nonrenormalisation theorems.
symplectic sigma models in superspace. we discuss a special "symplectic'' class of n = 4 supersymmetric sigma models in (0+1) dimension with 5r bosonic and 4r complex fermionic degrees of freedom. these models can be described off shell by n = 2 superfields (so that only half of supersymmetries are manifest) and also by n = 4 superfields in the framework of the harmonic superspace approach. using the latter, we derive the general form of the relevant bosonic target metric.
real-time tracking of time-varying velocity using a self-mixing laser diode. a new method is proposed for estimating the time-varying velocity of a moving target with a low-cost laser sensor using optical feedback interferometry. a new algorithm is proposed to track velocity variations from real-time analysis of the output signal of a self-mixing laser diode. this signal is strongly corrupted by a multiplicative noise caused by the speckle effect, which occurs very often with noncooperative targets used in many industrial applications. the proposed signal processing method is based on a second order adaptive linear predictor filter, which enables us to track the digital instantaneous doppler frequency, which is proportional to the velocity. a model of the laser diode output signal is proposed, and it is shown that the sensor and its associated algorithm have a global first-order lowpass transfer function with a cutoff frequency expressed as a function of the speckle perturbations, the signal to noise ratio and the mean doppler frequency. numerical as well as experimental results illustrate the properties of this sensor.
a double-laser diode onboard sensor for velocity measurements. in this paper, we validate the feasibility of an onboard velocity sensor using the self-mixing effect. roughness of the target surface, wet target surface, noncontrolled changes of incident angle, and speed vector vertical components have been considered during this development. a first prototype has been designed with an automotive application so to illustrate this feasibility. this low-cost and low-clutter prototype presents an interesting basic performance (/spl sigma//spl ap/0.22%). in order to improve the accuracy as well as the robustness of the system, a double-laser diode sensor has then been tested successfully (/spl sigma//spl ap/0.038%) by removing the influence of the pitching and the pumping effects.
interaction of eu(iii)/cm(iii) with alumina-bound poly(acrylic acid): sorption, desorption, and spectroscopic studies. this paper contributes to the comprehension of kinetic and equilibrium phenomena governing trace metal ion sorption on organic matter coated mineral particles. sorption and desorption experiments were carried out with trivalent metal ions m(iii) (m = eu, cm) and poly(acrylic acid) (paa)-coated alumina colloids at ph 5 in 0.1 m naclo4. under these conditions, m(iii) interaction with the solid is governed by sorbed paa. the results were compared with spectroscopic data obtained by time-resolved laser-induced fluorescence spectroscopy (trlfs). within less than 30 s, a state of local equilibrium is reached between m(iii) and adsorbed poly(acrylic acid). m(iii) bound to the organic-mineral surface and to dissolved paa keeps five water molecules in its first hydration sphere. interaction of m(iii) with alumina-bound paa appears to be stronger than with dissolved paa. with increasing contact time, a change of the metal ion speciation occurs at the organic-mineral surface. this change is explained quantitatively by kinetically controlled reactions, which succeed a rapid local equilibrium. the experimental findings suggest, in agreement with model calculations, that a part of the initially sorbed m(iii) is slowly converted to a kinetically stabilized species, thereby losing water molecules from the first coordination sphere as indicated by trlfs. this species might be assigned as a ternary al2o3-m(iii)-paa complex. the second part of the initially bound m(iii) appears to experience as well kinetically controlled reactions, however, without showing changes in the first coordination sphere. we assume that the kinetic stabilization is the consequence of rearrangement processes of the paa at the alumina surface.
electrochemical aspects of radiolytically enhanced uo$_2$ dissolution. experiments were performed, irradiating uo2 colloids of 3 nm diameter by a 5 mev alpha beam of a cyclotron. both the dissolution rate of these colloids and the production rate of hydrogen peroxide have been measured. the experimentally measured corrosion rate of these colloids is very similar to the corrosion rates measured for bulk uo2 samples, indicating on the one hand that similar reaction mechanism prevail and on the other hand that the uo2 colloids are well representative for bulk uo2. the results were compared with two current models for radiolytical dissolution of spent fuel. additionally a new coupling between water radiolysis reactions and spent fuel dissolution has been established. the coupling is not anymore based on rate constants for direct reaction of oxidants with uo2, but on the electrochemical coupling of anodic dissolution reactions with cathodic reduction of molecular and radical radiolysis products. the model not only describes well the experimental data, it also allows to predict both the establishment of corrosion potentials and of fractional reaction orders with respect to oxidant concentrations. model results can also be compared with experimental data (corrosion potential ...) using uo2 as an electrode.
measurement of nonrandom event-by-event fluctuations of average transverse momentum in $\sqrt{s_{nn}}$=200 gev au+au and p+p collisions. event-by-event fluctuations of the average transverse momentum of produced particles near midrapidity have been measured by the phenix collaboration in sqrt(s[sub nn])=200 gev au+au, and p+p collisions at the relativistic heavy ion collider. the fluctuations are observed to be in excess of the expectation for statistically independent particle emission for all centralities. the excess fluctuations exhibit a dependence on both the centrality of the collision and on the pt range over which the average is calculated. both the centrality and pt dependence can be well reproduced by a simulation of random particle production with the addition of contributions from hard-scattering processes.
simulation of a high performance $\gamma$-camera concept for pet based on liquid xenon and gaseous photomultiplier. we present the results of geant3 simulations of a full pet system made of liquid xenon (lxe)-tpc /spl gamma/-camera modules. in such camera both ionization and scintillation signals will be detected to provide the three coordinates and the energy of the converted /spl gamma/-ray. for that purpose, we will develop advanced ionization detectors operating in liquid xenon as well as fast cryogenic gaseous photomultipliers (gpms) with csi photocathodes, dedicated to the detection of the lxe scintillation signal. the measurement of the conversion coordinates and the energy will allow to reconstruct the correct compton sequences and thus to identify the first interaction vertices of both correlated annihilation /spl gamma/-rays. moreover, measurement of the depth of interaction (doi) will lead to a parallax-free tomograph. the geant3 simulation code of the lxe-tpc pet provided very promising expected performances from a realistic detector: good sensitivity to 511 kev /spl gamma/-rays (-93% for a 9 cm depth lxe module) and good 3d spatial resolution (250 /spl mu/m fwhm for first interaction vertex localization); the latter is close to the intrinsic physical limitations of the method.
high performance $\gamma$-camera concept for pet based on liquid xenon and a gaseous photomultiplier. the lxe-pet project aims at initiating an international scientific cooperation to study a new generation of γ-cameras for positron emission tomography (pet). our approach consists of coupling a liquid xe converter with an advanced large-area, fast gas-avalanche imaging photomultiplier (gpm). the foreseen method is based on detecting both the ionization and xe scintillation signals, to provide the three coordinates and the energy of the converted γ. for that purpose, we will develop ionization detectors based on micromegas or pim (parallel ionization multiplier) operating in liquid xenon and cryogenic gpms with csi photocathodes for the detection of the scintillation signal; these will include: multi-gems (gas electron multipliers), thick gem-like multipliers (thgem), pim, glass capillary plates and wire-type. expected characteristics of such γ-camera will be presented.
simulation and evaluation of a new pet system based on liquid xenon as detection medium. due to its intrinsic physical properties, high density and atomic number, fast scintillation, high scintillation light yield and low ionization potential, liquid xenon is an excellent medium for the tracking and the accurate energy measurement of γ-rays in the mev energy domain. the use of liquid xenon associated to a micro gap structure device[1] to measure 511 kev γ-rays in pet system is under investigation at subatech. a geant3 simulation of a full pet design made of lxe-tpc modules has been developed and the first estimations of the performances from a realistic detector are very promising: good overall sensitivity to 511 kev γ's (~ 93% for a 9 cm lxe module), good three-dimensional spatial resolution (250 µm fwhm, for first interaction vertex localization). the measurement of the 3 coordinates of the interaction vertices and the energy loss associated allow to reconstruct the correct compton sequence of correlated annihilation γ-rays. hence the capability to identify the first interaction vertex leads to major progresses in pet imaging: a parallax free pet tomograph with high detection sensitivity and spatial resolution. moreover, such lxe-pet camera have an excellent rejection power on scattered events in 3d reconstruction mode.
polymer-supported organotin reagents for regioselective halogenation of aromatic amines. polymer-supported triorganotin halides were used in the halogenation reaction of aromatic amines. treatment of aromatic amines with n-butyllithium and polymer-supported organotin halides gave the corresponding polymer-bound n-triorganostannylamines, which by treatment with bromine or iodine monochloride gave the para-halogenated aromatic amines with high yields and high selectivities. the polymer-supported organotin halides reagents regenerated during the course of the halogenation reaction can be reused without loss of efficiency. the presence of tin residues in halogenated aromatic amines was also investigated and evaluated at under 20 ppm after three runs.
a central back-coupling hypothesis on the organization of motor synergies: a physical metaphor and a neural model. we offer a hypothesis on the organization of multi-effector motor synergies and illustrate it with the task of force production with a set of fingers. a physical metaphor, a leaking bucket, is analyzed to demonstrate that an inanimate structure can show apparent error compensation among its elements. a neural model is developed using tunable back-coupling loops as means of assuring error compensation in a task-specific way. the model demonstrates non-trivial features of multi-finger interaction such as delayed emergence of force stabilizing synergies and simultaneous stabilization of the total force and total moment produced by the fingers. the hypothesis suggests that neurophysiological structures involving short-latency feedback may play a central role in the formation of motor synergies.
a high resolution electromagnetic calorimeter based on lead-tungstate crystals. a large-scale prototype of the phos electromagnetic spectrometer, which is part of the alice detector, has been built and tested. this prototype has 256 detector channels and is operated at −25 °c. each detector channel is a lead-tungstate crystal coupled to an avalanche photo-diode with a low-noise preamplifier. the prototype includes a 16×16 crystal matrix, photo-detectors, analog and digital electronics, a thermo-stabilized cooling system, a light-emitting diode monitoring system, and a charged-particle detector acting as veto counter. results of measurements using electron and hadron beams of the cern ps and sps accelerators are discussed, and the performance of the prototype is evaluated.
proton - lambda correlations in au-au collisions at $\sqrt{s_{nn}} = 200$ gev from the star experiment. the space-time evolution of the source of particles formed in the collision of nuclei can be studied through particle correlations. the star experiment is dedicated to study ultra-relativistic heavy ions collisions and allows to measure non-identical strange particle correlations. the source size can be extracted by studying p–λ, $\overline{\rm p}$–$\overline{\lambda}$, $\overline{\rm p}$–$\lambda$ and p–$\overline{\lambda}$ correlation functions. strong interaction potential has been studied for these systems using an analytical model. final state interaction (fsi) parameters have been determined and has shown a significant annihilation process present in $\overline{\rm p}$–$\lambda$ and p–$\overline{\lambda}$ systems not present in p–$\lambda$ and $\overline{\rm p}$–$\overline{\lambda}$.
on the multiple-humped fission barriers and half-lives of actinides. the energy of actinide nuclei has been determined within a generalized liquid drop model taking into account the proximity energy, the mass and charge asymmetry, an accurate nuclear radius in adding the shell and pairing energies. double and triple-humped potential barriers appear. the second maximum corresponds to the transition from compact and creviced one-body shapes to two touching ellipsoids. a third minimum and third peak appear in special asymmetric exit channels where one fragment is almost a magic nucleus with a quasi-spherical shape while the other one evolves from oblate to prolate shapes. the heights of the double and triple-humped fission barriers agree precisely with the experimental results in all the actinide region. the predicted half-lives follow the experimental data trend.
conformal properties of hypermultiplet actions in six dimensions. we consider scale-invariant interactions of 6d n=1 hypermultiplets with the gauge multiplet. if the canonical dimension of the matter scalar field is assumed to be 1, scale-invariant lagrangians involve higher derivatives in the action. though scale-invariant, all such lagrangians are not invariant with respect to special conformal transformations and their superpartners. if the scalar canonical dimension is assumed to be 2, conformal invariance holds at the classical, but not at the quantum level.
description of quantum entanglement with nilpotent polynomials. we propose a general method for introducing extensive characteristics of quantum entanglement. the method relies on polynomials of nilpotent raising operators that create entangled states acting on a reference vacuum state. by introducing the notion of tanglemeter, the logarithm of the state vector represented in a special canonical form and expressed via polynomials of nilpotent variables, we show how this description provides a simple criterion for entanglement as well as a universal method for constructing the invariants characterizing entanglement. we compare the existing measures and classes of entanglement with those emerging from our approach. we derive the equation of motion for the tanglemeter and, in representative examples of up to four-qubit systems, show how the known classes appear in a natural way within our framework. we extend our approach to qutrits and higher-dimensional systems, and make contact with the recently introduced idea of generalized entanglement. possible future developments and applications of the method are discussed.
6d superconformal theory as the theory of everything. we argue that the fundamental theory of everything is a conventional field theory defined in the flat multidimensional bulk. our universe should be obtained as a 3-brane classical solution in this theory. the renormalizability of the fundamental theory implies that it involves higher derivatives (hd). it should be supersymmetric (otherwise one cannot get rid of the huge induced cosmological term) and probably conformal (otherwise one can hardly cope with the problem of ghosts) . we present arguments that in conformal hd theories the ghosts (which are inherent for hd theories) might be not so malignant. in particular, we present a nontrivial qm hd model where ghosts are absent and the spectrum has a well defined ground state. the requirement of superconformal invariance restricts the dimension of the bulk to be d &lt; 7. we suggest that the toe lives in six dimensions and enjoys the maximum n = (2,0) superconformal symmetry. unfortunately, no renormalizable field theory with this symmetry is presently known. we construct and discuss an n = (1,0) 6d supersymmetric gauge theory with four derivatives in the action. this theory involves a dimensionless coupling constant and is renormalizable. at the tree level, the theory enjoys conformal symmetry, but the latter is broken by quantum anomaly. the sign of the beta function corresponds to the landau zero situation.
benign vs malicious ghosts in higher-derivative theories. interacting theories with higher derivatives involve ghosts. they correspond to instabilities that display themselves at the classical level. we notice that comparatively "benign" mechanical higher-derivative systems exist where the classical vacuum is stable with respect to small perturbations and the problems appear only at the nonperturbative level. we argue that benign higher-derivative field theories exist which are stable with respect to small fluctuations with nonzero momenta. a particular example is the 6d n=2 higher-derivative sym theory, which is finite and unitary at the perturbative level. the inflation-like instability with respect to small fluctuations of static modes is always present, however.
pentaquarks: review of the experimental evidence. pentaquarks, namely baryons made by 4 quarks and one antiquark have been predicted and searched for since several decades without success. theoretical and experimental advances in the last 2 years led to the observation of a number of pentaquark candidates. we review the experimental evidence for pentaquarks as well as their non-observations by some experiments, and discuss to which extend these sometimes contradicting informations may lead to a consistent picture.
review of the experimental evidence on pentaquarks and critical discussion. we review and discuss the experimental evidence on predicted baryonic states made by 4 quarks and one antiquark, called pentaquarks. theoretical and experimental advances in the last few years led to the observation of pentaquark candidates by some experiments, however with relatively low individual significance. other experiments did not observed those candidates. furthermore, the masses of the $\theta^+$(1540) candidates exhibit a large variation in different measurements. we discuss to which extend these contradicting informations may lead to a consistent picture.
performance of the alice photon spectrometer phos. we present in this paper the measured characteristics of a 64 lead–tungstate crystal array designed to detect high-energy photons and neutral mesons with the alice photon spectrometer phos. the array has been tested with electron and charged pion secondary beams delivered by the cern ps and sps synchrotrons. photon energy and π0 invariant mass resolutions are presented. the phos particle identification performance for data simulated with the aliroot package is studied.
applications of optimisation with xpress-mp. null
origin of fragments in multifragmentation reactions. null
entrance and exit channels for the heaviest elements. null
on the double and triple-humped fission barriers and half-lives of actinide elements. the deformation barriers standing in the quasi-molecular shape path have been determined in the actinide region within a macroscopic-microscopic energy derived from a generalized liquid drop model, the algebraic droplet model shell corrections and analytic expressions for the pairing energies. double and triple-humped fission barriers appear. the second barrier corresponds to the transition from one-body shapes to two touching ellipsoids. the third minimum and third peak, when they exist, come from shell rearrangements in the deformed fragment. the shape of the other almost magic one is close to the sphere. the barrier heights agreewith the experimental results, the energy of the second minimum being a little too high. the predicted half-lives follow the experimental data trend.
potential barriers in the fusionlike deformation path. null
improved measurement of double helicity asymmetry in inclusive midrapidity $\pi^0$ production for polarized p+p collisions at $\sqrt s$=200 gev. we present an improved measurement of the double helicity asymmetry for $\pi^0$ production in polarized proton-proton scattering at $\sqrt s$= 200 gev employing the phenix detector at the relativistic heavy ion collider (rhic). the improvements to our previous measurement come from two main factors: inclusion of a new data set from the 2004 rhic run with higher beam polarizations than the earlier run and a recalibration of the beam polarization measurements, which resulted in reduced uncertainties and increased beam polarizations. the results are compared to a next to leading order (nlo) perturbative quantum chromodynamics (pqcd) calculation with a range of polarized gluon distributions.
ghost-free higher-derivative theory. we present an example of the quantum system with higher derivatives in the lagrangian, which is ghost-free: the spectrum of the hamiltonian is bounded from below and unitarity is preserved.
hadronic matter is soft. the stiffness of the hadronic equation of state has been extracted from the production rate of $k^+$ mesons in heavy ion collisions around 1 $a$ gev incident energy. the data are best described with a compressibility coefficient $\kappa$ around 200 mev, a value which is usually called ``soft''. this is concluded from a detailed comparison of the results of transport theories with the experimental data using two different procedures: (i) the energy dependence of the ratio of $k^+$ from au+au and c+c collisions and (ii) the centrality dependence of the $k^+$ multiplicities. it is demonstrated that input quantities of these transport theories which are not precisely known, like the kaon-nucleon potential, the $\delta n \to n k^+ \lambda$ cross section or the life time of the $\delta$ in matter do not modify this conclusion.
lisor, a liquid metal loop for material investigation under irradiation. lisor is a liquid metal loop that will use psi's 72 mev philips cyclotron to irradiate stressed steel specimens in contact with flowing lead-bismuth with 50µa proton beam. lisor is a joint effort of psi and subatech with the support from cnrs and the institute of physics from riga. it has been initiated to explore whether or not liquid metal corrosion and liquid metal embrittlement are enhanced under irradiation in the presence of stress. numerical simulations showed that the damage levels and gas production in thin specimens induced by 72 mev protons are, within reasonable limits, comparable to those on the inside of the beam window of a spallation target at 600 mev, while much less radioactivity is produced. the paper describes the basic features of the experiment, the technical concept of the liquid metal loop with a special emphasis on the test section exposed to the proton beam and some of the relevant safety aspects.
common suppression pattern of $\eta$ and $\pi^0$ mesons at high transverse momentum in au+au collisions at $\sqrt{s_nn}$ = 200 gev. inclusive transverse momentum spectra of eta mesons have been measured within p_t = 2-10 gev/c at mid-rapidity by the phenix experiment in au+au collisions at sqrt(s_nn) = 200 gev. in central au+au the eta yields are significantly suppressed compared to peripheral au+au, d+au and p+p yields scaled by the corresponding number of nucleon-nucleon collisions. the magnitude, centrality and p_t dependence of the suppression is common, within errors, for eta and pi^0. the ratio of eta to pi^0 spectra at high p_t amounts to 0.40 &lt; r_eta/pi^0 &lt; 0.48 for the three systems in agreement with the world average measured in hadronic and nuclear reactions and, at large scaled momentum, in e^+e^- collisions.
tensile tests on manet ii steel in circulating pb-bi eutectic. off-beam tensile test have been performed on manet ii steel in eutectic pb-55.5bi (lbe) and ar during commissioning of the lisor loop, an experimental liquid metal loop, which was developed to investigate the influence of pb-bi on possible structural materials under static load and irradiationn. test temperatures were 180 - 300 °c. manet ii (11% crmovnb steel) exhibits good swelling and creep resistance behaviour under irradiationn up to around 500 °c. good corrosion resistance of this material is expected due to the asence of the element ni in the steel matrix which has a high solubility in lbe. all specimens showed a ductile fracture in ar. in lbe a loss of ductility was observed at the test temperature of 250 and 300 °c in comparison to ar samples. sem analysis of the fracture surface of these specimens revealed a mixed mode, i.e. dimple and brittle fracture and penetration of pb-bi along the grain boundaries, which is a typical finding for liquid metal embrittlement.
meson production in the $\vecp + d \to ^3\rm he + x$ reaction at spes 3 at e$_\rm inc$= 1450 mev. selected results of the meson production study in the vect(p)+d --&gt; 3he+x reaction realized with the spes 3 spectrometer at saturne will be presented and related to a semiphenomenological three nucleon model.
medium effects on high $\p_{t}$ particle production measured with the phenix experiment. transverse momentum $\p_{t}$ spectra measured by the phenix experiment at rhic in au + au, d + au and pp collisions at $\sqrt {^s{nn}} = 200 gev$ and in au + au collisions at $\sqrt {^s{nn}} = 62.4 gev$ are presented. a suppression of the yield of high $\p_{t}$ hadrons in central au + au collisions by a factor 4-5 at $\p_{t}&gt;5$ is found relative to the pp reference scaled by the nuclear overlap function $(\t_{ab})$ . in contrast, direct photons are not suppressed in central au + au collisions and no suppression of high $\p_{t}$ particles can be seen in d + au collisions. this leads to the conclusion that the dense medium formed in central au + au collisions is responsible for the suppression.
low mass dilepton production at rhic energies. recent results on low mass dilepton measurements from the phenix experiment are reported. invariant mass spectra of $\phi-&gt; e+e-$ are measured for the first time in au-au collisions at $\sqrt{^{s}nn} = 200 gev$ in run2. in d-au collisions, the yields and mt slopes of both $\phi -&gt;e+e- $ and $\phi-&gt; k+k- $ are measured. both results are consistent with each other within errors. in the future, a hadron blind detector will be installed in phenix which will enhance our capabilities of rejecting external photon conversions and dalitz pairs, that will result in a significant reduction of the large combinatorial background.
can $\phi$ mesons give an answer to the baryon puzzle at rhic?. the phenix experiment at rhic has observed a large enhancement of baryon and anti-baryon production at ~ 2-5 gev/c, compared to expectations from jet fragmentation. while a number of theoretical interpretations of the data are available, there is not yet a definitive answer to the “baryon puzzle”. we investigate the centrality dependence of -meson production at mid-rapidity in au + au collisions with $sqrt {^{s}nn}=200 gev$. comparison with the proton and anti-proton spectra reveal similar shapes, as expected for soft production described by hydrodynamics. however, the absolute yields show a different centrality dependence. the nuclear modification factors for $\phi$ are similar to those of pions, rather than (anti)protons that have similar mass. at intermediate , baryon/meson effects seem to be more important than the mass effects, in support of recombination models.
xps study of eu(iii) coordination compounds: core levels binding energies in solid mixed-oxo-compounds eu$_mx_xo_y$. literature is relatively sparse on xps studies of europium compounds: it is essentially restricted to metallic compounds (eum5, in which m is a transition metal) or to simple oxides. while particular interest have been devoted to understanding physical phenomenon in the beginning of “shake-down” and “shake-up” satellites evidenced on core-level regions of the lanthanides, few information on absolute binding energies (be) was available. this paper reports an xps binding energy data base for europium(iii) compounds, in which eu cation have various chemical environments: simple oxide eu2o3, eu mixed oxides with organic oxalate, acetylacetonate or inorganic sulfate, nitrate, carbonate ligands. the values of core-level be (o1s, eu3d and eu4d) and the characteristics of shake-down satellites of eu3d are reported, and their variations are attributed to ionicity/covalency changes. such interpretation was already published for group a mixed oxides and zeolites. these data are needed for determining eu(iii) species sorbed onto minerals in the presence of various ligands in the framework of retention studies for assessing the safety of future nuclear waste disposals.
hybrid materials applied to biotechnologies: coating of calcium phosphates for the design of implants active against bone resorption disorders. the reaction of a bisphosphonate bone resorption inhibitor (zoledronate) with calcium deficient hydroxyapatites (cdas) was studied as a potential route to local drug delivery systems active against bone resorption disorders. a simple mathematical model of the zoledronate–cda interaction was designed that correctly described the adsorption of zoledronate onto cdas. the resulting zoledronate-loaded materials were found to release the drug in different phosphate-containing media, with a satisfactory agreement between experimental data and the values predicted from the model.
nuclear modification of electron spectra and implications for heavy quark energy loss in au+au collisions at $\sqrt{s_{nn}}$=200 gev. the phenix experiment has measured mid-rapidity transverse momentum spectra (0.4 &lt; p_t &lt; 5.0 gev/c) of electrons as a function of centrality in au+au collisions at sqrt(s_nn)=200 gev. contributions from photon conversions and from light hadron decays, mainly dalitz decays of pi^0 and eta mesons, were removed. the resulting non-photonic electron spectra are primarily due to the semi-leptonic decays of hadrons carrying heavy quarks. nuclear modification factors were determined by comparison to non-photonic electrons in p+p collisions. a significant suppression of electrons at high p_t is observed in central au+au collisions, indicating substantial energy loss of heavy quarks.
dipole excitations of neutron-proton asymmetric nuclei. dipole excitations of unstable short-lived nuclei has been investigated experimentally by utilizing the electromagnetic-excitation process with high-energy secondary beams. from an exclusive measurement of the neutron-decay channels, differential cross sections with respect to excitation energy, which are directly related to the photo-absorption cross section and accordingly to the dipole-strength function, have been derived. light neutron-rich nuclei in the mass range froma = 11 toa = 23 with mass-over-charge ratios up toa/z≈ 2.8 have been investigated systematically. much in contrast to stable nuclei, low-lying dipole excitations well below the giant dipole resonance region have been observed as a general phenomenon for these neutron-proton asymmetric nuclei. for the neutron-rich oxygen isotopes, for instance, low-lyinge1 strength has been observed exhausting about 10% of the classical dipole sum rule below 15 mev excitation energy. a quantitative analysis of low-lying threshold strength for loosely bound nuclei indicates that the characteristics of the dipole strength is directly related to the ground-state single-particle structure of the valence nucleon in the projectile.
asymmetric fission of $^{149}$tb and $^{94}$mo leading to intermediate mass fragments. null
nucleon and light nucleus emission from excited nuclei. null
highly deformed rotating nuclei and superheavy elements in the fusionlike deformation valley. null
asymmetric fission barriers and total kinetic energies for $^{194}$hg, $^{149}$tb, $^{110–112}$in, $^{94}$mo, and $^{75}$br. null
multidimensional model of subbarrier heavy-ion fusion. null
evaporation, fission and fragmentation of hot rotating nuclei. null
non-cooled near infrared spectroscopy. we investigate the use of non-cryogenic instrumentation for near-infrared spectroscopy.
high-accuracy optical measurement of flatness for large objects. a high-accuracy non-contact optomechanical system has been designed for measuring the surface profile of relatively flat and large objects. the experimental set-up consists of a motorized gantry, a rangefinder, a ccd chip and a laser diode. this set-up permits discrete measurements to be performed on objects with a maximum plane surface area of 2.6 × 0.5 m2 along both the x and y axes. experiments were carried out on carbon sandwich panels. an uncertainty of ±8 µm has been obtained on flat and smooth surfaces; a ±30 µm uncertainty has been determined for a rough carbon sandwich panel.
a new algorithm for large surfaces profiling by fringe projection. a method for profiling surface objects based on the fringe projection method and a phase shifting algorithm is described. the application of this method to large surfaces is problematic since the calibration step requires the use of a reference plane as large as the object. a new algorithm based on least-squares method has also been developed to bypass this calibration step, and so the use of a reference plane. first experimental results on a carbon panel and on a parabolic aerial are presented to show the validity of the proposed algorithm. accuracy of 1mm has been obtained for an object of 1m and 20 cm long, while sensitivity has been proved to be of the order of 100 m.
radio detection of cosmic ray extensive air showers. at the radio observatory of nancay we study the possibility of detecting extremely high energy cosmic rays by radio detection of the electromagnetic pulse radiated during the development of extensive air showers in the atmosphere. details on the experimental setup and present results on the demonstrative experiment codalema are given.
cross-sections for pion and kaon production in proton nucleus collisions in the order of gev.&lt;br /&gt;measurement of double differential cross sections for light charged particles production in proton induced reaction at 62,9 mev on lead target. i - cross-sections for pion and kaon production in proton nucleus collisions in the order of gev.&lt;br /&gt;&lt;br /&gt;the study of the hadrons properties in nuclear matter is a subject of interest at the present time. positively charged kaons, having a strange anti-quark that can not be reabsorbed, are a very interesting tool to investigate the nuclear medium.&lt;br /&gt; the first part of this thesis studies cross sections for pion and kaon production in proton nucleus collisions at incident energy in the order of gev.&lt;br /&gt; the quantum molecular dynamics model to simulate proton nucleus collisions is described. then, total cross sections of elementary processes implemented in the model are presented. next, effects of the nucleon spectral function on pion and kaon production observed in double differential cross sections are discussed. finally, consequences of the correlations observed among nucleons in nucleus are investigated.&lt;br /&gt;&lt;br /&gt;ii - measurement of double differential cross sections for light charged particles production in proton induced reaction at 62,9 mev on lead target.&lt;br /&gt;&lt;br /&gt;in order to develop new options for nuclear waste management, studies are carrying out on the hybrid systems. the second part of this thesis takes place in the framework of the nuclear data linked to hybrid systems development. theoretical codes should have sufficient predictive power in the energy range from 20 to 150 mev. thus it is necessary to measure new cross sections to constrain these codes.&lt;br /&gt;the experiment aims to determine the double differential cross sections for light charged particles (p, d, t, 3he, a) production in proton induced reactions at 62,9 mev on lead target 208pb. the detection device consists of 7 triple telescopes, (si, si, csi(tl)).&lt;br /&gt;the general context of this work is presented in the first place. then, the experimental set up used for our measurements is described. the following chapters are dedicated to the data analysis.(particle discrimination, energy calibration) and double differential cross sections extraction. finally, a comparative study between our experimental results and some theoretical predictions is presented.
coulomb breakup of secondary beams of neutron-rich nuclei. secondary beams of unstable nuclei with kinetic energies of several hundred mev per nucleon are produced at, gsi in fragment~ationr eactions followed by in-flight, isot,ope separat'ion. results of studies of light) neutron-rich nuclei ranging from beryllium to oxygen including halo nuclei are presented. interaction of the projectiles with lead targets leads to coulomb breakup mediated t,hrough dipole excitabions into the continuum. non-resonant excitat,ions near the neutron separat,ion threshold deliver information on ground-stat,e properties such as spect,roscopic factors.
evidence for pygmy and giant dipole resonances in 130sn and 132sn. the dipole strength distribution above the one-neutron separation energy was measured in the unstable 130sn and the double-magic 132sn isotopes. the results were deduced from coulomb dissociation of secondary sn beams with energies around 500 mev/nucleon, produced by in-flight fission of a primary 238u beam. in addition to the giant dipole resonance, a resonancelike structure ("pygmy resonance") is observed at a lower excitation energy around 10 mev exhausting a few percent of the isovector e1 energy-weighted sum rule. the results are discussed in the context of a predicted new dipole mode of excess neutrons oscillating out of phase with the core nucleons.
the n=14 shell closure in $^{22}$o viewed through a neutron sensitive probe. to investigate the behavior of the n=14 neutron gap far from stability with a neutron-sensitive probe, proton elastic and 2+1 inelastic scattering angular distributions for the neutron-rich nucleus 22o were measured with a secondary beam intensity of only 1200 particles per second using the must silicon strip detector array at the ganil facility. a phenomenological analysis yields a deformation parameter bp;p' = 0.26 +- 0.04 for the 2+1 state, much lower than in 20o, showing a surprisingly weak neutron contribution to this state. a fully microscopic analysis was performed using optical potentials obtained from matter and transition densities generated by continuum skyrme-hfb and qrpa calculations, respectively. when the present results and those from a 22o + 197au scattering experiment are combined, the ratio of neutron to proton contributions to the 2+1 state is found close to the n/z ratio, demonstrating a strong n=14 shell closure in the vicinity of the neutron drip-line.
proton induced reactions on $^{natural}$u at 62.9 mev. double differential cross sections (ddcs) for light charged particles (proton, deuteron, triton, 3he, alpha) and neutrons produced by a proton beam impinging on a 238u target at 62.9 mev were measured at the cyclone facility in louvain-la-neuve (belgium). these measurements have been performed using two independent experimental set-ups ensuring neutron (demon counters) and light charged particles (si-si-csi telescopes) detection. the charged particle data were measured at 11 different angular positions from 25 degrees to 140 degrees allowing the determination of angle differential, energy differential and total production cross sections.
the megapie-test project. the goals of the megapie initiative are to design, build and operate a 1mw heavy liquid metal target. the first step towards the realization of the megapie target was the feasibility studies, which outlined the entire project. contextually to the feasibility studies the conceptual design phase started with the establishment of r&amp;d working groups assisting the design and validation of both the target and its ancillary systems. in this framework the eu project megapie-test has been structured in three work packages with tasks concerning the finalization of the engineering design, the components and subsystem testing, the integral test and the first irradiation period. the megapie-test consortium is composed by the 14 partners: fzk, psi, cea, enea, sck-cen, cnrs /idfe, in2p3, lmpgm, emn, ismra, univ-nantes, u-psudxi, ustl. currently the engineering design of the target has been finalized, its manufacturing has been launched and the design activities on the ancillary systems were almost completed. r&amp;d activities in the fields of materials, thermal – hydraulics, structural mechanics, neutronic and nuclear assessment and liquid metal technologies were performed in order to assist specific design issues. some subsystem and component tests were also performed and the preparation of the integral test is an ongoing activity.
final summary report on target design. the present document is the d13 deliverable report of work package 1, target development, from the megapie test project of the 5th european framework program. deliverable d13 is the final summary report on the activities performed within wp 1. the manufacturer (atea) has indicated to deliver the target to psi in the middle of 2005. assuming that this date is realistic, it can be foreseen that the integral out-of beam test will be conducted during the end of the year 2005 and the beginning of the year 2006. it can be assumed that the irradiation of the megapie target will start during the second quarter of 2006. the content of the present work package 1 final summary report reflects the status of the megapie target and ancillary system development at the end of 2004.
a framework for the detection and resolution of aspect interactions. aspect-oriented programming (aop) promises separation of concerns at the implementation level. however, aspects are not always orthogonal and aspect interaction is an important problem. currently there is almost no support for the detection and resolution of such interactions. the programmer is responsible for identifying interactions between conflicting aspects and implementing conflict resolution code. in this paper, we propose a solution to this problem based on a generic framework for aop. the contributions are threefold: we present a formal and expressive crosscut language, two static conflict analyses and some linguistic support for conflict resolution.
trace-based aspects. this chapter presents trace-based aspects which take into account the history of program executions in deciding what aspect behavior to invoke. such aspects are defined in terms of execution traces and may express relations between different events. weaving is accomplished through an execution monitor which modifies the base program execution as defined by the aspects. we motivate trace-based aspects and explore the trade-off between expressiveness and property enforcement/analysis. more concretely, we first present an expressive model of trace-based aspects enabling proofs of aspect properties by equational reasoning. using a restriction of the aspect language to regular expressions, we show that it becomes possible to address the difficult problem of interactions between conflicting aspects. finally, by restricting the actions performed by aspects, we illustrate how to keep the semantic impact of aspects under control and to implement weaving statically.
composition, reuse and interaction analysis of stateful aspects. aspect-oriented programming promises separation of concerns at the implementation level. however, aspects are not always orrthogonal and aspect interaction is a fundamental problem. in this paper, we extend previous work on a generic framework for the formal definition and interaction analysis of stateful aspects. we propose three important extensions which enhance expressivity while preserving static analyzability of interactions. first, we provide support for variables in aspects in order to share information between different execution points. this allows the definition of more precise aspects and to avoid detection of spurious conflicts. second, we introduce generic composition operators for aspects. this enables us to provide expressive support for the resolution of conflicts among interacting aspects. finally, we offer a means to define applicability conditions for aspects. this makes interaction analysis more precise and paves the way for reuse of aspects by making explicit requirements on contexts in which aspects must be used.
the next 700 krivine machines. the krivine machine is a simple and natural implementation of the normal weak-head reduction strategy for pure lambda-terms. while its original description has remained unpublished, this machine has served as a basis for many variants, extensions and theoretical studies. in this paper, we present the krivine machine and some well-known variants in a common framework. our framework consists of a hierarchy of intermediate languages that are subsets of the lambda-calculus. the whole implementation process (compiler + abstract machine) is described via a sequence of transformations all of which express an implementation choice. we characterize the essence of the krivine machine and locate it in the design space of functional language implementations. we show that, even within the particular class of krivine machines, hundreds of variants can be designed.
centrality and transverse momentum dependence of collective flow in 158 a gev pb+pb collisions measured via inclusive photons. directed and elliptic flow of inclusive photons near mid-rapidity in $158 $a gev pb+pb collisions has been studied. the data have been obtained with the photon spectrometer leda of the wa98 experiment at the cern sps. the flow strength has been measured for various centralities as a function of $p_t$ and rapidity over $0.18 &lt; p_t &lt; 1.5 \mathrm{gev}/c$ and $2.3 &lt; y &lt; 2.9$. the angular anisotropy has been studied relative to an event plane obtained in the target fragmentation region that shows the elliptic flow to be in-plane. the elliptic flow has also been studied using two-particle correlations and shown to give similar results. a small directed flow component is observed. both the directed and elliptic flow strengths increase with $p_t$. the photon flow results are used to estimate the corresponding neutral pion flow.
chemical and dynamics properties of heavy ion collisions at rhic energies by the measurement of the production of the doubly strange baryons in the star experiment. lattice qcd calculations predict, at vanishing chemical potential mu_b, a crossover from ordinary hadronic matter to a quark gluon plasma. heavy ion collisions have been proposed to recreate it in the laboratory and to study its properties. the au+au, d+au collisions at sqrt(s_(nn)) = 200 gev and the au+au ones at 62.4 gev delivered at rhic have been probed by the measurement of the xi- and xi+ particles in the star experiment. their yield evolution with collision energy and system size gives rize to the chemical properties of the reaction in the framework of hadronic and statistical models. the nuclear modification factor, r_(cp), of xi shows: (1) a meson/baryon dependence for 2 &lt; p_t &lt; 5 gev/c well reproduced by quark coalescence and recombination models, (2) the formation of a dense matter signed by a r_(cp) suppression at p_t &gt; 3 gev/c, (3) strong interactions between constituants suggesting the existence of strong collectivity in the medium. the xi transverse flow and more generally the multistrange baryon flow seems to be interesting to probe the early stage the collision with presumably partonic degrees of freedom. it is studied and discussed in this thesis.
strange particle correlations measured by the star experiment in ultra-relativistic heavy ion collisions at rhic. non-identical correlation functions allow to study the space-time evolution of the source of particles formed in ultra-relativistic heavy ion collisions. the star experiment is dedicated to probe the formation of a new state of nuclear matter called quark gluon plasma. the proton - lambda correlation function is supposed to be more sensitive to bigger source sizes than the proton - proton because of the absence of the final state coulomb interaction. in this thesis, proton - lambda, anti-proton - anti-lambda, anti-proton - lambda and proton - anti-lambda correlation functions are studied in au+au collisions at = 200 gev using an analytical model. the proton - lambda and anti-proton - anti-lambda correlation functions exhibit the same behavior as in previous measurements. the anti-proton - lambda and proton - anti-lambda correlation functions, measured for the first time, show a very strong signal corresponding to the baryon–anti-baryon annihilation channel. parameterizing the correlation functions has allowed to characterize final state interactions.
incident energy dependence of $pt$ correlations at relativistic energies. we present results for two-particle transverse momentum correlations, $&lt;\deltapt,i\deltapt,j&gt;$, as a function of event centrality for au+au collisions at $\sqrt{s_ {nn}}$=20, 62, 130, and 200 gev at the bnl relativistic heavy ion collider. we observe correlations decreasing with centrality that are similar at all four incident energies. the correlations multiplied by the multiplicity density increase with incident energy, and the centrality dependence may show evidence of processes such as thermalization, jet production, or the saturation of transverse flow. the square root of the correlations divided by the event-wise average transverse momentum per event shows little or no beam energy dependence and generally agrees with previous measurements made at the cern super proton synchrotron.
radio detection of cosmic ray air showers with codalema. studies of the radio detection of extensive air showers is the goal of the demonstrative experiment codalema. previous analysis have demonstrated that detection around $5.10^{16}$ ev was achieved with this set-up. new results allow for the first time to study the topology of the electric field associated to eas events on a event by event basis.
codalema: a cosmic ray air shower radio detection experiment. the codalema experimental device currently detects and characterizes the radio contribution of cosmic ray air showers : arrival directions and electric field topologies of radio transient signals associated to cosmic rays are extracted from the antenna signals. the measured rate, about 1 event per day, corresponds to an energy threshold around 5.10$^{16}$ev. these results allow to determine the perspectives offered by the present experimental design for radiodetection of ultra high energy cosmic rays at a larger scale.
features of radio detected extensive air showers with codalema. data acquisition and analysis for the codalema experiment, in operation for more than one year, has provided improved knowledge of the characteristics of this new device. at the same time, an important effort has been made to develop processing techniques for extracting transient signals from data containing interference.
radio detection of extensive air showers with codalema. the principle and performances of the codalema experimental device, set up to study the possibility of high energy cosmic rays radio detection, are presented. radio transient signals associated to cosmic rays have been identified, for which arrival directions and shower's electric field topologies have been extracted from the antenna signals. the measured rate, about 1 event per day, corresponds to an energy threshold around 5.10^16 ev. these results allow to determine the perspectives offered by the present experimental design for radiodetection of uhecr at a larger scale.
radio background measurements at the pierre auger observatory. null
radio-detection signature of high-energy cosmic rays by the codalema experiment. taking advantage of recent technical progress which has overcome some of the difficulties encountered in the 1960s in the radio detection of extensive air showers induced by ultra-high-energy cosmic rays (uhecr), a new experimental apparatus (codalema) has been built and operated. we will present the characteristics of this device and the analysis techniques that have been developed for observing electrical transients associated with cosmic rays. we find a collection of events for which both time and arrival direction coincidences between particle and radio signals are observed. the counting rate corresponds to shower energies $\geq 5 \times 10^{16}$ ev. the performance level which has been reached considerably enlarges the perspectives for studying uhecr events using radio detection.
evidence for radio detection of extensive air showers induced by ultra high energy cosmic rays. firm evidence for a radio emission counterpart of cosmic ray air showers is presented. by the use of an antenna array set up in coincidence with ground particle detectors, we find a collection of events for which both time and arrival direction coincidences between particle and radio signals are observed. the counting rate corresponds to shower energies $\gtrsim 5\times 10^{16}$ ev. these results open overwhelming perspectives to complete existing detection methods for the observation of ultra high-energy cosmic rays.
parton ladder splitting and the rapidity dependence of transverse momentum spectra in deuteron-gold collisions at rhic. we present a phenomenological approach (epos), based on the parton model, but going much beyond, and try to understand proton-proton and deuteron-gold collisions, in particular the transverse momentum results from all the four rhic experiments. it turns out that elastic and inelastic parton ladder splitting is the key issue. elastic splitting is in fact related to screening and saturation, but much more important is the inelastic contribution, being crucial to understand the data. we investigate in detail the rapidity dependence of nuclear effects, which is actually relatively weak in the model, in perfect agreement with the data, if the latter ones are interpreted correctly.
microcanonical pentaquark production in electron-positron annihilations. the existence of pentaquarks, namely baryonic states made up of four quarks and one antiquark, became questionable, because the candidates, i.e. the $\theta^+$ peak, are seen in certain reactions, i.e. p+p collisions, but not in others, i.e. $\ee$ annihilations. in this paper, we calculate the production of $\theta ^{+}(1540)$ and $\xi (1860)$ in $\ee$ annihilations at 91.2 gev with a microcanonical approach. a rather high production of pentaquark states is obtained. it is comparable with that from pp collisions at rhic energy, and higher than the yield from pp collisions at sps energy. if pentaquark states exist, the production is highly possible from high energy collisions, even without initial baryons.
leaching behaviour of unirradiated high temperature reactor (htr) uo$_2$–tho$_2$ mixed oxides fuel particles. the dissolution of different mixed oxide (u, th)o$_2$ particles under reducing conditions has been studied using a continuous flow-through reactor. the u/th ratio seems to have no or little influence on the normalised leaching rate of thorium or uranium, the release rate of uranium from the outer surface of a th rich matrix seems to follow the behaviour of pure uo2 even though u is a minor component in these phases and the dissolution rate of th is much lower. after long time u concentrations will become depleted at the solids surface and it will be expected that u release rates will become controlled by the release rates of thorium (rates at neutral ph &lt; 10$^{−6}$ g m^{−2}$ d$^{−1}$). under reducing conditions, the matrix of htr fuel particles presents significant intrinsic radionuclide confinement properties.
mechanical behavior of a borosilicate glass under aqueous corrosion. in france, fission products are being vitrified for a possible final geological disposal. under disposal conditions, corrosion of the glass by groundwater as well as stress corrosion because of stresses occurring at surface flaws cannot be excluded. within this framework, the mechanical behavior of the french simulated nuclear waste glass son68 was studied by vickers indentation and fracture experiments in air and in a corrosive solution. the glass was corroded at 90°c in a solution enriched with si, b, and na. the results showed that the glass corrosion enhances the cracks propagation relative to experiments in air. the indentation fracture toughness ($k_{ic}$) obtained using a four-point bending test showed that the $k_{ic}$ of the glass decreased with increasing corrosion time.
differential probes of medium-induced energy loss. results from the phenix experiment of measurements of high- $p_{\rm t}$ particle production presented at the hard probes 2004 conference are summarized. this paper focuses on a sub-set of the measurements presented at the conference, namely the suppression of $\pi^0$ production at moderate to high $p_{\rm t}$ as a function of angle with respect to the collision reaction plane, $\delta\phi$ , for different collision centralities. the data are presented in the form of nuclear modification factor as a function of angle with respect to the reaction plane, $r_{aa} (\delta \phi)$ . the data are analyzed using empirical estimates of the medium-induced energy loss obtained from the $r_{aa} (\delta \phi)$ values. a geometric analysis is performed with the goal of understanding the simultaneous dependence of raa on $\delta\phi$ and centrality. we find that the centrality and $\delta\phi$ dependence of the $\pi^0$ suppression can be made approximately consistent using an admittedly over-simplistic description of the geometry of the jet propagation in the medium but only if the energy loss is effectively reduced for short parton path lengths in the medium. we find that with a more "canonical" treatment of the quenching geometry, the $\pi^0$ suppression varies more rapidly with $\delta\phi$ than would be expected from the centrality dependence of the suppression.
renormalizable supersymmetric gauge theory in six dimensions. we construct and discuss a 6d supersymmetric gauge theory involving four derivatives in the action. the theory involves a dimensionless coupling constant and is renormalizable. at the tree level, it enjoys n = (1,0) superconformal symmetry, but the latter is broken by quantum anomaly. our study should be considered as preparatory for seeking an extended version of this theory which would hopefully preserve conformal symmetry at the full quantum level and be ultraviolet-finite.
particle production in proton-proton and deuteron-gold collisions at rhic. we try to understand recent data on proton-proton and deuteron-gold collisions at rhic, employing a parton model approach called epos.
a new string model: nexus 3. after discussing conceptual problems with the conventional string model, we present a new approach, based on a theoretically consistent multiple scattering formalism. first results for proton-proton scattering at 158 gev are discussed.
the nexus model. the interpretation of experimental results at rhic and in the future also at lhc requires very reliable and realistic models. considerable effort has been devoted to the development of such models during the past decade, many of them being heavily used in order to analyze data. there are, however, serious inconsistencies in the above-mentioned approaches. in this paper, we will introduce a fully self-consistent formulation of the multiple-scattering scheme in the framework of a gribov-regge type effective theory.
inconsistencies in models for rhic and lhc. the interpretation of experimental results at rhic and in the future also at lhc requires very reliable and realistic models. considerable effort has been devoted to the development of such models during the past decade, many of them being heavily used in order to analyze data. it is the purpose of this paper to point out serious inconsistencies in the above-mentioned approaches. we will demonstrate that requiring theoretical self-consistency reduces the freedom in modeling high energy nuclear scattering enormously. we will introduce a fully self-consistent formulation of the multiple-scattering scheme in the framework of a gribov-regge type effective theory. in addition, we develop new computational techniques which allow for the first time a satisfactory solution of the problem in the sense that calculations of observable quantities can be done strictly within a self-consistent formalism.
models for rhic and lhc: new developments. we outline inconsistencies in presently used models for high energy nuclear scattering, which make their application quite unreliable. many \"successes\" are essentially based on an artificial freedom of parameters, which does not exist when the models are constructed properly. the problem is the fact that any multiple scattering theory requires an appropriate treatment of the energy sharing between the individual interactions, which is technically very difficult to implement. lacking a satisfying solution to this problem, it has been simply ignored. we introduce a fully self-consistent formulation of the multiple-scattering scheme. inclusion of soft and hard components - very crucial at high energies - appears in a \"natural way\", providing a smooth transition from soft to hard physics. we can show that the effect of appropriately considering energy conservation has a big influence on the results, and must therefore be included in any serious calculation.
nuclear scattering at very high energies. we discuss the current understanding of nuclear scattering at very high energies. we point out several serious inconsistencies in nowadays models, which provide big problems for any interpretation of data at high energy nuclear collisions. we outline how to develop a fully self-consistent formalism, which in addition uses all the knowledge available from studying electron-positron annihilation and deep inelastic scattering, providing a solid basis for further developments concerning secondary interactions.
tools for rhic: review of models. we discuss the present status of microscopic models for rhic, with an emphasis on models being realized via the monte carlo technique. this review is to a large extent based on the oscar3 workshop, where general concepts and new trends in this field have been discussed.
early-reaction-phase energy transformation in heavy-ion reactions below 100 mev/u. an extensive study of maximal values that the main forms of energy take during the compact dynamical phase of heavy-ion reactions has been carried out within the framework of a semiclassical transport approach. several systems having different total mass and asymmetry have been studied over a full range of impact parameter and a wide range of incident energies below 100 mev/u. it has been found that maxima of compression and thermal energy as a function of impact parameter closely follow the reaction geometry in the spirit of the participant–spectator picture and in head-on collisions are proportional to the available center-of-mass energy. indication of a smooth transition between the low-energy reaction mechanism (deep inelastic model) and the higher-energy reaction mechanism (participant–spectator-like geometrical picture) is identified around the fermi energy.
inclusive $\pi^0$ spectra at high transverse momentum in d-au collisions at rhic. preliminary results on inclusive neutral pion production in d-au collisions at $\sqrt{s_{nn}}$ in the pseudo-rapidity range $0&lt;\eta&lt;1$ are presented. the measurement is performed using the star barrel electromagnetic calorimeter (bemc). in this paper, the analysis of the first bemc hadron measurement is described and the results are compared with earlier rhic findings. the $\pi^0$ invariant differential cross sections show good agreement with next-to-leading order (nlo) perturbative qcd calculations.
heavy-quarkonium hadron cross section in qcd at leading twist. we compute the total cross section of a heavy quarkonium on a hadron target in leading twist qcd, including target mass corrections. our method relies on the analytical continuation of the operator product expansion of the scattering amplitude, obtained long ago by bhanot and peskin. the cross section has a simple partonic form, which allows us to investigate the phenomenology of j/psi and upsilon dissociation by both pions and protons.
charmonium suppression in p-a collisions at rhic. we discuss charmonium production in proton-nucleus collisions at rhic energies under the assumption of xf and x2 scaling. we find that all the ambiguities due to energy loss are gone at this energy and therefore data will reveal the scaling law, if any. these p-a data will also be crucial to interpret nucleus-nucleus data with respect to a possible formation of a quark gluon plasma because the extrapolations for charmonium production from the present p-a data to rhic energies, based on the two scaling laws, differ by a factor of four.
quarkonium hadron interaction in qcd. the analytic continuation of the operator product expansion of the scattering amplitude allows to compute the heavy-quarkonium hadron total cross section. the energy dependence of the upsilon and upsilon' cross sections with a proton is discussed.
two large-area anode-pad micromegas chambers as the basic elements of a pre-shower detector. the design of a detector based on micromegas (micro mesh gaseous structure) technology is presented. our detector is characterized by a large active area of 398(\\times)281 mm(^{2}), a pad read-out with 20(\\times)22 mm(^{2}) segmentation, and an uniform amplification gap obtained by insulating spacers (100 (\\mu)m high and 200 (\\mu)m in diameter). the performances of several prototypes have been evaluated under irradiation with secondary beams of 2 gev/c momentum charged pions and electrons. we consider such a detector as the basic element for a pre-shower detector to equip the photon spectrometer (phos) of the alice experiment. its assets are modularity, small amount of material, robustness and low cost.
hard photon and neutral pion production in cold nuclear matter. the production of hard photons and neutral pions in 190 mev proton induced reactions on c, ca, ni, and w targets has been for the first time concurrently studied. angular distributions and energy spectra up to the kinematical limit are discussed and the production cross-sections are presented. from the target mass dependence of the cross-sections the propagation of pions through nuclear matter is analyzed and the production mechanisms of hard photons and primordial pions are derived. it is found that the production of subthreshold particles proceeds mainly through first chance nucleon-nucleon collisions. for the most energetic particles the mass scaling evidences the effect of multiple collisions.
on the relation between effective supersymmetric actions in different dimensions. we make two remarks: (i) renormalization of the effective charge in a 4--dimensional (supersymmetric) gauge theory is determined by the same graphs and is rigidly connected to the renormalization of the metric on the moduli space of the classical vacua of the corresponding reduced quantum mechanical system. supersymmetry provides constraints for possible modifications of the metric, and this gives us a simple proof of nonrenormalization theorems for the original 4-dimensional theory. (ii) we establish a nontrivial relationship between the effective (0+1)-dimensional and (1+1)-dimensional lagrangia (the latter represent conventional kahlerian sigma models).
effect of aqueous acetic, oxalic and carbonic acids on the adsorption of americium onto $\alpha$-alumina. the prediction of the migration for radionuclides in geologic media requires a quantitative knowledge of retardation phenomena. for this purpose, the sorption of am(iii) onto a model mineral — $\alpha$-alumina — is studied here, including the effects of groundwater chemistry: ph and concentrations of small organic ligands (acetate, oxalate and carbonate anions). this work presents some experimental evidences for the synergic sorption mechanism of americium–ligand cationic complexes onto the alumina. as its anionic complexes were not sorbed, am(iii) cations were desorbed as a result of the formation of anionic complexes in the aqueous phase. by using the ion-exchange theory, and a corresponding restricted set of parameters — exchange capacities and thermodynamic equilibrium constants — the whole set of sorption experiments of am(iii) cationic species onto the $\alpha$-alumina was modelled in various chemical conditions.
non-universality of transverse coulomb exchange at small x. within an explicit scalar qed model we compare, at fixed x &lt;&lt; 1, the leading-twist k_t-dependent `quark' distribution f_q(x, k_t) probed in deep inelastic scattering and drell-yan production, and show that the model is consistent with the universality of f_q(x, k_t). the extension of the model from the aligned-jet to the 'symmetric' kinematical regime reveals interesting properties of the physics of coulomb rescatterings when comparing dis and dy processes. at small x the transverse momentum induced by multiple scattering on a single centre is process dependent, as well as the transverse momentum broadening occurring in collisions on a finite size nuclear target.
retardation effect for collisional energy loss of hard partons produced in a qgp. we study the collisional energy loss suffered by an energetic parton travelling the distance l in a high temperature quark-gluon plasma and initially produced in the medium. we find that the medium-induced collisional loss -δe(l) is strongly suppressed compared to previous estimates which assumed the collisional energy loss rate -de/dx to be constant. the large l linear asymptotic behaviour of -δe(l) sets in only after a quite large retardation time. the suppression of -δe(l) is partly due to the fact that gluon bremsstrahlung arising from the initial acceleration of the energetic parton is reduced in the medium compared to vacuum. the latter radiation spectrum is sensitive to the plasmon modes of the quark-gluon plasma and has a rich angular structure.
single electrons from heavy flavor decays in p+p collisions at $\sqrt{s}$ = 200 gev. the invariant differential cross section for inclusive electron production in p+p collisions at sqrt(s) = 200 gev has been measured by the phenix experiment at the relativistic heavy ion collider over the transverse momentum range 0.4 &lt;= p_t &lt;= 5.0 gev/c at midrapidity (eta &lt;= 0.35). the contribution to the inclusive electron spectrum from semileptonic decays of hadrons carrying heavy flavor, i.e. charm quarks or, at high p_t, bottom quarks, is determined via three independent methods. the resulting electron spectrum from heavy flavor decays is compared to recent leading and next-to-leading order perturbative qcd calculations. the total cross section of charm quark-antiquark pair production is determined as sigma_(c c^bar) = 0.92 +/- 0.15 (stat.) +- 0.54 (sys.) mb.
measurement of identified pi^0 and inclusive photon v_2 and implication to the direct photon production in sqrt(s_nn) = 200 gev au+au collisions. the azimuthal distribution of identified pi^0 and inclusive photons has been measured in sqrt(s_nn) = 200 gev au+au collisions with the phenix experiment at the relativistic heavy ion collider (rhic). the second harmonic parameter (v_2) was measured to describe the observed anisotropy of the azimuthal distribution. the measured inclusive photon v_2 is consistent with the expected hadron decay and is also consistent with the lack of direct photon signal over the measured p_t range 1-6 gev/c. an attempt is made to extract v_2 of direct photons.
hanbury brown-ttiss correlation functions from event generators: a reliable approach to determine the size of the emitting source in ultrarelativistic heavy ion collisions?. employing nexus, one of the most recent simulation programs for heavy ions collisions, we investigate in detail the hanbury-twiss correlation function for charged pions for reactions 158 gev pb+pb. for this study we supplement the standard simulation program by electromagnetic interactions. we find that the string fragmentation introduces strong space-momentum correlations and the freeze out times of the correlated pairs have a wide distribution. both has a strong influence on the apparent source radius determined from the measured correlation correlation function.an equally strong influence is observed for the final state electromagnetic interaction.
multifragmentation - what the data tell us about the different models. we discuss what the presently collected data tell us about the mechanism of multifragmentation by comparing the results of two different models, which assume or show an opposite reaction scenario, with the recent high statistics $4\\pi$ experiments performed by the indra collaboration. we find that the statistical multifragmentation model and the dynamical quantum molecular dynamics approach produce almost the same results and agree both quite well with experiment. we discuss which observables may serve to overcome this deadlock on the quest for the reaction mechanism. finally we proof that even if the system is in equilibrium, the fluctuation of the temperature due to the smallness of the system renders the caloric curve useless for the proof of a first order phase transition.
kaon production at subthreshold and threshold energies. we summarize what we have learnt about the kaon production in nucleus-nucleus collisions in the last decade. we will address three questions: a) is the $k^+$ production sensitive to the nuclear equation of state? b) how can it happen that at the same excess energy the same number of $k^+$ and $k^-$ are produced in heavy ion collisions although the elementary cross section in pp collisions differs by orders of magnitudes? and c) why kaons don\'t flow?.
critical opacity: a possible explanation of the fast thermalisation times seen in bnl rhic experiments. the nambu jona-lasinio lagrangian offers an explication of the seemingly contradictory observations that a) the energy loss in the entrance channel of heavy ion reactions is not sufficient to thermalize the system and that b) the observed hadron cross sections are in almost perfect agreement with hydrodynamical calculations. according to this scenario, a critical opacity develops close to the chiral phase transition which equilibrates and hadronizes the expanding system very effectively. it creates as well radial flow and, if the system is not isotropic, finite $v_2$ values.
study of system- size effects in multi- fragmentation using quantum molecular dynamics model. we report, for the first time, the dependence of the multiplicity of different fragments on the system size employing a quantum molecular dynamics model. this dependence is extracted from the simulations of symmetric collisions of ca+ca, ni+ni, nb+nb, xe+xe, er+er, au+au and u+u at incident energies between 50 a mev and 1 a gev. we find that the multiplicity of different fragments scales with the size of the system which can be parameterized by a simple power law.
$\phi$ puzzle in heavy-ion collisions at 2 a gev: how many k$^-$ from $\phi$ decays?. the preliminary experimental data on $\\phi$ production in the reaction ni(1.93 agev) + ni point to a puzzling high $\\phi$ yield which can not be reproduced with present transport codes. we survey the experimental situation and present prospects for dedicated measurements of the $\\phi$ multiplicities with the $k^+ k^-$ and $e^+ e^-$ channels at hades and fopi.
strange baryons in a hot and dense medium within the nambu-jona-lasinio model. using an extended version of the nambu-jona-lasinio model we build a simple description of the baryons as diquark-quark bound states. first, a description of the diquarks in a dense and hot medium is presented. then, we introduce the formalism for the baryons based on the faddeev equation associated with the so-called `` static approximation'' which finally gives a bethe-salpeter equation in the diquarks-quarks channel. by identifying the baryons with the bound states, we can obtain a description of their properties. in particular, we obtain the right mass spectrum for the proton, $\lambda$, $\xi$, and $\sigma$ at t=0 and $\mu=0$. we extend the formalism to finite temperature and density to obtain a description of the mass change of these baryons in the medium.
analysis of kaon spectra at sis energies - what remains from the kn potential. we study the reaction au+au at 1.48 agev and analyze the influence of the kn optical potential on cm spectra and azimuthal distributions at mid-rapidity. we find a significant change of the yields but only slight changes in the shapes of the distributions when turning off the optical potential. however, the spectra show contributions from different reaction times, where early kaons contribute stronger to higher momenta and late kaons to lower momenta. azimuthal distributions of the kaons at mid-rapidity show a strong centrality dependence. their shape is influenced by the kn optical potential as well as by re-scattering.
heavy flavor production in phenix. the phenix experiment at rhic measured single electron spectra in p + p, d + au and au + au collisions at $\sqrt{s_{nn}} = 200$ gev, and in au + au collisions at $\sqrt{s_{nn}} = 62.4$ gev. in these spectra, electrons from semi-leptonic decays of charmed particles are the dominant contribution after subtraction of all 'photonic' sources (photon conversions, dalitz decays, decays of light vector mesons). the p + p open charm production cross-section is found to be in good agreement with pqcd nlo calculations. the shape of the distributions obtained for p + p interactions is compared with those observed for nucleus-nucleus collisions. from p + p to d + au and au + au interactions, open charm production is found to scale with the number of binary collisions $n_{\rm coll}$ . au + au data at $\sqrt{s_{nn}} = 62.4$ gev is compatible with the isr p + p results scaled by $n_{\rm coll}$ . the elliptic flow parameter v2 of heavy flavor electrons has also been measured, and is found to be non-zero in the intermediate pt range.
recent high- $p_{\rm t}$ results from star. the star collaboration has a broad range of recent results on intermediate and high- $p_{\rm t}$ phenomena in au + au collisions at $\sqrt{s_{nn}}$ = 200 and 62 gev and in d + au at $\sqrt{s_{nn}}$ = 200 gev. these include new measurements of spectra, azimuthal anisotropies and di-hadron correlations. the comparison of the 62 and 200 gev au + au results indicates that jet quenching, elliptic flow and di-hadron correlation measurements are very similar at the two energies. meson-baryon differences that have been seen at intermediate $p_{\rm t}$ in 200 gev au + au collisions are also present in 62 gev au + au collisions and in 200 gev d + au collisions. measurements of backward-forward inclusive hadron yield asymmetries and forward-midrapidity di-hadron correlations in d + au collisions are consistent with the saturation picture. a brief review of these results is presented.
open charm production from d + au collisions in star. charmed hadrons are interesting observables in heavy ion collisions. they are becoming more accessible to experimental scrutiny at rhic energies due to the increased production cross-section of charm with the larger centre-of-mass energy available at rhic compared to sps. one source of interest in charm production is due to the fact that gluon fusion dominates the charm production cross-section at high energy. hence, a measurement of charm hadrons is directly sensitive to the gluon distributions of the colliding particles. in addition, any measurement of production at rhic, and more importantly any observed suppression, must be compared to the overall production of pairs. a systematic study of charmed hadrons in all collision systems available at rhic is therefore an invaluable experimental tool in the characterization of the matter produced at rhic. in particular, d + au collisions are a necessary step for the comparison of any possible modification of charm production in au + au collisions. we present preliminary results on d meson production from d + au collisions in star at = 200 .
speciation of technetium(iv) chloride under gamma irradiation. speciation of tc in aqueous solution is a complex phenomenon because several parameters intervene simultaneously. we show under gamma radiations, it is completely modified. at ph 8.5 tco$_2$ is oxidized to tc(vii) with a yield of 1.38×10$^^{−7}$ mol j${−1}$. at ph 1.5, it was demonstrated that tc is present as hydroxylated species tc$_no_y^{(4n-2y)+}$ (l. vichot, m. fattahi, m. musikas: radiochim. acta 91, 263 (2003)). its oxidation leads also to tc(vii) with an experimental yield of 1.45×10$^{−7}$ mol j$^{−1}. in more acidic solutions, results are consistent with hydrolysis and oxidation of tc(iv) to tc(v). as for colloidal solutions of tc(iv) or tco$_2$, they give tc(vii) whereas solutions of tccl$_6^{2-}$ at ph 0.5 and 1.5 do not lead directly to tc(vii). reduced states of tc are very sensitive to radiation. hydrolysis as well as polymerisation of tc are accelerated. furthermore the radiolytic processes are ph dependent.
parity-violating electroweak asymmetry in polarized-e p scattering. null
measurement of transverse single-spin asymmetries for mid-rapidity production of neutral pions and charged hadrons in polarized p+p collisions at $\sqrt{s} = 200 gev. the transverse single-spin asymmetries of neutral pions and non-identified charged hadrons have been measured at mid-rapidity in polarized proton-proton collisions at sqrt(s) = 200 gev. the data cover a transverse momentum (p_t) range 0.5-5.0 gev/c for charged hadrons and 1.0-5.0 gev/c for neutral pions, at a feynman-x (x_f) value of approximately zero. the asymmetries seen in this previously unexplored kinematic region are consistent with zero within statistical errors of a few percent. in addition, the inclusive charged hadron cross section at mid-rapidity from 0.5 &lt; p_t &lt; 7.0 gev/c is presented and compared to nlo pqcd calculations. successful description of the unpolarized cross section above ~2 gev/c using nlo pqcd suggests that pqcd is applicable in the interpretation of the asymmetry results in the relevant kinematic range.
j/$\psi$ production and nuclear effects for $d+au$ and $p+p$ collisions at $\sqrt{s_nn}$ = 200 gev. j/psi production in d+au and p+p collisions at sqrt(s_nn) = 200 gev has been measured by the phenix experiment at rapidities -2.2 &lt; y &lt; +2.4. the cross sections and nuclear dependence of j/\psi production versus rapidity, transverse momentum, and centrality are obtained and compared to lower energy p+a results and to theoretical models. the observed nuclear dependence in d+au collisions is found to be modest, suggesting that the absorption in the final state is weak and the shadowing of the gluon distributions is small and consistent with dglap-based parameterizations that fit deep-inelastic scattering and drell-yan data at lower energies.
alice technical design report of the computing. null
the muon spectrometer of the alice experiment. the main goal of the muon spectrometer of the alice experiment is the measurement of heavy quarks in pp, pa and aa collisions at lhc energies, via the muonic channel. physics motivations, the apparatus and its physics performances are presented in this talk.
constructing the nuclear caloric curve from thermal bremsstrahlung. the behavior of the emission of thermal bremsstrahlung with the reaction centrality has been studied in $^{129}$xe + $^{nat}$sn reactions at 50a mev. a thermal hard photon component is present along the measured impact parameter range (0.1 $\leq$ b/b$_{max} \leq$ 0.6) showing the formation of a thermal equilibrated system. the temperature of the system (4.4 – 6.8 mev) exhibits a lower dependence on the excitation energy ($\epsilon^{\star}$, filled $\sim$ 2.8–8.7mev) than the expected for a fermi fluid.
charmonia enhancement in quark–gluon plasma with improved description of c-quarks phase distribution. we present a dynamical model of heavy quark evolution in the quark–gluon plasma (qgp) based on the fokker–planck equation. we then apply this model to the case of central ultra-relativistic nucleus–nucleus collisions performed at rhic and estimate the component of $j/\psi$ production (integrated and differential) stemming from $c–\bar{\rm c}$ pairs that are initially uncorrelated.
multi-strange baryon production in au+au collisions at top rhic energy as a probe of bulk properties. we report star preliminary results on multi-strange baryon production in au+au collisions at ${\sqrt{s_{\rm nn}}=200}\,{\rm gev}$ at rhic. its implication for the formation of a new state of matter is discussed. the system size dependence on the production of strange baryons is investigated to study the onset of strange quark equilibration in the medium. the nuclear modification factor of $\lambda, \xi$ and $\omega$ is also presented. its suppression at $p_t$ &gt; 3 gev/c supports the formation of a dense interacting medium at rhic. the spectra of multi-strange baryons reveal that within a hydro-inspired model, they may decouple earlier than lighter particles and that their flow may be mostly developed at a partonic level. this idea is emphasized by the measurement of the $v_2$ of ${\xi^{-}}+{\bar{\xi}^{+}}$ and ${\omega^{-}}+{\bar{\omega}^{+}}$ whose behaviour is close to the $\lambda+\bar{\lambda}$ baryon elliptic flow in the intermediate $p_t$ region where a constituent quark scaling of $v_2$ is observed.
transport theories for heavy-ion collisions in the 1 a gev regime. we compare multiplicities as well as rapidity and transverse momentum distributions of protons, pions and kaons calculated within presently available transport approaches for heavy-ion collisions around 1 a gev. for this purpose, three reactions have been selected: au+au at 1 and 1.48 a gev and ni+ni at 1.93 a gev.
physics of the muon spectrometer of the alice experiment. the main goal of the muon spectrometer of the alice experiment at lhc is the measurement of heavy quark production in p+p, p+a and a+a collisions at lhc energies, via the muonic channel. physics motivations and expected performances have been presented in this talk.
charm production in antiproton-nucleus collisions at the $j/\psi$ and the $\psi'$ thresholds. we discuss the production of charmonium states in antiproton–nucleus ollisions at the $\psi'$ threshold. it is explained that measurements in in $\overline{p}a$ collisions will allow to get new information about the strengths of the inelastic $j/\psin$ and $\psi'n$ interaction, on the production of $\lambda_c$ and $\overline{d}$ in charmonium interaction, on the production of $\delta_c$ and $\overline{d}$ in charmonium–nucleon interactions and for the first time about the nondiagonal transitions $\psi'n \to j/\psin$. the inelastic $j/\psi$-nucleon cross section is extracted from the comparison of hadron–nucleus collisions with hadron–nucleon collisions. we extract the total $j/\psi$-nucleon cross section from photon–nucleon collisions by accounting for the color transparency phenomenon within the frame of the gvdm (generalized vector meson dominance model). we evaluate within the gvdm the inelastic $\psi'$-nucleon cross section as well as the cross section for the nondiagonal transitions. predictions for the ratio of $j/\psi$ to $\psi'$ yields in antiproton–nucleus scatterings close to the threshold of $\psi'$ production for different nuclear targets are presented.
evolution of fragment distributions and reaction mechanisms for the ar+ni system from 32 to 95 a.mev. within the framework of flow and multifragmentation study, the 36ar+58ni experiment has been performed at seven incident energies from 32 to 95 a.mev with the indra detector at ganil. after a brief description of the experimental set-up, the main trends as well as the evolution of fragment distributions will be presented. some results about reaction mechanisms for particular classes of events will conclude this report.
the silicon strip detector for star. the star silicon strip detector (ssd) completes the three layers of the silicon vertex tracker (svt) already installed inside the time projection chamber (tpc). this additional fourth layer provides two-dimensional hit position and energy loss measurements for charged particles, improving the star tracking performances and the extrapolation of the tpc tracks to the svt hits. the design of the barrel is described; in particular the choice of the tape automated bonding (tab) technique to connect the detectors to their front-end electronics (fee). several hardware aspects of the detector are presented like the power supply, the readout chain and the cooling system, as well as the production chain from the detection module to the barrel.
direct decay of the gqr in $^{40}$ca through alpha-particle emission. inelastic scattering of 40ca on 40ca at 50 mev/a has been measured at the ganil facility in coincidence with light charged particles. for the first time, the speg spectrometer was associated with 240 csi(tl) scintillators of the indra 4π array. the missing energy method, successfully used in previous experiments to study the nucleon decay of collective states is applied to the study of alpha-particle decay of the gqr. a significant direct decay branch by alpha-particles of the gqr in 40ca is measured for the first time.
investigation of nucleon-induced reactions in the fermi energy domain within the microscopic dywan model. a microscopic investigation of nucleon-induced reactions is addressed within the dywan model, which is based on the projection methods of out of equilibrium statistical physics and on the mathematical theory of wavelets. due to a strongly compressed representation of the fermionic wave functions, the numerical simulations of the nucleon transport in target are therefore able to preserve the quantum nature of the colliding system, as well as a least biased many-body information needed to keep track of the cluster formation. a special attention is devoted to the fingerprints of the phase space topology induced by the fluctuations of the self-consistent mean-field. comparisons between theoretical results and experimental data point out that etdhf type approaches are well suited to describe reaction mechanisms in the fermi energy domain. the observed sensitivity to physical effects shows that the nucleon-induced reactions provide a valuable probe of the nuclear interaction in this range of energy.
midrapidity direct-photon production in $p+p$ collisions at $\sqrt{s}$ = 200 gev. a measurement of direct photons in $p+p$ collisions at $\sqrt{s}$=200 gev is presented. a photon excess above background from $\pi^0\rightarrow\gamma+\gamma$, $\eta\rightarrow\gamma+\gamma$, and other decays is observed in the transverse momentum range $5.5 &lt; p_t &lt; 7 gev/c$. the result is compared to a next-to-leading-order perturbative qcd calculation. within errors, good agreement is found between the qcd calculation and the measured result.
interferometry of direct photons in central 208pb+208pb collisions at 158a gev. two-particle correlations of direct photons were measured in central 208pb+208pb collisions at 158a gev. the invariant interferometric radii were extracted for 100.
alice: physics performance report, volume 1. alice is a general-purpose heavy-ion experiment designed to study the physics of strongly interacting matter and the quark-gluon plasma in nucleus-nucleus collisions at the lhc. it currently includes more than 900 physicists and senior engineers, from both nuclear and high-energy physics, from about 80 institutions in 28 countries. the experiment was approved in february 1997. the detailed design of the different detector systems has been laid down in a number of technical design reports issued between mid-1998 and the end of 2001 and construction has started for most detectors. since the last comprehensive information on detector and physics performance was published in the alice technical proposal in 1996, the detector as well as simulation, reconstruction and analysis software have undergone significant development. the physics performance report (ppr) will give an updated and comprehensive summary of the current status and performance of the various alice subsystems, including updates to the technical design reports, where appropriate, as well as a description of systems which have not been published in a technical design report.
pion, kaon, proton and anti-proton transverse momentum distributions from p+p and d+au collisions at $\sqrt{s_{nn}} = 200$ gev. identified mid-rapidity particle spectra of $\pi^{\pm}$, $k^{\pm}$, and $p(\bar{p})$ from 200 gev p+p and d+au collisions are reported. a time-of-flight detector based on multi-gap resistive plate chamber technology is used for particle identification. the particle-species dependence of the cronin effect is observed to be significantly smaller than that at lower energies. the ratio of the nuclear modification factor ($r_{dau}$) between protons $(p+\bar{p})$ and charged hadrons ($h$) in the transverse momentum range $1.2&lt;{p_{t}}&lt;3.0$ gev/c is measured to be $1.19\pm0.05$(stat)$\pm0.03$(syst) in minimum-bias collisions and shows little centrality dependence. the yield ratio of $(p+\bar{p})/h$ in minimum-bias d+au collisions is found to be a factor of 2 lower than that in au+au collisions, indicating that the cronin effect alone is not enough to account for the relative baryon enhancement observed in heavy ion collisions at rhic.
photon physics in heavy ion collisions at the lhc. various pion and photon production mechanisms in high-energy nuclear collisions at rhic and lhc are discussed. comparison with rhic data is done whenever possible. the prospect of using electromagnetic probes to characterize quark-gluon plasma formation is assessed.
retention of toxic substances on consolidated porous systems. characterization of the state of water by means of low field proton nmr and determination of kd values using capillaries. null
modelling the alteration gel composition of simplified borosilicate glasses by precipitation of an ideal solid solution in equilibrium with the leachant. the modelling of the alteration process of two model glasses (si–al–b–na and si–al–b–na–ca–zr), having the same molar ratio as the french reference son68 glass, with the geochemical code kindis was studied. the formation of the alteration layer was simulated by the precipitation of an ideal solid solution. this simulation study was compared then with the alteration experiments carried out in parallel: glasses were placed at 363 k in pure water at two different s/v ratios (1 and 80 cm−1) for duration of 30–180 days. the thermodynamic stability of the siliceous end-members of the solid solution has a fundamental influence on the simulation results. thus, with suitable end-members (chalcedony and hydroxides), the simulation allows one to reproduce with a good agreement the evolution of the silicon content in solution as well as the chemical composition of the gel layer. the experimental results and the simulations seem to indicate a control of the gel composition over the glass dissolution process.
coplanar ternary decay of hyper-deformed $^{56}$ni. ternary fission events from the decay of $^{56}$ni compound nuclei, formed in the $^{32}$s + $^{24}$mg reaction at $e_{lab}(^{32}s)$ = 163.5 mev, have been measured in a unique set-up consisting of two large area position sensitive (x,y) gas detector telescopes. very narrow out-of-plane correlations are observed for two fragments emitted in either purely binary events or in events with a missing mass consisting of 2 and 3 $\alpha$-particles. these correlations are interpreted as ternary fission decay from compound nuclei at high angular momenta through an elongated (hyper-deformed) shape with very large moments of inertia, where the lighter mass in the neck region remains at rest.
fragmentation in central pb + au collisions within a microscopic dynamic approach. null
on the stability of rotating nuclei against fission through creviced shapes. null
centrality dependence of direct photon production in sqrt(s_nn) = 200 gev au+au collisions. the first measurement of direct photons in au+au collisions at sqrt(s_nn) = 200 gev is presented. the direct photon signal is extracted as a function of the au+au collision centrality and compared to nlo pqcd calculations. the direct photon yield is shown to scale with the number of nucleon-nucleon collisions for all centralities.
experimental and theoretical challenges in the search for the quark gluon plasma : the star collaboration's critical assessment of the evidence from rhic collisions. we review the most important experimental results from the first three years of nucleus-nucleus collision studies at rhic, with emphasis on results from the star experiment, and we assess their interpretation and comparison to theory. the theory-experiment comparison suggests that central au+au collisions at rhic produce dense, rapidly thermalizing matter characterized by: (1) initial energy densities above the critical values predicted by lattice qcd for establishment of a quark-gluon plasma (qgp); (2) nearly ideal fluid flow, marked by constituent interactions of very short mean free path, established most probably at a stage preceding hadron formation; and (3) opacity to jets. many of the observations are consistent with models incorporating qgp formation in the early collision stages, and have not found ready explanation in a hadronic framework. however, the measurements themselves do not yet establish unequivocal evidence for a transition to this new form of matter. the theoretical treatment of the collision evolution, despite impressive successes, invokes a suite of distinct models, degrees of freedom and assumptions of as yet unknown quantitative consequence. we pose a set of important open questions, and suggest additional measurements, at least some of which should be addressed in order to establish a compelling basis to conclude definitively that thermalized, deconfined quark-gluon matter has been produced at rhic.
multiplicity and pseudorapidity distributions of photons in au+au collisions at $\sqrt{s_{nn}}$ = 62.4 gev. we present the first measurement of multiplicity and pseudorapidity distributions of photons in the pseudorapidity region 2.3 $\le$ $\eta$ $\le$ 3.7 for different centralities in au + au collisions at $\sqrt{s_{nn}}$ = 62.4 gev. we find that the photon yield in this pseudorapidity range scales with the number of participating nucleons at all collision centralities studied. the pseudorapidity distribution of photons, dominated by neutral pion decays, has been compared to those of identified charged pions, photons, and inclusive charged particles from heavy ion and nucleon-nucleon collisions at various energies. the photon production in the measured pseudorapidity region has been shown to be consistent with the energy and centrality independent limiting fragmentation scenario.
distributions of charged hadrons associated with high transverse momentum particles in pp and au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. charged hadrons in 0.15 &lt; pt &lt; 4 gev/c associated with particles of pt^trig &gt; 4 gev/c are reconstructed in pp and au+au collisions at sqrt(s_nn)=200 gev. the associated multiplicity and pt magnitude sum are found to increase from pp to central au+au collisions. the associated pt distributions, while similar in shape on the near side, are significantly softened on the away side in central au+au relative to pp and not much harder than that of inclusive hadrons. the results, consistent with jet quenching, suggest that the away-side fragments approach equilibration with the medium traversed.
probing bulk properties and partonic collectivity via multi-strange baryons in au+au collisions at top rhic energy. null
double helicity asymmetry in inclusive midrapidity$\pi^0$ production for polarized p+p collisions at $\sqrt s $=200 gev. we present a measurement of the double longitudinal spin asymmetry in inclusive pi^0 production in polarized proton-proton collisions at sqrt(s)=200 gev. the data were taken at the relativistic heavy ion collider with average beam polarizations of 26%. the measurements are the first of a program to study the longitudinal spin structure of the proton, using strongly interacting probes, at collider energies. the asymmetry is presented for transverse momenta 1-5 gev/c at mid-rapidity, where next-to-leading order perturbative quantum chromodynamic (nlo pqcd) calculations describe the unpolarized cross section well. the observed asymmetry is small and is compared with a nlo pqcd calculation with a range of polarized gluon distributions.
neutron and light-charged-particle productions in proton-induced reactions on $^{208}$pb at 62.9 mev. neutrons and light charged particles produced in 62.9mev proton-induced reactions on 208pb were measured during a single experiment performed at the cyclone facility in louvain-la-neuve (belgium). two independent experimental set-ups were used to extract double differential cross-sections for neutrons, protons, deuterons, tritons, 3he and alpha-particles. charged particles were detected using a set of si- si- csi telescopes from 25° to 155°, by step of 10 degrees. neutrons were measured using shielded demon counters, liquid ne213 scintillators, at 24°, 35°, 55°, 80° and 120°. these data allowed the determination of angle differential, energy differential and total production cross-sections. a comparison with theoretical calculations (mcnpx, fluka and talys) has been performed. it shows that the neutron and proton production rates are well predicted by mcnpx, using the incl4 option. all the other codes underestimate the neutron production whereas they overestimate the proton one. for composite particles, which represent 17% of the charged particle total reaction cross-section, neither the shape nor the amplitude of the cross-sections are correctly predicted by the models.
radio detection of cosmic ray air shower by the codalema experiment. the possibilities of measuring extremely high energy cosmic rays (ehecr) by radio detection of electromagnetic pulses radiated during the development of extensive air showers in the atmosphere are investigated. we present the demonstrative codalema experiment, set up at nancay radio-observatory (france). the radio-decametric array has been adapted to measure radio transients in time coincidence between antennas.
new prospects on particle detection with a parallel ionization multiplier (pim). we report on discharge rate measurements performed with parallel ionization multiplier (pim) detectors in a 10 gev hadron beam. various pim configurations with two amplification gaps are considered, and compared to a micromegas-like single-gap configuration. when the two stages of amplification are placed directly on top of one another, the discharge rate is found to be systematically greater than in the single-gap case, whatever the electric fields are in the two stages. moreover, the rate is strongly correlated with the total thickness of the amplification gap. however, when the two amplification stages are separated by a transfer gap, the discharge rate can be reduced by up to a factor of 50 at a gain of 5000 with a ne+10%co2 gas mixture.
vibration–displacement measurements based on a polarimetric extrinsic fibre fabry–perot interferometer. a polarization-based extrinsic fibre fabry–perot interferometric sensor for the measurement of velocity and displacement of a vibrating target is demonstrated in this work. it operates on the principle of dual interference within a single interferometric or 'sensing' arm which, in addition, provides a sense of direction of the moving target. this has been made possible by the introduction of a retarding film positioned along the optical path between the sensing fibre end and the intended target. two sets of interference signals are thus propagated and detected along one fibre arm to give robust and repeatable displacement–velocity information which has been found to be relatively insensitive to other external effects such as minor temperature changes. the sensor has been found to be capable of measuring the desired velocity information of an inclined target while typical excitation frequencies investigated ranged from ~ 2 to 60 hz. experimental results from a sensor configuration employing a polarization-maintaining (panda type) fibre as the sensing arm and operated at 1310 nm are presented.
parallel ionization multiplier (pim): application of a new concept of gaseous structure to tracking detectors. a new amplifying structure has been developed for beta imaging detectors: the parallel ionization multiplier (pim). pim is made of a thin sandwich of two metallic micromeshes separated by a new laser machined insulating spacer. this structure allows high gain (105) with a good efficiency and a two-dimensional spatial resolution below 50 small mu, greekm without strong discharge limitation. the capabilities of the pim are now investigated in order to be used by high-energy physics experiments. a prototype of a tracking detector for charged particles using the pim structure will be described. thanks to the very small avalanche extension, the use of a highly segmented anode is possible. this detector would support high rate running conditions of hadron experiment.
extrinsic fibre fabry-perot interferometer for vibration and displacement measurement: the benefit of polarization decomposition. we have successfully developed a new design of an extrinsic fibre fabry-perot interferometer (efpi) sensor dedicated to the characterization of vibration and displacement of a target. this device, based on a low finesse fabry-perot cavity formed by the end of a 'sensing' optical fibre and the target, gives information on the direction of the motion without the use of an additional reference arm. the incoming light, emitted by a 1310 nm laser diode, is decomposed according to two orthogonal polarization orientation inside the cavity. the two resulting interference signals are then carried back by the same optical fibre and sent to two photodiodes via a coupler and a polarizing beam splitter. with a relativelysimple signal processing, a precision of lambda/4 is achieved for the measurement of the displacement, for which the direction is also extracted. in addition, one can determine the velocity of the motion, that have been successfully compared with a reference sensor. the use of a polarization maintaining fibre as sensing arm, not mandatory formonitored laboratory set-up, allows the use of this sensor principle even with external perturbation (temperature changes, mechanical stress...).
$\beta$-imaging with the pim device. the $\beta$ autoradiography is an imaging technique which allows the two dimensional localisation of the distribution of a molecule labelled with 3h or 14c in tissue sample. in collaboration with the biospace mesures company, we developed a new micropattern gaseous detector called parallel ionization multiplier (pim) dedicated to the $\beta$ imaging. the images obtained show characteristics in adequation with the autoradiography requirements as a 2d spatial resolution (fwhm) of click to view the mathml source (respectively click to view the mathml source) and an efficiency of 85% (respectively 50%) for 3h (respectively 14c).
j/psi production in au-au collisions at sqrt(s_nn) = 200 gev at the relativistic heavy ion collider. first results on charm quarkonia production in heavy ion collisions at the relativistic heavy ion collider (rhic) are presented. the yield of j/psi's measured in the phenix experiment via electron-positron decay pairs at mid-rapidity for au-au reactions at sqrt(s_nn) = 200 gev are analyzed as a function of collision centrality. for this analysis we have studied 49.3 million minimum bias au-au reactions. we present the j/psi invariant yield dn/dy for peripheral and mid-central reactions. for the most central collisions where we observe no signal above background, we quote 90% confidence level upper limits. we compare these results with our j/psi measurement from proton-proton reactions at the same energy. we find that our measurements are not consistent with models that predict strong enhancement relative to binary collision scaling.
mid-rapidity neutral pion production in proton-proto collisions at sqrt(s_nn) = 200 gev. the invariant differential cross section for inclusive neutral pion production in p+p collisions at sqrt(s_nn) = 200 gev has been measured at mid-rapidity |eta| &lt; 0.35 over the range 1 &lt; p_t &lt;~ 14 gev/c by the phenix experiment at rhic. predictions of next-to-leading order perturbative qcd calculations are consistent with these measurements. the precision of our result is sufficient to differentiate between prevailing gluon-to-pion fragmentation functions.
azimuthal anisotropy of photon and charged particle emission in pb+pb collisions at 158 a gev/c. the azimuthal distributions of photons and charged particles with respect to the event plane are investigated as a function of centrality in pb + pb collisions at 158 a gev/c in the wa98 experiment at the cern sps. the anisotropy of the azimuthal distributions is characterized using a fourier analysis. for both the photon and charged particle distributions the first two fourier coefficients are observed to decrease with increasing centrality. the observed anisotropies of the photon distributions compare well with the expectations from the charged particle measurements for all centralities.
spin asymmetries for events with high p_t hadrons in dis and an evaluation of the gluon polarization. we present a measurement of the longitudinal spin cross section asymmetry for deep inelastic muon-nucleon interactions with two high transverse momentum hadrons in the final state. two methods of event classification are used to increase the contribution of the photon gluon fusion process to above 30%. the most effective one, based on a neural network approach, provides the asymmetries a_p(ln-&gt;lhhx)=0.030+/-0.057+/-0.010 and a_d(ln-&gt;lhhx)=0.070+/-0.076+/-0.010. from these values we derive an averaged gluon polarization delta(g)/g=-0.20+/-0.28+/-0.10 at an average fraction of nucleon momentum carried by gluons eta=0.07.
letter of intent for double-chooz: a search for the mixing angle theta13. tremendous progress has been achieved in neutrino oscillation physics during the last few years. however, the smallness of the $\t13$ neutrino mixing angle still remains enigmatic. the current best constraint comes from the chooz reactor neutrino experiment $\s2t13 &lt; 0.2$ (at 90% c.l., for $\adm2=2.0 10^{-3} \text{ev}^2$). we propose a new experiment on the same site, double-chooz, to explore the range of $\s2t13$ from 0.2 to 0.03, within three years of data taking. the improvement of the chooz result requires an increase in the statistics, a reduction of the systematic error below one percent, and a careful control of the cosmic ray induced background. therefore, double-chooz will use two identical detectors, one at $\sim$150 m and another at 1.05 km distance from the nuclear cores. the plan is to start data taking with two detectors in 2008, and to reach a sensitivity of 0.05 in 2009, and 0.03 in 2011.
a micro-canonical description of hadron production in proton-proton collisions. a micro-canonical treatment is used to study particle production in pp collisions. first this micro-canonical treatment is compared to some canonical ones. then proton, antiproton and pion 4(\pi) multiplicities from proton-proton collisions at various center of mass energies are used to fix the micro-canonical parameters (e) and (v). the dependences of the micro-canonical parameters on the collision energy are parameterised for the further study of pp reactions with this micro-canonical treatment.
micro-canonical hadron production in pp collisions. we apply a microcanonical statistical model to investigate hadron production in pp collisions. the parameters of the model are the energy e and the volume v of the system, which we determine via fitting the average multiplicity of charged pions, protons and antiprotons in pp collisions at different collision energies. we then make predictions of mean multiplicities and mean transverse momenta of all identified hadrons. our predictions on nonstrange hadrons are in good agreement with the data, the mean transverse momenta of strange hadron as well. however, the mean multiplicities of strange hadrons are overpredicted. this agrees with canonical and grandcanonical studies, where a strange suppression factor is needed. we also investigate the influence of event-by-event fluctuations of the e parameter.
a novel mechanism of h^0 di-baryon production in proton-proton interactions from parton based gribov-regge theory. a novel mechanism of h^0 and strangelet production in hadronic interactions within the gribov-regge approach is presented. in contrast to traditional distillation approaches, here the production of multiple (strange) quark bags does not require large baryon densities or a qgp. the production cross section increases with center of mass energy. rapidity and transverse momentum distributions of the h^0 are predicted for pp collisions at e_lab = 160 agev (sps) and \sqrt s = 200 agev (rhic). the predicted total h^0 multiplicities are of order of the omega-baryon yield and can be accessed by the na49 and the star experiments.
production of theta(1540) and xi pentaquark states in proton-proton interactions. the production of strange pentaquark states (e.g. theta baryons and xi^-- states) in hadronic interactions within a gribov-regge approach is explored. in this approach the theta^+(1540) and the xi pentaquark are produced by disintegration of remnants formed by the exchange of pomerons between the two protons. we predict the rapidity and transverse momentum distributions as well as the 4 pi multiplicity of the theta^+, xi^--, xi^-, xi^0 and xi^+ for sqrt{s} = 17 gev (sps) and 200 gev (rhic). for both energies more than 10^{-3} theta^+ and more than 10^{-5} xi per pp event should be observed by the present experiments.
production of pentaquark states in pp collisions within the microcanonical ensemble. the microcanonical statistical approach is applied to study the production of pentaquark states in pp collisions. we predict the average multiplicity and average transverse momentum of theta^{+}(1540) and xi(1860) and their antiparticles at different energies.
proton-proton and deuteron-gold collisions at rhic. we try to understand recent data on proton-proton and deuteron-gold collisions at rhic, employing a modified parton model approach.
ultra-high energy cosmic rays: some general features, and recent developments concerning air shower computations. we present an introductory lecture on general features of cosmic rays, for non-experts, and some recent developments concerning cascade equations for air shower developments.
first results of fast one-dimensional hybrid simulation of eas using conex. a hybrid simulation code is developed that is suited for fast one-dimensional simulations of shower profiles, including fluctuations. it combines the monte carlo simulation of high energy interactions with a fast numerical solution of cascade equations for the resulting distributions of secondary particles. results obtained with this new code, called conex, are presented and compared to corsika predictions.
pion interferometry in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. we present a systematic analysis of two-pion interferometry in au+au collisions at $\sqrt{\mathrm{s}_{_{\mathrm{nn}}}}$ = 200 gev using the star detector at rhic. we extract the hbt radii and study their multiplicity, transverse momentum, and azimuthal angle dependence. estimates of the geometrical and dynamical structure of the freeze-out source are extracted by fits with blast wave parameterizations. the expansion of the source and its relation with the initial energy density distribution is studied.
fission barriers in the quasi-molecular shape path. null
phenix high p(t) results. in heavy ion collisions, the high pt observables convey information about the initial conditions of the system under study. the suppression of high pt neutral pion and charged hadrons yields with respect to the nucleon-nucleon binary scaled p-p collisions and the particle composition in au+au collisions (as measured by the phenix experiment during the first two runs of the relativistic heavy ion collider (rhic)) are briefly reviewed. finally, the newest results from the rhic run 3 d+au, which disentangle initial and final state effects, are presented.
centrality dependence of charm production from a measurement of single electrons in au+au collisions at sqrt(s_nn) = 200 gev. the phenix experiment has measured mid-rapidity transverse momentum spectra (0.4 &lt; p_t &lt; 4.0 gev/c) of single electrons as a function of centrality in au+au collisions at sqrt(s_nn) = 200 gev. contributions to the raw spectra from photon conversions and dalitz decays of light neutral mesons are measured by introducing a thin (1.7% x_0) converter into the phenix acceptance and are statistically removed. the subtracted ``non-photonic'' electron spectra are primarily due to the semi-leptonic decays of hadrons containing heavy quarks (charm and bottom). for all centralities, charm production is found to scale with the nuclear overlap function, t_aa. for minimum-bias collisions the charm cross section per binary collision is n_cc^bar/t_aa = 622 +/- 57 (stat.) +/- 160 (sys.) microbarns.
systematic studies of the centrality and sqrt(s_nn) dependence of de_t/deta and dn_ch/deta in heavy ion collisions at mid-rapidity. the phenix experiment at rhic has measured transverse energy and charged particle multiplicity at mid-rapidity in au+au collisions at sqrt(s_nn) = 19.6, 130 and 200 gev as a function of centrality. the presented results are compared to measurements from other rhic experiments, and experiments at lower energies. the sqrt(s_nn) dependence of de_t/deta and dn_ch/deta per pair of participants is consistent with logarithmic scaling for the most central events. the centrality dependence of de_t/deta and dn_ch/deta is similar at all measured incident energies. at rhic energies the ratio of transverse energy per charged particle was found independent of centrality and growing slowly with sqrt(s_nn). a survey of comparisons between the data and available theoretical models is also presented.
formation of dense partonic matter in relativistic nucleus-nucleus collisions at rhic: experimental evaluation by the phenix collaboration. extensive experimental data from high-energy nucleus-nucleus collisions were recorded using the phenix detector at the relativistic heavy ion collider (rhic). the comprehensive set of measurements from the first three years of rhic operation includes charged particle multiplicities, transverse energy, yield ratios and spectra of identified hadrons in a wide range of transverse momenta (p_t), elliptic flow, two-particle correlations, non-statistical fluctuations, and suppression of particle production at high p_t. the results are examined with an emphasis on implications for the formation of a new state of dense matter. we find that the state of matter created at rhic cannot be described in terms of ordinary color neutral hadrons.
production of $\phi$ mesons at mid-rapidity in sqrt(s_nn) = 200 gev au+au collisions at relativistic energies. we present the first results of meson production in the k^+k^- decay channel from au+au collisions at sqrt(s_nn) = 200 gev as measured at mid-rapidity by the phenix detector at rhic. precision resonance centroid and width values are extracted as a function of collision centrality. no significant variation from the pdg accepted values is observed. the transverse mass spectra are fitted with a linear exponential function for which the derived inverse slope parameter is seen to be constant as a function of centrality. these data are also fitted by a hydrodynamic model with the result that the freeze-out temperature and the expansion velocity values are consistent with the values previously derived from fitting single hadron inclusive data. as a function of transverse momentum the collisions scaled peripheral.to.central yield ratio rcp for the is comparable to that of pions rather than that of protons. this result lends support to theoretical models which distinguish between baryons and mesons instead of particle mass for explaining the anomalous proton yield.
saturation of azimuthal anisotropy in au + au collisions at sqrt(s_nn) = 62 - 200 gev. new measurements are presented for charged hadron azimuthal correlations at mid-rapidity in au+au collisions at sqrt(s_nn) = 62.4 and 200 gev. they are compared to earlier measurements obtained at sqrt(s_nn) = 130 gev and in pb+pb collisions at sqrt(s_nn) = 17.2 gev. sizeable anisotropies are observed with centrality and transverse momentum (p_t) dependence characteristic of elliptic flow (v_2). for a broad range of centralities, the observed magnitudes and trends of the differential anisotropy, v_2(p_t), change very little over the collision energy range sqrt(s_nn) = 62-200 gev, indicating saturation of the excitation function for v_2 at these energies. such a saturation may be indicative of the dominance of a very soft equation of state for sqrt(s_nn) = 62-200 gev.
are vertical cosmic rays the most suitable to radio detection ?. the electric field induced by extensive air showers generated by high energy cosmic rays is considered and, more specifically, its dependence on the shower incident angle. it is shown that for distances between the shower axis and the observation point larger than a few hundred meters, non-vertical showers produce larger fields than vertical ones. this may open up new prospects since, to some extent, the consideration of non-vertical showers modifies the scope of the radio-detection domain.
pseudorapidity asymmetry and centrality dependence of charged hadron spectra in $d+au$ collisions at $\sqrt{s_{nn}}$ = 200 gev. null
nuclear modification factors for hadrons at forward and backward rapidities in deuteron-gold collisions at $\sqrt s_{nn}$ = 200 gev. we report on charged hadron production in deuteron-gold reactions at $\sqrt s_{nn}$ = 200 gev. our measurements in the deuteron-direction cover 1.4 &lt; $\eta$ &lt;$ 2.2, referred to as forward rapidity, and in the gold-direction -2.0$ &lt;$\eta$ &lt;$ -1.4, referred to as backward rapidity, and a transverse momentum range p$_t$ = 0.5-4.0 gev/c. we compare the relative yields for different deuteron-gold collision centrality classes. we observe a suppression relative to binary collision scaling at forward rapidity, sensitive to low momentum fraction (x) partons in the gold nucleus, and an enhancement at backward rapidity, sensitive to high momentum fraction partons in the gold nucleus.
fluctuations and correlations in star. null
measurement of jet modification at rhic. null
full jet reconstruction in d+au and p+p collisions at rhic. null
azimuthal anisotropy: the higher harmonics. null
elliptic flow of multistrange baryons $\xi$ and $\omega$ in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. null
open charm yields in 200 gev p+p and d+au collisions at rhic. null
high-$p_t$ electron distributions in d+au and p+p collisions at rhic. null
identified particle dependence of nuclear modification factors in d+au collisions at rhic. null
non-identical particle correlations in 130 and 200 a gev collisions at star. null
measurement of open charm production in d+au collisions at $\sqrt{s_{nn}}$ = 200 gev. null
highlights from star. null
radiodetection of cosmic ray extensive air showers: upgrade of the codalema experiment. we present the characteristics and performance of a demonstration experiment devoted to the observation of ultra high- energy cosmic ray extensive air showers using a radiodetection technique. in a first step, one antenna narrowed band filtered acting as trigger, with a 4 sigma threshold above sky background-level, was used to tag any radio transient in coincidence on the antenna array. recently, the addition of 4 particle detectors has allowed us to observe cosmic ray events in coincidence with antennas.
measurements of transverse energy distributions in au+au at $\sqrt{s_{nn}}$ = 200 gev. null
the dynamics of the quasielastic $^{16}o$(e,e'p) reaction at $q^2$ $\approx$ 0.8 (gev/c)$^2$. the physics program in hall a at jefferson lab commenced in the summer of 1997 with a detailed investigation of the 16o(e,e'p) reaction in quasielastic, constant (q,w) kinematics at q^2 ~ 0.8 (gev/c)^2, q ~ 1 gev/c, and w ~ 445 mev. use of a self-calibrating, self-normalizing, thin-film waterfall target enabled a systematically rigorous measurement. differential cross-section data for proton knockout were obtained for 0 &lt; emiss &lt; 120 mev and 0 &lt; pmiss &lt; 350 mev/c. these results have been used to extract the alt asymmetry and the rl, rt, rlt, and rl+tt effective response functions. detailed comparisons of the data with relativistic distorted-wave impulse approximation, relativistic optical-model eikonal approximation, and relativistic multiple-scattering glauber approximation calculations are made. the kinematic consistency of the 1p-shell normalization factors extracted from these data with respect to all available 16o(e,e'p) data is examined. the q2-dependence of the normalization factors is also discussed.
bose-einstein correlations of charged pion pairs in au+au collisions at $\sqrt s_{nn}$=200 gev. bose-einstein correlations of identically charged pion pairs were measured by the phenix experiment at midrapidity in au + au collisions at $\sqrt s_{nn}$=200 gev. the bertsch-pratt radius parameters were determined as a function of the transverse momentum of the pair and as a function of the centrality of the collision. using the standard core-halo partial coulomb fits, and a new parametrization which constrains the coulomb fraction as determined from the unlike-sign pion correlation, the ratio rout/rside is within 0.8–1.1 for 0.25$&lt;$kt$&lt;$1.2 gev/c. the centrality dependence of all radii is well described by a linear scaling in n, and rout/rside for kt$\sim$0.45 gev/c is approximately constant at unity as a function of centrality.
rapidity and centrality dependence of proton and antiproton production from $^{197}$au + $^{197}$au collisions at $\sqrt{s_{nn}}$ = 130 gev. null
production of $e^+e^-$ pairs accompanied by nuclear dissociation in ultraperipheral heavy-ion collisions. null
hadronization geometry and charge-dependent two-particle correlations on momentum subspace ($\eta, \phi$) in au-au collisions at $\sqrt{s_{nn}}$ = 130 gev. we present the first measurements of charge-dependent two-particle correlations on momentum-space difference variables $\eta_1 - \eta_2$ (pseudorapidity) and $\phi_1 - \phi_2$ (azimuth) for primary charged hadrons with transverse momentum $0.15 \leq p_t \leq 2$ gev/$c$ and $|\eta| \leq 1.3$ from au-au collisions at $\sqrt{s_{nn}} = 130$ gev. we observe correlation structures not predicted by theory but consistent with evolution of hadron emission geometry with increasing centrality from one-dimensional fragmentation of color strings to higher-dimensional fragmentation of a hadron-opaque bulk medium.
azimuthal anisotropy and correlations at large transverse momenta in $p+p$ and au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. results on high transverse momentum charged particle emission with respect to the reaction plane are presented for au+au collisions at $\sqrt{s_{_{nn}}}$= 200 gev. two- and four-particle correlations results are presented as well as a comparison of azimuthal correlations in au+au collisions to those in $p+p$ at the same energy. elliptic anisotropy, $v_2$, is found to reach its maximum at $p_t \sim 3$ gev/c, then decrease slowly and remain significant up to $p_t\approx 7$ -- 10 gev/c. stronger suppression is found in the back-to-back high-$p_t$ particle correlations for particles emitted out-of-plane compared to those emitted in-plane. the centrality dependence of $v_2$ at intermediate $p_t$ is compared to simple models based on jet quenching.
azimuthal anisotropy in au+au collisions at $\sqrt{s_{nn}} = 200 gev. the results from the star collaboration on directed flow ($v_1$), elliptic flow ($v_2$), and the fourth harmonic ($v_4$) in the anisotropic azimuthal distribution of particles from au+au collisions at $\sqrtsnn = 200$ gev are summarized and compared with results from other experiments and theoretical models. results for identified particles are presented and fit with a blast wave model. for $v_2$, scaling with the number of constituent quarks and parton coalescence is discussed. for $v_4$, scaling with $v_2^2$ and quark coalescence predictions for higher harmonic flow is discussed. the different anisotropic flow analysis methods are compared and nonflow effects are extracted from the data.
centrality and pseudorapidity dependence of charged hadron production at intermediate $p_t$ in au + au collisions at $\sqrt{s_{nn}}$= 130gev. null
entrance channels and alpha decay half-lives of the heaviest elements. the barriers standing against the formation of superheavy elements and their consecutive $\alpha$ decay have been determined in the quasimolecular shape path within a generalized liquid drop model including the proximity effects between nucleons in a neck, the mass and charge asymmetry, a precise nuclear radius and the shell effects given by the droplet model. for moderately asymmetric reactions double-hump potential barriers stand and fast fission of compact shapes in the outer well is possible. very asymmetric reactions lead to one hump barriers which can be passed only with a high energy relatively to the superheavy element energy. then, only the emission of several neutrons or an $\alpha$ particle can allow to reach an eventual ground state. for almost symmetric heavy-ion reactions, there is no more external well and the inner barrier is higher than the outer one.
kaon production and kaon to pion ratio in au + au collisions at $\sqrt{s_{nn}}$ = 130 gev. null
nuclear dynamics with the finite-range gogny force. null
nuclear dynamics with the (finite range) gogny force : flow effects. null
evidence for persisting mean field effects at e/a = 60 mev from particle-particle correlation measurements and theoretical investigations with the landau vlasov equation. null
temperature measurements at backward angles in $^{40}$ar induced reactions on ag at e/a=44mev. null
evidence for stopping in heavy ion collisions from a study of hard photon source velocities. null
hard photon as a probe to study dissipation mechanisms. null
event by event measurement of  of photons in s + au collisions at 200 a gev. null
$\delta^{++}$ production in 158 a gev $^{208}$pb+$^{208}$pb interactions at the cern sps. null
collective flow and hbt in pb+pb collisions at the cern-sps. null
elliptic emission of $k^+$ in 158a gev pb+pb collisions. null
elliptic emission of k$^+$ and $\pi^+$ in 158a.gev pb+pb collisions. null
central pb+pb collisions at 158 a gev/c studied by $\pi^-\pi^-$ interferometry. null
exclusive production of pion pairs in $\gamma\gamma$ collisions at large q$^2$. null
localized charged-neutral fluctuations in 158a gev pb+pb collisions. null
direct photons in wa98. null
event-by-event fluctuations in particle multiplicities and transverse energy produced in 158a gev pb+pb collisions. null
measurement of single electrons and implications for charm production in au+au collisions at $\sqrt {s_{nn}}$ = 130 gev. transverse momentum spectra of electrons from au+au collisions at sqrt[snn] = 130 gev have been measured at midrapidity by the phenix experiment at the relativistic heavy ion collider. the spectra show an excess above the background from photon conversions and light hadron decays. the electron signal is consistent with that expected from semileptonic decays of charm. the yield of the electron signal dne/dy for pt&gt;0.8 gev/c is 0.025±0.004(stat)±0.010(syst) in central collisions, and the corresponding charm cross section is 380±60(stat)±200(syst) μb per binary nucleon-nucleon collision.
centrality dependence of $\pi^{+/-}, k^{+/-}, p$ and $\bar p$ production from $\sqrt s_{nn}$ = 130 gev au+au collisions at rhic. null
midrapidity neutral-pion production in proton-proton collisions at $\sqrt{s}$ = 200 gev. the invariant differential cross section for inclusive neutral pion production in p+p collisions at sqrt(s_nn) = 200 gev has been measured at mid-rapidity |eta| &lt; 0.35 over the range 1 &lt; p_t &lt;~ 14 gev/c by the phenix experiment at rhic. predictions of next-to-leading order perturbative qcd calculations are consistent with these measurements. the precision of our result is sufficient to differentiate between prevailing gluon-to-pion fragmentation functions.
centrality dependence of charged-neutral particle fluctuations in 158a gev $^{208}$pb+$^{208}$pb collisions. null
centrality dependence of the high $p_t$ charged hadron suppression in au+au collisions at $\sqrt{s_{nn}}$ = 130 gev. null
cross sections and transverse single-spin asymmetries in forward neutral-pion production from proton collisions at $\sqrt{s} $ = 200 gev. null
azimuthally sensitive hanbury brown-twiss interferometry in au + au collisions at $\sqrt{s_{nn}}$ = gev. null
radio detection of cosmic ray extensive air showers: present status of the codalema experiment. data acquisition and analysis for the codalema experiment, in operation for more than one year, has provided improved knowledge of the characteristics of this new device. at the same time, an important effort has been made to develop processing techniques for extracting transient signals from data containing interference.
nucleon-induced reactions at intermediate energies: new data at 96 mev and theoretical status. double-differential cross sections for light charged particle production (up to a = 4) were measured in 96 mev neutron-induced reactions, at the tsl laboratory cyclotron in uppsala (sweden). measurements for three targets, fe, pb, and u, were performed using two independent devices, scandal and medley. the data were recorded with low-energy thresholds and for a wide angular range (20°–160°). the normalization procedure used to extract the cross sections is based on the np elastic scattering reaction that we measured and for which we present experimental results. a good control of the systematic uncertainties affecting the results is achieved. calculations using the exciton model are reported. two different theoretical approaches proposed to improve its predictive power regarding the complex particle emission are tested. the capabilities of each approach is illustrated by comparison with the 96 mev data that we measured, and with other experimental results available in the literature.
new adsorbents from oil shales: preparation characterization and u, th isotope adsorption tests. null
virtual compton scattering in the resonance region up to the deep inelastic region at backward angles and momentum transfer squared of $q^2$ = 1.0 $gev^2$. we have made the first measurements of the virtual compton scattering process via the e p -&gt; e p gamma exclusive reaction at q**2 = 1 gev**2 in the nucleon resonance region. the cross section is obtained at center of mass (cm) backward angle, theta_gamma_gamma*, in a range of total (gamma* p) cm energy w from the proton mass up to w = 1.91 gev. the data show resonant structures in the first and second resonance regions, and are well reproduced at higher w by the bethe-heitler+born cross section, including t-channel pi0-exchange. at high w, our data, together with existing real photon data, show a striking q**2 independence. our measurement of the ratio of h(e,e'p)gamma to h(e,e'p)pi0 cross sections is presented and compared to model predictions.
measurement of the generalized polarizabilities of the proton in virtual compton scattering at $q^2=0.92 and 1.76 gev^2$: i. low energy expansion analysis. null
measurement of the generalized polarizabilities of the proton in virtual compton scattering at $q^2=0.92 and 1.76 gev^2$: ii. dispersion relation analysis. null
measurement of the generalized polarizabilities of the proton in virtual compton scattering at $q^2$ = 0.92 and 1.76 gev$^2$. we report a virtual compton scattering study of the proton in hall a at the thomas jefferson national accelerator facility at low cm energies. we have determined the structure functions $p_{ll}-p_{tt}/\epsilon$ and $p_{lt}$, and the electric and magnetic generalized polarizabilities (gp) $\alpha_e(q2)$ and $\beta_m(q2)$ at momentum transfer q2= 0.92 and 1.76 gev2. all these observables show a strong fall-off with q2, and neither the electric nor magnetic gp follows a simple dipole form.
identified charged particle spectra and yields in au + au collisions at$\sqrt{^snn}$=200 gev. null
high-$p_t$ charged hadron suppression in au-au collisions at $\sqrt s_{nn}$=200 gev. null
$j/\psi$ production from proton-proton collisions at $\sqrt s$=200 gev. null
azimuthal anisotropy at the relativistic heavy ion collider: the first and fourth harmonics. null
backward electroproduction of $\pi^0$ mesons on protons in the region of nucleon resonances at four momentum transfer squared $q^2 = 1.0 gev^2$. exclusive electroproduction of pi0 mesons on protons in the backward hemisphere has been studied at q2=1.0 gev2 by detecting protons in the forward direction in coincidence with scattered electrons from the 4 gev electron beam in jefferson lab's hall a. the data span the range of the total (gamma*p) center-of-mass energy w from the pion production threshold to w=2.0 gev. the differential cross sections sigmat+epsilonsigmal, sigmatl, and sigmatt were separated from the azimuthal distribution and are presented together with the maid and said parametrizations.
scaling properties of proton and antiproton production in $\sqrt{s_{nn}}$=200 gev au+au collisions. null
shape isomerism of rotating $^{44}$ti and $^{48}$cr. null
hindas-a european nuclear data program for accelerator-driven systems. null
cross section data and kerma coefficients for 95 mev neutrons for medical applications. null
be, li, he and h decay half-lives at low excitation energy. null
photon flow in 158 a gev pb+pb collisions. null
new results for hadronic collisions in the framework of the parton-based gribov-regge theory. null
the cross sections between charmonia and comovers within pqcd. null
transfer of eu(iii) associated with polymaleic acid to bacillus subtilis. null
what determines the $k^-$ multiplicity at energies around (1-2)a gev?. null
constraints on models for proton-proton scattering from multistrange baryon data. null
stability of rotating $^{44}$ti, $^{56}$ni, and $^{126}$ba nuclei in the fusionlike deformation path. null
alpha and light nucleus emission within a generalized liquid drop model. null
physics of event generators. null
baryon production in proton-proton collisions. null
hot hadronic matter with the njl model. null
2d localization using resistive strips associated to the micromegas structure. null
consistent treatment of soft and hard processes in hadronic interactions. null
energy loss and x(2) scaling breakdown in j / psi nuclear production. null
the emitting source: can it be determined by the hbt correlation function in ultrarelativistic heavy ion collisions?. null
multiplicity of different hadrons in e+ e- , p p, and a a collisions. null
multistrange baryon production from identical pomerons in proton-proton collisions. null
probing hadronization with strangeness. null
is the existence of a softest point in the directed flow excitation function an unambiguous signal for the creation of a quark gluon plasma?. we investigate the claim that a minimum of the excitation function of the in-plane directed flow of nucleons in ultrarelativistic heavy-ion collisions is a ‘smoking gun' signature for the creation of a quark–gluon plasma (qgp). employing a non-equilibrium transport approach we demonstrate that such a minimum is expected in heavy ion collisions and not related to the formation of a qgp. the minimum has its origin in the interplay between a decreasing scattering angle of the elastic collisions with increasing beam energy, which lowers the in-plane flow, and the onset of particle production which increases the in-plane flow. thus, the interpretation of this minimum as a ‘smoking gun' signature for the creation of a qgp seems premature.
transverse-mass dependence of two-pion correlations in au+au collisions at $\sqrt s_{nn}$ = 130 gev. null
overview of phenix results from the first rhic run. null
thermal bremsstrahlung probing the thermodynamical state of multifragmenting systems. null
thermal bremsstrahlung photons probing the nuclear caloric curve. null
first results from rhic-phenix. null
event reconstruction in the phenix central arm spectrometers. null
suppression of hadrons with large transverse momentum in central au+au collisions at $\sqrt s_{nn}$= 130gev. transverse momentum spectra for charged hadrons and for neutral pions in the range 1 gev/c.
precipitation of technetium by subsurface sulfate-reducing bacteria. null
micro-organism effects on radionuclides migration. null
multiplicity distributions and charged-neutral fluctuations. null
the production of k$^+$ mesons in heavy ion reactions. null
initial condition for quark-gluon plasma evolution. null
analysis of kaon production around the threshold. null
heavy-quarkonium hadron interaction from a leading twist qcd analysis. null
the highly deformed nucleus $^{40}$ca in the fusionlike deformation valley. null
entrance and exit channels for very heavy and superheavy elements. null
cluster and alpha radioactivities and asymmetric fission. null
fission through quasi-molecular shapes and fragmentation. null
deformation valleys through quasi-molecular and toroidal shapes. null
overpopulation of $\bar \omega$ in $pp$ collisions: a way to distinguish statistical hadronization from string dynamics. null
thermodynamics of the three-flavor nambu-jona-lasinio model: chiral symmetry breaking and color superconductivity. null
strange resonance production: probing chemical and thermal freeze-out in relativistic heavy ion collisions. null
fusion and alpha emission within a liquid drop model and heaviest element formation and decay. null
transverse mass distributions of neutral pions from $^{208}$pb-induced reactions at 158.a gev. null
deuteron and triton production with high energy sulphur and lead beams. null
one-, two- and three-particle distributions from central pb+pb collisions at 158a gev/c. null
event texture search for critical fluctuations in pb+pb collisions. null
on the formation and alpha decay of superheavy elements. null
dissipative and fluctuating effects in nuclear dynamics with a wavelet representation. null
l-dependent potential barriers and superdeformed states. null
asymmetric fission for $^{70,76}$se and $^{90,94,98}$mo via quasimolecular shapes and related formulas. null
parton-based gribov-regge theory. null
first-order dissolution rate law and the role of surface layers in glass performance assessment. null
one and two-dimensional analysis of 3$\pi$ correlations measured in pb+pb interactions. null
suppression of charmonia in nuclear matter. null
performance of a gas avalanche pixel detector of 50 mu m pitch. null
on the role of energy conservation in high-energy nuclear scattering. we argue that the most commonly used models for nuclear scattering at ultra-relativistic energies do not treat energy conservation in a consistent fashion. demanding theoretical consistency as a minimal requirement for a realistic model, we provide a solution for the above-mentioned problem, the so-called 'parton-based gribov-regge theory'. in order to keep a clean picture, we do not consider secondary interactions. we provide a very transparent extrapolation of the physics of more elementary interactions towards nucleus-nucleus scattering, without considering any nuclear effects due to final state interactions. in this sense we consider our model a realistic and consistent approach to describe the initial stage of nuclear collisions.
self-consistency requirement in high-energy nuclear scattering. null
scaling of particle and transverse energy production in $^{208}$pb+$^{208}$pb collisions at 158 a.gev. null
transverse flow of nuclear matter in collisions of heavy nuclei at intermediate energies. null
dynamical effects on the quasiprojectile temperature in the ar+al reaction. null
coulomb interaction of pions in central pb+pb collisions at 158 agev. null
light nucleus emission within a generalized liquid-drop model and quasimolecular shapes. null
analytic expressions for the proximity energy, the fusion process and the alpha emission. null
thermodynamics-a valuable approach to multifragmentation?. null
simulated annealing clusterization algorithm for studying the multifragmentation. null
multiboson effects in multiparticle production. null
anti-deuteron and kaon production in pb+pb collisions. null
coalescence, interferometry and flow. null
three-pion interferometry results from central pb+pb collisions at 158a gev/c. null
observation of direct photons in central 158a gev $^{208}$pb+$^{208}$pb collisions. null
recent results from the wa98 experiment. null
search for disoriented chiral condensates in 158.a gev pb+pb collisions. null
an intelligent instrument for tracking and adaptive filtering of oscillatory signals using hebbian learning rules. null
external modulation: an emerging technology in hybrid fibre coaxial networks. null
isomerism and novel magnetic order in mn$_{13}$ cluster. null
magnetic ordering and granularity effects in la$_{1-x}$ba$_x$mno$_3$. null
onset of binary processes: a new observable for the in-medium nn cross section. null
raman scattering characterization of si(100) implanted with mega-electron-volt sb. null
a new approach to nuclear collisions at rhic energies. null
contribution of evanescent waves to the far field: the atomic point of view. null
quantum-mechanical description of ionization, capture, and excitation in proton collisions with atomic oxygen. null
charmonium cross section from p-a production. null
fusion and alpha emission within a liquid drop model and superheavy element formation and decay. null
energy conservation in modeling ultrarelativistic heavy ion collisions. null
energy sharing in ultrarelativistic nuclear collisions. null
conceptual problems in simulations of nucleus-nucleus collisions. null
determination of radium-226 in aqueous solutions by alpha spectrometry. null
presentation on the recent results in high energy heavy ion physics and on the problem on the quark-gluon plasma formation. null
alice off-line organization. null
photon physics at rhic. null
alpha emissions as an asymmetric fission and superheavy element decay. null
fusion, fission and alpha emission within a liquid drop model and quasi-molecular shapes. null
states of $^{76}$ge via (pp') inelastic scattering at 22 mev. null
spectroscopy of $^{28}$p levels using the ($^3$he, t) reaction. null
investigation of states in $^{30}$p via the $^{30}$si($^3$he,t)$^{30}$p reaction at 30 mev. null
elastic and inelastic scattering of 22 mev protons from natural even-even zn and ge isotopes - 1 - spectroscopy of the $^{64,66}$zn isotopes. null
coulomb displacement energy calculations for the a = 28, t = 1 triad. null
timescale of particle emission using nuclear interferometry. null
states of $^{74}$ge via the (pp') inelastic scattering at 22 mev. null
search for nuclear structure dependence of analyzing powers for the (tp) reaction in medium mass nuclei. null
octupole modes of oscillation via analyzing power measurements. null
fusion excitation function of the $^{40}$ca + $^{40}$ca system close to the threshold. null
fission following capture reactions of $^{32}$s + $^{208}$pb. null
analyzing power measurements for two-nucleon transfer reactions in a shape transitional nucleus. null
alpha-particle transfer to 0+ states in the germanium nuclei and the role of proton pairing correlations. null
three dissipative regimes in heavy ion reactions. null
the fusion process in landau-vlasov dynamics. null
semi-classical dynamics of heavy-ion reactions. null
semi-classical approaches to the phase space evolutions in intermediate energy heavy ion collisions. null
semiclassical approaches to proton emission in intermediate-energy heavy-ion reactions. null
quantal fluctuations and invariant operators for a gneeral time-dependent harmonic oscillator. null
quantal dynamics of charge equilibration in damped nuclear collisions. null
peripheral reactions at intermediate energies in landau-vlasov dynamics. null
geometry and dynamics of a zero-temperature fermi-gas model for preequilibrium emission of nucleons with application to $^{16}$o + $^{93}$nb at $e_{lab}$ = 204 mev. null
general properties of gausson-conserving descriptions of quantal damped motion. null
fast nucleon emission: a probe of the heavy ion reactions at energies below 100 mev/u. null
fast fission phenomenon deep inelastic reactions and compound nucleus formation described within a dynamical macroscopic model. null
fast fission phenomenon. null
fast fission: a dissipate mechanism following sometimes fusion. null
damping of the wave packet motion in a general time-dependent quadratic field. null
constants of motion and non-stationary wave functions for the damped time-dependent harmonic oscillator. null
cold production of fast nucleons in central heavy-ion reactions. null
coherent state propagation in open systems. null
coherent production of fast nucleons in heavy-ion reactions. null
cluster jets as a possible alpha emission mechanism in intermediate energy heavy ion collisions. null
ternary fission through compact and creviced shapes. null
symmetrical fusion of heavy ions around the coulomb barrier energy. null
static and dynamic fusion barriers in heavy-ion reactions. null
relaxed-density potential and fusion cross section saturation for light and medium nuclei. null
on the projectile fragmentation in heavy-ion reactions at intermediates energies. null
on the fission barrier of heavy and superheavy nuclei. null
fission processes through compact and creviced shapes. null
a geometrical model for the fusion of identical nuclei. null
selection of violent collisions by neutron calorimetry for interferometry measurements. null
search for lifetime effects in the decay of an evaporative source using two particle correlations. null
hard photon interferometry in the reaction xe + au at 44 a mev. null
hard photon intensity interferometry in heavy ion collisions. null
two-proton correlation function measured at very small relative momenta. null
hyperdeformation of rotating nulei within the generalized liquid-drop model. null
on the production and decay of hot nuclei. null
mean-field dissipation and fluctuations in heavy-ion reactions. null
lorentz-covariant description of intermediate energy heavy-ion reactions in relativistic quantum molecular dynamics. null
the quantum molecular dynamics approach. null
total cross sections for the pp $\pi^0$ channel in the inelastic pp interaction. null
times scales for spinodals decomposition in nuclear matter with pseudoparticle models. null
subthreshold pion production in nucleus-nucleus collisions within the quantum molecular dynamics approach. null
sub-barrier fusion of $^{64}$ni + $^{100}$mo. null
study of energy deposition in heavy-ion reactions. null
semiclassical simulation of sudden nucleus scission with two-body collisions. null
relaxed-density potential of deformed u + u nuclei. null
relaxed and quasi-projectile fragments in heavy-ion reactions. null
prolate deformations induced by the proximity forces in the scission region. null
potential surfaces in symmetric heavy-ion reactions. null
on the symmetric fragmentation barrier at finite temperature. null
on the plane fragmentation barriers. null
on the fission of $^{56}$ni and $^{48}$cr rotating nuclei. null
on the energy shape dependences of ellipsoidal leptodermous systems. null
on nuclear ternary fission. null
observation of lifetime effects in two-proton correlations for well-characterized sources. null
nuclear matter with a pseudo-particle model : static bulk and surface properties. null
multifragmentation near the threshold. null
mass dependence of the disappearance of flow in nuclear collisions. null
mass dependence of pion production in heavy ion collisions near but below threshold. null
isospin effects on dynamics of heavy-ion collisions. null
impact-parameter-selected two-proton intensity interferometry for $^{36}$ar + $^{45}$sc at e/a = 80 mev. null
hyperdeformation in $^{152}$dy at very high spins. null
high energy gamma ray production in proton-induced reactions at 104, 145 and 195 mev. null
fusion barrier lowering induced by nuclear deformations. null
from stochastic phase-space evolution to brownian motion in collective space. null
from fission to scattering in the $^{100}$mo (18.7 mev/u) + $^{100}$mo reaction within a microscopic dynamic approach. null
fluctuations of one-body observables. null
fluctuations in one-body dynamics. null
fission barrier of projectiles in heavy-ion reactions. null
dynamics of hot rotating nuclei. null
deexcitation of single excited nuclei in the qmd model. null
cosmic rays in taps. null
comparison between the symmetric fission and fusion paths. null
classical and quantum approach to light-particle correlations in intermediate-energy heavy-ion reactions. null
binary and ternary fission of hot and rotating nuclei. null
study of the resistive plate chambers for the alice dimuon arm. the trigger system for the alice dimuon arm will be based on resistive plate chambers. an rpc prototype, with electrodes made of low resistivity bakelite (rho ~ 3.10^9 ? cm) has been tested both at the sps and at the gif. the results for operation in streamer mode are presented here.
influence of temperature and humidity on bakelite resistivity. the use of phenolic or melaminic bakelite as rpc electrodes is widespread. the electrode resistivity is an important parameter for the rpc performance. as recent studies have pointed out, the bakelite resistivity changes with temperature and is influenced by humidity. in order to gain a quantitative understanding on the influence of temperature and humidity on rpc electrodes, we assembled an apparatus to measure resistivity in well-controlled conditions. a detailed description of the experimental set-up as well as the first resistivity measurements for various laminates in different environmental conditions are presented.
centrality dependence of neutral pion production in 158 a gev $^{208}pb +^{208}pb$ collisions. the production of neutral pions in 158agev pb+pb collisions has been studied in the wa98 experiment at the cern sps. transverse momentum spectra are studied for the range 0.3 gev/c mt-m0 4.0 gev/c. the results for central collisions are compared to various models. the centrality dependence of the neutral pion spectral shape and yield is investigated. an invariance of the spectral shape and a simple scaling of the yield with the number of participating nucleons is observed for centralities with greater than about 30 participating nucleons which is most naturally explained by assuming an equilibrated system.
influence of the coulomb field on charged particle emission in ar + ni reaction at 77 mev/u. ar+ni collisions at 77 mev/u were studied in the experiment e286 performed at ganil. an important advantage of this experiment was an application of the neutron detector demon for registration of both neutral and charged particles. this feature allows to compare characteristics of neutrons and protons detected by the same detector and gives a possibility to determine the influence of the coulomb field on the proton emission. estimation of a charge of the emitting source was performed by comparing energy spectra of neutrons and protons detected under identical experimental conditions. the experimental results were compared with the prediction of the simon model.
$\rho^0$ production and possible modification in au+au and p+p collisions at $\sqrt{s_{nn}}$ = 200 gev. null
particle-type dependence of azimuthal anisotropy and nuclear modification of particle production in au + au collisions at $\sqrt{s_{nn}}$ = 200 gev. null
an update from star--using strangeness to probe relativistic heavy ion collisions. null
three-pion hanbury brown-twiss correlations in relativistic heavy-ion collisions from the star experiment. null
$\rho$(770)$^0$ and $f_0$(980) production in au+au and pp collisions at $\sqrt{s_{nn}}$ = 200 gev. null
$\lambda$ (1520) and $\sigma$(1385) resonance production in au+au and p+p collisions at rhic ($\sqrt{s_{nn}}$ = 200 gev). null
strange and multi-strange particle ratios in p+p reactions at $\sqrt{s}$ = 200 gev at rhic. null
widths of the balance function for charged kaon pairs from au+au at $\sqrt{s_{nn}}$ = 200 gev at rhic. null
charged and neutral kaon correlations in au-au collisions at $\sqrt{s_{nn}}$ = 200 gev using the solenoidal tracker at rhic (star). null
strange particle spectra from $\sqrt{s}$ = 200 gev p + p collisions in star at rhic. null
probing thermal freeze-out conditions via multi-strange baryons in au + au collisions at $\sqrt{s_{nn}}$ = 130 gev. null
nuclear modification of identified strange particles at moderate $p_t$ in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev at rhic. null
charged strange and non-srange particle ratios as a function of $p_t$ and centrality in star at rhic. null
data on light-fragment correlations in $^{40}$ar + $^{58}$ni at 77 mev/nucleon. null
transverse-momentum and collision-energy dependence of high-$p_t$ hadron suppression in au + au collisions at ultrarelativistic energies. null
erratum: midrapidity antiproton-to-proton ratio from au + au collisions at $\sqrt{s_{nn}}$ = 130 gev [phys.rev.lett.86,4778(2001)]. null
evidence from d + au measurements for final-state suppression of high-p$_t$ hadrons in au + au collisions at rhic. null
coherent vector meson production in ultra-peripheral heavy-ion collisions at star. null
$\xi^-$ and $\overline{\xi}^+$ baryon production in au + au collisions at $\sqrt{s_{nn}}$ = 130 gev. null
correlations, fluctuations and flow measurements from the star experiment. null
elastic neutron scattering at 96 mev from $^{12}$c and $^{208}$pb. null
alice addentum to the technical design report of the time of flight system (tof). null
hydrogen isotope double differential production cross sections induced by 62.7 mev neutrons on a lead target. null
light charged particle production cross sections in 62.9 mev proton induced reactions on $^{208}$pb. null
suppression of soft nuclear bremsstrahlung in proton-nucleus collisions. photon energy spectra up to the kinematic limit have been measured in 190 mev proton reactions with light and heavy nuclei to investigate the influence of the multiple-scattering process on the photon production. relative to the predictions of models based on a quasifree production mechanism, a strong suppression of bremsstrahlung is observed in the low-energy region of the photon spectrum. we attribute this effect to the interference of photon amplitudes due to multiple scattering of nucleons in the nuclear medium.
pion interferometry of $\sqrt{s_{nn}}$ = 130 gev au + au collisions at rhic. null
a facility for measurements of nuclear cross sections for fast neutron cancer therapy. null
two-neutron correlations at small relative momenta in $^{40}$ar + $^{197}$au collisions at 60 mev/nucleon. null
possible dynamical limitations to excitation energy storage in nuclei. null
mass identification in heavy ion collection with the spectrometer soleno. null
the star time projection chamber. null
heavy ion colomb excitation and gamma decay studies of the one and two phonon giant dipole resonances in $^{208}$pb and $^{209}$bi. null
subthreshold pion dynamics as a source for hard photons beyond proton-neutron bremsstrahlung in heavy in heavy-ion collisions. null
dynamics of violent collisions induced by ar on ag between 27 and 60 mev/nucleon: persistance of binary dissipative collisions and temperature limits. null
two-photon correlations: from young experiments to heavy-ion collision dynamics. null
high transverse momentum proton emission in ar+ta collisions at 94 mev per nucleon. null
directed collective flow and azimuthal distributions in $^{36}$ar+$^{27}$al collisions from 55 to 95 mev per nucleon. null
experimental search for coherent subthreshold pion production. null
photon production in heavy-ion collisions close to the pion threshold. null
coulomb instability in collisions between very heavy nuclei around 30 mev per nucleon. null
two-neutron interferometry measurements. null
properties of very hot nuclei formed in $^{64}$zn + $^{nat}$ti collisions at intermediate energies. null
dynamical analysis of dissipative collisions between ar and ag nuclei in the fermi domain. null
hard photons and neutral pions as probes of hot and dense nuclear matter. null
evidence for thermal equilibration in multifragmentation reactions probed with bremstrahlung photons. null
elliptic flow of identified hadrons in au+au collisions at $\sqrt{s_{nn}}$=200 gev. null
the star silicon strip detector (ssd). null
star detector overview. null
measurements of photon and $\pi^0$ production in heavy ion collisions at rhic. null
measurement of source chaoticity for particle emission in au + au collisions at $\sqrt{s}_{nn}}$ = 130 gev using 3-particle hbt correlations. null
non-identical particle correlation analysis as a probe of transverse flow. null
$\omega^-$ and $\overline{\omega}^+$ production in au + au collisions at $\sqrt{s_{nn}}$ = 130 and 200 gev. null
$\rho(770)^0$, k$^{\star}(892)^0$ and f$_0$(980) production in au-au and pp collisions at $\sqrt{s_{nn}}$ = 200 gev. null
mid-rapidity $\pi^{\pm}, k^{\pm}$, and $\overline{p}$ spectra and particle ratios from star. null
soft physics in star. null
narrowing of the balance function with centrality in au + au collisions at $\sqrt{s_{nn}}$ = 130 gev. null
phenix detector overview. null
charged hadron spectra in phenix. null
two particle azimuthal correlation measurements in phenix. null
charged particle angular correlations from leading photons at rhic. null
high $p_t$ identified hadron ratios in $\sqrt{s_{nn}}$ = 200 gev au + au collisions. null
measurement of the neutral pion cross section in proton-proton collisions at $\sqrt{s}$ = 200 gev with phenix. null
single leptons from heavy-flavor decays at rhic. null
$j/\psi \rightarrow ee$ and $j/\psi \rightarrow \mu\mu$ measurements in au au and $pp$ collisions at $\sqrt{s_{nn}}$ = 200 gev. null
charge fluctuations at mid-rapidity in au + au collisions in the phenix experiment at rhic. null
identified charged particle azimuthal anisotropy in phenix at rhic. null
two-particle correlations measured by phenix in au-au collisions at $\sqrt{s_{nn}}$ = 200 gev. null
source parameters from identified hadron spectra and hbt radii for au-au collisions at $\sqrt{s_{nn}}$ = 200 gev in phenix. null
charge particle multiplicity and transverse energy measurements in au-au collisions in phenix at rhic. null
disappearance of back-to-back high-p$_t$ hadron correlations in central au + au collisions at $\sqrt{s_{nn}}$ = 200 gev. null
azimuthal anisotropy and correlations in the hard scattering regime at rhic. null
centrality dependence of charged particle multiplicity in au-au collisions at $\sqrt s_{nn}$ = 130 gev. null
$k^*(892)^0$ production in relativistic heavy ion collisions at $\sqrt{s_{nn}}$=130gev. null
coherent $\rho^0$ production in ultraperipheral heavy ion collisions. null
centrality dependence of high $p_t$ hadron suppression in au+au collisions at $\sqrt{s_{nn}}$=130 gev. null
leptonic observables in ultra-relativistic heavy ion collisions. null
results on identified hadrons from the phenix experiment at rhic. null
high p$_{t}$ measurements from phenix. null
elliptic flow from two- and four-particle correlations in au+au collisions at $\sqrt{s_{nn}}$=130 gev. null
azimuthal anisotropy of k$^0_s$ and $\lambda + \overline{\lambda}$ production at midrapidity from au + au collisions at $\sqrt{s_{nn}}$ = 130 gev. null
strangeness in au + au collisions at $\sqrt{s_{nn}}$ = 130 gev observed with the star detector. null
strangeness production at rhic. null
results from the star experiment. null
midrapidity antiproton-to-proton ratio from au + au collisions at $\sqrt{s_{nn}}$ = 130 gev. null
complexation and luminescence spectroscopic studies of europium (iii) with polymaleic acid. null
measurements of sideward flow around the balance energy. null
production tests of microstrip detector and electronic frontend modules for the star and alice trackers. null
tab bonded ssd module for the star and alice trackers. null
emission time scale of light particles in the system xe+sn at 50 a.mev - a probe for dynamical emission ?. null
multiplicity distribution and spectra of negatively charged hadrons in au+au collisions at $\sqrt{s_{nn}}$=130 gev. null
topology analysis for the identification of energetic photons in a segmented electromagnetic calorimeter. null
test of spatial resolution of micromegas detectors. null
detection of charged pions and protons in the segmented electromagnetic calorimeter taps. null
new method for the discrimination of single-source events in heavy-ion collisions. this paper introduces a new method for the selection of central single-source events, based on classical multivariate techniques. the resulting discriminating variable is shown to be valid for different hypotheses on the nuclear source deexcitation mechanism. it enables the selection of events which are representative of the whole set of single-source events. application to the ni+ni at 32a mev system measured with the indra multidetector has allowed the determination of the fusion probability as a function of the impact parameter and the evaluation of the corresponding cross section.
study of resistive plate chambers for the alice dimuon spectrometer. null
first results for two-nucleon interferonmetry experiment, e286, at ganil. null
measurement of double differential cross sections for light charged particles production in neutron induced reactions at 62.7 mev on lead target. null
a low-resistivity rpc for the alice dimuon arm. in view of alice, the dedicated heavy-ion experiment at lhc, a resistive plate chamber (rpc) with electrodes made of low-resistivity bakelite ( rho approximately=3.5*10/sup 9/ omega cm) has been tested at the cern sps both in streamer and in avalanche mode. the chamber has shown a stable behaviour and excellent rate capability: its efficiency is better than 95% for local particle fluxes of about 1 and 10 khz/cm/sup 2/ for operation in streamer and in avalanche mode, respectively. the cluster size and the time resolution have also been measured for both modes of operation. (22 refs).
response of a resistive plate chamber to particles leaking laterally from a thick absorber. null
alpha emission and spontaneous fission through quasi-molecular shapes. null
macroscopic-microscopic energy of rotating nuclei in the fusion-like deformation valley. null
two-nucleon correlations at small relative velocities in heavy ion collisions. null
two kaon correlations from pb + pb collisions at 160 a gev from na44. null
deuteron and triton production in pb + pb collisions at 158 a gev. null
two-particle correlations in 158a gev collisions. null
first results from two-nucleon interferometry experiment, e286, at ganil. null
hadronization and strangeness production in a chirally symmetric nonequilibrium model. null
the alice dimuon trigger: overview and electronics prototypes. alice is the lhc experiment (2005) dedicated to the study of heavy ion collisions. amongst the alice sub-detectors, the muon spectrometer will investigate the dimuon production from heavy resonance (j/psi, gamma) decays, which is believed to be a promising signature of the qgp (quark gluon plasma) formation. for maximum efficiency of the spectrometer, a dedicated dimuon trigger is presently built. the detector partis itself based on rpcs operated in streamer mode and is the topic of another contribution to this conference. this paper gives the principle and the simulated performances of the trigger and is also focussed on the description of the electronics prototypes and future developments. the rpcs are read-out by x and y orthogonal strips: the front-end chips are presently developed. the signals are sent to the trigger electronics which basically performs a pt cut on the tracks to reduce the background. a prototype of fast (decision time 200 ns) programmable electronics working in a pipelined mode at 40 mhz has been built and tested. this prototype handles simultaneously 160 digital information from the strips. the tests of the trigger card have required the construction of a pattern generator (160 bits at 40 mhz).
the trigger of the alice dimuon arm: architecture and detectors. the trigger system of the alice dimuon arm is based on resistive plate chambers (rpc). besides a short description of the trigger system, the test results of an rpc prototype with electrodes made of low resistivity bakelite (~3.109cm) are presented. rate capability, time resolution and cluster size have been measured for the rpc operated both in streamer and in avalanche mode. although the rate capability is obviously higher in avalanche mode (few khz/cm2), remarkable results have been achieved even in streamer mode (several hundreds of hz/cm2).
a unified treatment of high-energy interactions. null
apparent temperatures in hot quasi-projectiles and the caloric curve. null
complexation of uranium (vi) with humic acid at low metal ion concentrations by indirect speciation methods. null
beyond mean field confrontation of different models with high transverse momentum proton spectra. null
geochemical assessment of actinide isolation in a german salt repository environment. null
uranium removal onto chitosan : competition with organics substances. organic materials and especially humic substances are often present in natural waters. the influence of organic compounds on uranium removal onto chitosan is studied. the ph of the media is fixed at 6. in operating conditions, uranium major species present in solution are : (uo2)(3)(oh)(5)(+) and (uo2)(4)(oh)(7)(+). capacity of fixation is found to be 450 mg g(-1), considering precipitation phenomena. experimental results show that chitosan offers a selective adsorption depending on the different organics used: the adsorption capacity is maximum with benzaldehyde, average with humic acids, limited with phenol and insignificant with benzoic acid. phenol does not affect the fixation capacity of uranium significantly whereas benzoic add and benzaldehyde have an average effect upon it and humic acids decrease it to 200 mg g(-1) and less as the concentration of the acid increases.
gas avalanche pixel detector with amorphous silicon carbide overcoating. null
performance of micrograp gas chambers fabricated with selected anode metals. null
m+/h+ ion exchange behavior of the phosphoantimonic acids hnsbnp2o3n+5. xh20 (n=1,3) for m=cs and other alkali metal ions. null
evidence for dynamical proton emission in peripheral xe + sn collisions at 50 mev/u. relative angle correlation functions for mid-rapidity protons built up from 50 mev/u xe+sn data recorded by the indra multidetector exhibit the characteristics of a non-equilibrated emission: they show an energy dependent anisotropy which cannot be accounted for by standard evaporation processes; a possible explanation lies in the dynamical origin of these protons as suggested by a landau-vlasov simulation.
selective actinide solvent extraction used in conjunction with liquid scintillation counting. null
recent results from the cern lead beam experiments : how close have we come to the quark-gluon-plasma ?. in a pragmatic view of media communication, we present a new method for studying cognitions during reception (labeled ecer method) to go beyond some theoretical and methodological limitations of dual-process theories of attitude formation and change. this implementation of real-time process-tracking operationalizes cognitive elaboration and cues that set processing in motion. it enables picking up viewers' cognitive responses during exposure to the message and analysing them with a cognitivo-discursive analysis (tropes software). in a first application, the ecer method is integrated into an experiment on the role of the viewer's degree of involvement in cognitive processing. the results show that the more the viewers are involved, the more they engage in cognitive elaboration and the more they develop cognitive strategies of discourse that express a “ real world ”. inconsistent with current models, we show that involvement does not result in processing one type of cues (peripheral or central) more than another.
the dywan model and the description of transport phenomena in nuclei. title: the influence of advertising without message recall: the implicit effects of brand mere exposure ; abstract: the experiment reproduces the advertising influence conditions when the consumers, at the time to buy, do not remember the messages seen very quickly, several days before. by mobilizing the concepts of implicit attitude and implicit memory, this research studies the implicit effects of mere exposure of new brand, seven days later, when the subjects forgot to have been beforehand exposed. when the subjects think that they see the brand for the first time, the mere exposure makes positive the initially moderate evaluations. the positive and negative initially attitudes become more accessible. if one says to the subjects that they already saw the brand during advertising messages, these effects disappear. the discussion explain that the model of the misattribution of the perceptive fluidity cannot explain to him only the results. those underline that the brand negative evaluations would be implicitly memorized.
fission, cluster emission and hyperdeformation at high angular momentum in the fusion-like deformation valley. the multimedia information systems, in the context of a generalized communication, have to become an effective and relevant tool in the service of the social world. a first modelling of their conception as their realization looked in the daytime to the method of territorial intelligence catalyse. the hypothesis founder of this research is that it is possible to translate concretely the contribution of the sciences of information and communication in the process of methodological and technological transfer necessary for the development of catalyse observatories, by combining them in theories stemming from the other disciplines (didactics, cognitive psychology, sociology, geography...). from the statement of a situation where sic seem practically absent in methods of territorial intelligence, i operate in this research an empirical evaluation of the development of several catalyse observatories: - state of the relations between the territorial intelligence and the sciences of information and communication - analyses of a corpus of territorial experiments at the european level - modelling of the contributions of the sciences of information and the communication in the catalyse method and in the procedures of transfers this research succeeds concretely to propose procedures of transfer and educational tools organized in an optics of improvement of the conception of the systems of production and access to information. the procedures of transfer must be estimated and redefined in researches to come, also by taking into account evolutions of the society, and researches in sciences of information and communication.
nuclear clusters in entrance and exit channels. null
the use of the n, n-diethyldodecanamide for actinide extractionin conjunction with liquid scintillation counting. if simplified, every information retrieval problem can be solved when the information need implied by its expression has been identified. we are interested in the criteria used in realising a good information retrieval problem expression. we have listed these criteria through some principles and maxims which first characterized the communication between two persons are applied. we choose to use the gricean maxims because they are the most favoured for this type of situation. secondly, we have tried to identify some others principles that can be used to realise a good information retrieval problem expression. the principles by grice can not resolve all forms of error associated with this particular form of communication. in our work, we defined three other principles namely: adhesion principle, reformulation principle, memorization principle. we give some examples of situations where the principles we have formulated are not applicable and the consequences. we present the possible applications of our new model: mirabel, which can help in the description of information retrieval problem from. it also compels its user to use essential good expression principle implicitly.
production of $\pi^0$ and $\eta$ mesons in carbon-induced relativistic heavy-ion collisions. null
luminescence kinetics of pbwo4 scintillator crystals. this article particularly proposes an approach of the communication of the total marks while being interested in those exploiting their strong capacity symbolic system. it tries to explain their success while reconsidering the history of the concept of advertising brand and their strategies to conclude on their current step. it proposes finally a certain number of research orientations opened by this object.
low pt phenomena in a + a and p + a collisions at midrapidity. null
formation and stability of coloids under simulated near field conditions. null
long-term disposal of waste packages with spent fuel in gorleben salt. null
on the influence of baryonic resonances on the dynamics of kaons : prepared for workshop on the structure of mesons, baryons, and nuclei, cracow. electricité de france, the french electricity provider is moving into new markets, and needs to know very well its clients and tracks their satisfaction. thus, different kinds of text mining tools were tested in order to analyze a huge quantity of heterogeneous textual documents, including mails, open-ended customer survey questions, discussion forums, and comments contained in large databases concerning customer contacts. in this context, it seemed unavoidable to build a test grid, in order to facilitate the comparison between different tools. this paper describes the test grid carrying out, the software selection, the way the evaluation of four tools (alceste, sas text miner, temis insight discoverer and spad/crm) was achieved and the results.
coulomb correlations for interferometry analysis of expanding hadron systems. the extraction of relevant information in texts constitutes a fundamental process of text mining. we realize this process with a linguistic platform (ilc) for recognition and extraction of terms and their linguistic variants from corpora. we present a methodology to enhance the recognition of syntactic term variation in english using syntactic and morphosyntactic features. those modifications contribute to improve filtering of the variants and to assist the expert in validating indexation.
ph dependency of hlw glass dissolution and sorption behaviour of eu, th and u on glass corrosion products. the instructions put together below fall into three categories. the publisher would be grateful to authors for respecting these indications. the length of this summary may attain a dozen lines. it is to be written in size 9 italic times. an abstract in french will be joined.
source terms for performance assessement of hlw glass and spent fuel as waste forms. kernel methods have recently been introduced to solve natural language processing and text mining problems. kernels define a generalised similarity measure between objects of arbitrary structure, with three interesting properties, namely the ability to incorporate prior knowledge about the problem, the implicit mapping of the data into a new feature space, which allows for very richer representation and where problem solving is easier, and finally the independence of learning algorithms from the dimension of this new feature space (—the kernel trick“). these properties, coupled with robust learning algorithms (for classification, clustering, dimension reduction, filtering, ...) provide some remarkable results in text mining tasks, such as document categorization, concept clustering, word sense disambiguation, information extraction, relationship extraction and automatic multilingual lexicon extraction.
different nn cross sections and multifragmentation. this paper reports our contribution to the poesia project, aimed at the automatic filtering of illicit contents on the internet. in this context, we present an original text classification system, based on information theory principles, which is able to detect and filter with a high accuracy pornographic texts.
neutral meson production in p-be and p-au collisions at 450 gev beam energy. we propose a "targeted" xml markup to computerize old dictionaries whose structure is "vague". it stands for a markup halfway between the "minimal" markup and the "analytic" markup already available. as regards this paper, it is meant to supply a brief survey of the use of the "ciblé" mark up through the development of a c++ application which can be used on a large number of xml corpuses.
systematic study of low-mass electron pair production in p-be and p-au collisions at 450 gev/c. null
microscopic models for ultrarelativistic heavy ion collisions. null
multistep production of $\eta$ and hard $\pi^0$ mesons in subthreshold au-au collisions. we present an unsupervised approach to cluster sequences. this method is inspired by topology learning methods for hidden markov models, and is built upon the definition of a distance between markov models. this type of technique may be used to learn markovian character models from data or to identify allographs or handwriting styles.
high energy pb + pb collisions viewed by pion interferometry. null
kaon and proton ratios from central pb + pb collisions at the cern sps. null
one, two and three particle hadron spectra : recent results from cern/sps experiment na44. null
proton and anti-proton distribution at midrapidity in proton nucleus and sulphur nucleus collisions. null
a novel approach to the oxidation potentiel of the tc (vii) tc (iv) couple in hydroclorique acid medium through the reaction. null
search for disoriented chiral condensates in 158 gev/ a pb pb collisions. null
multiplicity and pseudorapidity distributions of photons in s + au reactions at 200 gev. null
transverse momentum distributions of neutral pions from nuclear collisions at 2000 agev. null
nuclear temperature measurements with helium isotopes. null
sensitivity of two-fragment correlation functions to initial-state momentum correlations. null
isospin independence of the h-he double isotope ration. null
examining the cooling of hot nuclei. null
reaction mechanisms and multifragmentation processes in $^{64}$zn+$^{58}$ni at 35a-79 mev. null
cluster radioactivity and very asymmetic fission through compact and creviced shapes. null
analytic description of the fusion and fission processes through quasi-molecular shapes. null
domain walls in supersymmetric gauge theories. for an anthropology of communication what do we mean by “anthropology of communication”? the term is used for the ethnographic analysis of human communication as well as for behavioural changes due to the application of new information technologies. we put forward an introduction (below), to be followed by seven texts: three reflections on “anthropology”, three on “communication” and one concerned with the overlap between these two concepts. starting with december 2004, these papers will be accessible at the rate of one paper a month. our intention is to search for all terms associated with “anthropology” and “communication”, to create an inventory of representative semantic fields for them, to indicate current overlaps established by colleagues carrying out communications and information research as well as possible overlaps in the future.
physics of thermal qcd. null
the multifragmentation of spectator matter. null
contribution of $\pi^0$ and $\eta$ dalitz decays of the dilepton invariant-mass spectrum in 1a gev heavy-ion collisions. null
exclusive and meson production in ar + ca at 800 mev. null
can one extract source radi from transport theories. in this paper we define a digital document according to three levels of structure: its organization structure, its control structure and its interactivity structure. whereas these three structures exist inherently in every document, they are redefined when dealing with digital documents, since they can be abstracted and formalised, and consequently manipulated. we show how these three structures are implemented within the scenari publishing chain, which provided us a framework of application in real project.
multiplicity dependence of pion source size in heavy-ion collisions. the document concept commonly deals with data that documents contain, but rarely with (inter)actions that documents can handle. as a consequence, it is hard to specify how users can interact on documents : this is mainly done by dedicated applications from which users have to swap in order to achieve a single editing task. in this paper, we suggest the use of a model for interactive components based on three metaphors, the document, presentation and instrument. the proposed model aims at replacing the application concept by the document and interaction instrument concepts within interactive workplaces, thus resulting in a higher powerfulness / ease-of-use ratio.
collective expansion in hight energy heavy ion collisions. today, we more than ever need useful abstractions in order to reason on complex document transformations, to assert properties on document manipulation systems, and to inspire perhaps revolutionary approaches of document creation and processing. this paper sketches perspectives and proposes markers toward inventing the next document generation.
comment on search for direct photons from s + au collisions at 200 gev nucleon by the ceres collaboration. a multidisciplinary research work on electronic document was used in the call for this issue of the journal. it is summarized in this introduction, namely its three approaches on the form, the text and the medium. the eleven selected articles are briefly presented. they propose to develop different modeling, to point changing between traditional and electronic document and to show the change in progress on different fields. this work in progress have already showed real results and consequently must be continued.
azimuthal anisotropy of in s + au reactions at 200 a gev. internet provokes a double effect on rumors: it lets diffuse it more than ever and, in the same time, gives the power to control the occurrence of a suspected rumor. in this aim, some specialized or "reference websites" are available on the net. the text of this communication presents some of them: the first one, afu in 1991, was the faq of the newsgroup alt.folklore.urban. thereafter were hoaxbusters.ciac.org (1995) and snopes.com (1997), and a couple of "sequels" (urbanlegends.about.com, 1997; truthorfiction.com, 1998; hoaxbuster.com, 2000). finally, some questions will arouse from the fact that none of these websites seem to have any means of investigation, neither any routine method.
soft photon production in central collisions of 200 a gev s + au. null
dynamical aspects of particle emission in dissipative binary collisions : effects on hot-nuclei formation. null
effects on the mean-field dynamics and phase space geometry on the cluster formation. null
azimuthal 2 alpha correlations and projectile-residue distributions selected by neutron and charged-particle multiplicity measurement. null
how to measure which sort of particles was emetted earlier and which later. null
thermal energy analysis around the fermi energyin ar+ag reaction z. the steps of territorial intelligence are based on the emergence of new fashions of exchange within the territory . it acts thus on the territorial visibility and hustles the places of strategic reflexion; by doing this, it takes part within the country, to make move the bond sociétal. this paper subjects a posture of collection and mutualisation of information within the territory.
screening v.s confinement in 1 + 1 dimensions. null
structure constants of the d5, chiral, minimal model. what is the real nature of document changing during the digital revolution ? to answer, we need a "digital discours sociology", which analyse convergence between economics, sign and technics.
dynamical effects and intermediate mass fragment production in peripheral and semicentral collisions of xe + sn at 50 mev/nucleon. this article presents the results of a content analysis of magazines produced and distributed in quebec for female adolescents. the aim of the research was to examine how femininity and male-female relations are represented in such magazines. it validated the hypothesis that these magazines are conservative rather than egalitarian, and allows this media form to be seen as an agent of traditional socialization. distribution of the various articles (n=253) in the magazines by theme revealed that nearly two thirds (64.8%) deal with beauty, fashion, boys, heterosexual relations and male singers or actors. and while over one third (35.2%) discuss personal and social development, a careful reading of these articles revealed that the focus was on the personal rather than the social dimension of such development.
description of sub-barrier heavy ion fusion in a semi classical quantum tunneling model. information and communication technologies change the way people consider information and knowledge. their use in e-learning raises orientation problems while navigating within technological devices. first of all, the learner-subject must locate the tools and services at his disposal during all of the training. to help orientation, resorting to the spatial metaphor is frequent. this research is based on results of some empirical survey. it is the opportunity of questioning oneself as to the limits of this type of metaphor by studying mental representations that users may have. solutions could be brought by making spatial metaphor intervene more as a tool of information processing than as a tool of software localization.
energetic protons induced radioactivity in space detectors. null
comment on analysis of hard two-photon correlations measured in heavy-ion reactions at intermediate energies. f. fialkoff's has led us to wonder about the most widely borrowed novels in public libraries and how they are promoted. we analysed the best lending statistics in three libraries in lorraine (north-east france) and tried to identify the most frequently borrowed novels. this project led us to exchange ideas with librarians. we would like to find out about their presuppositions and thus, know more about libraries because we will know about librarians better. this study allows to compare how the librarians apprehend reading and how readers do. we would like to show the gap between the two representations.
importance of-one and two-body dissipation at intermediate energies studied by hard photons. null
linking dynamical and thermal models of ultrarelativistic nuclear scattering. the university of sud (toulon, france) is experimenting a socio-technical system aiming at reinforcing collaborative work among students. the present paper summarizes the theoretical foundations of such a system and explains how it works in practice and how network analysis can help interpreting operational data.
fragment formation in heavy ion collisions. null
midrapidity protons in 158 a gev pb + pb collisions. the author describes the present document delivery situation in france with particular reference to copyright and the eu directive, consortia, portals, funding and some suggestions for the future.
influence of the emitter coulomb field on two proton correlations function. this article uses the results of a thesis to analyse the changing structure of access services to scientific and technical information in french university libraries, and corresponding changes in the role of inter-library loan services. these changes have mainly occurred with the growing integration of electronic resources in collections and an overhaul of the services on offer. given the specific features of access systems to scientific information and the wide range of users in the various sections of university libraries, we will be attempting a detailed analysis of the situation in the french university network.
freeze out conditions in ultrarelativistic heavy-ion collisions. null
kaon spectra from p + be to pb + pb collisions. null
pestov spark counter prototype development for the cern lhc alice experiment. null
azimuthal correlations in the target fragmentation region of high energy nuclear collisions. null
a preshower photon multilicity detector for the wa93 experiment at cern. null
limits on the production of direct photons in 200 a gev s 32 + au collisions. null
extended pulse shape discrimination capabilities using a csi-bgo phoswich. null
compressibility probed by linear momentum transfer. null
real and virtual comptom scattering (experiments). null
elliptic flow : transition from out-of plane emission in au + au collisions by e895 collaboration. null
systematics of inclusive photon production in 158a gev pb induced reactions on ni, nb, and pb targets. null
elliptic of k+ and ii+ in 158a gev pb+pb collisions. null
deep-subthreshold n and n production probing pion dynamics in the reaction ar+ca at 180a mev. null
freeze-out parameters in central 158 a gev pb+pb collisions. null
the uniform description of hight energy hadron production mechanisms in the new venus model. null
on the origin of the radial flow in low energy heavy ion reactions. this article consists of a creative detour through the benjaminian argumentation in order to presenting certain artistic experiences and scientific research as a situational exercise of the aura, which is at work in the process of the perception and cognition of the infinite world by one unique subject.
on the flow of kaons produced in relativistic heavy ion collision. null
two proton correlations near midrapidity in p + pb and s + pb collisions at the cern - sps. this article consists of a creative detour through the benjaminian argumentation in order to presenting certain artistic experiences and scientific research as a situational exercise of the aura, which is at work in the process of the perception and cognition of the infinite world by one unique subject.
darboux method and search and invariants lotka volterra and complex quadratic system. null
persistence of collective fluctuations in n body metaequilibrium gravitationnal and plasma systems. the object of this research commanded by the european community was to understand the mechanisms and the efficiency of the discourses of prevention both explicit (officials campaigns toward the public) and others type of discourses which are broadcasted to inform people through the mass media in order to form professionals of health care. during several years it is realy an instrumentation of tv programs which appears bringing cognitive contributions and sharing emotionnals ressources with the tv audience. rather than the objective discourse of information brought by the classical tv journalism, an other type of speech mixing testimony and private advices has taken place, and apparently with more success. ß the turning point in public opinion of the scandal and a real change in the way to see the disease and the people. the event is amplified and the plight is constructed as nearer. it is no more the disease of other people (homosexual, toxicomane).
signature of geometrical effects in heavy ion collisions below 100 mev/u. null
exclusive electroproduction of vector mesons and transversity distributions. null
expansion and hadronization of a chirally symmetric quark-meson plasma. null
microorganisms as potential vectors of the migration of radionucleides. the migration of heavy metals and radionuclides depends on many factors of which the microbiological life. in this paper, we describe in a first step, the microorganisms-metals interaction processes and secondly, our works on biosorption. the important factors affecting the biosorption are the cell wall structure and the growth conditions. bacteria are shown to exhibit a fixation selectivity for some ions (th4+). finally we give some examples of the characterization of the binding sites.
normalized vacuum states in n=4 supersymmetric yang-mills quantum mechanics with any gauge group. null
microscopic calculations of stopping and flow from 160 a mev to 160-a gev. null
the formation of fragments in symmetric heavy ion collisions. the attempt to bring to light an urban press is indissociable of taking city (or "l'urbain") into account as global concept. from such context, it's a matter of question how far it's possible to perceive "urban" press discourses through some main discourse types, spotted at a global level. the instituting discourse approach privileged here is tested from french, brazilian and american papers examples, in different times.
rotating bubble and toroidal nuclei and fragmentation. the concepts of communicative space, media sphere and public sphere are sometimes used like synonyms one of the other. however, according to us, they are three different concepts: public sphere and media sphere are two distinct spaces symbolic systems which, both, are anchored in communicative space.
dynamical analysis of isospin and angular momentum effects in peripheral heavy-ion reactions. the institute for scientific and technical information (inist) is a service unit of the french national centre for scientific research (cnrs). a leading integrated scientific and technical information center, inist provides the major public research and academic institutions as well as the socio-economic sector with resources and services designed to improve dissemination of and access to international scientific and technical information. committed to the new information and communication technologies, inist offers a whole range of access services to scientific and technical information on the internet. the article highlights the place and the future of document supply in this context.
deformation energy of a toroidal nucleus and plane fragmentation barriers. the internet is a profound cultural revolution. yet, the importance of this radical change in our way of treating information has not yet entered mainstream intellectual discourse. this paper considers how researchers in the humanities can take internet seriously, by using it to broadcast their work but also to communicate with peers and public about it.
search for production of strangelets in quark matter using particle correlations. null
unlike particle correlations and the strange quark matter distillation process. the article looks back on the 183 workshops and briefing sessions of the 15 annual uksg conferences from 1990 to 2004. the content of this particular form of professional training reflects the development of the professional environment, interests and activities of librarians, especially the emergence of the digital library. nine major subjects of information and debate are identified: human resource management, new software, acquisition of e-serials, legal aspects, emerging standards, usage statistics, library/vendor relationship, document delivery, and publishing. an analysis of attendance and some remarks on specific features of the sessions complete this “historical study”.
search for an enhanced production of low energy pions in c($p, \pi^+)x$, cu($p, \pi^+)x$, and cu($p, \pi^0)x$ reactions for 300 mev $\leq t_p \leq$ 400 mev. null
tof : proposal for a neutron time of flight facility, european collaboration for high-resolution measurements of neutron cross sections between 1ev and 250 mev. null
nuclear interferometry for two-nucleon systems (experiment e286 at ganil). null
what can we learn on the nuclear equation of state from the inversion of the flow in heavy ion reactions ?. many different approaches to the geometric and statistical analysis of document layouts have been proposed in the literature. the development of practical branchand- bound algorithms for solving geometric matching problems under noise and uncertainty has enabled the formulation of new classes of geometric layout analysis methods based on globally optimal maximum likelihood interpretations for well-defined models of the spatial statistics of document images. i review this approach to geometric layout analysis using text line finding and column finding in the presence of noise and uncertainty as examples and compare the approach with selected other statistical and geometric layout analysis methods.
study of intermediate velocity products in the ar+ni collisions between 52 and 95 a.mev. intermediate velocity products in ar+ni collisions from 52 to 95 a.mev are studied in an experiment performed at the ganil facility with the 4$\pi$ multidetector indra. it is shown that these emissions cannot be explained by statistical decays of the quasi-projectile and the quasi-target in complete equilibrium. three methods are used to isolate and characterize intermediate velocity products. the total mass of these products increases with the violence of the collision and reaches a large fraction of the system mass in mid-central collisions. this mass is found independent of the incident energy, but strongly dependent on the geometry of the collision. finally it is shown that the kinematical characteristics of intermediate velocity products are weakly dependent on the experimental impact parameter, but strongly dependent on the incident energy. the observed trends are consistent with a participant-spectator like scenario or with neck emissions and/or break-up.
stopping and radial flow in central $^{58}$ni + $^{58}$ni collisions between 1a and 2a gev. the production of charged pions, protons and deuterons has been studied in central collisions of 58ni on 58ni at incident beam energies of 1.06, 1.45 and 1.93 agev. the dependence of transverse-momentum and rapidity spectra on the beam energy and on the centrality of the collison is presented. it is shown that the scaling of the mean rapidity shift of protons established for ags and sps energies is valid down to 1 agev. the degree of nuclear stopping is discussed; the iqmd transport model reproduces the measured proton rapidity spectra for the most central events reasonably well, but does not show any sensitivity between the soft and the hard equation of state (eos). a radial flow analysis, using the midrapidity transverse-momentum spectra, delivers freeze-out temperatures t and radial flow velocities beta_r which increase with beam energy up to 2 agev; in comparison to existing data of au on au over a large range of energies only beta_r shows a system size dependence.
mass scaling of reaction mechanisms in intermediate energy heavy ion collisions. null
comparison between the fragmentation processes in central pb + ag and pb + au collisions. null
surveying the nuclear caloric curve. null
is reducibility in nuclear multifragmentation related to thermal scaling?. null
multifragmentation in xe(50 amev)+sn: confrontation of theory and data. null
thermal and chemical equilibrium for vaporizing sources. null
analysis of the transverse momentum collective motion in heavy-ion collisions below 100 mev/nucleon. a theoretical analysis of collective momentum transfer is performed in heavy-ion reactions below 100 mev/nucleon in the landau-vlasov approach. the nucleon-nucleon cross section, atomic mass, compressibility, and effective mass dependences are analyzed. the simulation of detector acceptances and of finite number of detected particles are discussed. in connection with recent experiments, theoretical results and experimental data are confronted taking into account the experimental constraints. finite range forces of the gogny type connected with different nuclear matter incompressibilities are used and the ensuing sensitivity of the flow is studied. the question whether the flow provides information on out-of-equilibrium matter properties is investigated.
observation of a saturation in the time scale for multifragment emission in symmetric heavy-ion collisions. null
dynamical effects in peripheral and semi-central collisions at intermediate energy. null
kinematical properties and composition of vaporizing sources: is thermodynamical equilibrium achieved ?. null
neutrons from the breakup of $^{19}$c. null
possible observation of medium effects using a pion correlation technique. null
azimuthal anisotropies as stringent test for nuclear transport models. azimuthal distributions of charged particles and intermediate mass fragments emitted in au+au collisions at 600amev have been measured using the fopi facility at gsi-darmstadt. data show a strong increase of the in-plane azimuthal anisotropy ratio with the charge of the detected fragment. intermediate mass fragments are found to exhibit a strong momentum-space alignment with respect of the reaction plane. the experimental results are presented as a function of the polar center-of-mass angle and over a broad range of impact parameters. they are compared to the predictions of the isospin quantum molecular dynamics model using three different parametrisations of the equation of state. we show that such highly accurate data provide stringent test for microscopic transport models and can potentially constrain separately the stiffness of the nuclear equation of state and the momentum dependence of the nuclear interaction.
effects of gogny-type interactions on the nuclear flow. a flow analysis on symmetric and asymmetric reactions from 100 to 400 mev/nucleon is performed in the framework of the semiclassical landau-vlasov approach. in this energy range our results present two different trends. at lower energies it is governed by the momentum dependence of the nuclear optical potential, whereas at higher energies its density dependence plays a crucial role leading to a rather pronounced sensitivity to the incompressibility modulus. the nonlocality of the nuclear interaction is relevant for asymmetric colliding systems. with an incompressibility modulus in the vicinity of 200 mev, an excellent quantitative description of the flow behavior with incident energy and impact parameter or the system mass is provided.
vaporization events from binary dissipative collisions. null
onset of collective expansion in nucleus-nucleus collisions below 100 mev/u. null
disappearance of flow and the in-medium nucleon-nucleon cross section for $^{64}zn$ + $^{27}al$ collisions at intermediate energies. null
pion reabsorption in heavy-ion collisions interpreted in terms of the delta capture process. null
multi-boson effects. null
chiral phase transition in an expanding quark-antiquark plasma. null
instability of the filtering method for vlasov's equation. null
does one create very hot nuclei in heavy-ion reactions below 100 mev/u ?. a meticulous analysis of emitted charged particles in heavy-ion reactions has been carried out in the framework of the dynamical semiclassical landau-vlasov approach for the ar + al collisions at 65 mev/u. in accordance with most of the recent experimental results, the binary reaction mechanism is the main reaction feature. contrary to the expectations that below 100 mev/u a mechanism reminiscent of low energy deep-inelastic reaction could create two very excited sources (the primary quasiprojectile and quasitarget), the simulation shows that this reaction mechanism is closely connected to the participant-spectator picture. due to an abundant dynamical (participant) emission mainly centered at midrapidity, the primary quasiprojectile and quasitarget can be identified as not very hot spectators.
a novel approach to rescattering in ultrarelativistic heavy ion collisions. in ultrarelativistic heavy-ion collisions the phase space density of produced hadrons is that high that their interaction cannot be treated anymore as a sequence of binary hadron-hadron collisions. here many hadrons overlap simultaneously. we assume that they form a parton cluster which decays according to the available phase space. this concept has been implemented in the string model venus (4.00) and tested by comparison with s + s data. it turns out that in this approach the yet not understood distribution of image and ks can be reproduced for the first time.
wavelet representation of the nuclear dynamics. null
magnetoconductance autocorrelation function for few-channel chaotic microstructures. null
the importance of initial - final state correlations for the formation of fragments in heavy ion collision. null
disappearance of elliptic flow, a new probe for the nuclear equation of state. using a relativistic hadron transport model, we investigate the utility of the elliptic flow excitation function as a probe for the stiffness of nuclear matter and for the onset of a possible quark-gluon-plasma (qgp) phase-transition at ags energies 1 e_beam 11 agev. the excitation function shows a strong dependence on the nuclear equation of state, and exhibits characteristic signatures which could signal the onset of a phase transition to the qgp.
probing partonic structure in $\gamma* \gamma \longrightarrow \pi \pi$ near threshold. hadron pair production gamma* gamma -&gt; h hbar in the region where the c.m. energy is much smaller than the photon virtuality can be described in a factorized form, as the convolution of a partonic handbag diagram and generalized distribution amplitudes which are new non-perturbative functions describing the exclusive fragmentation of a quark-antiquark pair into two hadrons. scaling behavior and a selection rule on photon helicity are signatures of this mechanism. the case where h is a pion is emphasized.
time ordering in off-diagonal parton distributions. we investigate the relevance of time ordering in the definition of off-diagonal parton distributions in terms of products of fields. the method we use easily allows determination of their support properties and provides a link to their interpretation from a parton point of view. it can also readily be applied to meson distribution amplitudes.
modelling the many-body dynamics of heavy ion collisions : present status and future perspective. null
signal of fragment structures from a semiclassical transport equation in heavy-ion collisions. null
excitation energies and temperatures of hot nuclei produced in the reactions of cu+au at 35 a mev. null
impact parameter determination in experimental analysis using neural network. null
a hot expanding source in 50 a mev xe+sn central reactions. null
independence of fragment charge distributions of the size of heavy multifragmenting sources. null
