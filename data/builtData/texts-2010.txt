single-machine scheduling with time window-dependent processing times. in the one-machine scheduling problems analysed in this paper, the processing time of a job depends on the time at which the job is started. more precisely, the horizon is divided into time windows and with each one a coefficient is associated that is used to determine the actual processing time of a job starting in it. two models are introduced, and one of them has direct connections with models considered in previous papers on scheduling problems with time-dependent processing times. various computational complexity results are presented for the makespan criterion, which show that the problem is np-hard, even with two time windows. solving procedures are also proposed for some special cases.
ninth international workshop on the pragmatics of ocl and other textual specification languages. this paper reports on the 9th ocl workshop held at the models conference in 2009. the workshop focused on the challeges of using ocl in a variety of new scenarios (e.g., model verification and validation, code generation, test-driven development, transformations) and application domains (e.g., domain-specific languages, web semantics) in which ocl is now being used due to the increasing popularity of model-driven development processes and the important role ocl play in them. the workshop included sessions with paper presentations and a final round discussion.
coupled transport and chemistry in clay stone studied by advective displacement. experiments and model . null
finite wordlength controller realizations using the specialized implicit form. null
approche archéologique et environnementale des premiers peuplements alpins autour du col du petit-saint-bernard (savoie -vallée d’aoste) : un bilan d’étape. null
une économie pastorale dans le nord du vercors : analyse pluridisciplinaire des niveaux néolithiques et protohistoriques de la grande rivoire (sassenage, isère). the neolithic levels of the rock-shelter “la grande rivoire” are composed of a multitude of sedimentary layers of very contrasting colours. sedimentological analyses show that the fine fractions, mainly silty and rather strongly carbonated, have two main origins: on the one hand, an important accumulation of herbivores faeces due to the repeated penning of flocks (presence of calcite spheruliths, high concentrations of organic matter, high rate of phosphate), on the other hand, a large production of ashes from fire kindled directly on the dung or in nearby hearths. animal bones of domestic species found in these dung levels show a large predominancy of goat / sheep over ox and pig. archaeobotanical studies suggest a fodder / litter supply based on leafy and flowering tree branches. the protohistoric levels are different from the neolithic ones: ashy layers are missing whereas natural detritic components are better represented. nevertheless, the analysis of thin sections from these levels shows the lasting presence of spheruliths and phytoliths. this indicates that stabling was during the bronze and iron ages still practiced in the rock-shelter.
detection and characterization of the cosmic ray air shower radio emission with the codalema experiment. null
evidence for a geomagnetic effect in the codalema radio data. null
characteristics of high energy cosmic rays observed with codalema: evidence for a geomagnetic radio emission mechanism. null
radiodetection of astronomical phenomena in the cosmic ray dedicated codalema experiment. null
le projet codalema. this chapter presents the methodology of the radio detection of cosmic ray extensive air showers developed for the codalema experiment. we illustrate the performances and results obtained in the first four years of operation of codalema.
codalema : how to detect fast radio transients from cosmic ray air showers. null
radiodetection of extensive air showers : the codalema experiment. null
corr\'elation ${e}_0$ vs ${e}_\text{cic}$. null
first detection of radio signals from cosmic ray air showers with a self triggered, fully autonomous system. null
geomagnetic feature of the events observed by the self triggered, fully autonomous radiodetectors system installed at the clf. null
timing accuracy of the coincidences between the autonomous radio stations at the clf and auger sd. null
analysis of the global behavior of the clf autonomous radio stations. understanding electric field and weather effects. null
first threefold detection of a coincidence between the self-triggered radio stations at the clf and auger sd. null
radio spectrum measurements at auger, part 2. null
radio spectrum measurements at auger. null
entropy: a consolidation manager for clusters. clusters provide powerful computing environments, but in practice much of this power goes to waste, due to the static allocation of tasks to nodes, regardless of their changing computational requirements. consolidation is an approach that migrates tasks within a cluster as their computational requirements change, both to reduce the number of nodes that need to be active and to eliminate temporary overload situations. previous consolidation strategies have relied on task placement heuristics that use only local optimization and typically do not take migration overhead into account. however, heuristics based on only local optimization may miss the globally optimal solution, resulting in unnecessary resource usage, and the overhead for migration may nullify the benefits of consolidation. in this paper, we propose the entropy resource manager for homogeneous clusters, which performs consolidation based on constraint programming and takes migration overhead into account. the use of constraint programming allows entropy to find mappings of tasks to nodes that are better than those found by heuristics based on local optimizations, and that are frequently globally optimal in the number of nodes. because migration overhead is taken into account, entropy chooses migrations that can be implemented efficiently, incurring a low performance overhead.
antenna development for astroparticle and radioastronomy experiments. an active dipole antenna is in operation since five years at the nançay radio observatory (france) in the codalema experiment. a new version of this active antenna has been developed, whose shape gave its name of "butterfly" antenna. compared to the previous version, this new antenna has been designed to be more efficient at low frequencies, which could permit the detection of atmospheric showers at large distances. despite a size of only 2 m×1 m in each polarization, its sensitivity is excellent in the 30-80 mhz bandwidth. three antennas in dual polarization were installed on the codalema experiment, and four other have been recently installed on the auger area in the scope of the aera project. the main characteristics of the butterfly antenna are detailed with an emphasis on its key features which make it a good candidate for the low frequency radioastronomy and the radio detection of transients induced by high energy cosmic rays.
report on the selection of the reference xt-ads target design and specifications. the xt-ads is an experimental accelerator driven system (ads) that is being developed in the framework of the european fp6 eurotrans project. in this deliverable, the specifications of the spallation target and the selection of its reference design are discussed. justification of the design options, in relation to the performance requirements of the xt-ads and the interlinking with the design of the sub-critical core and the primary system, are given.
general synthesis report of the different ads design status. establishment of a catalogue of the r&amp;d needs. this document is a general synthesis report of the different ads design status being designed within the eurotrans integrated project; an fp6 european commission partially funded programme. this project had the goal to demonstrate the possibility of nuclear waste transmutation/burning in accelerator driven systems (ads) at industrial scale.the focus is on a pb-cooled ads for the european facility on industrial scale transmuter (etd/efit) with a back-up solution based on an he cooled ads.as an intermediate step towards this industrial-scale prototype, an experimental transmuter based on ads concept (etd/xt-ads) able to demonstrate both the feasibility of the ads concept and to accumulate experience when using dedicated fuel sub-assemblies or dedicated pins within a mox fuel core has been also studied.the two machines (xt-ads and pb cooled efit) have been designed in a consistent way bringing more credibility to the potential licensing of these plants and with sufficient details to allow definition of the critical issues as regards design, safety and associated technological and basic r&amp;d needs. the different designs fit rather well with the technical objectives fixed at the beginning of the project in consistency with the european roadmap on ads development.for what concerns the accelerator, the superconducting linac has been clearly assessed as the most suitable concept for the three reactors in particular with respect to the stringent requirements on reliability. associated r&amp;d needs have been identified and will be focused on critical components (injector, cryomodule) long term testing.the design of the different ads has been performed in view of what is reasonably achievable pending the completion of r&amp;d programmes. the way the eurotrans integrated project has been organised with other domains than the dm1 design being specifically devoted to r&amp;d tasks in support to the overall etd/efit and etd/xt-ads design tasks has been helpful. the other domains were centred on the assessment of reactivity measurement techniques (dm2 ecats), on the development of u-free dedicated fuels (dm3 aftra), on materials behaviour and heavy liquid metal technology (dm4 demetra) and on nuclear data assessment (dm5 nudatra). pending questions associated to technology gaps have been identified through the different appropriate r&amp;d work programmes and a catalogue of the r&amp;d needs has been established.finally, the work within the eurotrans integrated project has provided an overall assessment of the feasibility at a reasonable cost for an ads based transmutation so that a decision can be taken to launch a detailed design and construction of the intermediate step experimental ads now already launched within the 7th fp programme under the name of common design team (cdt).
final report on the feasibility of the xt-ads spallation target. in the framework of the eurotrans project an experimental accelerator driven system “xt-ads” is being designed. this document is the final deliverable of work package 1.4, the project in which the design of the spallation target for the xt-ads is developed. the present status of the design of a windowless spallation target for the xt-ads is discussed with particular focus on the modifications made as compared to myrrha draft 2. the document is composed of two major parts. after the definition of the design boundary conditions, a first major part deals with the general layout of the spallation target loop with a discussion of the current lay-out options. in the second part the different design support studies that have been performed are described. these include the spallation target zone and spallation loop thermal-hydraulics, system dynamics, beam impact studies, neutronics, mechanical integration and safety analyses. in the last section some conclusions are formulated.
design and r&amp;d support of the xt-ads spallation target. the experimental accelerator-driven system xt-ads is being developed within the framework of the european fp6 project eurotrans. the device will serve a twofold purpose. firstly it will act as an ads concept demonstrator and secondly it will serve as a flexible fast neutron spectrum experimental irradiation tool for materials, fuel materials and radioactive isotopes studies. because of a functional similarity between the xt-ads and the myrrha concept that was developed earlier at sck-cen in mol, belgium, the design file of the latter was chosen as a starting point for the development of the xt-ads.
webexpir: windowless target electron beam experimental irradiation. the windowless target electron beam experimental irradiation (webexpir) program was set-up as part of the myrrha/xt-ads r&amp;d effort on the spallation target design to investigate the interaction of a proton beam with a liquid lead–bismuth eutectic (lbe) free surface. in particular, possible free surface distortion or shockwave effects in nominal conditions and during sudden beam on/off transient situations, as well as possible enhanced evaporation were assessed. an experiment was conceived at the iba tt-1000 rhodotron, where a 7 mev electron beam was used to simulate the high power deposition at the myrrha/xt-ads lbe free surface. the geometry and the lbe flow characteristics in the webexpir set-up were made as representative as possible of the actual situation in the myrrha/xt-ads spallation target. irradiation experiments were carried out at beam currents of up to 10 ma, corresponding to 40 times the nominal beam current necessary to reproduce the myrrha/xt-ads conditions. preliminary analyses show that the webexpir free surface flow was not disturbed by the interaction with the electron beam and that vacuum conditions stayed well within the design specifications.
evaluation of potential innovative positron-emitting radionuclides for pet applications. the opportunities offered by the construction in nantes (france) of a high energy (70 mev) and high intensity (2 simultaneous 350 μa proton beams and one 30 μa alpha beam) cyclotron (arronax) for radiochemistry and nuclear medicine research has prompted us to reinvestigate the production of several beta+ isotopes. fluorine-18 is widely used in clinical routine practice for pet examination, but fails to achieve all the potential benefits of pet imaging for one major reason: its short half-life is not suited to the long kinetics of large molecules, e.g. antibodies, which may be used for diagnostics (immuno-pet) and radioimmunotherapy. since pet cameras achieve better image quantification than spect cameras, beta+/beta- couples of the same element, which will follow the same metabolic pathways, could be used to monitor radionuclide distribution, and thus to obtain more precise dosimetry, especially in the field of radioimmunotherapy (rit).
lower bounds for resource constrained project scheduling problem. we review the most recent lower bounds for the makespan minimization variant of the resource constrained project scheduling problem. lower bounds are either based on straight relaxations of the problems (e.g., single machine, parallel machine relaxations) or on constraint programming and/or linear programming formulations of the problem.
a reconfiguration language for virtualized grid infrastructures. the growing needs in computational power to answer to the increasing number of on-line services and the complexity of applications makes it mandatory to build corresponding hardware infrastructures and to share several distributed hardware and software resources thanks to grid computing. to help with optimizing resource utilization, system virtualization is a more and more adopted technique in data centers. however, this software layer adds to the administration complexity of servers and it requires speciﬁc management tools to deal with hypervisor functionalities like live migration. to address this problem, we propose vmscript, a domain speciﬁc language for administration of virtualized grid infrastructures. this language relies on set manipulation and is used to introspect physical and virtual grid architectures thanks to query expressions and notably to modify vm placement on machines.
methods to solve multi-skill project scheduling problem. this is a summary of the author's phd thesis supervised by p. martineau and e. néron and defended on 28 november 2006 at the université françois-rabelais de tours. the thesis is written in french, and is available upon request from the author. this work deals with the problem of scheduling a project. the activities of this project requires skills that may not be mastered by all persons involved. first of all, the problem is defined in the introduction part. then we propose different methods to solve it: lower bounds in part 2, different heuristics and meta-heuristics in part 3, and finally a branch-and-bound procedure in the last part.
working on innovation. null
production of porous carbonaceous adsorbent from physical activation of sewage sludge: application to wastewater treatment. with an objective of production of carbonaceous sorbent for industrial effluent treatment, physical activation by steam of biological sludge collected from the municipal wastewater treatment plant of nantes (france) was studied and optimised using experimental design. thus, this activation process consists of a carbonisation under n-2 atmosphere at 600 degrees c for 1 h, followed by a thermal oxidation using steam (760 degrees c, 0.5 h, 2.5 l/min). the global mass yield of the process is equal to 38%. the thermal treatment allows a specific surface area of up to 225 m(2)/g to be reached, the porous structure being composed of both micropores and mesopores. the content of acidic surface groups is 0.71 meq/g whereas that of basic surface groups is 0.55 meq/g. the adsorption properties of the sorbent made from sludge are estimated with regard to various pollutants representative of industrial pollution of wastewaters and compared with those of commercial activated carbon. whereas the adsorption capacities of organic micropollutants are quite low because of proportionality to the microporosity, the important mesoporosity of the sorbent leads to interesting properties for macromolecules removal from aqueous solutions, such as dyes (q(m) = 175-200 mg/g). furthermore, the surface functional groups and ca2+ ions within the materials allow high copper ion adsorption capacities of 140 mg/g to be obtained. finally, a techno-economic approach shows that the sludge activation process seems to be economically competitive with regard to incineration.
production of fibrous activated carbons from natural cellulose (jute, coconut) fibers for water treatment applications. different fibrous activated carbons were prepared from natural precursors (jute and coconut fibers) by physical and chemical activation. physical activation consisted of the thermal treatment of raw fibers at 950 degrees c in an inert atmosphere followed by an activation step with co2 at the same temperature. in chemical activation, the raw fibers were impregnated in a solution of phosphoric acid and heated at 900 degrees c in an inert atmosphere. the characteristics of the fibrous activated carbons were determined in the following terms: elemental analysis, pore characteristics, sem observation of the porous surface, and surface chemistry. as the objective of this study was the reuse of waste for industrial wastewater treatment, the adsorption properties of the activated carbons were tested towards pollutants representative of industrial effluents: phenol, the dye acid red 27 and cu2+ ions. chemical activation by phosphoric acid seems the most suitable process to produce fibrous activated carbon from cellulose fiber. this method leads to an interesting porosity (s-bet up to 1500 m(2) g(-1)), which enables a high adsorption capacity for micropollutants like phenol (reaching 181 mg g(-1)). moreover, it produces numerous acidic surface groups, which are involved in the adsorption mechanisms of dyes and metal ions. (c) 2006 elsevier ltd. all rights reserved.
preparation of adsorbents from sewage sludge by steam activation for industrial emission treatment. with an objective of production of carbonaceous sorbent for industrial effluent treatment, physical activation by steam of biological sludge collected from the municipal wastewater treatment plant of nantes (france) is studied. this work focuses on the optimization of the carbonization and activation conditions on physico-chemical and adsorptive properties of adsorbent materials produced. the carbonization is performed in a vertical pyrolysis furnace. its duration is fixed at 1 h, based on tga analysis. the carbonization temperature is optimized between 400 and 1000 degrees c to improve the bet specific surface area, the micropore volume and the quantity of surface functional groups of the char. these parameters seem to be improved by a temperature increase, the specific surface area varying from 20 to 96 m(2) g(-1). an intermediate temperature of 600 degrees c is selected, in order to obtain an interesting specific surface area (60 m(2) g(-1)) and satisfying mass yield. the activation conditions are optimized in terms of temperature (750-850 degrees c) and duration (30-90 min). a factorial design is carried out, based on the following responses: mass yield, porosity (bet surface area, mesopore and micropore volume), surface chemistry and adsorption properties for current pollutants (phenol, copper ions, dyes and cov). the activation step shows an improvement of pore and adsorption characteristics of the adsorbent, with specific surface area reaching 230 m(2) g(-1) and equilibrium adsorption capacities of phenol and copper equal to 50 mg g(-1), and 80 mg g(-1), respectively. the adsorption of microorganic compounds and dyes may be related respectively with micro- and meso-porous properties, whereas copper adsorption efficiency is due to an ion-exchange mechanism as demonstrated by exchange with ca2+ ions contained in the raw water sludge. experimental design responses are optimized using surface response methodology and are validated experimentally. finally, techno-economical analysis of the process shows a higher cost than farmland re-use but steam activation of sludge seems to be competitive towards incineration.
photocatalytic oxidation of indoor vocs at ppb levels: kinetics of by-products formation. null
modelling and analysis of permeability of anisotropic compressed non-woven filters. an existing geometrical pore-scale model for flow through isotropic spongelike media is adapted to predict flow through anisotropic non-woven glass fibre filters. model predictions are compared to experimental results for the permeability obtained for a filter under different stages of compression to demonstrate the capability of the model to adjust to changes in porosity. the experimental data used are for a glass fibre paper with a uniform fibre diameter. the input parameters of the pore-scale model are the porosity, fibre diameter and some measure of the anisotropy between the in-plane and normal directions to the paper. correlation between the predictions and the experimental results is satisfactory and provides confidence in the modelling procedure. it is shown that the permeability is very sensitive to changes in the level of anisotropy, i.e. the level of compression of the nonwoven material.
influence on permeability of the structural parameters of heterogeneous porous media. predicting the macroscopic properties of porous media used in treatment processes is a complex task regarding 3-d structures at micro-, meso- and macro-levels. currently, information is scarce concerning the influence, at a microscopic level, of the 3-d structure of fibrous media on the physical laws governing their macroscopic behaviour. nevertheless, the relationship between macroscopic properties (pressure drop, treatment efficiency) and microstructure can be assessed thanks to suitable structure modelling theories. in this context, the present study proposes and compares different methods (mercury porosimetry and image analysis) for the structure characterization at a microscopic level of filtering fibrous media, such as nonwoven and woven fabrics. the results obtained show a porous structure gradient in the thickness of the nonwoven media studied in terms of porosity, pore size and tortuosity factor. moreover, the influence on structural parameters of media compression, when submitted to friction forces exerted by flow during filtration tests, is established. a model for the determination of multi-level pore size distributions from mercury porosimetry data is proposed. the "equivalent pore" model is used to estimate the tortuosity factor. the influence of measured structural parameters on fibrous media permeability is studied in a classical model for flow through fibrous media.
indoor air particulate filtration onto activated carbon fiber media. due to their bad effects on human health and comfort, removing particles and volatile organic compounds from indoor air has become an issue of major interest. in this study, the potential use of five media for particle removal was investigated: a felt, a cloth, and a knitted fabric made entirely of activated carbon fibers, and two prototype nonwovens made of different proportions of activated carbon and glass fibers. dynamic filtration measurements were performed in experimental conditions as representative as possible of indoor air, with alumina particles (modal diameter: 0.37 mu m), at an inlet concentration of 2,500 particles cm(-3) and for two different frontal velocities of air: 0.37 and 0.50 m s(-1). although this medium was not designed for filtration, felt exhibited a high initial filtration efficiency (74%) for a low pressure drop (less than 210 pa). similarly, associating several layers of woven/knitted media in series led to high performances, as it reduced preferential paths for the airflow. finally, prototype nonwovens appeared more efficient than activated carbon felt, but exhibited higher pressure drops.
collection efficiency of a woven filter made of multifiber yarn: experimental characterization during loading and clean filter modeling based on a two-tier single fiber approach. due to their good adsorption capacities, woven activated carbon filters are promising materials for the removal of volatile organic compounds (voc) and odors in air treatment applications. as these fibrous media can easily be implemented in various filtration configurations, their use in a combined filter device for voc and particulate matter (pm) removal is considered. for this purpose, experiments are performed to characterize their collection efficiencies versus particle size and mass loading for the removal of particulate matter smaller than 10 mu m. the specific fiber arrangement, due to multi-fiber yam weaving, prevents the use of initial collection efficiency models developed for non-woven media. this study aims to characterize collection efficiency during loading and to model initial collection efficiency in a woven structure made of multi-fiber yams. (c) 2005 elsevier ltd. all rights reserved.
application of sludge-based carbonaceous materials in a hybrid water treatment process based on adsorption and catalytic wet air oxidation. this paper describes a preliminary evaluation of the performance of carbonaceous materials prepared from sewage sludges (sbcms) in a hybrid water treatment process based on adsorption and catalytic wet air oxidation; phenol was used as the model pollutant. three different sewage sludges were treated by either carbonisation or steam activation, and the physico-chemical properties of the resultant carbonaceous materials (e.g. hardness, bet surface area, ash and elemental content, surface chemistry) were evaluated and compared with a commercial reference activated carbon (pica f22). the adsorption capacity for phenol of the sbcms was greater than suggested by their bet surface area, but less than f22; a steam activated, dewatered raw sludge (sa_draw) had the greatest adsorption capacity of the sbcms in the investigated range of concentrations (\textless0.05 mol l(-1)). in batch oxidation tests, the sbcms demonstrated catalytic behaviour arising from their substrate adsorptivity and metal content. recycling of sa_draw in successive oxidations led to significant structural attrition and a hardened sa_draw was evaluated, but found to be unsatisfactory during the oxidation step. in a combined adsorption oxidation sequence, both the pica carbon and a selected sbcm showed deterioration in phenol adsorption after oxidative regeneration, but a steady state performance was reached after 2 or 3 cycles. (c) 2010 elsevier ltd. all rights reserved.
dimensional modeling of biomass pyrolysis based on a nodal approach. the purpose of the present research is to develop a numerical model for the biomass gasification processes suitable for the dimensioning of industrial installation. the end use of the producer gas being often internal combustion engines systems, the model concentrates on the quantitative and qualitative aspects of the process, with a particular interest in the problem of tar formation and destruction. this study focuses on the first two steps of the process: drying and pyrolysis. the pyrolysis model presented in this paper consists in a coupling between a heat transfer model based on a nodal approach and a chemical model for the thermal decomposition of the pyrolysed material. drying is considered as a sub process included in pyrolysis. the shown results are in good agreement with experimental data.
detection of knock occurrence in a gas si engine from a heat transfer analysis. this paper focuses on the transient thermal signal around an engine cylinder in order to propose a new and non-intrusive method of knock detection. numerical simulations of unsteady heat transfer through the cylinder and inside the coolant flow are performed. the amplification of wall heat transfer due to knock is taken into account by a self-developed program. it enables one to fix the instantaneous heat flux on the internal side of the cylinder. the transient component of the thermal signal is then calculated and analysed. the effect of a machining, consisting of a transverse groove, which reduces the cylinder liner thermal resistance, is investigated. this groove disturbs the turbulent coolant flow, and its shape mainly influences the heat transfer from the wall to the fluid. the numerical results show that the use of a square groove would multiply by three the amplitude of the transient thermal signal recorded within the water compared to a smooth wall case. moreover, the amplitude can be enhanced by using a nanofluid as coolant because of its higher thermal diffusivity. (c) 2005 elsevier ltd. all rights reserved.
a comparative study of different methods of using animal fat as a fuel in a compression ignition engine. this work explores a comparative study of different methods of using animal fat as afuel in a compression ignition engine. a single-cylinder air-cooled, direct-injection diesel engine is used to test the fuels at 100% and 60% of the maximum engine power output conditions. initially, animal fat is tested as fuel at normal temperature. then, it is preheated to 70 degrees c and used as fuel. finally, animal fat is converted into methanol and ethanol emulsions using water and tested as fuel. a drop in cylinder peak pressure, longer ignition delay, and a lower premixed combustion rate are observed with neat animal fat as compared to neat diesel. with fat preheating and emulsions, there is an improvement in cylinder peak pressure and maximum rate of pressure rise. ignition delay becomes longer with both the emulsions as compared to neat fats. however preheating shows shorter ignition delay. improvement in heat release rates is achieved with all the methods as compared to neat fats. at normal temperature, neat animal fat results in higher specific energy consumption and exhaust gas temperature as compared to neat diesel at both power outputs. preheating and emulsions of animal fat show improvement in performance as compared to neat fat. smoke is lower with neat fat as compared to neat diesel. it reduces further with all the methods. at peak power output, the smoke level is found as 0.89 m(-1) with methanol, 0.28 m(-1) with ethanol emulsions, and 1.7 m(-1) with fat preheating, whereas it is 3.7 m(-1) with neat fat and 6.3 m(-1) with neat diesel. methanol and ethanol emulsions significantly reduce no emissions due to the vaporization of water and alcohols. however no increases with fat preheating due to high in-cylinder temperature. higher unburned hydrocarbon and carbon monoxide emissions are found with neat fat as compared to neat diesel at both power outputs. however these emissions are considerably reduced with all the methods. it is finally concluded that adopting emulsification with the animal fat can lead to a reduction in emissions and an improvement in combustion characteristics of a diesel engine.
ethanol animal fat emulsions as a diesel engine fuel - part 1: formulations and influential parameters. this paper focuses on effective solution to improve the combustion of low quality animal fat by making stable emulsions with water. animal fat emulsions are prepared by mixing the fat with water, surfactant and co-surfactant. ethanol is chosen as the co-surfactant because of its dilution ability. span 83 also called sorbitan sesquiolate is used as the surfactant because it well stabilizes and forms stable animal fat emulsions. emulsions and micro-emulsions are prepared for different co-surfactant/surfactant (c/s) ratios. a number of formulations are made and the sauter mean diameter of water droplets are estimated using electron microscope images. results are presented in pseudo ternary diagrams. influence of different parameters affecting the emulsion characteristics are studied experimentally. according to the stability, structure, viscosity, fat content and economical aspects, the optimum emulsion is found as the emulsion with 36.4% of ethanol, 3.6% of span.83, 10% of water and 50% of animal fat by volume. (c) 2006 elsevier ltd. all rights reserved.
numerical investigation of the partial oxidation in a two-stage downdraft gasifier. null
dimensional modelling of wood pyrolysis using a nodal approach. new gasification installations and techniques are being tested today but they all struggle with mainly the same drawbacks such as removal of various pollutants in the producer gas or clogging of material pathways. this work is oriented on developing a new model for the non-oxidative pyrolysis step of a gasification process as a part of a wider research conducted on the overall gasification of wood waste. a batch reactor is modelled by means of nodal modelling, a technique widely used for simple heat transfer processes. additionally to the heat transport inside the batch reactor the model uses a simple and versatile generic chemistry and simplified mass transfer principles. thermal data from modelling is compared with data obtained from an experimental batch pyrolysis reactor using wood sawdust and cutter shavings. experimental and theoretical results regarding thermal phenomena are in good agreement. (c) 2008 elsevier ltd. all rights reserved.
pollution duality in turbocharged heavy duty diesel engine. diesel engine designers are faced with increasingly stringent social demands to reduce emissions while maintaining high performance. several strategies are considered, such as the advanced fuel system, the cooled exhaust gas recirculation (egr), the particulate filter, the no(x) after-treatment, the oxidation catalyst, the advanced control techniques and the alternative combustion. these strategies have been tuned to achieve the lowest engine exhaust gas emissions. the major problem of diesel engine pollution is the no(x) and soot formation. their antagonistic evolution according to the air/fuel ratio is well-known, and requires a good compromise. in this article, a numerical investigation was carried out using the kiva-3v code. the aim deals with the influence of some engine parameters on the performances and the pollutant (no(x)-soot) formation of a turbocharged heavy duty direct injection diesel engine. the numerical simulations were achieved to capture independently the effects of engine operating parameters such as the fuel injection timing, the fuel injection duration, the piston bowl diameter and the egr rate. the obtained results are discussed and some conclusions are developed.
thermo chemical equilibrium modelling of a biomass gasifying process using aspen plus. this article deals with the use of aspen plus to model the thermo chemical processes occurring in wood biomass gasifiers. an original equilibrium gasification model using aspen plus was first built and validate based on existing data of a downdraft gasifier (ddg). the thermo chemical models assume that reactants reach chemical equilibrium. the simulations enabled to predict the composition of the flaming pyrolysis gas as well as the final composition of the producer gas. a parametric study based on air/fuel ratio variation was then conducted. the model was finally adapted to the operating conditions of a staged gasifier developed at the technical university of denmark (dtu). parametric studies were conducted to observe the effect of humidity with a low air injection. aspen plus was shown to be well adapted to model the gasification process (equilibrium), and it was possible to simulate both a ddg and a dtu using the same model. data about the composition of flaming pyrolysis and the exit gas were obtained.
prediction of micro-explosion delay of emulsified fuel droplets. burning a water-in-oil emulsion enables reduction in solid and gaseous pollutants in comparison with neat oil. in the emulsion, heavy fuel-oil and water lie in distinct phases, having a high difference in boiling point (up to 200 k). in an emulsion droplet injected and subsequently heated inside a flame, the internal water droplets are enclosed inside the emulsion and do not systematically vaporise at boiling point. they are known to reach a metastable state, breaking up at a temperature below the spinodal limit of water. from this moment, the surrounding fuel-oil is fragmented into numerous faster and smaller droplets by the suddenly expanding steam. this physical phenomenon is called ''micro-explosion''. this work demonstrates a numerical modelisation of unsteady heat and mass transfer at the surface and inside of the emulsion droplet, and provides a prediction of its micro-explosion delay, using homogeneous nucleation hypothesis. this assumption of homogeneous nucleation for internal water droplets matches the use of a ''drop tower'' experimental facility. finally, comparisons between predicted ranges for micro-explosion delays and experimental delays from literature are discussed, along with combustion parameters (ambient temperature, relative velocity) and combustible emulsion parameters. as a result, the experimental and numerical micro-explosion delays decrease with liquid or ambient temperature and relative velocity, and increase with water content and radius of emulsion droplet. their low average deviation reveals the accuracy of the assumption of homogeneous nucleation in the considered situations. (c) 2008 elsevier masson sas. all rights reserved.
the use of biofuel emulsions as fuel for diesel engines: a review. animal fats/vegetable oils (called biofuels) and their emulsions are quite promising alternative fuels for diesel engines. this article reports a comprehensive study on the use of animal fats/vegetable oils and their emulsions as fuel in compression ignition engines. emulsions preparation method and their effects on engine performance, emission, and combustion characteristics have been studied in detail. information indicates that biofuel emulsions in diesel engines enhanced the combustion efficiency with improved performance as compared to neat fuels. the maximum percentage of water addition to biofuel was found as 30 per cent by volume for maximum efficiency. they reduced no,, smoke, and particulate emissions considerably. emulsions resulted in higher ignition delay as a result of vaporization of water as compared to neat fuels. peak pressure, rate of pressure rise, and premixed combustion rate in the heat release curve were found to be higher when compared to neat oils because of longer ignition delay. further improvements could be achieved by adding oxygenated fuels like methanol, dimethyl carbonate, and cetane number improvers like diethyl ether with biofuels in small quantities. it has also been suggested that dual fuel operation can significantly reduce particulate and no., emissions with biofuels. exhaust gas recirculation can reduce ignition delay considerably with reduced no., emissions. finally, modelling techniques were presented because they can help in in-depth analysis of the combustion process of biofuel emulsions in diesel engines.
a numerical comparison of spray combustion between raw and water-in-oil emulsified fuel. heavy fuel-oils, used engine oils and animal fat can be used as dense, viscous combustibles within industrial boilers. burning these combustibles in the form of an emulsion with water enables to decrease the flame length and the formation of carbonaceous residue, in comparison with raw combustibles. these effects are due to the secondary atomization among the spray, which is a consequence of the micro-explosion phenomenon. this phenomenon acts in a single emulsion droplet by the fast (&lt; 0.1 ms) vaporization of the inside water droplets, leading to complete disintegration of the whole emulsion droplet. first, the present work demonstrates a model of spray combustion of raw fuel. secondly, the spray combustion of water-in-oil emulsified fuel is exposed to the same burning conditions, taking into account the micro-explosion phenomenon. finally, the comparison between the results with and without second atomization shows some similar qualitative tendencies with experimental measurements from the literature.
formulation and combustion of emulsified fuel: the changes in emission of carbonaceous residue. burning dense, viscous combustibles such as heavy fuel-oil as a water-in-oil emulsified combustible enables to decrease the emission of solid carbonaceous residue, in comparison with raw, non-emulsified combustible. this is due to the phenomenon of micro-explosion, meaning the rapid (&lt;0.1 ms) vaporization of the water droplets inside the emulsion, breaking up the initial emulsion droplet into numerous and faster 'daughter-droplets'. the present work is based on a small-scale furnace (300 kw max.) feed with heavy fuel-oil mixed with 10-20% of gasoil, with and without emulsion of water. the emulsification of combustible enables to record a reproducible lowering in emission of carbonaceous residue from the combustion of emulsified fuel, in comparison with raw fuel. this is added to a variation in granulometry of carbonaceous residue, hereby considered as an indicator of second atomization. copyright (c) 2009 john wiley &amp; sons, ltd.
liquid fuel recovery through pyrolysis of polyethylene waste. despite significant advances in recent years, 61% of the plastic waste generated in western europe is still disposed of to landfill. the polyethylene, high and low density (hdpe and ldpe), is the main part of waste plastics. it is proposed a more effective way to valorise these wastes, which appear to be the pyrolysis. this solution has two main advantages: the quantity of waste is reduced up to 99%, depending on plastic composition, while liquid and gaseous fractions with lhv almost like diesel fuel and natural gas will be produced during the process.
biodiesel elaboration from municipal fat wastes. the purposes of this study are to elaborate a diesel engine bio-fuel using animal fat wastes (waf) as raw material, and to identify the effect of principal elements on the process design. in order to accomplish our goals, we studied the transesterification of these waf with methanol and using sulfuric acid (h(2)so(4)) as catalyst. also the physical characteristics of produced bio-fuel were studied and compared with the european standard en 14214. the waf used in this study was collected from the area of bordeaux in france by ctma, a waste treatment station. the effects of: catalyst amount, time of reaction and alcohol to oil molar ratio on the reaction conversion have been established. the waf have a very high acid number (60 mg (koh)/g (fat)), which is equivalent to a content of 30% (wt.) in free fatty acids, and a cinematic viscosity of 18.75 mm(2)/s at 40 degrees c. after processing, we obtained a liquid with a low acidity (0.4 mg (koh)/g (fuel)) and a viscosity of 4 mm2/s at 40 degrees c.
modeling the temperature dependence of adsorption equilibriums of voc(s) onto activated carbons. in order to optimize the efficiency of the removal of volatile organic compounds (vocs) by adsorption onto activated carbon beds, process simulations taking into account exothermicity effects are helpful. significant temperature increases may arise in the bed during the voc adsorption cycle, especially when high concentrations have to be treated. consequently, reliable and easy-to-handle isotherms remain a key hurdle to build realistic models. in this study, adsorption models were tested to describe a set of experimental data obtained for three vocs (acetone, ethyl formate, and dichloromethane) adsorbed onto five commercial activated carbons at four different temperatures (20, 40, 60, and 80°c ). a new expression of the freundlich equation [qe=(a1t+a2t2)ce(1/nf)] was shown to be statistically the most efficient to describe the adsorption isotherms of vocs, single or in mixtures. a second-order polynomial temperature-dependence was introduced in this expression. the so-adapted freundlich relationship gave a mean coefficient of determination of 0.97 for single-component adsorption and a correlation coefficient of 0.98 for binary mixtures.
formulation of synthetic greywater as an evaluation tool for wastewater recycling technologies. on-site greywater recycling is one of the main ways of preserving water resources in urban or arid areas. this study aims to formulate model synthetic greywater (sgw) in order to evaluate and compare the performances of several recycling processes on a reproducible effluent. the formulated sgw is composed of septic effluent to provide indicators of faecal contamination, and technical quality chemical products to simulate organic pollution of greywater. to ensure that the sgw developed is representative of household greywater, its analysis was compared to real greywater collected and analysed (rgws) and to real greywater mentioned in previous publications (rgwl). the performance of a direct nanofiltration process with a concentration factor of 87.5% at 35 bar was then tested on both real greywater and sgw. the laboratory experimental results are promising: fluxes and retention rates were high, and similar for both effluents. the permeation flux was higher than 50 l h-1 m-2. retentions greater than 97% for biochemical oxygen demand for 5 days (bod5) and 92% for anionic surfactants were observed. no enterococcus were detected in the two permeates. these results confirm that the model sgw developed in this study shows the same behaviour as real greywater when recycled. thus, the use of this sgw developed in this study was validated for the evaluation of membrane efficiency to treat greywater. this new tool will be a real asset for future studies.
membrane process treatment for greywater recycling: investigations on direct tubular nanofiltration. on-site greywater recycling and reuse is one of the main ways to reduce potable water requirement in urban areas. direct membrane filtration is a promising technology to recycle greywater on-site. this study aimed at selecting a tubular nanofiltration (nf) membrane and its operating conditions in order to treat and reuse greywater in buildings. to do so, a synthetic greywater (sgw) was reconstituted in order to conduct experiments on a reproducible effluent. then, three pci nf membranes (afc30, afc40 and afc80) having distinct molecular weight cut-offs were tested to recycle this sgw with a constant concentration at 25°c at two different transmembrane pressures (20 and 35 bar). the best results were obtained with afc80 at 35 bar: the flux was close to 50 l m-2 h-1, retentions of 95% for chemical oxygen demand and anionic surfactants were observed, and no enterococcus were detected in the permeate. the performances of afc80 were also evaluated on a real greywater: fluxes and retentions were similar to those observed on sgw. these results demonstrate the effectiveness of direct nanofiltration to recycle and reuse greywater.
optimal maintenance and replacement decisions under technological change with consideration of spare parts inventories. classical spare parts inventory models assume that the same vintage of technology will be utilized throughout the planning horizon. however, replacement often occurs in the form of a new technology that renders existing spare parts inventories obsolete. this paper aims to study the impact of spare parts inventory on maintenance and replacement decisions under technological change via a markov decision process formulation. the replacement decision is complex in that one must decide with which technology available on the market to replace the current asset. under technological change, the do nothing and repair options have significantly more value as they allow the appearance of even better technologies in the future.
simultaneous optimization of x-bar control chart and age-based preventive maintenance policies under an economic objective. null
integrating rule-based modelling and constraint programming for solving industrial packing problems. null
the increasing nvalue constraint. this paper introduces the increasing_nvalue constraint, which restricts the number of distinct values assigned to a sequence of variables so that each variable in the sequence is less than or equal to its successor. this constraint is a specialization of the nvalue constraint, motivated by symmetry breaking. propagating the nvalue constraint is known as an np-hard problem. however, we show that the chain of non strict inequalities on the variables makes the problem polynomial. we propose an algorithm achieving generalized arc-consistency in o(σdi) time, where σdi is the sum of domain sizes. this algorithm is an improvement of filtering algorithms obtained by the automaton-based or the slide-based reformulations. we evaluate our constraint on a resource allocation problem.
on matrices, automata, and double counting. matrix models are ubiquitous for constraint problems. many such problems have a matrix of variables m, with the same constraint defined by a finite-state automaton a on each row of m and a global cardinality constraint gcc on each column of m . we give two methods for deriving, by double counting, necessary conditions on the cardinality variables of the gcc constraints from the automaton a. the first method yields linear necessary conditions and simple arithmetic constraints. the second method introduces the cardinality automaton, which abstracts the overall behaviour of all the row automata and can be encoded by a set of linear constraints. we evaluate the impact of our methods on a large set of nurse rostering problem instances.
sweeping with continuous domains. the geost constraint has been proposed to model and solve discrete placement problems involving multi-dimensional boxes (packing in space and time). the filtering technique is based on a sweeping algorithm that requires the ability for each constraint to compute a forbidden box around a given fixed point and within a surrounding area. several cases have been studied so far, including integer linear inequalities. motivated by the placement of objects with curved shapes, this paper shows how to implement this service for continuous constraints with arbitrary mathematical expressions. the approach relies on symbolic processing and defines a new interval arithmetic.
gluon radiation by heavy quarks. null
charm and beauty searches using electron-d0 azimuthal correlations and microvertexing techniques in star experiment at rhic. null
effect of organic solvents on oxygen mass transfer in multiphase systems: application to bioreactors in environmental protection. the absorption of oxygen in aqueous-organic solvent emulsions was studied in a laboratory-scale bubble reactor at a constant gas flow rate. the organic and the gas phases were dispersed in the continuous aqueous phase. volumetric mass transfer coefficients (k(l)a) of oxygen between air and water were measured experimentally using a dynamic method. it was assumed that the gas phase contacts preferentially the water phase. it was found that addition of silicone oils hinders oxygen mass transfer compared to air-water systems whereas the addition of decane, hexadecane and perfluorocarbon pfc40 has no significant influence. by and large, the results show that, for experimental conditions (organic liquid hold-up &lt;= 10% and solubility ratio &lt;= 10), the k(l)a values of oxygen determined in binary air-water systems can be used for multiphase (gas-liquid-liquid) reactor design with applications in environmental protection (water and air treatment processes). (c) 2006 elsevier b.v. all rights reserved.
bacteria removal in septic effluent: influence of biofilm and protozoa. numerous biological, physical and chemical parameters are involved in the retention and removal of bacteria in wastewater treatment systems. biological parameters, such as biofilms and protozoa grazing activity, are often mentioned but few studies provide a better understanding of their influence. in this study, the effect of bacterivorous protozoa on pathogenic indicator bacteria removal was investigated in septic effluent and in the presence of a biofilm coating glass slides. endogenous bacteria from septic effluent were quantified. first, bacteria removal was compared between septic effluents treated or not with an inhibitor of protozoa (cycloheximide). the mortality rates were 10 times lower in treated effluent (96 cfu ml(-1) d(-1)) than in untreated effluent (1100 cfu ml(-1) d(-1)). secondly, the efficiency of bacteria removal was studied (i) with a biofilm surface and active protozoa, (ii) with a biofilm surface and inactivated protozoa, (iii) with a clean surface. protozoa in the presence of a biofilm were responsible for 60% of bacteria removal. biofilm without protozoa and a clean surface each removed similar quantities of bacteria. grazing by protozoa could be an important biological mechanism for bacterial elimination in wastewater treatment systems. (c) 2006 elsevier ltd. all rights reserved.
mass transfer coefficients of styrene and oxygen into silicone oil emulsions in a bubble reactor. the absorption of oxygen and styrene in water-silicone oil emulsions was independently studied in laboratory-scale bubble reactors at a constant gas flow rate for the whole range of emulsion compositions (0-10% v/v). the volumetric mass transfer coefficients to the emulsions were experimentally measured using a dynamic absorption method. it was assumed that the gas phase contacts preferentially the water phase. in the case of oxygen absorption, it was found that the addition of silicone oil hinders oxygen mass transfer compared to an air-water system. decreases in k(l)a(oxygen) of up to 25% were noted. such decreases in the oxygen mass transfer coefficient, which imply longer aeration times to transfer oxygen, could represent a limiting step in biotechnological processes strongly dependent on oxygen concentration. nevertheless, as the large affinity of silicone oil for oxygen enables greater amounts of oxygen to be transferred from the gas phase, it appears that the addition of more than 5% silicone oil should be beneficial to increase the oxygen transfer rate. in the case of styrene absorption, it was established that the volumetric mass transfer coefficient based on the emulsion volume is roughly constant with the increase in the emulsion composition. in spite of the relatively high cost of silicone oil, water-silicone oil emulsions remain relevant to treat low-solubility volatile organic compounds, such as styrene, in low-concentration gas streams. (c) 2006 elsevier ltd. all rights reserved.
wood bark as packing material in a biofilter used for air treatment. biotechnology has been applied to find green and low cost environmental processes. in the waste gas treatments (odours and volatile organic compounds voc) one of the main biological systems used is biofilters. this technology works at normal operating conditions of temperature and pressure, and therefore it is relatively cheap with high efficiencies when the waste gas is characterized by high flow and low pollutant concentration. the aim of this work is to use wood barks (pinus) as packing material in the biofilter. for this purpose, the influence of various parameters such as residence time of the gas and pollutant loads on removal efficiencies was studied for a biofilter pilot unit. ethanol, methyl ethyl ketone, dichloromethane and toluene were used as pollutant compounds, because they are representative of both volatile organic compounds. packing material stability and good biodegradation performances were found.
biofiltration of volatile organic compounds by a fluidized bed of sawdust. in this study, the use of a natural material, sawdust, in a fluidised biofiltrer has been considered. the performance of the biofiltration of ethanol and toluene was estimated in the presence of the native microorganisms of the material and also after the addition, and a period of acclimatization, of external microorganisms. modifications of the physical and biological characteristics of the material were studied in order to better understand the process. the influence of biofilter shutdown periods was also considered to evaluate the effect of a period of inactivity on subsequent performances. this study shows that a significant degradation of the pollutants is obtainable provided that the following steps are performed: seeding with activated sludge, introduction of nutrients, and control of the changes in the material characteristics and the bed moisture. during the operation of the fluidized bed biofilter, the moisture of the bed had an important effect on the biofilter performance, but was rather difficult to control because of its dependence on the ambient and inlet air temperatures, which changed during the day and the seasons. during the tests with the batch of sawdust particles used as delivered without any sludge enrichment, a reduction in abatement performances was measured with time. in the case of ethanol alone, for a concentration of 0.02 g.m-3, abatement decreased from 24% to 18% then to 7%. the partial or complete addition of sawdust particles previously activated with sludge significantly enhanced the performance of the biofilter, both for ethanol and toluene pollution. abatements of ethanol of 85% and 60% were achieved when the sawdust particles were activated by sludge. in contrast, a 5-week shutdown of the reactor produced a decrease in abatement, either by a loss of microorganism efficiency during their ''starvation'' or by their destruction.
antibacterial effects of chitosan powder: mechanisms of action. chitosan, the deacetylated derivative of chitin, is a natural d-glucosamine polymer that can be extracted from the shells of seafood such as prawns crabs and lobsters. it can be used as a flocculent, plant disease resistant promoter, anti-cancer agent, wound healing promotion agent and antimicrobial agent. the aim of this paper is the study of the interaction between chitosan powder and various kinds of pathogen microorganisms potentially present in water. first of all, physico-chemical characterisations of chitin and chitosan powder were performed. the deacetylation yields were 35 %, 60 % and 80 +/- 10 %. the experimental studies focused on the measurements of the mortality constant rate for various bacterial strains, escherichia coli, pseudomonas aeruginosa, enterococcus faecalis and staphylococcus saprophyticus. an explanation of the antibacterial mechanisms is proposed involving the cell wall disruption due to free amino groups present in chitosan.
sfgp 2007 - microbial growth onto filter media used in air treatment devices. this work deals with microbial growth onto filter media and focuses on the ability of microbial communities to proliferate onto filter media. two microorganism types are studied: microorganisms from activated sludge of wastewater treatment plant (sm) and a toluene specific consortium (tsc). the filter media considered for this study contain activated carbon fibres (acf), combined volatile organic compounds (voc), particles treatment purposes, activated carbon fibres felt (acff) and activated carbon and cellulose fibres felt (ac(2)f(2)). using a static growth procedure during 10 days under 100 % relative humidity, artificially contaminated filters are submitted to microbial colonisation. the final concentration of microorganisms per gram of filter have been assessed using a method developed in the lab, based on filter protein content assay. the average surface charge of inocula and filter's fibres are measured to assess the influence of microorganisms adhesion on contamination. the influence of soot particles on tsc proliferation onto ac(2)f(2) filter is then studied. zeta measures enable the assessment of the implication of soot in microorganisms adhesion onto filter fibres. consequences of microbial contamination on filter permeability and downstream particles released have then been assessed in a filtration device. results demonstrate a better resistance of ac(2)f(2) to microbial colonisation. however, sm have more difficulties to proliferate on acff than tsc, whereas sm colonise easier ac(2)f(2) than tsc. charge surface assay has defined an optimal electrostatic compatibility for tsc and ac(2)f(2) and a minimum for sm and acff. when soot is added to tsc solution before introduction in ac(2)f(2), high contamination shapes were observed whereas only a slight one occur without soot addition. zeta potential measures show favourable charge conditions for adhesion of soot on ac(2)f(2) fibres and tsc on soot particles. the soot may thus have played an interface role in microbial adhesion onto media. this means that electrostatic compatibility between particles is a good approach for assessing microbial adhesion onto filters but could not explain the whole mechanism of microbial proliferation. other parameters like nutrition preferences are certainly involved. the contamination have induced filter characteristics modification. permeability have decreased until 20 % with microbial concentration. colonised filters have released up to 450 microbial particles/cm3. such a release was only observed when ac(2)f(2) was contaminated with fungi using spore as the reproduction vector (inoculum containing tsc and soot).
packing material formulation for odorous emission biofiltration. in biological gas treatment, like biofiltration of volatile organic compounds or odorous substances, the microbial nutritional needs could be a key factor of the process. the aim of this work is to propose a new packing material able to provide the lacking nutrients. in the first part of this study, two kinds of material composed of calcium carbonate, an organic binder and two different nitrogen sources, 3 ammonium phosphate and urea phosphate (up), were compared. the new supports present bulk densities between 0.88 and 1.15 g cm(-3) moisture retention capacities close to 50% and 70%, and water cohesion capacities greater than six months for the material with 20% binder. in the second part, oxygen consumption measurements in liquid experiments show that these packing materials could enhance bacterial growth compared to pine bark or pozzolan and have no inhibitory effect. the biodegradation of different substrates (sodium sulfide and ammonia) and the support colonization by the biomass were evaluated. finally, up 20 was chosen and tested in a hydrogen sulfide or ammoniac biofiltration process. this showed that, for h2s concentrations greater than 100 mg m(-3), up 20 has a real advantage over pine bark or pozzolan. (c) 2067 elsevier ltd. all rights reserved.
biodegradation of endocrine disrupters: case of 17 beta-estradiol and bisphenol a. the biodegradation of 17 beta-estradiol (e2) and bisphenol a (bpa) was compared to that of a reference pollutant, sodium benzoate (sb), known for its high biodegradability. the biodegradation was measured using the sturm test (iso 9439 modified sturm test). the susceptibility of the target pollutants to be degraded by microorganisms of activated sludge from a wastewater treatment plant (wwtp) was evaluated by the production of carbon dioxide (co,). sorption experiments onto inactivated sludge were carried out to assess the contribution of sorption in e2 and bpa removal during biological treatment in a wwtp. e2 was more adsorbed than bpa onto inactivated sludge, probably making it less accessible to assimilation by microorganisms. in fact, e2 was less biodegradable than bpa with 66 % and 74 % of theoretical co2 formation (th-co2) in 28 days, respectively. however, e2 showed faster biodegradation than bpa due to the shorter adaptation time of the microorganisms to start the assimilation. final concentrations were measured and revealed that, under sturm test conditions, e2 was totally removed from the aqueous phase while some traces of bpa were detected. this result could be explained by the lower adsorbability of bpa observed in adsorption experiments onto inactivated sludge. to investigate competition in a bi-component solution, sturm tests were carried out with bpa/sb and e2/sb. moreover, the biodegradation curves obtained did not indicate a toxicity of the target compounds towards microorganisms, which rapidly degraded sb. in the case of bpa/sb, an inflection in the curve confirmed the adaptation time of 4-5 days for bpa to be degraded.
biosorption of cu(ii) from aqueous solution by fucus serratus: surface characterization and sorption mechanisms. in this work, the brown alga fucus serratus (fs) used as a low cost sorbent has been studied for the biosorption of copper(ii) ions in batch reactors. firstly, the characterization of the surface functional groups was performed with two methods: a qualitatively analysis with the study of ft-ir spectrum and a quantitatively determination with potentiometric titrations. from this latter, a total proton exchange capacity of 3.15 mmol g(-1) was extrapolated from the fs previously protonated. this value was similar to the total acidity of 3.56 mmol g(-1) deduced from the gran method. using the single extrapolation method, three kinds of acidic functional groups with three intrinsic pk(a) were determined at 3.5, 8.2 and 9.6. the point of zero net proton charge (pznpc) was found close to ph 6.3. secondly, the biosorption of copper ions was studied. the equilibrium time was about 350 min and the adsorption equilibrium data were well described by the langmuir's equation. the maximum adsorption capacity has been extrapolated to 1.60 mmol g(-1). the release of calcium and magnesium ions was also measured in relation to the copper biosorption. finally, the efficiency of this biosorbent in natural tap water for the removal of copper was also investigated. all these observations indicate that the copper biosorption on fs is mainly based on ion exchange mechanism and this biomass could be then a suitable sorbent for the removal of heavy metals from wastewaters. (c) 2008 elsevier ltd. all rights reserved.
evolution of bacterial community in experimental sand filters: physiological and molecular fingerprints. biofilm development in wastewater treatment system by soil infiltration is often mentioned for its participation to purification efficiency and clogging zone formation. it appears necessary to understand its evolution in order to better control the operation of these systems. the objective of this study was to improve knowledge about the temporal evolution of the biofilm structure in the first centimetres of infiltration system. for this purpose, metabolic fingerprints by biolog ecoplate (tm) and molecular fingerprints by ribosomal intergenic spacer analysis (risa) were carried out on sand, septic effluent and treated effluent samples from two experimental reactors supplied with different hydraulic loads collected at different times. the metabolic capabilities of sand microflora decreased in time. in the same way, molecular structure of the biofilm community changed and converged to similar structure in time. principal components analysis on risa gel revealed a ''buffering effect'' of the sand filter on the genetic structure of the bacterial community from treated effluent. the kinetics of evolution of the both metabolic and genetic fingerprints showed a reduction of the metabolic and genetic potentials of septic and treated effluents for the same times. the population dynamic within the biofilms appears interesting evidence in the comprehension of the operation of the treatment systems.
evaluation of a new packing material for h2s removed by biofiltration. this study aims to evaluate the feasibility of using a new packing material (up20) in treating h2s. three identical laboratory-scale biofilters, filled with, respectively, up20 alone, pine bark, and a configuration made of two layers of pozzolan/up20 (80/20, v/v), were used for critical comparison. various concentrations of h2s (up to 100 ppmv) were used to determine the optimum biofilter performances. the superficial velocity of the polluted gas on each biofilter was 65 m h(-1) (0.018 m s(-1); gas flow rate 0.5 n m(3) h(-1)) corresponding to an empty bed residence time of 57s. changes in elimination capacity, removal efficiency, moisture content, temperature and ph were tracked during 95 days. the pressure drops along each biofilter were also measured by varying the gas flow rate from 0.5 to 4n m(3) h(-1). after 63 days of operation, the loading rate was significantly increased to 10 g m(-3) h(-1) and the up20 biofilter retained a removal efficiency of more than 93%, indicating a strong ability to stimulate microbial activity (compared to 69% for the pine bark biofilter and 74% for the biofilter filled with a configuration of two layers of pozzolan/up20). a michaelis-menten type equation was applied and the maximum removal rate (v-m) and saturation constant stant (k-s) were calculated. v-m was evaluated at 35g h2s m(biofilter)(-3) h(-1) for up20 (14 and 15g h2s m(biofilter)(-3) h(-1) for pine bark and pozzolan/up20, respectively). the saturation constant k-s was 70 ppmv for up20 (18 ppmv for pine bark and 20 ppmv for pozzolan/up20) indicating that the new packing material will be effective in treating large pollutant concentrations. at low concentrations of pollutant, the results suggest that a biofilter with a configuration of two layers of pozzolan/up20 is the most suitable choice for treating h2s. (c) 2008 elsevier b.v. all rights reserved.
natural seaweed waste as sorbent for heavy metal removal from solution. biosorption is a suitable heavy metal remediation technique for the treatment of aqueous effluents of large volume and low pollutant concentration. however, today industrial applications need the selection of efficient low-cost biosorbents. the aim of this work is to investigate brown alga such as fucus serratus (fs) as a low-cost biosorbent, for the fixation of metallic ions, namely cu2+, zn2+, pb2+, ni2+, cd2+ and ce3+, in a batch reactor. biosorption kinetics and isotherms have been performed at ph 5.5. for all of the studied metallic ions, the equilibrium time is about 450 min and a tendency based on the initial sorption rate has been established: ce3+ zn2+ ni2+ cu2+ cd2+ pb2+. the adsorption equilibrium data are well described by the langmuir equation. the sequence of the maximum adsorption capacity is pb2+ cu2+ &gt;&gt; ce3+ ni2+ cd2+ zn2+ and values are ranged between 1.78 and 0.71 mmol g-1. these results indicate that the fs biomass is a suitable biosorbent for the removal of heavy metals from wastewater and can be tested in a dynamic process. the selected pilot process involves a hybrid membrane process: a continuous stirred tank reactor is coupled with a microfiltration immersed membrane, in order to confine the fs particles. a mass balance model is used to describe the adsorption process and the breakthrough curves are correctly modelled. based on these results, it is demonstrated that fs is an interesting biomaterial for the treatment of water contaminated heavy metals.
characterization techniques of packing material colonization in gas biofiltration processes. null
evaluation of innovative packing materials for the biodegradation of h2s: a comparative study. the biofiltration of gas polluted with h2s was carried out using innovative configurations of packing materials (i.e. a new synthetic material called up20, sapwood, peat, pozzolan and pine bark). a comparison of seven different configurations (media alone or in combination) was made based on biofilter performances and pressure drop measurements. biofilters were operated continuously for at least 95 days at a constant flow rate (0.5 n m(3) h(-1) corresponding to a superficial velocity of 65 m h(-1) and an empty bed residence time of 57 s). elimination capacities and removal efficiencies were calculated according to loading rates varying from 0 to 25.59 m(-3) h(-1) (inlet concentration up to 400 mg m(-3)). biofilter performances were modelled and biokinetic constants were calculated using the ottengraf model and a modified michaelis-menten model. in terms of elimination capacity, packing materials can be ordered from the most efficient to the least efficient: peat-up20 in a mixture &gt; peat-up20 in two layers &gt; peat &gt; pozzolan-up20 in two layers &gt; pine bark &gt; sapwood-up20 in two layers &gt; sapwood. a maximal removal rate, v-m, of 55 g m(-3) h(-1) was calculated for biofilters filled with peat-up20 (in a mixture or in two layers) and peat (in comparison, v-m = 8.3 m(-3) h(-1) for a biofilter filled with sapwood). peat is the best material to treat high h2s concentrations and the addition of up20 can significantly increase the removal efficiency. the pozzolan-up20 combination represents an interesting packing material to treat pollutant loading rates up to 5 g m(-3) h(-1) with low pressure drops. for low h2s concentrations, sapwood can be considered as a good support for h2s degradation with pollutant loading rates up to 4 g m(-3) h(-1). (c) 2010 society of chemical industry.
silicone oil: an effective absorbent for the removal of hydrophobic volatile organic compounds. background: hydrophobic volatile organic compounds (vocs), such as toluene, dimethyl sulfide (dms) and dimethyl disulfide (dmds), are poorly soluble in water and classical air treatment processes like chemical scrubbers are not efficient. an alternative technique involving an absorption step in an organic solvent followed by a biodegradation phase was proposed. the solvent must fulfil several characteristics, which are key factors of process efficiency, and a previous study allowed polydimethylsiloxane (or pdms, i.e. silicone oil) to be selected for this purpose. the aim of this paper was to determine some of its characteristics like absorption capacity and velocity performances (henry's constant, diffusivity and mass transfer coefficient), and to verify its non-biodegradability. results: for the three targeted vocs, henry's constants in silicone oil were very low compared to those in water, and solubility was infinite. diffusivity values were found to be in the range 10(-10) to 10(-11) m(2) s(-1) and mass transfer coefficients did not show significant differences between the values in pure water and pure silicone oil, in the range 1.0 x 10(-3) to 4.0 x 10(-3) s(-1) for all the vocs considered. silicone oil was also found to be non-biodegradable, since its biological oxygen demand (bod(5)) value was zero. conclusion: absorption performances of silicone oil towards toluene, dms and dmds were determined and showed that this solvent could be used during the first step of the process. moreover, its low biodegradability and its absence of toxicity justify its use as an absorbent phase for the integrated process being considered. (c) 2010 society of chemical industry.
removal of arsenic(v) onto chitosan: from sorption mechanism explanation to dynamic water treatment process. the aim of this work consists in a feasibility study to understand how arsenate ions could be removed from contaminated water by sorption onto chitosan, a biopolymer extracted from the wastes of the seafood industries. firstly, a batch adsorption study investigates different models, namely langmuir, freundlich, tempkin, and redlich-peterson. the sorption mechanism is shown to be sorption by an electrostatic attraction, with thermodynamic parameters indicating an exothermic and spontaneous reaction. the main influencing parameters are the temperature, the ph and the presence of other ions. secondly, a semi-dynamic membrane process is proposed: a stirred batch reactor is coupled with a microfiltration immersed-membrane. a mass balance model is used to describe the adsorption process and the breakthrough curves are well simulated. (c) 2010 elsevier b.v. all rights reserved.
determination of partition coefficients of three volatile organic compounds (dimethylsulphide, dimethyldisulphide and toluene) in water/silicone oil mixtures. the main objective of this work was to propose a model able to predict the partition coefficient of odorous or toxic gaseous pollutants (dimethylsulphide, dimethyldisulphide and toluene) in water/silicone oil mixtures. experimental measurements using a static headspace method were carried out for pure water (h(voc,water)), for pure silicone oil (h(voc,solvent)) and for mixtures of varying composition (h(voc,mixture)). the dramatic decrease in the partition coefficient (h(voc,mixture)) with oil addition clearly showed a deviation from linearity, which was more pronounced for increasing h(voc,water)/h(voc,solvent) ratios. moreover, experiments using a dynamic absorption method underlined that the absorption capacity of a biphasic water/silicone oil mixture can be classed as the absorption capacity of a pseudo-homogeneous phase whose physical properties (molecular weight and density) can be calculated from the physical properties of water and solvent, and balanced using the ''equivalent absorption coefficients'' h(voc,mixture)/h(voc,water) and h(voc,mixture)/h(voc,solvent). an ''equivalent absorption capacity'' concept is then proposed, which should be useful to design absorption units using two-phase liquid mixtures for the treatment of industrial air loaded with volatile organic compounds. (c) 2010 elsevier b.v. all rights reserved.
laundry water recycling in ship by direct nanofiltration with tubular membranes. the present study deals with the feasibility to implement, on board ship, a direct nanofiltration process in order to treat laundry grey waters and recycle 80% to the inlets of the washing machines. at first, a specific methodology for real laundry grey water production was set up at the laboratory for the purpose. characteristics of the lab-produced grey water are close to those observed on board (ph 7,1300 mg(cod)/l, 80 mg(tss)/l). then, a membrane screening in view of a selection was realised at the lab scale. a direct nanofiltration process (without pre-treatment) on tubular pci-afc80 membrane (35 bar, 25 degrees c, volumic-reduction-factor 5) allowed us to produce a permeate free of micro-organisms and suspended solids and with only 48 mg(cod)/l and 7 mg(toc)/l based on these satisfactory preliminary results, a techno-economic study was conducted (up-scaling) with the aim of producing daily 52 m(3) of recyclable permeate from 65 m(3) of polluted grey water. finally, orders of magnitude, both for energetic consumption and processing costs, are proposed. (c) 2010 elsevier b.v. all rights reserved.
identifying and exploiting problem structures using explanation-based constraint programming. identifying structures in a given combinatorial problem is often a key step for designing efficient search heuristics or for understanding the inherent complexity of the problem. several operations research approaches apply decomposition or relaxation strategies upon such a structure identified within a given problem. the next step is to design algorithms that adaptively integrate that kind of information during search. we claim in this paper, inspired by previous work on impact-based search strategies for constraint programming, that using an explanation-based constraint solver may lead to collect invaluable information on the intimate dynamically revealed and static structures of a problem instance. moreover, we discuss how dedicated or solving strategies (such as benders decomposition) could be adapted to constraint programming when specific relationships between variables are exhibited.
vehicle routing problem in mixed flows for reverse logistics: a modeling framework. null
multiobjective optimization and constraint programming. null
subcontractors scheduling on residential buildings construction sites. null
integrating strong local consistencies into constraint solvers. null
idempotent version of the fréchet contingency array problem. in this paper we study the idempotent version of the so-called fréchet correlation array problem. the problem is studied using an algebraic approach. the major result is that there exists a unique upper bound and several lower bounds. the formula for the upper bound is given. an algorithm is proposed to compute one lower bound. another algorithm is provided to compute all lower bounds, but the number of lower bounds may be a very large number. note that all these results are only based on the distributive lattice property of the idempotent algebraic structure.
a particle swarm approach for the mllp. this contribution presents a discrete particle swarm optimization (dpso) approach for the multi-level lot-sizing problem (mllp), which is an uncapacitated lot sizing problem dedicated to materials requirements planning (mrp) systems. the originality of the proposed dpso approach is that it is based on cost modification. by the way, we use pso for that it has been developed: the continuous optimization. each particle of the swarm is represented by a matrix of logistic costs. a sequential approach heuristic, using wagner-whitin algorithm, is then used to determine the associated production planning. the first results obtained are very encouraging. our dpso outperforms the results recently published with other nature-inspired algorithms.
tactical planning for optimal cash flow and value sharing in supply chain. in this paper, we discuss optimisation of cash flow and value sharing in the context of supply chain planning. based on the previous work of (comelli et al., 2007), a generic objective function which allows cash flow optimisation is presented. it could be adapted to any supply chain planning model by using links between physical and financial flow. a mathematical model is also given: its aim is to share the cash flow created by the whole supply chain among the partners thanks to transfer pricing. then, a framework composed with a linking between tactical planning model and the latter, is presented. finally, a real case study is given to illustrate our approach.
metaheuristic for the capacitated lot sizing problem: a software tool for mps elaboration. the master production schedule elaboration plays a major part in tactical planning. among mathematical models which deal with the tactical planning, a particular one is dedicated to it: the capacitated lot‐sizing problem. literature about its resolution is huge, but few metaheuristics have been developed in order to solve it: we propose to use optimisation methods based on a simulated annealing: the data encoding are based on a production planning matrix and the neighbourhood system is maked up of several possible moves. we also proposed a bi‐objective function which integrates logistic costs and an evaluation of the degree of the capacities' temporarily leave the set of feasible solutions in order to escape from local minimas. we have tested our optimisation methods on benchmarks from the literature and some best results are outperformed. these methods have been integrated into a software tool.
chemical and biological evaluation of scandium(iii)-polyaminopolycarboxylate complexes as potential pet agents and radiopharmaceuticals. null
minimization of absorption contrast for accurate amorphous phase quantification: application to zro2 nanoparticles. monoclinic and tetragonal zirconia samples were characterized by x-ray diffraction, pycnometry, thermogravimetric analysis (tga), fourier transform (ft) ir and mass (ms) spectroscopies, and scanning and transmission electron (tem) microscopies. the results show, for the particular case of a tetragonal zirconia sample, an x-ray-undetected subproduct identified as an amorphous organic phase by ftir-atr (attenuated total reflection) and tga-ms. the observations by tem allowed this amorphous phase to be localized on the surface as a shell coating the nanoparticles. moreover, this amorphous phase was quantified by rietveld refinement via the addition of an internal silicon standard. because zirconia and silicon have different linear absorption coefficients, the microabsorption effect was minimized by using small particle sizes. the amorphous phase was calculated to constitute 11.4 (30)% of the initial mass before brindley correction and 10.6 (30)% of the initial mass after brindley correction. the closeness of these values shows that the contribution of the brindley correction can be neglected if precautions are taken on the microabsorption effect. this work has also highlighted the importance of thoroughly characterizing commercial products, which are not necessarily pure. indeed, the presence of impurities could become a non-negligible parameter for physical and chemical properties studies related to commercial materials.
femtoscopy application of the new epos model to the star experiment. the space-time structure at hadronization was studied within new epos model using femtoscopical methods. the results of the study was compared with the star hbt data for auau collision and first alice hbt data for pp collisions. the model predicted mt and centrality dependence of r out, r side and r long femtoscopy parameters were found to be in accordance with the star data.
the lateral distribution function of coherent radio emission from extensive air showers; determining the chemical composition of cosmic rays. the lateral distribution function (ldf) for coherent electromagnetic radiation from air showers initiated by ultra-high-energy cosmic rays is calculated using a macroscopic description. a new expression is derived to calculate the coherent radio pulse at small distances from the observer. it is shown that for small distances to the shower axis the shape of the electric pulse is determined by the 'pancake' function, describing the longitudinal distribution of charged particles within the shower front, while for large distances the pulse is determined by the shower profile. this reflects in a different scaling of the ldf at small and at large distances. as a first application we calculate the ldf for proton- and iron-induced showers and we show that this offers a very sensitive measure to discriminate between these two. we show that due to interference between the geo-magnetic and the charge-excess contributions the intensity pattern of the radiation is not circular symmetric.
longitudinal spin transfer to $\lambda$ and $\bar{\lambda}$ hyperons in polarized proton-proton collisions at $\sqrt{s}$ = 200 gev. the longitudinal spin transfer, $d_{ll}$, from high energy polarized protons to $\lambda$ and $\bar{\lambda}$ hyperons has been measured for the first time in proton-proton collisions at $\sqrt{s} = 200 \mathrm{gev}$ with the star detector at rhic. the measurements cover pseudorapidity, $\eta$, in the range $|\eta| &lt; 1.2$ and transverse momenta, $p_\mathrm{t}$, up to $4 \mathrm{gev}/c$. the longitudinal spin transfer is found to be $d_{ll}= -0.03\pm 0.13(\mathrm{stat}) \pm 0.04(\mathrm{syst})$ for inclusive $\lambda$ and $d_{ll} = -0.12 \pm 0.08(\mathrm{stat}) \pm 0.03(\mathrm{syst})$ for inclusive $\bar{\lambda}$ hyperons with $&lt;\eta&gt; = 0.5$ and $ = 3.7 \mathrm{gev}/c$. the dependence on $\eta$ and $p_\mathrm{t}$ is presented.
scaling properties at freeze-out in relativistic heavy ion collisions. identified charged pion, kaon, and proton spectra are used to explore the system size dependence of bulk freeze-out properties in cu+cu collisions at $\sqrt{s_{nn}}$=200 and 62.4 gev. the data are studied with hydrodynamically-motivated blast-wave and statistical model frameworks in order to characterize the freeze-out properties of the system. the dependence of freeze-out parameters on beam energy and collision centrality is discussed. using the existing results from au+au and $pp$ collisions, the dependence of freeze-out parameters on the system size is also explored. this multi-dimensional systematic study furthers our understanding of the qcd phase diagram revealing the importance of the initial geometrical overlap of the colliding ions. the analysis of cu+cu collisions, which expands the system size dependence studies from au+au data with detailed measurements in the smaller system, shows that the bulk freeze-out properties of charged particles studied here scale with the total charged particle multiplicity at mid-rapidity, suggesting the relevance of initial state effects.
weighted poisson mixed model for underdispersed longitudinal count data. null
on strong stability and stabilizability of systems of neutral type. for linear stationary systems, the infinite dimensional framework allows one to distinguish different notions of stability: weak, strong or exponential. the purpose of this chapler is to investigate the problem of strong stability, i.e. asymptotic non-exponential stability for linear systems of neutral type in order to use this characterization in the study of the stabilizability problem for this type of systems. an important tool in this investigation is the riesz basis property of generalized eigenspaces of the neutral system, because that the generalized eigenvectors do not form, in general, a riesz basis. this allows one to describe more precisely asymptotic non-exponential stability of neutral systems. for a particular case, conditions of strong stabilizability of neutral type systems are given with a feedback law without derivative of the delayed state.
transforming bpmn process models to bpel process definitions with atl. null
towards an advanced model-driven engineering toolbox. null
an amma/atl solution for the grabats 2009 reverse engineering case study. this paper presents a solution to the reverse engineering case study of grabats 2009 implemented using the atlanmod transformation language (atl), and the atlanmod model management architecture (amma). two scalability approaches are presented: a classical one, as well as an optimization for multiple queries on the same model. the task showing the genericity of the transformation tool is also solved.
achieving rule interoperability using chains of model transformations. null
on the use of higher-order model transformations. the level of maturity that has been reached by model transformation technologies is proved by the growing literature on transformation libraries that address an increasingly wide spectrum of applications. with the success of the modeling and transformation paradigm, the need arises to address more complex applications that require a direct manipulation of model transformations. the uniformity and flexibility of the model-driven paradigm allows this class of applications to make use of the same transformation infrastructure. this is possible because transformations can be translated into transformation models and given as objects to a different class of model transformations, called higher-order transformations (hot). this paper provides an introduction to hots and a survey of the several application cases where their use is relevant. a number of possible future applications of hots is also proposed.
measuring discovered models. model driven engineering (mde) is based on the principle of uniﬁcation. most of the artifacts used or produced in a software project may be uniformly represented by models conforming to various metamodels. in this paper we show how two independent projects could be bridged to produce additional results. the ﬁrst one is about discovering models from legacy code and the second one is about measuring various kind of models. due to the interoperability properties of mde, the integration of these projects was straightforward and allowed to provide a new generic legacy measurement infrastructure.
integration by model-driven virtual tool. null
program comprehension case study for grabats 2009. null
experiments with a higher-level navigation language. writing navigation expressions is an important part of the task of developing a model transformation deﬁnition. when navigation is complex and the size of source models is signiﬁcant, performance issues cannot be neglected. model transformation languages often implement some variants of ocl as their navigation language. writing eﬃcient code in ocl is usually a diﬃcult task because of the nature of the language and the lack of optimizing ocl compilers. moreover, optimizations generally reduce readability. an approach to tackle this issue is to raise the level of abstraction of the navigation language. we propose to complement the regular navigation language of model transformation languages with a high-level navigation language, in order to improve both performance and readability. this paper reports on the initial results of our experiments creating the hln language: a declarative high-level navigation language. we will motivate the problem, and will we describe the language as well as the main design guidelines.
coupling static and dynamic models information. in model driven engineering (mde) a system may be represented by a model conforming to a given metamodel. the joint use of several models representing the same system is called multimodeling. in this work we show how different models representing the same legacy system may be used for program comprehension. more precisely, we show how to jointly exploit static (structural) and dynamic (behavioral) models of the same program to enhance understandability. the key advantage of mde is that all models are based on a uniform representation and thus the joint use of these models is greatly facilitated.
supporting tool reuse with model transformation. null
parsing sbvr-based controlled languages. null
possible benefits of bridging eclipse-emf &amp; microsoft "oslo". null
megamodeling software platforms: automated discovery of usable cartography from available metadata. model-driven reverse engineering focuses on automatically discovering models from different kinds of available information on existing software systems. although the source code of an application is often used as a basic input, this information may take various forms such as: design "models", bug reports, or any kind of documentation in general. all this metadata may have been either built manually or generated (semi)automatically during the whole software life cycle, from the specification and development phase to the effective running of the system. this paper proposes an automated and extensible mde approach to build a usable cartography of a given platform from available metadata by combining several mde techniques. as a running example, the approach has been applied to the topcased mde platform for embedded &amp; real-time systems.
study and desgin of a very high spatial resolution beta imaging system. the b autoradiography is a widely used technique in pharmacology or biological fields. it is able to locate in two dimensions molecules labeled with beta emitters. the development of a gaseous detector incorporating micromesh called pim in the subatech laboratory leads to the construction of a very high spatial resolution apparatus dedicated to b imaging. this device is devoted to small analysis surface of a half microscope slide in particular of 3h or 14c and the measured spatial resolution is 20 μm fwhm. the recent development of a new reconstruction method allows enlarging the field of investigation to high energy beta emitters such as 131i, 18f or 46sc. a new device with a large active area of 18x18 cm2 has been built with a user friendly design. this allows to image simultaneously 10 microscope slides. thanks to a multi-modality solution, it retains the good characteristics of spatial resolution obtained previously on a small surface. moreover, different kinds of samples, like microscope slides or scotches can be analysed. the simulation and experimentation work achieved during this thesis led to an optimal disposition of the inner structure of the detector. these results and characterization show that the pim structure has to be considered for a next generation of b-imager.
complexation with metal ions and colloidal aggregation of natural organic matter in aqueous solutions: a computational molecular modeling perspective (invited talk). null
metal cation complexation with natural organic matter in aqueous solutions: molecular dynamics simulations and potentials of mean force. null
the megapie-test project: supporting research and lessons learned in first-of-a-kind spallation target technology. the megawatt pilot experiment (megapie) has been launched by six european institutions (psi, fzk, cea, sck-cen, enea and cnrs), jaea (japan), doe (us) and kaeri (korea) with the aim to carry out an experiment, in the sinq target location at psi (switzerland), to demonstrate the safe operation of a liquid metal (lead–bismuth eutectic, lbe) spallation target hit by a not, vert, similar1 mw proton beam. the european commission has joined the megapie project through the 5-year (2001–2006) project named megapie-test. this project has been formally concluded with an international workshop, where the results and the lessons learned during the project have been summarised. this work presents a review of the outcome of that workshop.
azimuthal anisotropy of π0 production in au+au collisions at √snn=200 gev: path-length dependence of jet quenching and the role of initial geometry. we have measured the azimuthal anisotropy of π0 production for 1.
transverse momentum dependence of j/ψ polarization at midrapidity in p+p collisions at √s=200 gev. we report the measurement of the transverse momentum dependence of inclusive j/ψ polarization in p+p collisions at √s=200 gev performed by the phenix experiment at the relativistic heavy ion collider. the j/ψ polarization is studied in the helicity, gottfried-jackson, and collins-soper frames for pt&lt;5 gev/c and |y|&lt;0.35. the polarization in the helicity and gottfried-jackson frames is consistent with zero for all transverse momenta, with a slight (1.8 sigma) trend towards longitudinal polarization for transverse momenta above 2 gev/c. no conclusion is allowed due to the limited acceptance in the collins-soper frame and the uncertainties of the current data. the results are compared to observations for other collision systems and center of mass energies and to different quarkonia production models.
high pt direct photon and π0 triggered azimuthal jet correlations and measurement of kt for isolated direct photons in p+p collisions at √s=200 gev. correlations of charged hadrons of 1.
the ordered distribute constraint. in this paper we introduce a new cardinality constraint: ordereddistribute. given a set of variables, this constraint limits for each value v the number of times v or any value greater than v is taken. it extends the global cardinality constraint, that constrains only the number of times a value v is taken by a set of variables and does not consider at the same time the occurrences of all the values greater than v. we design an algorithm for achieving generalized arc-consistency on ordereddistribute, with a time complexity linear in the sum of the number of variables and the number of values in the union of their domains. in addition, we give some experiments showing the advantage of this new constraint for problems where values represent levels whose overrunning has to be under control.
new models for the multi-skill project scheduling problem. null
non-proliferation studies with double chooz detectors. null
design and sizing of an hybrid sailboat, based on a power modeling approach. nowadays, the ecological impacts are took into account by collective consciousness in all fields; in the manufacturing with the concern of the products end-of-life, in construction, or in transport. for this last context, oil is no more considered as the only energy source, and electric or even multi-sources propulsions, named "hybrid" propulsions, can be found. if the hybrid propulsion solution is now well known in the field of automotive, it is not so well integrated in the field of boating. however, with the potential availability of three energy sources, oil, electricity and wind, and a quite free environment of evolution, pleasure boats could be a promising field of application for the hybrid technology. that is the point analyzed in this thesis. to carry through this study, a sizing process had to be set up. several methodologies have already been proposed in the literature since the 90's in the automotive engineering, but most of them operate with a limited set of the constitutive components of the propulsion architecture to be designed. our work background is different. we propose an iterative sizing methodology based on power exchanges modelling. our objective is to represent by a single parameterized model each set of components, to reduce the complexity of the optimization problem linked to the sizing process. the proposed methodology is applied on two sizing problems; for a sailing boat and a hybrid car.
underwater robot navigation around a sphere using electrolocation sense and kalman filter. the aim of this paper is to perform the navigation of an underwater robot equipped with a sensor using the electric sense. the robot navigates in an unbounded environment in presence of spheres. this sensor is inspired of some species of electric fish. a model of this sensor composed of n spherical electrodes is established. the variations of the current due to the presence of the sphere is related to the model of rasnow [3]. unscented kalman filter is used to localize the robot with respect to the sphere and to estimate the size of the sphere. we show that bio-inspired motions improve the detection of the spheres. we illustrate the efficiency of the method in two cases: a two electrodes sensor and a four electrodes sensor.
sensor model for the navigation of underwater vehicles by the electric sense. we present an analytical model of a sensor for the navigation of underwater vehicles by the electric sense. this model is inspired from the electroreception structure of the electric fish. in our model, that we call the poly-spherical model (psm), the sensor is composed of n spherical electrodes. some electrodes play the role of current-emitters whereas others play the role of current-receivers. by imposing values of the electrical potential on each electrode we create an electric field in the vicinity of the sensor. the region where the electric field is created is considered as the bubble of perception of the sensor. each object that enters this bubble is electrically polarized and creates in return a perturbation. this perturbation induces a variation of the measured current by the sensor. the model is tested on objects for which the expression of the polarizability is known. a unique off-line calibration of the poly-spherical model permits to predict the measured current of a real immersed sensor in an aquarium. comparisons in a basic scene between the predicted current given by the poly-spherical model and the measured current given by our test bed show a very good agreement, which confirms the interest of using such fast analytical models for the purpose of navigation.
radiodetection and characterization of the cosmic rays air showers radio emission for energies higher than 10^16 ev with the codalema experiment. null
investigation on heat transfer evaluation for a more efficient two-zone combustion model in the case of natural gas si engines. two-zone model is one of the most interesting engine simulation tools, especially for si engines. however, the pertinence of the simulation depends on the accuracy of the heat transfer model. in fact, an important part of the fuel energy is transformed to heat loss from the chamber walls. also, knock appearance is closely related to heat exchange. however, in the previous studies using two-zone models, many choices are made for heat transfer evaluation and no choice influence study has been carried out, in the literature. the current study aims to investigate the effect of the choice of both the heat transfer correlation and burned zone heat transfer area calculation method and provide an optimized choice for a more efficient two-zone thermodynamic model, in the case of natural gas si engines. for this purpose, a computer simulation is developed. experimental measurements are carried out for comparison and validation. the effect of correlation choice has been first studied. the most known correlations have been tested and compared. our experimental pressure results, supported for more general and reliable conclusions, by a literature survey of many other studies, based on measured heat transfer rates for several si engines, are used for correlation selection. it is found that hohenberg's correlation is the best choice. however, the influence of the burned zone heat transfer area calculation method is negligible.
production of innovative radionuclides at arronax and 211at rit. null
charm and beauty production at rhic. null
collective effects in pp scattering. null
hypernuclei production in heavy ion collisions within a thermal model approach. null
selected highlights from the star experiment at rhic. null
nuclear matter properties from subthreshold kaon production. null
charm and beauty searches using electron-d0 azimuthal correlations and microvertexing techniques. null
correlation results from an event-by-event simulation of the three-dimensional hydrodynamic evolution in ultrarelativistic heavy ion collisions. null
energy loss from heavy quarks in a dense medium. null
bulk and shear viscosities of the gluon plasma. null
is there a way to validate the different models for pp collisions?. null
on the possible experimental determination of the k+n potential on cross section in matter. null
witten index in supersymmetric 3d theories revisited. we have performed a direct calculation of witten index in n = 1,2,3 supersymmetric yang-mills chern-simons 3d theories. we do it in the framework of born-oppenheimer (bo) approach by putting the system into a small spatial box and studying the effective hamiltonian depending on the zero field harmonics. at the tree level, our results coincide with the results of witten, but there is a difference in the way the loop effects are implemented. in witten's approach, one has only take into account the fermion loops, which bring about a negative shift of the (chosen positive at the tree level) chern-simons coupling k. as a result, witten index vanishes and supersymmetry is broken at small k. in the effective bo hamiltonian framework, fermion, gluon and ghost loops contribute on an equal footing. fermion loop contribution to the effective hamiltonian can be evaluated exactly, and their effect amounts to the negative shift k -&gt; k - h/2 for n =1 and k -&gt; k - h for n = 2,3 in the tree-level formulae for the index. in our approach, with rather natural assumptions on the structure of bosonic corrections, the shift k -&gt; k + h brought about by the gluon loops also affects the index. since the total shift of k is positive or zero, witten index appears to be nonzero at nonzero k, and supersymmetry is not broken. we discuss possible reasons for such disagreement.
studies of isolated photon production in simulated proton-proton collisions with alice-emcal. null
quarkonia propagation in qgp: study of elastic and inelastic scattering processes. null
collective effects in proton proton and heavy ion scattering, and the "ridge" at rhic. null
a two degrees of freedom gain scheduled controller design methodology for a multi-motors web transport systems. in web transport systems, the main problem is to control the web velocity and tensions independently, to prevent web breaks, folding, or damage. interesting results have been obtained using multi-variable control strategies. unfortunately, most of the existing methodologies are either not systematic or deal with the tracking and disturbance rejection problems as a whole, and not separately. this paper presents a complete methodology in order to design two-degrees-of-freedom (2dof) controller. the feedforward part is based on a reference model allowing operators to obtain the desired tracking performances (in particular, web tensions and velocity decoupling). the feedback part ensures robustness and disturbance rejection and is designed using two high-level tuning parameters only, thanks to the standard state control (ssc) methodology. however, the system dynamics change greatly during the winding/unwinding process due to the winder/unwinder radius and inertia variations. therefore, a gain-scheduling controller is derived from the interpolation of consistent realizations of the h2 controllers obtained at different points of the operating domain. the resulting controller is tested on a realistic simulator first, and after discretization, on a 3-motor web-handling experimental platform.
megapie spallation target: irradiation of the first protypical spallation target for future ads. null
achievements and lessons learned within the domain 1 "design" of the ip_eurotrans integrated project. null
strangeness enhancement - more than a core corona effect. null
beta decay studies of neutron rich nuclei using total absorption gamma-ray spectroscopy and delayed neutron measurements. null
xt-ads windowless spallation target design and corresponding r&amp;d topics. null
reactor neutrino detection for non proliferation with the nucifer experiment. null
from single heavy-quark to quarkonia formation. null
transport coefficients of the gluon plasma. null
hydrodynamical evolution (ebe) from flux-tube initial conditions in auau@rhic. null
simulation of the expansion and the phase transition of a quark plasma with the nambu-jona-lasinio model. null
quarkonia in urhic can the teach us something about qgp (properties)?. null
centrality dependence of observables more than a core - corona effect ?. null
importance of the conditioning of the chitosan support in a catalyst-containing ionic liquid phase immobilised on chitosan: the palladium-catalysed allylation reaction case. null
first dark matter results from the xenon100 experiment. null
the interaction of $\rho$ and k+ mesons with matter. null
parton energy loss in heavy-ion collisions via direct-photon and charged-particle azimuthal correlations. null
the lateral distribution function of coherent radio emission from extensive air showers: determining the chemical composition of cosmic rays. null
model ingredients and peak mass production in heavy-ion collisions. null
coherent radiation from extensive air showers. the generic properties of the emission of coherent radiation from a moving charge distribution are discussed. the general structure of the charge and current distributions in an extensive air shower are derived. these are subsequently used to develop a very intuitive picture for the properties of the emitted radio pulse. using this picture can be seen that the structure of the pulse is a direct reflection of the shower profile. at higher frequencies the emission is suppressed because the wavelength is shorter than the important length scale in the shower. it is shown that radio emission can be used to distinguish proton- and iron-induced air showers.
radio detection of cosmic ray air showers by the rauger experiment, a fully autonomous and self-triggered system installed at the pierre auger observatory. rauger is a radio experiment constituting three fully autonomous and self-triggered radio stations installed at the center of the pierre auger observatory's surface detector (sd). it aims at the radio detection of the electric field emitted by the secondary charged particles of the atmospherical shower initiated by ultra-high energy cosmic rays. installed in november 2006, we recorded the first atmospherical showers in coincidence with the sd in july 2007. up to now, 65 such coincidences have been obtained. the skymap in local coordinates (zenith angle, azimuth) of these events presents a strong azimuthal asymmetry in agreement with what was observed in the northern hemisphere by the codalema experiment (the asymmetry is simply switched by 180° in azimuth). we also recorded a threefold coincidence making possible a complete reconstruction: both the radio reconstructed shower axis and the shower energy are in perfect agreement with the sd estimations.
first results of the tianshan radio experiment for neutrino detection. we present the first results of a set-up called tianshan radio experiment for neutrino detection (trend) being presently deployed on the site of the 21 cm array (21cma) radio telescope, in xinjiang, china. we describe here its detection performances as well as the analysis method we applied to the data recorded with a small scale prototype. we demonstrate the ability of the trend set-up for an autonomous radio-detection of extended air showers induced by cosmic rays. the full set-up will consist of 80 antennas deployed over a 4 km2 area, and could result in a very attractive and unequalled radio-detection facility for the characterization of showers induced by ultra-high energy neutrinos with energies around 1017 ev.
coherent radio emission from cosmic ray air showers computed by monte-carlo simulation with selfas. an active dipole antenna is in operation since five years at the nançay radio observatory (france) in the codalema experiment. a new version of this active antenna has been developed, whose shape gave its name of "butterfly" antenna. compared to the previous version, this new antenna has been designed to be more efficient at low frequencies, which could permit the detection of atmospheric showers at large distances. despite a size of only 2 m×1 m in each polarization, its sensitivity is excellent in the 30-80 mhz bandwidth. three antennas in dual polarization were installed on the codalema experiment, and four other have been recently installed on the auger area in the scope of the aera project. the main characteristics of the butterfly antenna are detailed with an emphasis on its key features which make it a good candidate for the low frequency radioastronomy and the radio detection of transients induced by high energy cosmic rays.
enhanced optimisation techniques for off-line programming of laser cutting robots. recent advances in laser technology, and especially essential increase of the cutting speed, motivate amendment of the existing robot path methods, which do not allow complete utilisation of the actuator capabilities and also neglect some particularities in the mechanical design of the manipulator arm wrist. this research addresses the optimisation of the 6-axes robot motions for the continuous contour tracking taking into account the redundancy caused by the tool axial symmetry. the particular contribution of the paper is in the area of multi-objective path planning using the graph-based search space representation. the developed path planning algorithm is based on the dynamic programming, which incorporates the constraint checking for each segment of a candidate solution and the penalty assignment for the constraint violation. to generate the smooth motion, each joint trajectory is evaluated by a set of performance indices such as the coordinate deviation, maximum increment, and total displacement of the axes. during the optimisation, the vector objective is converted into the scalar one using the weighted sum or minimax criterion, while the distance between the successive tool locations is evaluated using three types of the distance metrics. in contrast to the previous works, the developed optimisation technique explicitly incorporates verification of the velocity/acceleration constrains, allowing the designer interactively define their importance with respect to the path-smoothness objectives. in addition, it takes into account the capacity of some manipulator wrist axes for unlimited rotation in order to produce more economical motions. the efficiency of the developed algorithms has been carefully investigated via computer simulation. the presented results are implemented in a commercial software package and verified for real-life applications in automotive industry.
event-by-event simulation of the three-dimensional hydrodynamic evolution from flux tube initial conditions in ultrarelativistic heavy ion collisions. we present a sophisticated treatment of the hydrodynamic evolution of ultrarelativistic heavy ion collisions, based on the following features: initial conditions obtained from a flux tube approach, compatible with the string model and the color glass condensate picture; an event-by-event procedure, taking into the account the highly irregular space structure of single events, being experimentally visible via so-called ridge structures in two-particle correlations; the use of an efficient code for solving the hydrodynamic equations in 3+1 dimensions, including the conservation of baryon number, strangeness, and electric charge; the employment of a realistic equation of state, compatible with lattice gauge results; the use of a complete hadron resonance table, making our calculations compatible with the results from statistical models; and a hadronic cascade procedure after hadronization from the thermal matter at an early time.
non-disturbing characterization of natural organic matter (nom) contained in clay rock pore water by mass spectrometry using electrospray and atmospheric pressure chemical ionization modes. we have investigated the composition of the mobile natural organic matter (nom) present in callovo-oxfodian pore water using electrospray ionization mass spectrometry (esi-ms), atmospheric pressure chemical ionization mass spectrometry (apci-ms) and emission-excitation matrix (eem) spectroscopy. the generation of knowledge of the composition, structure and size of mobile nom is necessary if one wants to understand the interactions of these compounds with heavy metals/radionuclides, in the context of environmental studies, and particularly how the mobility of these trace elements is affected by mobile nom. the proposed methodology is very sensitive in unambiguously identifying the in situ composition of dissolved nom in water even at very low nom concentration, due to innovative non-disturbing water sampling and ionization (esi/apci-ms) techniques. it was possible to analyze a quite exhaustive inventory of the small organic compounds of clay pore water without proceeding to any chemical treatment at naturally occurring concentration levels. the structural features observed were mainly acidic compounds and fatty acids as well as aldehydes and amino acids.
probing nuclear compressibility via fragmentation in au+au reactions at 35 amev. the molecular dynamics study of fragmentation in peripheral $^{197}$au +$^{197}$au collisions at 35 mev/nucleon is presented to probe the nuclear matter compressibility in low density regime. the yields of different fragment species, rapidity spectra, and multiplicities of charged particles with charge $3\leq z \leq 80$ are analyzed at different peripheral geometries employing a soft and a hard equations of state. fragment productions is found to be quite insensitive towards the choice of nucleon-nucleon cross sections allowing us to constrain nuclear matter compressibility. comparison of calculated charged particle multiplicities with the experimental data indicates preference for the \emph{soft} nature of nuclear matter.
isospin effects on the system size dependence of balance energy in heavy-ion collisions. we study the effect of isospin degree of freedom on balance energy throughout the mass range between 50 and 350 for two sets of isobaric systems with n/a = 0.5 and 0.58. our fndings indicate that different values of balance energy for two isobaric systems may be mainly due to the coulomb repulsion. we also demonstrate clearly the dominance of coulomb repulsion over symmetry energy.
role of isospin degree of freedom on the mass dependence of balance energy. the effect of isospin degree of freedom on balance energy and its mass dependence has been studied for the mass range between 50 and 350. our results shows the dominance of coulomb potential in isospin effects.
impact parameter dependence of isospin effects on the mass dependence of balance energy. we study the effect of isospin degree of freedom on the balance energy as well as its mass dependence throughout the mass range 48-270 for two sets of isobaric systems with n/z = 1 and 1.4 using isospin-dependent quantum molecular dynamics (iqmd) model. our fndings reveal the dominance of coulomb repulsion in isospin effects on balance energy as well as its mass dependence throughout the range of the colliding geometry.
dependence of balance energy on isospin degrees of freedom. using the isospin-dependent quantum molecular dynamics model we study the isospin effects on the disappearance of flow for the reactions of 58ni+58ni and 58fe+58fe as a function of impact parameter. we found good agreement between our calculations and experimentally measured energy of vanishing flow at all colliding geometries. our calculations reproduce the experimental data within 5%(10%) at central (peripheral) colliding geometries.
effect of nuclear compressibility on the fragmentation in peripheral au+au collisions at 35 amev. we studied the fragmentation in au(35 amev)+au collisions at reduced impact parameters in the range b/b_max=0.55 and 0.95 using soft and hard equations of state. the comparison of of qmd simulations at 100 fm/c as a function of reduced impact parameter $b/b_{max}$ with multics miniball data showed that soft eos accurately reproduces the experimental trend of declining fragment multiplicity with impact parameter. the hard eos on the contrary, seems too explosive to explain the data.
csi-thgem gaseous photomultipliers for rich and noble-liquid detectors. the properties of uv-photon imaging detectors consisting of csi-coated thgem electron multipliers are summarized. new results related to detection of cherenkov light (rich) and scintillation photons in noble liquid are presented.
n=4, 3d supersymmetric quantum mechanics in non-abelian monopole background. using the harmonic superspace approach, we construct the three-dimensional n=4 supersymmetric quantum mechanics of the supermultiplet (3,4,1) coupled to an external su(2) gauge field. the off-shell n=4 supersymmetry requires the gauge field to be a static form of the 't hooft ansatz for the 4d self-dual su(2) gauge fields, that is a particular solution of bogomolny equations for bps monopoles. we present the explicit form of the corresponding superfield and component actions, as well as of the quantum hamiltonian and n=4 supercharges. the latter can be used to describe a more general n=4 mechanics system, with an arbitrary bps monopole background and on-shell n=4 supersymmetry. the essential feature of our construction is the use of semi-dynamical spin (4,4,0) multiplet with the wess-zumino type action.
four-point vector correlators and ads/qcd correspondence. we derive the four-point vector correlators in qcd from ads/qcd correspondence. it is shown that meson poles are correctly reproduced. the final expression also suggests a nonzero amplitude in the limit of zero virtuality of two longitudinal gluons. this fact does not mean that one can produce, absorb or scatter real longitudinal gluons.
center of mass energy and system-size dependence of photon production at forward rapidity at rhic. we present the multiplicity and pseudorapidity distributions of photons produced in au+au and cu+cu collisions at \sqrt{s_nn} = 62.4 and 200 gev. the photons are measured in the region -3.7 &lt; \eta &lt; -2.3 using the photon multiplicity detector in the star experiment at rhic. the number of photons produced per average number of participating nucleon pairs increases with the beam energy and is independent of the collision centrality. for collisions with similar average numbers of participating nucleons the photon multiplicities are observed to be similar for au+au and cu+cu collisions at a given beam energy. the ratios of the number of charged particles to photons in the measured pseudorapidity range are found to be 1.4 +/- 0.1 and 1.2 +/- 0.1 for \sqrt{s_nn} = 62.4 gev and 200 gev, respectively. the energy dependence of this ratio could reflect varying contributions from baryons to charged particles, while mesons are the dominant contributors to photon production in the given kinematic region. the photon pseudorapidity distributions normalized by average number of participating nucleon pairs, when plotted as a function of \eta - ybeam, are found to follow a longitudinal scaling independent of centrality and colliding ion species at both beam energies.
essential aop: the a calculus. null
manipulator motion planning for high-speed robotic laser cutting. recent advances in laser technology, especially the increase of the cutting speed, has motivated the amendment of the existing robot path methods, which do not allow the complete utilisation of the actuator capabilities and neglect certain particularities in the mechanical design of the wrist of the manipulator arm. this research addresses the optimisation of the six-axis robot motion for continuous contour tracking while considering the redundancy caused by the tool axial symmetry. the particular contribution of the paper is in the area of multi-objective path planning using the graph-based search space representation. in contrast to previous work, the developed optimisation technique is based on dynamic programming and explicitly incorporates verification of the velocity/acceleration constraints. this allows the designer to define interactively their importance with respect to the path-smoothness objectives. in addition, this optimisation technique takes into account the capacity of certain manipulator wrist axes for unlimited rotation in order to produce more economical motion. the efficiency of the developed algorithms has been carefully investigated via computer simulation. the presented results are implemented in a commercial software package and verified for real-life applications in the automotive industry.
study of fragmentation using clusterization algorithm with realistic binding energies. we here study fragmentation using a simulated annealing clusterization algorithm (saca) with binding energy at a microscopic level. in an earlier version, a constant binding energy (4 mev/nucleon) was used. we improve this binding energy criterion by calculating the binding energy of different clusters using a modified bethe-weizsäcker mass (bwm) formula. we also compare our calculations with experimental data of the aladin group. nearly no effect of this modification is visible.
the northern site of the pierre auger observatory. the pierre auger observatory is an international facility dedicated to the full-sky study of the highest-energy cosmic rays. the southern site of the auger observatory was completed in june 2008. data collected since january 2004 have yielded important information on the energy spectrum, the primary particle composition, the fluxes of photons and neutrinos and on the anisotropic distribution of the arrival directions of the most energetic particles. on this basis, the scientific motivation for the northern auger observatory site in colorado, usa, is discussed. the overall layout, the key components and the expected performance of this 20 000 km2 hybrid observatory comprised of an array of 4400 surface detectors and 39 fluorescence telescopes are described.
spectroscopic study of 26si for application to nova gamma-ray emission. 26al was the first cosmic radioactivity ever detected in the galaxy. its nucleosynthesis in novae outbursts is still uncertain mainly due to the lack of nuclear information concerning the 25al(p,g )26si reaction. we report here on a neutron-gamma coincidence measurement of the 24mg(3he,ng )26si reaction performed at the orsay tandem facility aiming at the spectroscopic study of astrophysically important 26si states. a new level in the gamow peak is observed at ex = 5:888 mev and the gamma-ray decay scheme of all levels below the proton threshold is confirmed.
charged and strange hadron elliptic flow in cu+cu collisions at $\sqrt{s_{nn}}$ = 62.4 and 200 gev. we present the results of an elliptic flow analysis of cu+cu collisions recorded with the star detector at 62.4 and 200gev. elliptic flow as a function of transverse momentum is reported for different collision centralities for charged hadrons and strangeness containing hadrons $k_{s}^{0}$, $\lambda$, $\xi$, $\phi$ in the midrapidity region $|eta|&lt;1.0$. significant reduction in systematic uncertainty of the measurement due to non-flow effects has been achieved by correlating particles at midrapidity, $|\eta|&lt;1.0$, with those at forward rapidity, $2.5&lt;|\eta|&lt;4.0$. we also present azimuthal correlations in p+p collisions at 200 gev to help estimating non-flow effects. to study the system-size dependence of elliptic flow, we present a detailed comparison with previously published results from au+au collisions at 200 gev. we observe that $v_{2}$($p_{t}$) of strange hadrons has similar scaling properties as were first observed in au+au collisions, i.e.: (i) at low transverse momenta, $p_t&lt;2gev/c$, $v_{2}$ scales with transverse kinetic energy, $m_{t}-m$, and (ii) at intermediate $p_t$, $2.
performances and stability of a 2.4 ton gd organic liquid scintillator target for antineutrino detection. in this work we report the performances and the chemical and physical properties of a (2 x 1.2) ton organic liquid scintillator target doped with gd up to ~0.1%, and the results of a 2 year long stability survey. in particular we have monitored the amount of both gd and primary fluor actually in solution, the optical and fluorescent properties of the gd-doped liquid scintillator (gdls) and its performances as a neutron detector, namely neutron capture efficiency and average capture time. the experimental survey is ongoing, the target being continuously monitored. after two years from the doping time the performances of the gd-doped liquid scintillator do not show any hint of degradation and instability; this conclusion comes both from the laboratory measurements and from the "in-tank" measurements. this is the largest stable gd-doped organic liquid scintillator target ever produced and continuously operated for a long period.
charge collection in the silicon drift detectors of the alice experiment. a detailed study of charge collection efficiency has been performed on the silicon drift detectors (sdd) of the alice experiment. three different methods to study the collected charge as a function of the drift time have been implemented. the first approach consists in measuring the charge at different injection distances moving an infrared laser by means of micrometric step motors. the second method is based on the measurement of the charge injected by the laser at fixed drift distance and varying the drift field, thus changing the drift time. in the last method, the measurement of the charge deposited by atmospheric muons is used to study the charge collection efficiency as a function of the drift time. the three methods gave consistent results and indicated that no charge loss during the drift is observed for the sensor types used in 99% of the sdd modules mounted on the alice inner tracking system. the atmospheric muons have also been used to test the effect of the zero-suppression applied to reduce the data size by erasing the counts in cells not passing the thresholds for noise removal. as expected, the zero suppression introduces a dependence of the reconstructed charge as a function of drift time because it cuts the signal in the tails of the electron clouds enlarged by diffusion effects. these measurements allowed also to validate the correction for this effect extracted from detailed monte carlo simulations of the detector response and applied in the offline data reconstruction.
a model-driven traceability framework for software product lines. software product line (spl) engineering is a recent approach to software development where a set of software products are derived for a well defined target application domain, from a common set of core assets using analogous means of production (for instance, through model driven engineering). therefore, such family of products are built from reuse, instead of developed individually from scratch. spl promise to lower the costs of development, increase the quality of software, give clients more flexibility and reduce time to market. these benefits come with a set of new problems and turn some older problems possibly more complex. one of these problems is traceability management. in the europe an ample project we are creating a common traceability framework across the various activities of the spl development. we identified four orthogonal traceability dimensions in spl development, one of which is an extension of what is often considered as "traceability of variability". this constitutes one of the two contributions of this paper. the second contribution is the specification of a metamodel for a repository of traceability links in the context of spl and the implementation of a respective traceability framework. this framework enables fundamental traceability management operations, such as trace import and export, modification, query and visualization. the power of our framework is highlighted with an example scenario.
$k^-$ and $\bar p$ spectra for au+au collisions at $\sqrt{s}$ = 200 gev from star, phenix and brahms in comparison to core-corona model predictions. based on results obtained with event generators we have launched the core-corona model. it describes in a simplified way but quite successfully the centrality dependence of multiplicity and $$ of identified particles observed in heavy-ion reaction at beam energies between $\sqrt{s}$ = 17 gev and 200 gev. also the centrality dependence of the elliptic flow, $v_2$, for all charged and identified particles could be explained in this model. here we extend this analysis and study the centrality dependence of single particle spectra of $k^-$ and ${\bar p}$ measured by the phenix, star and brahms collaborations. we find that also for these particles the analysis of the spectra in the core-corona model suffers from differences in the data published by the different experimental groups, notably for the pp collisions. as for protons and $k^+$ for each experience the data agree well with the prediction of the core-corona model but the value of the two necessary parameters depends on the experiments. we show as well that the average momentum as a function of the centrality depends in a very sensitive way on the particle species and may be quite different for particles which have about the same mass. therefore the idea to interpret this centrality dependence as a consequence of a collective expansion of the system, as done in blast way fits may be premature.
surrounding effects and sensitivity of the codalema experiment. future autonomous systems of cosmic ray radiodetection will be installed over large areas, encountering various environmental and noise conditions. it is thus essential to check and evaluate the influence of the vicinity on the sensitivity of detection. in this paper, the main environmental influences on the performances of the codalema experiment are presented. it will be shown that the performances and sensitivity of the detector are not affected by the environment, and that the new codalema autonomous detection station can reach the ultimate accessible sensitivity even in a quite noisy environment. this allows deconvolving the detector's response and recovering the real spectral characteristics of the cosmic ray air showers.
radio emission modelization - observables and interpretation. null
[acute hemiparesis revealing a neuroborreliosis in a child]. we report on a 11-year-old boy who had 2 acute hemiparesis episodes over a period of 1 month. he suffered from headache and fatigue since 1 year. he could not remember neither a tick bite nor a local erythematous skin lesion. the diagnosis of neuroborreliosis was based on intrathecal production of specifics antibodies. furthermore, the csf/blood glucose ratio was decreased (0.14), which was rarely described. cranial mri showed left capsulothalamic inflammation and a vasculitis. the patient was successfully treated by ceftriaxone. neuroborreliosis should be considered in all children with stroke-like episode, even in the absence of a history of a tick bite.
statistical method for the determination of the ignition energy of dust cloud-experimental validation. powdery materials such as metallic or polymer powders play a considerable role in many industrial processes. their use requires the introduction of preventive safeguard to control the plants safety. the mitigation of an explosion hazard, according to the atex 137 directive (1999/92/eu), requires, among other things, the assessment of the dust ignition sensitivity. prisme laboratory (university of orléans) has developed an experimental set-up and methodology, using the langlie test, for the quick determination of the explosion sensitivity of dusts. this method requires only 20 shots and ignition sensitivity is evaluated through the e50 (energy with an ignition probability of 0.5). a hartmann tube, with a volume of 1.3 l, was designed and built. many results on the energy ignition thresholds of partially oxidised previous termaluminiumnext term were obtained using this experimental device (baudry, 2007) and compared to literature. e50 evolution is the same as previous mie but their respective values are different and previous mie is lower than e50 however the link between e50 and previous mie has not been elucidated. in this paper, the langlie method is explained in detail for the determination of the parameters (mean value e50 and standard deviation σ) of the associated statistic law. the ignition probability versus applied energy is firstly measured for lycopodium in order to validate the method. a comparison between the normal and the lognormal law was achieved and the best fit was obtained with the lognormal law. in a second part, the langlie test was performed on different dusts such as previous aluminium, cornstarch, lycopodium, coal, and pa12 in order to determine e50 and σ for each dust. the energies e05 and e10 corresponding respectively to an ignition probability of 0.05 and 0.1 are determined with the lognormal law and compared to previous mie find in literature. e05 and e10 values of ignition energy were found to be very close and were in good agreement with previous mie in the literature.
discrepancies in thorium oxide solubility values: study of attachment/detachment processes at the solid/solution interface. the solubility of thorium under oxide and/or hydroxide forms has been extensively studied for many years. nevertheless, a large discrepancy in the solubility values is noticed in the literature. we study th atom exchange between thorium oxide surfaces and various aqueous solutions (0.01 mol*l−1 nacl for 0.0 &lt; ph &lt; 5.2) to address this issue. by solid-state characterization [x-ray photoelectron spectroscopy (xps), scanning electron microscopy, and atomic force microscopy], we determined that 80% of the xps accessible near the surface region of sintered thorium oxide is represented by the less reactive tho2(cr) grains. the remaining 20% corresponds to thox(oh)y(h2o)z, which is largely associated with grain boundaries. only the latter fraction is involved in solid/solution exchange mechanisms. local conditions (thorium concentrations, ph values, etc.) in grain boundaries lead to an adjustment of the "local solubility constraints" and explain the thorium concentration measured in our experiments. for ph &lt;5.2, the thorium concentration and ph gradient between the bulk solution and grain-boundary regions imply that the solubility values mainly depend on the availability and accessibility of thox(oh)y(h2o)z. we have performed two solubility experiments with a 232tho2(cr) solid in a 0.01 mol*l−1 nacl solution for 300 days. in a first experiment, we measured 232th concentrations in dissolution experiments in order to determine the detachment rates of th atoms from the solid surface. in a subsequent step, we added 229th to the solution in order to measure the surface attachment rate for dissolved th atoms. this allowed an assessment of the net balance of th atom exchange at the solid/solution interface. the empirical solubility data do not correspond to the thermodynamic bulk phase/solution equilibrium because measured solution concentrations are controlled by site-specific exchange mechanisms at the solid/solution interface. therefore, for sparingly soluble solids, one needs to quantify site-specific surface attachment and detachment rates if one wants to assess solubility constraints.
one step purification process for no-carrier-added (64)cu produced using enriched nickel target. null
study of the composition of ultra-high energy cosmic rays detected by the pierre auger observatory and analysis of the associated hadronic mechanisms. ultra high energy cosmic rays (uhecr), i.e. e &gt; 1 eev, raise many questions about their origin and constitute a challenge to modern physics. these cosmic rays entering the atmosphere dissipate their huge energy by generating a shower of secondary particles whose development is significantly different depending on the nature of the primaries. the study of the composition of uhecr is therefore a major interest both in understanding the hadronic processes which govern the evolution of showers and in identifying the sources of this radiation. given its hybrid structure and the size of its unmatched network of ground detectors, the pierre auger observatory can provide clear answers to the issues raised by uhecr. in this thesis, we are particularly interested in the muon component of air showers. first, we show how the hadronic parameters define the production of muons. then we present an original method to extract this muon component and deduce the implications on the composition of uhecr. the results of this approach suggest a transition from a heavy composition to a light one when the energy increases. finally, we address the measurement of cosmic-air cross section and present the first results derived from the pierre auger observatory data.
alendronate-doped apatitic cements as a potential technology for the prevention of osteoporotic hip fractures. null
aspects preserving properties. aspect oriented programming can arbitrarily distort the semantics of programs. in particular, weaving can invalidate crucial safety and liveness properties of the base program. in this article, we identify categories of aspects that preserve some classes of properties. specialized aspect languages can be then designed to ensure that aspects belong to a specific category and therefore that woven programs will preserve the corresponding properties. our categories of aspects, inspired by katz's, comprise observers, aborters and confiners. observers introduce new instructions and a new local state but they do not modify the base program's state and control-flow. aborters are observers which may also abort executions. confiners only ensure that executions remain in the reachable states of the base program. % these categories (along with three other) are defined precisely based on a language independent abstract semantics framework. the classes of preserved properties are defined as subsets of ltl for deterministic programs and ctl* for non-deterministic ones. we can formally prove that, for any program, the weaving of any aspect in a category preserves any property in the related class. we present, for most aspect categories, a specialized aspect language which ensures that any aspect written in that language belongs to the corresponding category. it can be proved that these languages preserve the corresponding classes of properties by construction. the aspect languages share the same expressive pointcut language and are designed \wrt a common imperative base language. each category and language are illustrated by simple examples. the appendix provides semantics and examples of proofs: the proof of preservation of properties by a category and the proof that all aspects written in a language belong to the corresponding category.
environmental impact assessment of urban mobility plan : a methodology including socio-economic consequences. the project objective is to develop an optimized methodology to assess the environmental impacts of urban mobility plans (ump, in french pdu),taking into account their social and economic consequences. the main proposed methodology is based on a systemic approach : multi-factor (air quality, noise, energy consumption, greenhouse gas emission) numerical simulations with a chain of physically-based models, based on alternative and comparative scenarios. the social and economic consequences of these alternative simulations are assessed by means of econometric models. two alternative approaches are explored: (i) the use of composite environmental indicators to correlate the sources to the impacts, especially health impacts, and (ii) the analysis of sample surveys on what makes inhabitants'quality of life, well-being and territorial satisfaction and on citizens'behavioral changes linked to transport offer changes.
modelling and control of flexible manipulators. null
poincaré-cosserat equations for lighthill three-dimensional dynamic model of a self propelled eel devoted to robotics. in this article, we propose a dynamic model of the three-dimensional eel swim. this model is analytical and suited to the on-line control of eel-like robots. the proposed solution is based on the large amplitude elongated body theory of lighthill and a working frame recently proposed in [1] for the dynamic modeling of hyper-redundant robots. this working frame was named "macro-continuous" since at this macroscopic scale, the robot (or the animal) is considered as a cosserat beam internally (and continuously) actuated. this article proposes new results in two directions. firstly, it achieves an extension of the lighthill theory to the case of a self propelled body swimming in three dimensions, while including a model of the internal control torque. secondly, this generalization of the lighthill model is achieved due to a new set of equations which is also derived in this article. these equations generalize the poincaré equations of a cosserat beam to the case of an open system containing a fluid stratified around the slender beam.
fast dynamics of a three dimensional eel-like robot: comparisons with navier-stokes simulations. this article proposes a dynamic model of the swim of elongated ﰣshes suited to the on-line control of bio-mimetic eel-like robots. the approach is analytic and can be considered as an extension of the original reactive "large-elongated-body-theory" of lighthill to the three dimensional self propulsion augmented of a resistive empirical model. while all the mathematical fundamentals are detailed in [1], this article essentially focuses on the numerical validation and calibration of the model and the study of swimming gaits. the proposed model is coupled to an algorithm allowing us to compute the motion of the ﰣsh head and the ﰣeld of internal control torque from the knowledge of the imposed internal strain ﰣelds. based on the newton-euler formalism of robots dynamics, this algorithm works faster than real time. as far as precision is concerned, many tests obtained with several planar and three dimensional gaits are reported and compared (in the planar case) with a navier-stokes solver, devoted until today to the planar swim. the comparisons obtained are very encouraging since in all the cases we tested, the diﰢerences between our simpliﰣed and reference simulations do not exceed ten per cent.
dynamic modeling and simulation of a 3-d serial eel-like robot. null
further results on the controllability of the satellite with two rotors: open-loop control and path planning. null
macro-continuous computed torque algorithm for a three-dimensional eel-like robot. this paper presents the dynamic modeling of a continuous three-dimensional swimming eel-like robot. the modeling approach is based on the "geometrically exact beam theory" and on that of newton-euler, as it is well known within the robotics community. the proposed algorithm allows us to compute the robot's galilean movement and the control torques as a function of the expected internal deformation of the eel's body.
fast motions in biomechanics and robotics". springer, chapitre: "re-injecting the structure in nmpc schemes: application to the constrained stabilization of a snakeboard. null
modelling a maintenance scheduling problem with alternative resources. effective management of maintenance in buildings can have a signi cant impact on the total life cycle costs and on the building energy use. nevertheless, the building maintenance scheduling problem has been infrequently studied. in this paper, we present constraint-based scheduling models for the building maintenance scheduling problem, where each activity has a set of alternative resources. we consider two di erent models, one using basic constraints, and the other using our new and modi fied global constraints, which handle alternative disjunctive resources for each activity to allow propagation before activities are assigned to resources. we evaluate these models on randomly generated problems and show that while the basic model is faster on smaller problems, the global con- straint model scales better.
closing the open shop: contradicting conventional wisdom. this paper describes a new approach for solving disjunctive temporal problems such as the open shop and job shop scheduling domains. much previous research in systematic search approaches for these problems has focused on developing problem specific constraint propagators and ordering heuristics. indeed, the common belief is that many of these problems are too difficult to solve without such domain specific models. we introduce a simple constraint model that combines a generic adaptive heuristic with naive propagation, and show that it often outperforms state-of-the-art solvers for both open shop and job shop problems.
azimuthal correlations of electrons from heavy-flavor decay with hadrons in p+p and au+au collisions at sqrt(s_nn)=200 gev. measurements of electrons from the decay of open-heavy-flavor mesons have shown that the yields are suppressed in au+au collisions compared to expectations from binary scaled p+p collisions. these measurements indicate that charm and bottom quarks interact with the hot-dense matter produced in heavy-ion collisions much more than expected. here we extend these studies to two-particle correlations where one particle is an electron from the decay of a heavy-flavor meson and the other is a charged hadron from either the decay of the heavy meson or from jet fragmentation. these measurements provide more detailed information about the interactions between heavy quarks and the matter, such as whether the modification of the away-side-jet shape seen in hadron-hadron correlations is present when the trigger particle is from heavy-meson decay and whether the overall level of away-side jet suppression is consistent. we statistically subtract correlations of electrons arising from background sources from the inclusive electron-hadron correlations and obtain two-particle azimuthal correlations at sqrt(s_nn)=$200 gev between electrons from heavy-flavor decay with charged hadrons in p+p and also first results in au+au collisions. we find the away-side-jet shape and yield to be modified in au+au collisions compared to p+p collisions.
a new experimental setup for a high performance double electropneumatic actuators system. this paper presents design, modelization and control of a new electropneumatic test bench. this latter has been designed for many applications given that it allows high accurancy control and dynamic perturbation force. in fact, the main originality (with respect to previous test benchs) of this test bench is that it is composed by two actuators, the first one being controlled in position, the second one generating perturbation forces. this latter one allows to evaluate the performance of control laws with respect to dynamical forces.
design and optimization of an hybrid sailboat by a power modeling approach. in this paper an original modeling approach is proposed. this approach is based on power exchanges and highlights the most influential parameters of a complex system, without having to choose particular technological solutions. this is thus a particularly well suited solution for a first and global design or for the validation of a concept. this approach has been efficiently used for the optimal sizing and control of a hybrid sailboat which comprises a thermal engine and two electric motors in serie configuration associated with sails. the control strategy is organized in two levels: a global one, which defines power objectives, and a local one, which maximizes the efficiency, while guaranteeing the security of each component.
a multivariable centralized controller design methodology for a steer-by-wire system. this paper presents a multivariable centralized control law for a steer-by-wire system. the controller is designed by solving an h2 problem leaning on a model-matching approach. a particular steering column model is used to define the expected performances for the steer-by-wire system. a driver-vehicle model is used to evaluate the behaviour of the steering system (through the h2 criterion) in a realistic way . this make the approach proposed original and the resulting steer-by-wire system robust.
power modeling for the optimization of a marine hybrid propulsion. in this paper an original modeling approach is proposed. this approach is based on power exchanges and highlights the most influential parameters of a complex system, without having to choose particular technological solutions. this is thus a perfect solution for a first and global design or for the validation of a concept. this approach has been efficiently employed for the optimal sizing and control of a marine hybrid propulsion which comprises a thermal engine and two electric motors in series configuration associated with sails. the control strategy is organized in two levels: a global one, which defines power objectives, and a local one which maximizes the efficiency and guarantees the component security.
heavy quark energy loss in high multiplicity proton proton collisions at lhc. one of the most promising probes to study deconfined matter created in high energy nuclear collisions is the energy loss of (heavy) quarks. it has been shown in experiments at the relativistic heavy ion collider (rhic) that even charm and bottom quarks, despite their high mass, experience a remarkable medium suppression in the quark gluon plasma. in this exploratory investigation we study the energy loss of heavy quarks in high multiplicity proton-proton collisions at lhc energies. although the colliding systems are smaller than compared to those at rhic (p+p vs. au+au) the higher energy might lead to multiplicities comparable to cu+cu collisions at rhic. recently the cms collaboration has shown that these particles likely interact among each other. the interaction of charm quarks with this environment gives rise to a non-negligible suppression of high momentum heavy quarks in elementary collisions.
new constructive heuristics for the multi-compartment vehicle routing problem with stochastic demands. null
a decomposition approach to the batch processing problem. null
solving the vehicle routing problem with stochastic demands with a multiple scenario approach. traditional approaches for the vrpsd aim at designing a-priori robust plans that avoid potential route failures. however, the widespread and inexpensive real-time communication and geolocalization technologies have opened promising perspectives in this field. we illustrate on the vrpsd the flexibility of jmsa, a generic framework for multiple scenario approach. preliminary results show that a continuous re-optimization leads to reductions in route failures and improvements in cost efficiency.
urban mobility plan environmental impacts assessment: a methodology including socio-economic consequences - the eval-pdu project. the project objective is to develop an optimized methodology to assess the environmental impacts of urban mobility plans (ump, in french pdu), taking into account their social and economic consequences. the main proposed methodology is based on a systemic approach: multi-factor (air quality, noise, energy consumption, greenhouse gas emission) numerical simulations with a chain of physically-based models, based on alternative and comparative scenarios. the social and economic consequences of these alternative simulations are assessed by means of econometric models. two alternative approaches are explored: (i) the use of composite environmental indicators to correlate the sources to the impacts, especially health impacts, and (ii) the analysis of sample surveys on what makes inhabitants' quality of life, well-being and territorial satisfaction and on citizens' behavioral changes linked to transport offer changes.
geomagnetic origin of the radio emission from cosmic ray induced air showers observed by codalema. the new setup of the codalema experiment installed at the radio observatory in nançay, france, is described. it includes broadband active dipole antennas and an extended and upgraded particle detector array. the latter gives access to the air shower energy, allowing us to compute the efficiency of the radio array as a function of energy. we also observe a large asymmetry in counting rates between showers coming from the north and the south in spite of the symmetry of the detector. the observed asymmetry can be interpreted as a signature of the geomagnetic origin of the air shower radio emission. a simple linear dependence of the electric field with respect to ∧ is used which reproduces the angular dependencies of the number of radio events and their electric polarity.
the pourbaix diagram of astatine in aqueous medium. null
advances in the development of astatine-radiolabelling protocols: exploring the metallic character of astatine. null
temperature effect on u(vi) sorption onto na-bentonite. u(vi) sorption on a purified na-bentonite was investigated from 298±2 to 353±2 k by using a batch experimental method as a function of ph, u(vi) concentration, carbonate concentration and solid-to-liquid ratio (m/v). the data at 298±2 k could be well described by a surface complexation model (scm) with a complex located on layer sites (x2uo2) and three complexes located on edge sites (≡souo2+, ≡so(uo2)3(oh)5, and ≡so(uo2)3(oh)72-). the intrinsic equilibrium constants (kint) of the surface reactions at 333±2 k and 353±2 k were obtained by fitting u(vi) sorption curves versus ph on the na-bentonite. the model enables u(vi) sorption in the presence of carbonate ( =10-3.58 atm) to be described without considering any ternary surface complexes involving carbonate, except for underestimation around ph 7 (6 &lt; ph &lt; 7.5). the standard enthalpy changes ( ) of the surface reactions were evaluated from the kint values obtained at three temperatures (298±2, 333±2 and 353±2 k) via the van't hoff equation. the proposed scm and of the surface reactions enable u(vi) sorption on the na-bentonite at other temperatures to be predicted.
a rapid microwave-assisted procedure for easy access to nx polydentate ligands for potential application in α-rit. heterocycles bearing a hydrazine moiety react with bisaldehydes or bisketones to afford new nx polydentate ligands suitable for α-radioimmunotherapy. we developed a fast and efficient method using microwave-assisted technology to obtain chelators with variable size and number of coordination centres which were fully characterized. the complexation efficiency with astatine will be assessed.
quarkonium production in high energy proton-proton and proton-nucleus collisions. null
lightweight executability analysis of graph transformation rules. domain specific visual languages (dsvls) play a cornerstone role in model-driven engineering (mde), where (domain specific) models are used to automate the production of the final application. graph transformation is a formal, visual, rule-based technique, which is increasingly used in mde to express in-place model transformations like refactorings, animations and simulations. however, there is currently a lack of methods able to perform static analysis of rules, taking into account the dsvl meta-model integrity constraints. in this paper we propose a lightweight, efficient technique that performs static analysis of the weak executability of rules. the method determines if there is some scenario in which the rule can be safely applied, without breaking the meta-model constraints. if no such scenario exists, the method returns meaningful feedback that helps repairing the detected inconsistencies.
impact of hot carrier stress on small signal mosfet rf parameters. null
from dc to rf mosfet reliability (45mn). null
probing pre-formed alpha particles in the ground state of nuclei. in this proceeding we report on alpha particle emission through the nuclear break-up in the reaction 40ca on a 40ca target at 50a mev. it is observed that alpha particles are emitted to the continuum with very specific angular distribution during the reac- tion. the alpha particle properties seem to be compatible with an alpha cluster in the daughter nucleus that is perturbed by the short range nuclear attraction of the collision partner and emitted as described by a time-dependent theory. this mechanism offers new possibilities to study alpha particle properties in the nuclear medium.
on the mobility and potential retention of iodine in the callovovian-oxfordian formation. iodide sorption experiments were conducted on clay stone samples originating from the callovian-oxfordian formation under experimental conditions as close as possible to in situ conditions. the total natural iodine content of the formation is shown to be very constant throughout the formation, ranging from 2 to 3 ppm. this range is in agreement with a past iodine accumulation in the marine organic matter of the sediment before and during deposition, and early diagenesis. at variance with total iodine, the leached iodine concentrations are variable. if leached iodine is considered to represent porewater solute iodine, its concentration can be calculated and ranges from 0 (below detection limit) to ~60 µmol/l and represents 0 to 25 % of the total iodine. the reason for this variability is not understood. sorption isotherms were determined either for natural 127i- solutions or for 131i- spiked 127i- solutions, with concentrations ranging from 10-9 to 10-3 mol/l at solid to liquid ratios from 10 to 200 g/l. no or little sorption was encountered, kd values being in the range 0-0.5 l/kg with statistical and analytical error bands being greater than the kd values, with the exception of one experiment at low solid to liquid ratio (10 g/l), showing significant kd values of ~25 l/kg. in sorption experiments with natural 127i- and at the lowest added iodide concentrations (&lt; 10-6 mol/l) an apparent negative kd was obtained due to the iodide content in the solid porewater that was leached once the solid was suspended. the low affinity of iodide for argilite is thus confirmed. however, based only on these results and given the extent of the error bands, one cannot discard a limited iodide uptake. literature data on iodide diffusion on similar rock materials have already shown that iodide does not behave like chloride. the retention mechanism of radio-iodide is discussed in the light of the present results and diffusion data. a model involving isotopic exchange between the natural iodine content of the geological formation and radio-iodine allows all of the results to be described. not all the iodine in the formation appears to participate in isotopic exchange reactions with the solution. a quantification of the isotopically labile fraction of iodine would allow the effect of isotopic exchange on radio-iodide migration throughout the callovian-oxfordian formation to be assessed and predicted.
analytic relations for partial alpha decay half-lives and barrier heights and positions. from an adjustment on a recent selected data set of partial α-decay half-lives of 344 ground state to ground state transitions, analytic formulae are proposed depending on the angular momentum of the α particle. in particular, an expression allows to reproduce precisely the partial α-decay half-lives of even-even heavy nuclei and, then, to predict accurately the partial α-decay half-lives of other very heavy elements from the experimental or predicted qα. simple expressions are also provided to calculate the potential barrier radius and height.
community of inquiry in e-learning : a critical analysis of garrison and anderson model. this article is based on a constructively critical analysis of the model of community of inquiry developed by garrison and anderson (2003) as part of a research conducted in the area of e-learning. the authors claim that certain collaborative interactions create "distant presence" fostering the emergence of a community of inquiry which has a positive influence on individual and collective learning. more specifically, the article points out that until now, the model's theoretical foundations had not been made explicit and provides important insights concerning these epistemological considerations. it also suggests a number of theoretical perspectives which strengthen the authors' presentation of the conceptual anchorings of the model. thus the major contribution of this article is to show the potential of garrison and anderson's model for the research in the field of e-learning.
measuring cosmic ray radio signals at the pierre auger observatory. the recent results of the lopes and codalema experiments open the door to a renewal of the radio technique for cosmic ray induced shower measurements. the demonstration has been done of its potential and performances at energies below 1018 ev, this upper limit being due to the small scale of the current experiments. a natural stage toward the improvement of the method is thus to install radio detectors in association with a large cosmic ray detector such as auger. besides surface and fluorescence detection, radio detection could be an alternative method, providing a complementary information. the pierre auger collaboration has thus engaged a r&amp;d effort which will lead to the installation of a radio engineering array covering 20 km2 on its southern site. outline of the technique, results of the first phase of the tests and current plans for the future engineering array will be presented.
development of a cryogenic gaseous photomultiplier dedicated to a liquid-xenon compton telescope for medical imaging. a novel imaging technique based on the tridimensional localization of a (beta, gamma) radioisotope emitter with a liquid-xenon compton telescope was proposed at subatech in 2003. this technique named 3 gamma imaging combines a classical positron emission tomography device and a compton telescope for the reconstruction of two back-to-back annihilation gamma-rays and the third one respectively. the interaction of the last one with liquid-xenon induces a scintillation signal read by a vacuum photomultiplier tube to trigger the acquisition of the simultaneous ionization signal read by a micromegas (micro mesh gaseous structure) which allows the measurement of each interaction position and corresponding energy. in this experimental framework, we propose an original way of scintillation reading, replacing the traditionnal photomultiplier tubes devices by a large-area cryogenic gaseous photomultiplier. this photodetector consists of a reflective solid cesium iodide photocathode for the photoconversion of uv light and a combination of three micro-pattern gaseous detectors : the thgem (thick gaseous electron multiplier), the micromegas and the pim (parallel ionization multiplier). it should allow a virtual segmentation of the liquid xenon volume to reduce the telescope occupancy. first results obtained with a small area prototype at liquid xenon temperature (173 k) are presented.
stability and stabilizability of mixed retarded-neutral type systems. we analyze the stability and stabilizability properties of mixed retarded-neutral type systems when the neutral term is allowed to be singular. considering an operator model of the system in a hilbert space we are interesting in the critical case when there exists a sequence of eigenvalues with real parts approaching to zero. in this case the exponential stability is not possible and we are studying the strong asymptotic stability property. the behavior of spectra of mixed retarded-neutral type systems does not allow to apply directly neither methods of retarded system nor the approach of neutral type systems for analysis of stability. in this paper two technics are combined to get the conditions of asymptotic non-exponential stability: the existence of a riesz basis of invariant finite-dimensional subspaces and the boundedness of the resolvent in some subspaces of a special decomposition of the state space. for unstable systems the technics introduced allow to analyze the concept of regular strong stabilizability for mixed retarded-neutral type systems. the present paper extends the results by r.~rabah, g.m.~sklyar, a.v.~rezounenko on stability obtained in [j. diff. equat., 214(2005), no. 2, 391-428] and on stabilizability from [j. diff. equat., 245(2008), no. 3, 569-593].
an integrated production and maintenance planning model with time windows and shortage cost. in this paper, we tackle the problem of integrating production and maintenance. production problem addresses the issue of determining the production lot sizes of various items. preventive maintenance is carried out in time windows to restore the production line to an 'as-good-as-new' status, and when a production line fails, a minimal repair is carried out to restore it to an 'as-bad-as-old' status. the resulting problem is modelled as a linear mixed-integer program. it takes into account demand shortage and the reliability of the production line. computational experiments are carried out to show the effectiveness of the integrated model compared to classical separate model for different instances, and the obtained results are analyzed in detail.
on controllabilty and stabilizability of linear neutral type systems. linear systems of neutral type are considered using the infinite dimensional approach. conditions for exact controllability and regular asymptotic stabilizability are given. the main tools are the moment problem approach and the existence of a riesz basis of invariant subspaces.
on exact controllability of linear time delay systems of neutral type. the problem of exact null controllability is considered for linear neutral type systems with distributed delay. a characterization of this problem is given. the minimal time of controllability is precised. the results are based on the analysis of the riesz basis property of eigenspaces in hilbert space. recent results on the moment problem and properties of exponential families are used.
the analysis of exact controllability of neutral type systems by the moment problem approach. the problem of exact null-controllability is considered for a wide class of linear neutral-type systems with distributed delay. the main tool of the analysis is the application of the moment problem approach and the theory of the basis property of exponential families. a complete characterization of this problem is given. the minimal time of controllability is specified. the results are based on the analysis of the riesz basis property of eigenspaces of the neutral-type systems in hilbert space. key words. neutral-type systems, exact controllability, moment problem, riesz basis, distributed delays ams subject classifications. 93b05, 93c23, 93c25 doi. 10.1137/060650246 1.
stability analysis of mixed retarded-neutral type systems in hilbert space. we investigate the stability property of mixed retarded-neutral type systems. considering an operator model of the system in hilbert space we are interesting in the critical case when the spectrum of the operator belongs to the open left half-plane and there exists a sequence of eigenvalues with real parts approaching to zero. in this case the exponential stability is not possible and we are studying the strong asymp- totic stability property. the present paper extends the results obtained in [r. rabah, g.m. sklyar, a. v. rezounenko, stability analysis of neutral type systems in hilbert space. j. of dierential equations, 214(2005), no. 2, 391{428] in which stability of systems of neutral type was studied using the existence of a riesz basis of invariant finite-dimensional subspaces. however, for mixed retarded-neutral type systems such a basis may not exist for the whole state space. though the main result on stability remains the same for mixed retarded-neutral type systems, the technic of its proof had to be changed and it involves a proof of resolvent boundedness on some invariant subspace. we show that the property of asymptotic stability is determinated not only by the spectrum of the system but also depends on geometrical characteristics of its main neutral term which in our situation may be singular. we also give an explicit example of two systems having the same spectrum in the open left half-plane, but one of them is asymptotically stable while the other one is unstable.
on strong regular stabilizability for linear neutral type systems. the problem of strong stabilizability of linear systems of neutral type is investigated. we are interested in the case when the system has an infinite sequence of eigenvalues with vanishing real parts. this is the case when the main part of the neutral equation is not assumed to be stable in the classical sense. we discuss the notion of regular strong stabilizability and present an approach to stabilize the system by regular linear controls. the method covers the case of multivariable control and is essentially based on the idea of infinite-dimensional pole assignment proposed in [g.m. sklyar, a.v. rezounenko, a theorem on the strong asymptotic stability and determination of stabilizing controls, c. r. acad. sci. paris ser. i math. 333 (8) (2001) 807-812]. our approach is based on the recent results on the riesz basis of invariant finitedimensional subspaces and strong stability for neutral type systems presented in [r. rabah, g.m. sklyar, a.v. rezounenko, stability analysis of neutral type systems in hilbert space, j. differential equations 214 (2) (2005) 391-428].
a versatile palladium-triphosphane system for direct arylation of heteroaromatics with chlorides at low catalyst loading. put a ring on it: the use of an air-stable, robust palladium/tridentate phosphane catalyst in direct c[bond]h and c[bond]cl activation reactions is reported (see scheme; dmac=n,n-dimethylacetamide, tbab=tetra-n-butylammonium bromide). electron-rich, electron-poor, and polysubstituted furans (x=o), thiophenes (x=s), pyrroles (x=nr5), and thiazoles were arylated with chloroarenes in the presence of the catalyst.
an application of dc programming approach for a logistics network design problem. null
designing a decision support system for the transportation of disabled persons. null
a linear relaxation heuristic for solving logistics network design and planning problems. null
a lagrangean relaxation based heuristic for logistics network design. we consider a logistics network design problem with multiple periods, echelons, facilities and commodities. the initial network has some operating facilities and a set of potential locations to settle new ones. over a strategic horizon, the optimisation model aims at locating operating facilities; planning the capacity, the production levels and the product flows. this problem is modelled as a mixed integer linear programme (milp) with binary variables associated to facility status and continuous variables associated to material flows. the proposed scheme is based on the lagrangean relaxation of some coupling constraints. a decomposition level is performed by considering the main echelons: suppliers / plants; plants / warehouses and customers; warehouses / customers. characteristics of the corresponding echelon the resolution of this decomposed problem provides a lower bound. on the other hand, the feasible solutions provide upper bounds that can be improved by applying a d.c. (difference of convex functions) algorithm for milp. a procedure for updating lagrangean multipliers is also proposed. all these elements are combined to build an iterative heuristic for solving the original problem. stopping conditions are based on the classical gap between the two bounds.
a linear relaxation-based heuristic for logistics network design. we address the problem of designing and planning a multi-period, multi-echelon, multicommodity logistics network with deterministic demands. this consists of making strategic and tactical decisions: opening, closing or expansion of facilities, supplier selection and definition of the products flows. we use a heuristic approach based on the linear relaxation of the original mixed integer linear problem (milp). the main idea is to fix as many binary variables as possible with the linear relaxation coupled with rounding procedures. the number of binary variables in the resulting milp is small enough to enable solving it with a commercial solver. we compare the computational time and the quality of the results obtained with the heuristic and the commercial solver.
a dynamic model for the facility location in supply chain design. in this paper, we propose a mixed integer linear program for the design and planning of a multi-echelon, multi-commodity production-distribution network with deterministic demands. within a strategic time horizon, this study aims to help strategic and tactical decisions for each period of time: opening, closing or enlargement of facilities, supplier election, flows along the supply chain network. several families of products, with bills of materials, are considered. we present one application of our model: how to plan the expansion of a company which has to face increasing demands. we report numerical experiments with a milp solver.
static output feedback control design for descriptor systems. the static output feedback (sof) synthesis problem for descriptor systems is considered in this paper. lmi-based algorithms are proposed to find potentially structured sof gains ensuring admissibility and even h∞ performance of the closed-loop system. these algorithms are then used to propose a (descriptor) observer-based h∞ controller design method. an alternative technique for determining such separated estimation/control structure, after the design step, is also proposed. several numerical examples, throughout the paper, demonstrate the effectiveness of the proposed algorithms.
mass predictions of exotic nuclei within a macro-microscopic model. different liquid drop model mass formulae have been studied. they include a coulomb diffuseness correction z2/a term and pairing and shell energies of the thomas-fermi model. the influence of the selected charge radius, the curvature energy and different forms of the wigner term has been investigated. their coefficients have been determined by a least square fitting procedure to 2027 experimental atomic masses. the different fits lead to a surface energy coefficient of 17-18 mev. a large equivalent rms radius (r0 = 1.22 − 1.24 fm) or a shorter central radius may be used. a rms deviation of 0.54 mev can be reached between the experimental and theoretical masses. the remaining differences come from the determination of the shell and pairing energies. mass predictions are given for exotic nuclei.
study of selenium nanoparticles dissolution in environmental and human fluids. null
a route feasibility algorithm for the dial a ride problem with transfers. null
towards autonomous radio detection of ultra high energy cosmic rays. the radiodetection of extensive air showers, investigated for the first time in the 1960's, obtained promising results but plagued by the technical limitations. at that time, h.r. allan summed up the state of the art in an extensive review article whose conclusions and predictions are still used today. set up in 2001 at the nancay observatory, the codalema experiment was built first as a demonstrator and successfully showed the feasibility of the radiodetection of extensive air showers. radically modified in 2005, it allowed to obtain a clear energy correlation, and put in evidence an unambiguous signature of the geomagnetic origin of the electric field emission process associated to the air shower. the switch towards large areas is the next step of the technique's development. therefore, the autonomy of the detectors becomes essential. after test prototypes installed in 2006 at the pierre auger observatory, a generation of new autonomous detectors was developed. their first results will be presented. this work is also dedicated to the issues related to the radiodetection technique : the antenna response, the sensitivity, the surrounding effects, the monitoring of a big array. the determination of the shower characteristics independently of other detectors such as the lateral distribution, the energy correlation and the frequency spectrum of the radio transient will be discussed.
creole: a universal language for creating, requesting, updating and deleting resources. in the context of service-oriented computing, applications can be developed following the rest (representation state transfer) architectural style. this style corresponds to a resource oriented model, where resources are manipulated via crud (create, request, update, delete) interfaces. the diversity of crud languages due to the absence of a standard leads to composition problems related to adaptation, integration and coordination of services. to overcome these problems, we propose a pivot architecture built around a universal language to manipulate resources, called creole, a crud language for resource edition. in this architecture, scripts written in existing crud languages, like sql, are compiled into creole and then executed over different crud interfaces. after stating the requirements for a universal language for manipulating resources, we formally describe the language and informally motivate its definition with respect to the requirements. we then concretely show how the architecture solves adaptation, integration and coordination problems in the case of photo management in flickr and picasa, two well-known service-oriented applications. finally, we propose a roadmap for future work.
hadron-hadron and cosmic-ray interactions at multi-tev energies. the workshop on "hadron-hadron and cosmic-ray interactions at multi-tev energies" held at the ect* centre (trento) in nov.-dec. 2010 gathered together both theorists and experimentalists to discuss issues of the physics of high-energy hadronic interactions of common interest for the particle, nuclear and cosmic-ray communities. qcd results from collider experiments -- mostly from the lhc but also from the tevatron, rhic and hera -- were discussed and compared to various hadronic monte carlo generators, aiming at an improvement of our theoretical understanding of soft, semi-hard and hard parton dynamics. the latest cosmic-ray results from various ground-based observatories were also presented with an emphasis on the phenomenological modeling of the first hadronic interactions of the extended air-showers generated in the earth atmosphere. these mini-proceedings consist of an introduction and short summaries of the talks presented at the meeting.
efficient algorithms for singleton arc consistency. in this paper, we propose two original and efficient approaches for enforcing singleton arc consistency. in the first one, the data structures used to enforce arc consistency are shared between all subproblems where a domain is reduced to a singleton. this new algorithm is not optimal but it requires far less space and is often more efficient in practice than the optimal algorithm sac-opt. in the second approach, we perform several runs of a greedy search (where at each step, arc consistency is maintained), possibly detecting the singleton arc consistency of several values in one run. it is an original illustration of applying inference (i.e., establishing singleton arc consistency) by search. using a greedy search allows benefiting from the incrementality of arc consistency, learning relevant information from conflicts and, potentially finding solution(s) during the inference process. we present extensive experiments that show the benefit of our two approaches.
a new consistent vehicle routing problem for the transportation of handicapped persons. null
study of the isolated photon production in proton-proton collisions with the emcal calorimeter of the alice experiment at lhc. prompt photons produced in the hard initial parton-parton scatterings in high-energy proton-proton collisions – such as those at the cern large hadron collider (lhc) – are an excellent tool to study perturbative quantum chromodynamics. in particular, high transverse momentum(pt) photons provide crucial information on the parton distribution functions (pdfs) of the proton. experimentally, in order to measure prompt photons, one needs first to get rid of various other photon sources, especially those from the large background due to pi0 decays. the electromagnetic calorimeter (emcal) of the alice (a large ion collider experiment) experiment at the lhc covers the central rapidities and provides improved capabilities to measure high-pt photons in alice.we present here studies of photon production in high-energy proton-proton collisions, as well as a complete analysis of their measurement with the emcal detector, giving details in particular on the methods developped to separate them from the pi0 decay photons with the help of isolation cuts.
the parabasisphenoid complex in mesozoic turtles and the evolution of the testudinate basicranium. null
architecture for the next generation system management tools. to get more results or greater accuracy, computational scientists execute parallel or distributed applications, and try to scale these applications up. clusters, grids and clouds are by nature different computing platforms: some span across multiple sites and multiple administrative domains, whereas others are single site/domain. as a consequence, scientists have to manage technical details related to both hardware and software constraints of each platform in order to execute their applications. from our point of view, scientists should focus on their research rather than dealing with platform considerations. in this article, we advocate for a system management framework that aims to automatically adapt both hardware and software resources to the applications' needs through a unique method. for each application, scientists describe the requirements through the definition of a virtual platform (vp) and a virtual system environment (vse). relying on these definitions, the framework is in charge of (i) the deployment and the configuration of the virtual platform upon the physical infrastructure, and (ii) the customization of the execution environment upon the former vp. furthermore, we present a refinement of the concept of emulation and virtualization introduced by goldberg. this allows scientists to define complex vps relying on different concepts based on system virtualization (identity, partitioning, aggregation) and emulation (simple, abstraction). leveraging several system management tools (such as oscar, the grid'5000 toolkit, or xtreemos), our current prototype follows a modular approach. we anticipate to be able to easily interface our architecture with recent cloud management solutions (such as opennebula or eucalyptus).
stiffness modelling of parallelogram-based parallel manipulators. the paper presents a methodology to enhance the stiffness analysis of parallel manipulators with parallelogram-based linkage. it directly takes into account the influence of the external loading and allows computing both the non-linear ``load-deflection" relation and relevant rank-deficient stiffness matrix. an equivalent bar-type pseudo-rigid model is also proposed to describe the parallelogram stiffness by means of five mutually coupled virtual springs. the contributions of this paper are highlighted with a parallelogram-type linkage used in a manipulator from the orthoglide family.
viscosities of the quasigluon plasma. we investigate bulk and shear viscosities of the gluon plasma within relaxation time approximation to an effective boltzmann-vlasov type kinetic theory by viewing the plasma as describable in terms of quasigluon excitations with temperature dependent self-energies. the found temperature dependence of the transport coefficients agrees fairly well with available lattice qcd results. the impact of some details in the quasigluon dispersion relation on the specific shear viscosity is discussed.
performance evaluation of parallel manipulators for milling application. this paper focuses on the performance evaluation of the parallel manipulators for milling of composite materials. for this application the most significant performance measurements, which denote the ability of the manipulator for the machining are defined. in this case, optimal synthesis task is solved as a multicriterion optimization problem with respect to the geometric, kinematic, kinetostatic, elastostostatic, dynamic properties. it is shown that stiffness is an important performance factor. previous models operate with links approximation and calculate stiffness matrix in the neighborhood of initial point. this is a reason why a new way for stiffness matrix calculation is proposed. this method is illustrated in a concrete industrial problem.
stiffness analysis of parallel manipulators with preloaded passive joints. the paper presents a methodology for the enhanced stiffness analysis of parallel manipulators with internal preloading in passive joints. it also takes into account influence of the external loading and allows computing both the non-linear ``load-deflection'' relation and the stiffness matrices for any given location of the end-platform or actuating drives. using this methodology, it is proposed the kinetostatic control algorithm that allows to improve accuracy of the classical kinematic control and to compensate position errors caused by elastic deformations in links/joints due to the external/internal loading. the results are illustrated by an example that deals with a parallel manipulator of the orthoglide family where the internal preloading allows to eliminate the undesired buckling phenomena and to improve the stiffness in the neighborhood of its kinematic singularities.
optimal maintenance and replacement decisions under technological change. the requirement of equipment improvement in order to satisfy the safety and reliability of system motivates the development of technology. the presence or expectation of technologically better equipment will influence managerial decisions on whether to invest in the maintenance of current equipment, invest in replacement with an equivalent model, replacement with a higher technology model currently available on the market, or wait for a potentially even better technology to appear in the near future. hence, the consideration of technological change is a very important aspect for maintenance and replacement decisions. this paper aims to define a model that allows us to gain insight into how maintenance/replacement policies will be influenced by the expectation of future technology. we then use stochastic dynamic programming (i.e., markov decision process) to solve for the optimal maintenance and replacement policy of the equipment as a function of performance and cost. finally, we illustrate the problem through several numerical examples.
enabling tool reuse and interoperability through model-driven engineering. null
new developments of epos 2. since 2006, epos hadronic interaction model is being used for very high energy cosmic ray analysis. designed for minimum bias particle physics and used for having a precise description of sps and rhic heavy ion collisions, epos brought more detailed description of hadronic interactions in air shower development. thanks to this model it was possible to understand why there were less muons in air shower simulations than observed in real data. with the start of the lhc era, a better description of hard processes and collective effects is needed to deeply understand the incoming data. we will describe the basic physics in epos and the new developments and constraints which are taken into account in epos 2.
dealing with non-functional requirements in model-driven development,. null
automatic generation of basic behavior schemas from uml class diagrams. null
verification and validation of declarative model-to-model transformations through invariants. in this paper we propose a method to derive ocl invariants from declarative model-to-model transformations in order to enable their verification and analysis. for this purpose we have defined a number of invariant-based verification properties which provide increasing degrees of confidence about transformation correctness, such as whether a rule (or the whole transformation) is satisfiable by some model, executable or total. we also provide some heuristics for generating meaningful scenarios that can be used to semi-automatically validate the transformations. as a proof of concept, the method is instantiated for two prominent model-to-model transformation languages: triple graph grammars and qvt.
adopting agile methods: can goal-oriented social modeling help?. the heavy reliance on the human factor in agile methods poses new challenges for organizations intent on adopting them. improper role assignment, neglected team dependencies, and overlooked required skills have all been reported as reasons for failures during the introduction of an agile method. current process modelling languages are not designed for describing or analyzing such human-related issues, and thus, provide little assistance to organizations in the process of adopting an agile method. this paper advocates the use of goal-oriented modeling techniques for depicting social aspects of agile methods. these social models can be used to identify the key factors that contribute to the success or failure of an agile method, thus providing guidance early during the introduction of the method in an organization. the approach is illustrated using the scrum process.
tools for modeling and generating safe interface interactions in web applications. modern web applications that embed sophisticated user interfaces and business logic have rendered the original interaction paradigm of the web obsolete. in previous work, we have advocated a paradigm shift from static content pages that are browsed by hyperlinks to a state-based model where back and forward navigation is replaced by a full-fledged interactive application paradigm, featuring undo and redo capabilities, with support for exception management policies and transactional properties. in this demonstration, we present an editor and code generator designed to build applications based on our approach.
synthesis of ocl pre-conditions for graph transformation rules. graph transformation (gt) is being increasingly used in model driven engineering (mde) to describe in-place transformations like animations and refactorings. for its practical use, rules are often complemented with ocl application conditions. the advancement of rule post-conditions into pre-conditions is a well-known problem in gt, but current techniques do not consider ocl. in this paper we provide an approach to advance post-conditions with arbitrary ocl expressions into pre-conditions. this presents benefits for the practical use of gt in mde, as it allows: (i) to automatically derive pre-conditions from the meta-model integrity constraints, ensuring rule correctness, (ii) to derive pre-conditions from graph constraints with ocl expressions and (iii) to check applicability of rule sequences with ocl conditions.
specifying aggregation functions in multidimensional models with ocl. multidimensional models are at the core of data warehouse systems, since they allow decision makers to early define the relevant information and queries that are required to satisfy their information needs. the use of aggregation functions is a cornerstone in the definition of these multidimensional queries. however, current proposals for multidimensional modeling lack the mechanisms to define aggregation functions at the conceptual level: multidimensional queries can only be defined once the rest of the system has already been implemented, which requires much effort and expertise. in this sense, the goal of this paper is to extend the object constraint language (ocl) with a predefined set of aggregation functions. our extension facilitates the definition of platform-independent queries as part of the specification of the conceptual multidimensional model of the data warehouse. these queries are automatically implemented with the rest of the data warehouse during the code-generation phase. the ocl extensions proposed in this paper have been validated by using the use tool.
situational evaluation of method fragments: an evidence-based goal-oriented approach. despite advances in situational method engineering, many software organizations continue to adopt an ad-hoc mix of method fragments from well-known development methods such as scrum or xp, based on their perceived suitability to project or organizational needs. with the increasing availability of empirical evidence on the success or failure of various software development methods and practices under different situational conditions, it now becomes feasible to make this evidence base systematically accessible to practitioners so that they can make informed decisions when creating situational methods for their organizations. this paper proposes a framework for evaluating the suitability of candidate method fragments prior to their adoption in software projects. the framework makes use of collected knowledge about how each method fragment can contribute to various project objectives, and what requisite conditions must be met for the fragment to be applicable. pre-constructed goal models for the selected fragments are retrieved from a repository, merged, customized with situational factors, and then evaluated using a qualitative evaluation procedure adapted from goal-oriented requirements engineering.
industrialization of research tools: the atl case. research groups develop plenty of tools aimed at solving real industrial problems. unfortunately, most of these tools remain as simple proof-of-concept tools that companies consider too risky to use due to their lack of proper user interface, documentation, completeness, support, etc that companies expect from commercial-quality level tools. based on our tool development experience in the atlanmod research team, specially regarding the evolution of our atl model transformation tool, we argue in this paper that the best solution for research teams aiming to create high-quality and widely-used tools is to industrialize their research prototypes through a partnership with a technology provider.
combining model-driven engineering and cloud computing. service-orientation and model-driven engineering are two of the most dominant software engineering paradigms nowadays. this position paper explores the synergies between them and show how they can benefit from each other. in particular, the paper introduces the notion of modeling as a service (maas) as a way to provide modeling and model-driven engineering services from the cloud.
model transformation chains in model-driven performance engineering: experiences and future research needs. null
towards incremental execution of atl transformations. up to now, the execution of atl transformations has always followed a two-step algorithm: 1) matching all rules, 2) applying all matched rules. this algorithm does not support incremental execution. for instance, if a source model is updated, the whole transformation must be executed again to get the updated target model. in this paper, we present an incremental execution algorithm for atl, as well as a prototype. with it, changes in a source model are immediately propagated to the target model. our approach leverages previous works of the community, notably on live transformations and incremental ocl. we achieve our goal on a subset of atl, without requiring modifications to the language.
charged-particle multiplicity density at midrapidity in central pb-pb collisions at sqrt(snn) = 2.76 tev. the first measurement of the charged-particle multiplicity density at mid-rapidity in pb-pb collisions at a centre-of-mass energy per nucleon pair sqrt(snn) = 2.76 tev is presented. for an event sample corresponding to the most central 5% of the hadronic cross section the pseudo-rapidity density of primary charged particles at mid-rapidity is 1584 +- 4 (stat) +- 76 (sys.), which corresponds to 8.3 +- 0.4 (sys.) per participating nucleon pair. this represents an increase of about a factor 1.9 relative to pp collisions at similar collision energies, and about a factor 2.2 to central au-au collisions at sqrt(snn) = 0.2 tev. this measurement provides the first experimental constraint for models of nucleus-nucleus collisions at lhc energies.
optimal condition-based resurfacing decisions for roads. we develop a condition-based maintenance optimization approach for the road cracking problem in order to derive an optimal action-planning policy from a markov decision process that minimizes the expected cost. our model deals with multiple imperfect actions that consist of different resurfacing thicknesses and that have varying effects on the future deterioration law. we model the deterioration after maintenance to be dependent upon both the state of the road before maintenance and the type of maintenance performed. another new aspect of our model is the possibility for a maintenance action to render the road to a state better than as-good-as-new. the application of new road layers is, however, constrained by a maximum total road thickness. some maintenance actions may then become infeasible due to the thickness of the road relative to the constraint. as the road is constrained by a maximum total thickness, the maintenance decision is made complex by the determination of not only how thick of a layer to apply, but also how much old road to remove. we model the road state by two deterioration variables: the longitudinal cracking percentage and its associated growth rate. our degradation model will take into account the changing road composition which is a function of the successive application of new and removal of old layers.
a condition-based imperfect maintenance model with action dependent deterioration. we propose a maintenance decision framework where the state of the system is based on a percentage of total degrada- tion and the rate of change of the degradation. at each decision interval, the degradation percentage may be observed perfectly as well as the rate of change with respect to the last observation. we consider ﬁve possible actions based on these two observable phenomenon: do nothing, three imperfect maintenance actions, and a complete renewal. prior imperfect maintenance models do not consider a scenario where imperfect maintenance not only restores the system to a state less than “as good as new” but also under a new deterioration law. in this case, it is necessary to consider that the rate of degradation after imperfect maintenance will be altered based on the action performed and the state of the system prior to the maintenance. we propose to develop an imperfect maintenance optimization model based on a markov decision process framework to determine the optimal maintenance actions given the state of the system at each decision interval and motivate the problem through road maintenance.
optimal highway maintenance policies under uncertainty. we develop an inspection and maintenance policy to minimize the cost of maintaining a given section of road or highway when there is a great deal of uncertainty in the degradation process. we propose to model the degradation of a section of road based on the proliferation and growth of cracks. we utilize a combination of a poisson and gamma process to account for the tremendous amount of uncertainty and difficulty in predicting the proliferation of cracks. our policy defines the optimal inspection interval as well as the minimum threshold at which to perform crack repairs. furthermore, our policy contains a safety constraint to prevent the probability of a "catastrophic" failure from exceeding a pre-determined reliability value. numerical calculations have shown that our model will extend the lifecycle of the road by performing preventive, conditioned-based maintenance to slow down the growth of cracks. classical preventive maintenance policies usually shorten the lifecycle by forcing earlier renewals.
multiperiodic planning and routing on a rolling horizon for field force optimization logistics. field force planning and optimization is a new challenge in logistics for the service sector. in the energy or water distribution areas, it consists in planning the visits of commercial or technical personnel on the field over a set of time periods (days) to visit industrial facilities or customers in order to satisfy requests for service. demands can be for planned maintenance of equipments, commercial activities or emergency maintenance activities. our application is relative to water distribution and field force optimization for planning the visits of personnel to clients over a multiperiod horizon and a specific geographic area. visits to customers can be planned through a call center upon customer demand or upon the company's request. in order to provide a tool for optimization on a rolling horizon in this context and test these hypotheses, we have proposed a reoptimization procedure. we have developed a demand generator and solution technique using the memetic algorithm and column generation methods previously developed for the fixed horizon planning problem. experiences on realistic data validate our approach and allow us to suggest some further researches for this problem.
multiperiod planning and routing on a rolling horizon for field force optimization logistics. we address the problem of the planning and scheduling of technicians visits to customers on the field, in the context of service logistics over a multiperiod horizon. we have developed a vrp type memetic algorithm and a column generation/branch and bound heuristic in order to optimize the plan over a static horizon. we have then adapted our procedures to cope with a rolling horizon context, when a new plan has to be determined after the execution of each first daily period of the previous plan. we have tested our procedures on realistic data from the water distribution context, and obtained good solutions in a reasonable amount of time. we show in particular the advantage of reutilization of partial solutions from the previous plan, for the optimization of each new plan. directions for further research are then indicated.
a tabu search algorithm for the integrated scheduling problem of container handling systems in a maritime terminal. the scheduling problem in a container terminal is characterized by high level of coordination of different types of equipment. in this paper we present an integrated model to schedule these equipment at the same time with the objective to minimize the makespan, or the time it takes to serve a given set of ships. the problem is formulated as a hybrid flow shop scheduling problem with precedence and blocking constraints (hfss-b). a tabu search algorithm is proposed to solve this problem. certain mechanisms are developed and introduced into the algorithm to assure the quality and efficiency of the algorithm. the performance of the tabu search algorithm is analyzed from the computational point of view.
analytic expressions for alpha-decay half-lives and potential barriers. from an adjustment on a recent selected data set of partial $\alpha$-decay half-lives of 344 ground state to ground state transitions, analytical formulas are proposed for $log_{10}t_{1/2}(s)$ depending or not on the angular momentum of the $\alpha$ particle. in particular, an expression allows to reproduce precisely the partial $\alpha$-decay half-lives of even-even heavy nuclei and, then, to predict accurately the partial $\alpha$-decay half-lives of other very heavy elements from the experimental or predicted $q_\alpha$. comparisons have been done with other empirical approaches. moreover, the potential barrier against $\alpha$-decay or $\alpha$-capture has been determined within a liquid drop model including a proximity energy term. simple expressions are provided to calculate the potential barrier radius and height.
elliptic flow of charged particles in pb-pb collisions at 2.76 tev. we report the first measurement of charged particle elliptic flow in pb-pb collisions at 2.76 tev with the alice detector at the cern large hadron collider. the measurement is performed in the central pseudorapidity region (|eta|&lt;0.8) and transverse momentum range 0.2&lt; p_t&lt; 5.0 gev/c. the elliptic flow signal v_2, measured using the 4-particle correlation method, averaged over transverse momentum and pseudorapidity is 0.087 +/- 0.002 (stat) +/- 0.004 (syst) in the 40-50% centrality class. the differential elliptic flow v_2(p_t) reaches a maximum of 0.2 near p_t = 3 gev/c. compared to rhic au-au collisions at 200 gev, the elliptic flow increases by about 30%. some hydrodynamic model predictions which include viscous corrections are in agreement with the observed increase.
managing virtual resources: fly through the sky. virtualization technologies have been a key element in the adoption of infrastructure-as-a-service (iaas) cloud computing platforms as they radically changed the way in which distributed architectures are exploited. however, a closer look suggests that the way of managing virtual and physical resources still remains relatively static.
fiesta toolkit: model-driven software product lines in practice. model-driven software product lines (md-spls) are those product lines whose members are created by using models and model transformations as first-class artifacts. in this paper we present the fiesta toolkit, a set of tools to assist product line architects and products designers when creating md-spls. the main characteristic of our toolkit is that it facilitates the creation of fine-grained configurations of products, i.e. configurations at the level of each element in models that will be transformed into product line members. for that, the toolkit includes a set of tools for the creation of md-spl projects, feature models, constraint models, binding models, ocl-type expressions to validate binding models against constraint models, and decision models.
automated reasoning for derivation of model-driven spls. model-driven spl approaches use metamodels and transformation rules to obtain concrete software artifacts departing from models. most of such approaches use also feature models to express variability. because of the variability, to derive products, they have to adapt the transformation rules according to user choices captured in feature configurations. in this paper we propose an approach based on constraint programming to derive model-driven spls. our contribution is twofold. first, we assist product line architects when relating transformation rules and features in order to derive prod- ucts based on feature configurations; the novelty is that we facilitate the management of feature interactions to architects. second, current approaches to reason on feature models in spl engineering only deal with problems related to product configuration. we improve such approaches adding facilities for product derivation.
controlling contractors with monads for hybrid dynamical systems. physical systems with intelligent behaviors result from inter-actions of different fields: sensor networks, robotics, optimization, reasoning, etc. rooted in this philosophy of interdisciplinary, this paper makes a connexion between hybrid dynamical systems, interval-based constraint propagation and functional programming. it shows how to build a monadic program in haskell to control contractors (constraint propagators) for the state estimation of multi-model (hy- brid) dynamical systems, subject to partial and uncertain measurements. the example of system taken here is an elevator that can either be moving upward, downward or stopped. the altitude is measured directly and the estimation problem is simply to track its motion. the purpose of the haskell library is to offer both a high-level and flexible framework for building propagation strategies based on user knowledge or user requirements.
clusters in light nuclei. a great deal of research work has been undertaken in the alpha-clustering study since the pioneering discovery, half a century ago, of 12c+12c molecular resonances. our knowledge of the field of the physics of nuclear molecules has increased considerably and nuclear clustering remains one of the most fruitful domains of nuclear physics, facing some of the greatest challenges and opportunities in the years ahead. in this work, the occurence of "exotic" shapes in light n=z alpha-like nuclei is investigated. various approaches of superdeformed and hyperdeformed bands associated with quasimolecular resonant structures are presented. results on clustering aspects are also discussed for light neutron-rich oxygen isotopes.
dilated lmi characterizations for linear time-invariant singular systems. this article explores, based on projection lemma, dilated linear matrix inequality (lmi) characterisations for linear time-invariant singular systems. the deduced formulations cover not only the existing results reported in the literature, but also complete some missing lmi conditions. this article also lightens the mutual relations of these characterisations and clarifies the relation between the dilated lmis for the conventional state-space systems and those for singular systems. the well-known results for the state-space setting reported in the literature can be reviewed as special cases.
self-optimisation of the energy footprint in service-oriented architectures. the impact of it on the global energy consumption has frighteningly increased over the last years. one of the reasons for this is the demand for infrastructure to support the increasing number of online (24x7) services and data, followed by the popularisation of practices like cloud computing. from the infrastructure point of view, hardware throttling and server consolidation are techniques used to deal with energy efficiency. however, details about the application behavior are not visible from the infrastructure layer, which prevents a more complete energy-efficient treatment. this paper presents an approach for self-optimisation of the energy consumption at the application layer. we rely on service-oriented architectures, since they allow rapid and seamless service composition and eases the application adaptation. the energy efficiency properties of services are defined by means of quality of service criteria and a set of event-condition-actions is defined to enable the application to react to environmental changes and optimise its energy consumption. as a proof of concept, we present a prototype for energy-aware self-adaptation in soa-based applications as well as an example scenario that shows the practical usage of our approach.
modisco: a generic and extensible framework for model driven reverse engineering. nowadays, almost all companies, independently of their size and type of activity, are facing the problematic of having to manage, maintain or even replace their legacy systems. many times, the first problem they need to solve is to really understand what are the functionalities, architecture, data, etc of all these often huge legacy applications. as a consequence, reverse engineering still remains a major challenge for software engineering today. this paper introduces modisco, a generic and extensible open source reverse engineering solution. modisco intensively uses mde principles and techniques to improve existing approaches for reverse engineering.
model-driven interoperability of dependencies visualizations. software tools and corresponding knowledge tend to be collected and packaged into platforms like eclipse, mathlab or kde. their success and usefulness combined with their growing size and complexity rise issues about management of dependencies between their components and between the platform and other applications which rely on its plug-in system and/or provided functionalities. such problems imply need for dependencies management tools in which visualization is a core feature. as dependencies are also a concern in domains like object-oriented programming or operating system packaging, we may expect to reuse corresponding works in visualization. but each domain and its related dependencies problem have induced their own hard coded viewing and browsing tools. in this article we present how we have reuse existing visualization tools for our platform cartography together with our own displays using a model-driven interoperability approach to easily realize bindings between visualization tools.
how to deal with your it legacy? reverse engineering with modisco... the modisco (model discovery) project is an eclipse modeling project dedicated to reverse engineering. to this end, it provides a customizable model-driven reverse engineering (mdre) framework. legacy systems and corresponding data currently embrace a large number of heterogeneous technologies, making the design, development and maintenance of tools dealing with the reuse or evolution of such legacy a tedious and time consuming task. as reverse-engineering projects usually face with both the combination of many technologies and various different scenarios, model-driven approaches and related tools offer the required abstraction level to build up mature and flexible solutions. the modisco generic and extensible framework is dedicated to the resolution of these concrete problems by allowing: 1) the description of the information extracted out of the legacy as models; 2) the understanding of these models in order to take the most efficient decisions; 3) the transformation of these models into other exploitable artifacts (source code, documentation, metrics, etc). this framework has been designed to be applied on many different reverse-engineering use cases, such as those mentioned in this non-exhaustive list: 1) the migration/modernization of existing systems considering their architecture, used technologies or just available data; 2) the documentation of complex legacy in order to better understand their different aspects and specificities; 3) the evaluation of such legacy in terms of quality (computation of metrics, detection of anti-patterns, etc). this talk will present the overall status of the situation within the modisco project. it will start by briefly summarizing the main objectives of modisco and will introduce its general organization and actual development team. the focus will be then set on more precisely describing the various components now available from the provided mdre framework, emphasizing on different possible concrete applications of these tools and underlying approach. finally, the future of modisco will be discussed interactively with the audience considering as potential subjects the global project roadmap, the next components to be released, the possible evolution of the community, etc.
model driven tool interoperability in practice. model driven engineering (mde) advocates the use of models, metamodels and model transformations to revisit some of the classical operations in software engineering. mde has been mostly used with success in forward and reverse engineering (for software development and better maintenance, respectively). supporting system interoperability is a third important area of applicability for mde. the particular case of tool interoperability is currently receiving a lot of interest. in this paper, we describe some experiments in this area that have been performed in the context of open source modeling efforts. taking stock of these achievements, we propose a general framework where various tools are associated to implicit or explicit metamodels. one of the interesting properties of such an organization is that it allows designers starting some software engineering activity with an informal light-weight tool and carrying it out later on in a more complete or formal context. we analyze such situations and discuss the advantages of using mde to build a general tool interoperability framework.
towards model driven tool interoperability: bridging eclipse and microsoft modeling tools. successful application of model-driven engineering approaches requires interchanging a lot of relevant data among the tool ecosystem employed by an engineering team (e.g., requirements elicitation tools, several kinds of modeling tools, reverse engineering tools, development platforms and so on). unfortunately, this is not a trivial task. poor tool interoperability makes data interchange a challenge even among tools with a similar scope. this paper presents a model-based solution to overcome such interoperability issues. with our approach, the internal schema/s (i.e., metamodel/s) of each tool are explicited and used as basis for solving syntactic and semantic dierences between the tools. once the corresponding metamodels are aligned, model-to model transformations are (semi)automatically derived and executed to perform the actual data interchange. we illustrate our approach by bridging the eclipse and microsoft (dsl tools and sql server modeling) modeling tools.
inter-dsl coordination support by combining megamodeling and model weaving. model-driven engineering (mde) advocates the use of models at every step of the software development process. within this context, a team of engineers collectively and collaboratively contribute to a large set of interrelated models. even if the main focus can be on a single model (e.g. a class diagram model), related elements in other models (e.g. a requirement model) often have to be considered and/or accessed. moreover, all the involved models do not necessarily conform to the same metamodel and thus may have been built using different independent domain-specific languages (dsls). such a situation has already been frequently observed in many large-scale industrial deployments of mde. manually coordinating all the involved models, i.e. being able to both manage and use the links existing between them, can become a cumbersome and difficult task. as a proposal to solve this inter-dsl coordination issue, we introduce in this paper a generic and extensible inter-model traceability and navigation environment based on the complementary use of megamodeling and model weaving. we illustrate our solution with a concrete working example.
measurement of bottom versus charm as a function of transverse momentum with electron-hadron correlations in p+p collisions at sqrt(s)=200 gev. the momentum distribution of electrons from semi-leptonic decays of charm and bottom for mid-rapidity |y|&lt;0.35 in p+p collisions at sqrt(s)=200 gev is measured by the phenix experiment at the relativistic heavy ion collider (rhic) over the transverse momentum range 2 &lt; p_t &lt; 7 gev/c. the ratio of the yield of electrons from bottom to that from charm is presented. the ratio is determined using partial d/d^bar --&gt; e^{+/-} k^{-/+} x (k unidentified) reconstruction. it is found that the yield of electrons from bottom becomes significant above 4 gev/c in p_t. a fixed-order-plus-next-to-leading-log (fonll) perturbative quantum chromodynamics (pqcd) calculation agrees with the data within the theoretical and experimental uncertainties. the extracted total bottom production cross section at this energy is \sigma_{b\b^bar}= 3.2 ^{+1.2}_{-1.1}(stat) ^{+1.4}_{-1.3}(syst) micro b.
adaptation and evaluation of generic model matching strategies. model matching is gaining importance in model-driven engineering (mde). the goal of model matching is to identify correspondences between the elements of two metamodels or two models. one of the main application scenarios is the derivation of model transformations from metamodel correspondences. model correspondences, in turn, offer a potential to address other mde needs. manually finding of correspondences is labor intensive and error-prone when (meta)models are large. to automate the process, research community proposes matching strategies combining multiple heuristics. a problem is that the heuristics are limited to certain representation formalisms instead of being reusable. another problem is the difficulty to systematically evaluate the quality of matching strategies. this work contributes an approach to deal with the mentioned issues. to promote reusability, the approach consists of strategies whose heuristics are loosely coupled to a given formalism. to systematize model matching evaluation, the approach automatically extracts a large set of modeling test cases from model repositories, and uses megamodels to guide strategy execution. we have validated the approach by developing the aml domain specific language on top of the amma platform. by using aml, we have implemented a library of strategies and heuristics. to demonstrate that our approach goes beyond the modeling context, we have tested our strategies on ontology test cases as well. at last, we have contributed three use cases that show the applicability of (meta)model matching to interesting mde topics: model co-evolution, pivot metamodel evaluation, and model synchronization.
applying mde for the validation of correct eclipse plugin bundles. abstract. complex software systems are often constructed by assembling bundles from repositories. eclipse is one of these systems; build on top of a platform accepting different sets of bundles according to the user needs. this adaptability is one of the main benefits of this kind of systems but implies also several configuration problems. the consistency of eclipse plug‐in's bundles is one of them. this problem involves a need for the configuration validation. to adress this problem, this paper proposes an approach using model driven engineering. the presented solution combines different mde techniques such as global model management and model transformations to check the coherency of eclipse plug‐in's bundles.
multi-level tests for model driven web applications. null
improving higher-order transformations support in atl. null
search computing: a model-driven perspective. null
commissioning of the alice muon spectrometer trigger at lhc. alice (a large ion collider experiment) is the lhc experiment dedicated to the study of ultra-relativistic heavy ion collisions. the alice muon spectrometer covers a large range in pseudo-rapidity and is designed to study quarkonia and heavy flavours decaying into (di-)muons. the high particle multiplicities environment in such collisions require a specific, fast and efficient trigger system, the muon trigger. it consists of four planes of rpc detectors, covering an area of 36 m2 each, 21k front-end channels and a fast-decision electronics. the muon trigger is designed to reconstruct (muon) tracks and deliver a trigger signal each 25 ns (40 mhz) with a total latency of 800 ns. the hit position on the rpc is measured in two orthogonal directions with an accuracy of about 1 cm. the performance measured with the first p–p collisions at sqrt (s)= 900 gev carried out in december 2009 is reported.
double helicity dependence of jet properties from dihadrons in longitudinally polarized p+p collisions at sqrt(s) = 200 gev. it has been postulated that partonic orbital angular momentum can lead to a significant double-helicity dependence in the net transverse momentum of drell-yan dileptons produced in longitudinally polarized p+p collisions. analogous effects are also expected for dijet production. if confirmed by experiment, this hypothesis, which is based on semi-classical arguments, could lead to a new approach for studying the contributions of orbital angular momentum to the proton spin. we report the first measurement of the double-helicity dependence of the dijet transverse momentum in longitudinally polarized p+p collisions at sqrt(s) = 200 gev from data taken by the phenix experiment in 2005 and 2006. the analysis deduces the transverse momentum of the dijet from the widths of the near- and far-side peaks in the azimuthal correlation of the dihadrons. when averaged over the transverse momentum of the triggered particle, the difference of the root-mean-square of the dijet transverse momentum between like- and unlike-helicity collisions is found to be -37 +/- 88(stat) +/- 14(syst) mev/c.
a linear relaxation-based heuristic approach for logistics network design. we address the problem of designing and planning a multi-period, multi-echelon, multi-commodity logistics network with deterministic demands. this consists of making strategic and tactical decisions: opening, closing or expanding facilities, selecting suppliers and defining the product flows. we use a heuristic approach based on the linear relaxation of the original mixed integer linear problem (milp). the main idea is to solve a sequence of linear relaxations of the original milp, and to fix as many binary variables as possible at every iteration. this simple process is coupled with several rounding procedures for some key decision variables. the number of binary decision variables in the resulting milp is small enough for it to be solved with a solver. the main benefit of this approach is that it provides feasible solutions of good quality within an affordable computation time.
the time-consistent vehicle routing problem. null
a new time-consistent vehicle routing problem for the transportation of handicapped persons. null
proceedings of the first international workshop on multiple partonic interactions at the lhc (mpi08). the objective of this first workshop on multiple partonic interactions (mpi) at the lhc is to raise the profile of mpi studies, summarizing the legacy from the older phenomenology at hadronic colliders and favouring further specific contacts between the theory and experimental communities. the mpi are experiencing a growing popularity and are currently widely invoked to account for observations that would not be explained otherwise: the activity of the underlying event, the cross sections for multiple heavy flavour production, the survival probability of large rapidity gaps in hard diffraction, etc. at the same time, the implementation of the mpi effects in the monte carlo models is quickly proceeding through an increasing level of sophistication and complexity that in perspective achieves deep general implications for the lhc physics. the ultimate ambition of this workshop is to promote the mpi as unification concept between seemingly heterogeneous research lines and to profit of the complete experimental picture in order to constrain their implementation in the models, evaluating the spin offs on the lhc physics program.
cross section and double helicity asymmetry for eta mesons and their comparison to neutral pion production in p+p collisions at sqrt(s)=200 gev. measurements of double-helicity asymmetries for inclusive hadron production in polarized p+p collisions are sensitive to helicity--dependent parton distribution functions, in particular to the gluon helicity distribution, delta(g). this study focuses on the extraction of the double-helicity asymmetry in eta production: polarized p+p --&gt; eta + x, the eta cross section, and the eta/pi^0 cross section ratio. the cross section and ratio measurements provide essential input for the extraction of fragmentation functions that are needed to access the helicity-dependent parton distribution functions.
first pp results... from the aa physics perspective. null
comparison of the expressiveness of arc, place and transition time petri nets. null
scoping strategies for distributed aspects. dynamic deployment of aspects brings greater flexibility and reuse potential, but requires a proper means for scoping aspects. scoping issues are particularly crucial in a distributed context: adequate treatment of distributed scoping is necessary to enable the propagation of aspect instances across host boundaries and to avoid inconsistencies due to unintentional spreading of data and computations in a distributed system. we motivate the need for expressive scoping of dynamically-deployed distributed aspects by an analysis of the deficiencies of current approaches for distributed aspects. extending recent work on scoping strategies for non-distributed aspects, we then introduce a set of high-level strategies for specifying locality of aspect propagation and activation, and illustrate the corresponding gain in expressiveness. we present the operational semantics of our proposal using scheme interpreters, first introducing a model of distributed aspects that covers the range of current proposals, and then extending it with dynamic aspect deployment and scoping strategies. this work shows that, given some extensions to their original execution model, scoping strategies are directly applicable to the expressive scoping of distributed aspects.
update on the correlation of the highest energy cosmic rays with nearby extragalactic matter. data collected by the pierre auger observatory through 31 august 2007 showed evidence for anisotropy in the arrival directions of cosmic rays above the greisen-zatsepin-kuz'min energy threshold, \nobreak{$6\times 10^{19}$ev}. the anisotropy was measured by the fraction of arrival directions that are less than $3.1^\circ$ from the position of an active galactic nucleus within 75 mpc (using the véron-cetty and véron $12^{\rm th}$ catalog). an updated measurement of this fraction is reported here using the arrival directions of cosmic rays recorded above the same energy threshold through 31 december 2009. the number of arrival directions has increased from 27 to 69, allowing a more precise measurement. the correlating fraction is $(38^{+7}_{-6})%$, compared with $21%$ expected for isotropic cosmic rays. this is down from the early estimate of $(69^{+11}_{-13})%$. the enlarged set of arrival directions is examined also in relation to other populations of nearby extragalactic objects: galaxies in the 2 microns all sky survey and active galactic nuclei detected in hard x-rays by the swift burst alert telescope. a celestial region around the position of the radiogalaxy cen a has the largest excess of arrival directions relative to isotropic expectations. the 2-point autocorrelation function is shown for the enlarged set of arrival directions and compared to the isotropic expectation.
macro-microscopic mass formulae and nuclear mass predictions. different mass formulae derived from the liquid drop model and the pairing and shell energies of the thomas-fermi model have been studied and compared. they include or not the diffuseness correction to the coulomb energy, the charge exchange correction term, the curvature energy, different forms of the wigner term and powers of the relative neutron excess i = (n − z)/a. their coefficients have been determined by a least square fitting procedure to 2027 experimental atomic masses [1]. the coulomb diffuseness correction z2/a term or the charge exchange correction z4/3/a1/3 term plays the main role to improve the accuracy of the mass formula. the wigner term and the curvature energy can also be used separately but their coefficients are very unstable. the different fits lead to a surface energy coefficient of around 17-18 mev. a large equivalent rms radius (r0 = 1.22−1.24 fm) or a shorter central radius may be used. a rms deviation of 0.54 mev can be reached between the experimental and theoretical masses. the remaining differences come probably mainly from the determination of the shell and pairing energies. mass predictions of selected expressions have been compared to 161 new experimental masses and the correct agreement allows to provide extrapolations to masses of 656 selected exotic nuclei.
on solving inverse problems for electric fish like robots. this paper relates preliminary results concerning the solution of inverse problems arising in electric sense based navigation. this sense is used by electric fishes to move in dark waters using the electric current measurements perceived by the epidermal sensors as these are affected by the presence of obstacles. the latter change the resulting induced measures by instantaneously disturbing the fish self-produced electric field. the approach lies on a recently proposed graphical signature based classification methodology to overcome the computational burden associated to an explicit inversion of the mathematical equations. a preliminary validation of the proposed solution is obtained using a dedicated experimental setting.
elliptic flow of direct photons in au+au collisions at 200 gev. the rapidity dependence of the elliptic flow of direct photons in au+au collisions at √snn = 200 gev are predicted, based on a three-dimensional ideal hydrodynamic description of the hot and dense matter. the rapidity dependence of the elliptic flow v2(y) of direct photons (mainly thermal photons) is very sensitive to the initial energy density distribution along longitudinal direction, which provides a useful tool to extract the realistic initial condition from measurements.
measurement of transverse single-spin asymmetries for j/psi production in polarized p+p collisions at sqrt(s) = 200 gev. we report the first measurement of transverse single-spin asymmetries in $j/\psi$ production from transversely polarized $p+p$ collisions at $\sqrt{s} = 200$ gev with data taken by the phenix experiment in 2006 and 2008. the measurement was performed over the rapidity ranges $1.2 &lt; |y| &lt; 2.2$ and $ |y| &lt; 0.35$ for transverse momenta up to 6 gev/$c$. $j/\psi$ production at rhic is dominated by processes involving initial-state gluons, and transverse single-spin asymmetries of the $j/\psi$ can provide access to gluon dynamics within the nucleon. such asymmetries may also shed light on the long-standing question in qcd of the $j/\psi$ production mechanism. asymmetries were obtained as a function of $j/\psi$ transverse momentum and feynman-$x$, with a value of $-0.086 \pm 0.026^{\rm stat} \pm 0.003^{\rm syst}$ in the forward region. this result suggests possible nonzero trigluon correlation functions in transversely polarized protons and, if well defined in this reaction, a nonzero gluon sivers distribution function.
a genetic local search algorithm for minimizing total weighted tardiness in the job-shop scheduling problem. this paper considers the job-shop problem with release dates and due dates, with the objective of minimizing the total weighted tardiness. a genetic algorithm is combined with an iterated local search that uses a longest path approach on a disjunctive graph model. a design of experiments approach is employed to calibrate the parameters and operators of the algorithm. previous studies on genetic algorithms for the job-shop problem point out that these algorithms are highly depended on the way the chromosomes are decoded. in this paper, we show that the efficiency of genetic algorithms does no longer depend on the schedule builder when an iterated local search is used. computational experiments carried out on instances of the literature show the efficiency of the proposed algorithm.
computation of the robust preference relation combining a choquet integral and utility functions. null
spectroscopy of the unbound nucleus 18na. the unbound nucleus 18na, the intermediate nucleus in the two-proton radioactivity of 19mg, is studied through the resonant elastic scattering 17ne(p,17ne)p. the spectroscopic information obtained in this experiment is discussed and put in perspective with previous measurements and the structure of the mirror nucleus 18n.
a gac algorithm for a class of global counting constraints. this paper presents the constraint class seq bin(n;x;c;b) where n is an integer variable, x is a sequence of integer variables and c and b are two binary constraints. a constraint of the seq bin class enforces the two following conditions: (1) n is equal to the number of times that the constraint c is satised on two consecutive variables in x, and (2) b holds on any pair of consecutive variables in x. providing that b satises the particular property of neighborhood-substitutability, we come up with a ltering algorithm that achieves generalized arc-consistency (gac) for seq bin(n;x;c;b). this algorithm can be directly used for the constraints change, smooth, increasing nvalue, among and increasing among, in time linear in the sum of domain sizes. for all these constraints, this time complexity either improves the best known results, or equals those results.
resonances as a possible observable of hot and dense nuclear matter. one of the most fundamental questions in the field of relativistic heavy ion physics is how to reach and explore densities which are needed to cross the chiral and/or the deconfinement phase transition. in this analysis we investigate the information we can gather by analyzing baryonic and mesonic resonances on the hot and dense phase in such nuclear reactions. the decay products of these resonances carry information on the resonances properties at the space time point of their decay. we especially investigate the percentage of reconstructable resonances as a function of density for heavy ion collisions in the energy range between $e_{lab}$ = 30 agev and $\sqrt{s}$ = 200 agev, the energy domain between the future fair facility and the present rhic collider.
operation and calibration of the silicon drift detectors of the alice experiment during the 2008 cosmic ray data taking period. the calibration and performance of the silicon drift detector of the alice experiment during the 2008 cosmic ray run will be presented. in particular the procedures to monitor the running parameters (baselines, noise, drift speed) are detailed. other relevant parameters (sop delay, time-zero, charge calibration) were also determined.
inconsistencies in the determination of a capacity. null
method for producing solutions to a concrete multicriteria optimisation problem. null
mcs - a new algorithm for multicriteria optimisation in constraint programming. in this paper we propose a new algorithm called mcs for the search for solutions to multicriteria combinatorial optimisation problems. to quickly produce a solution that offers a good trade-off between criteria, the mcs algorithm alternates several branch \&amp; bound searches following diversified search strategies. it is implemented in cp in a dedicated framework and can be specialised for either complete or partial search.
microbial corrosion of p235gh steel under geological conditions. the role of microbial activity on the alteration of steel containers used for nuclear waste disposal is increasingly discussed. in this work we isolated and identified sulphate-reducing bacteria (srb) in the callovo–oxfordian clay rock studied as a potential host rock formation for a repository for high-level and long-lived radioactive waste in france. then, the effect of the srb growth on the overpack steel corrosion was investigated. the corrosion rate of the steel coupons was high under biotic conditions (not, vert, similar30 μm/year) in comparison to blank sterilized runs (not, vert, similar14 μm/year). culture experiments in compacted conditions with clay–stone cores and steel coupons under 120 bar, simulating deep geological conditions, gave results similar to those obtained in batch experiments (e.g. production of h2s). this indicates the plausibility of srb growth during the construction and operational phases of the repository and their survival at least temporarily after the disposal closure if water is available, which may cause fast corrosion of the steel containers under disposal conditions.
high $p_t$ resonances as a possibility to explore hot and dense nuclear matter. one of the fundamental objectives of experiments with ultrarelativistic heavy ions is to explore strongly interacting matter at high density and high temperature. in this investigation we study in particular the information which can be obtained by analyzing baryonic and mesonic resonances. the decay products of these resonances carry information on the resonances properties at the space time point of their decay. we especially investigate the percentage of reconstructable resonances as a function of density for heavy ion collisions in the energy range between $e_{lab}$ = 30 agev and $\sqrt{s}$ = 200 agev, the energy domain between the future fair facility and the present rhic collider.
wyfiwif: a haptic communication paradigm for collaborative motor skills learning. motor skills transfer is a challenging issue for many applications such as surgery, design and industry. in order to design virtual environments that support motor skills learning, a deep understanding of humans' haptic interactions is required. to ensure skills transfer, experts and novices need to collaborate. this requires the construction of the common frame of reference between the teacher and the learner in order to understand each other. in this paper, human-human haptic collaboration is investigated in order to understand how haptic information is exchanged. furthermore, wyfiwif (what you feel is what i feel), a haptic communication paradigm is introduced. this paradigm is based on a hand guidance metaphor. the paradigm helps operators to construct an efficient common frame of reference by allowing a direct haptic communication. a learning virtual environment is used to evaluate this haptic communication paradigm. hence, 60 volunteer students performed a needle insertion learning task. the results of this experiment show that, compared to conventional methods, the learning method based on haptic communication improves the novices' performance in such a task. we conclude that the wyfiwif paradigm facilitate expert-novice haptic collaboration to teach motor skills.
fics 2010. informal proceedings of the 7th workshop on fixed points in computer science (fics 2010), held in brno, 21-22 august 2010.
structured and flexible gray-box composition: application to task rescheduling for grid benchmarking. the evolution of complex distributed software systems often requires intricate composition operations in order to adapt or add functionalities, react to unanticipated changes to security policies, or do performance improvements, which cannot be modularized in terms of existing services or components. they often need controlled access to selected parts of the implementation, e.g., to manage exceptional situations and crosscutting within services and their compositions. however, existing composition techniques typically support only interface-level (black-box) composition or arbitrary access to the implementation (gray-box or white-box composition). in this paper, we present a more structured approach to the composition of complex software systems that require invasive accesses. concretely, we provide two contributions, we (i) present a small kernel composition language for structured gray-box composition with explicit control mechanisms and a corresponding aspect-based implementation; (ii) present and compare evolutions using this approach to gray-box composition in the context of two real-world software systems: benchmarking of grid algorithms with nasgrid and transactional replication with jboss cache.
sensitivity of isolated photon production at tev hadron colliders to the gluon distribution in the proton. we compare the single inclusive spectra of isolated photons measured at high transverse energy in proton-antiproton collisions at sqrt(s)=1.96 tev with next-to-leading order perturbative qcd predictions with various parametrizations of the parton distribution functions (pdfs). within the experimental and theoretical uncertainties, the tevatron data can be reproduced equally well by the recent cteq6.6, mstw08 and nnpdf1.2 pdf sets. we present also the predictions for isolated gamma spectra in proton-proton collisions at sqrt(s)=14tev for central (y=0) and forward (y=4) rapidities relevant for lhc experiments. different proton pdfs result in maximum variations of order 30% in the expected e_t-differential isolated gamma cross sections. the inclusion of the isolated photon data in global pdf fits will place extra independent constraints on the gluon density.
three-particle coincidence of the long range pseudorapidity correlation in high energy nucleus-nucleus collisions. we report the first three-particle coincidence measurement in pseudorapidity (δη) between a high transverse momentum (p⊥) trigger particle and two lower p⊥ associated particles within azimuth |δϕ|&lt;0.7 in √snn=200  gev d+au and au+au collisions. charge ordering properties are exploited to separate the jetlike component and the ridge (long range δη correlation). the results indicate that the correlation of ridge particles are uniform not only with respect to the trigger particle but also between themselves event by event in our measured δη. in addition, the production of the ridge appears to be uncorrelated to the presence of the narrow jetlike component.
an experimental exploration of the qcd phase diagram: the search for the critical point and the onset of de-confinement. the qcd phase diagram lies at the heart of what the rhic physics program is all about. while rhic has been operating very successfully at or close to its maximum energy for almost a decade, it has become clear that this collider can also be operated at lower energies down to 5 gev without extensive upgrades. an exploration of the full region of beam energies available at the rhic facility is imperative. the star detector, due to its large uniform acceptance and excellent particle identification capabilities, is uniquely positioned to carry out this program in depth and detail. the first exploratory beam energy scan (bes) run at rhic took place in 2010 (run 10), since several star upgrades, most importantly a full barrel time of flight detector, are now completed which add new capabilities important for the interesting physics at bes energies. in this document we discuss current proposed measurements, with estimations of the accuracy of the measurements given an assumed event count at each beam energy.
longitudinal scaling property of the charge balance function in au + au collisions at 200 gev. we present measurements of the charge balance function, from the charged particles, for diverse pseudorapidity and transverse momentum ranges in au + au collisions at 200 gev using the star detector at rhic. we observe that the balance function is boost-invariant within the pseudorapidity coverage [-1.3, 1.3]. the balance function properly scaled by the width of the observed pseudorapidity window does not depend on the position or size of the pseudorapidity window. this scaling property also holds for particles in different transverse momentum ranges. in addition, we find that the width of the balance function decreases monotonically with increasing transverse momentum for all centrality classes.
is the centrality dependence of the elliptic flow $v_2$ and of the average $$ in rhic experiments more than a core-corona effect?. recently we have shown that the centrality dependence of the multiplicity of different hadron species observed in rhic and sps experiments can be well understood in a simple model, dubbed core-corona model. there it is assumed that those incoming nucleons which scatter only once produce hadrons as in pp collisions whereas those which scatter more often form an equilibrated source which decays according to phase space. in this article we show that also kinematical variables like $v_2/\epsilon_{part} (n_{part})$ as well as $v_2^i/\epsilon_{part} (n_{part})$ and $$ of identified particles are well described in this model. the correlation of $$ between peripheral heavy ion collisions and pp collisions for different hadrons, reproduced in this model, questions whether hydrodynamical calculations are the proper tool to describe non-central heavy ion collision. the model explains as well the centrality dependence of $v_2/\epsilon_{part}$ of charged particles, considered up to now as an observable which allows to determine the viscosity of the quark gluon plasma. the observed dependence of $v_2^i/\epsilon_{part}(n_{part})$ on the particle species is a simple consequence of the different ratios of core to corona particles.
measurement of the bottom quark contribution to non-photonic electron production in $p+p$ collisions at $\sqrt{s} $=200 gev. the contribution of b meson decays to non-photonic electrons, which are mainly produced by the semi-leptonic decays of heavy flavor mesons, in p+p collisions at $\sqrt{s} =$ 200 gev has been measured using azimuthal correlations between non-photonic electrons and hadrons. the extracted b decay contribution is approximately 50% at a transverse momentum of $p_{t} \geq 5$ gev/c. these measurements constrain the nuclear modification factor for electrons from b and d meson decays. the result indicates that b meson production in heavy ion collisions is also suppressed at high pt even under the extreme case for the ratio of b to d contributions to non-photonic electrons.
alice emcal physics performance report. the alice detector at the lhc (a large ion collider experiment) will carry out comprehensive measurements of high energy nucleus-nucleus collisions, in order to study qcd matter under extreme conditions and the phase transtion between conã¯â¬âned matter and the quark-gluon plasma (qgp). this report presents our current state of understanding of the physics performance of the large acceptance electromagnetic calorimeter (emcal) in the alice central detector. the emcal enhances aliceã¢ââs capabilities for jet measurements. the emcal enables triggering and full reconstruction of high energy jets in alice, and augments existing alice capabilities to measure high momentum photons and electrons. combined with aliceã¢ââs excellent capabilities to track and identify particles from very low pt to high pt , the emcal enables a comprehensive study of jet interactions in the medium produced in heavy ion collisions at the lhc.
proving fixed points. we propose a method to characterize the fixed points described in tarski's theorem for complete lattices. the method is deductive: the least and greatest fixed points are "proved" in some inference system defined from deduction rules. we also apply the method to two other fixed point theorems, a generalization of tarski's theorem to chain-complete posets and bourbaki-witt's theorem. finally, we compare the method with the traditional iterative method resorting to ordinals and the original impredicative method used by tarski.
a new challenge for the energy efficiency evaluation community: energy savings and emissions reductions from urban transportation policies. the energy efficiency evaluation community has a large experience about programs for industries, residential and commercial sectors. but now the largest share of the energy consumption growth is due to the transportation sector. moreover, as the stakes related to the transport sector are considerable, relying on separate actions for technological energy efficiency improvements will not be sufficient. therefore transport policies now support the development of more integrated approaches. all together, this raises new evaluation issues. this paper first look at what makes or would make the transport specific and different from the other “usual” sectors, as far as the evaluation of energy savings and avoided co2 emissions is concerned. this is illustrated by a comparison between two simple action types, a car replacement for the transport sector and a boiler replacement for the building sector. then, taking into account the change toward more integrated action plans, the common evaluation methods used for energy efficiency programs and urban transport planning are discussed, to what extent they can be applied to evaluation of urban mobility plans. key differences between the building and transport sectors that have an influence on what evaluation methods can be used are: 1) the level of complexity for the definition of the service delivered, 2) the relative importance of variables having long term perspective (building stock) and short term perspective (mobility behaviors). the evaluation of integrated approaches raises additional issues, mainly interactions between the distinct policy measures implemented on a given territory, and causality between the measures implemented and the changes observed. two interesting tracks stand out: - developing new portfolio approaches starting from available bottom-up methods; - adapting methods used for transport infrastructure design or transport planning. this emphasizes progress that could be achieved by crossing experiences from both scientific communities, evaluation of energy efficiency programs and transport planning.
compatibility of the french white certificate program to fulfil the objective of energy savings claimed by the energy service directive. the commission has proposed a directive on the promotion of end-use efficiency and energy services (esd) to enhance the cost-effective and efficient end-use of energy in member states. according to the directive, the member states shall adopt and aim to achieve an overall national indicative energy savings target of 9% (or beyond) in 2016. this target is to be reached by way of energy services and other energy efficiency measures. the french national energy efficiency action plan to comply with the esd includes a white certificates scheme (or fwc) as one of the important measures to fulfil the target. as the accountings of energy savings in the fwc scheme and in the esd are different (e.g. lifetime-cumulated and discounted kwh for fwc and annual kwh for esd), an analysis of the compliance of both methodologies and a comparison of the assessed savings are necessary. in this paper, we evaluate the compliance with the esd requirements of two different end-use actions (insulation, heating boiler) included in the fwc scheme. this is done through the concrete case of certificates filed by edf. the main objective of this evaluation is to assess the contribution of the savings of these fwc actions to the target of the esd. finally, general conclusions are drawn about the use of a white certificates scheme as a monitoring and evaluation tool for the esd purpose.
can energy savings from operations promoting energy efficient behaviors in office buildings be accounted for?. when looking for solutions to mitigate the growth of energy consumption in the commercial buildings sector, research works often focus on the energy performance of buildings. indeed, many studies established how large the technical improvement potential was in this sector. but cost-effective energy savings can also be achieved in a complementary way by an improved energy management promoting energy efficient behaviors, because energy consumptions depend on both energy performance of buildings and equipments, and end-users behaviors. past experiences tend to show that if awareness operations were widely disseminated, a significant amount of energy savings could be realized. it is likely that more and more organizations engage such operations. unfortunately, their real impacts remain rather unknown and uncertain, mainly because they are not perceived as a serious option. consequently they are implemented in very heterogeneous ways. thus, their results may vary a lot too. this paper first reminds success factors analyzed in previous works, before presenting monitoring guidelines to ensure that energy savings can be accounted for. this methodological approach could be an entry to consider the inclusion of behavioral actions in schemes accounting for energy savings, such as white certificates. the option to include awareness operations in an energy management service appears to create good conditions ensuring the quality of the operations and therefore an accounting system reliable enough for certified energy savings. admitting this new kind of energy service in white certificates schemes would on the one side provide a clear recognition of behavioral actions, and on the other side promote quality standards ensuring more homogeneity and effectiveness among this kind of operation.
evaluation as a "learning-by-doing" tool for the implementation of local energy efficiency activities. with the "think global, act local" trends, local levels are taking an increasing role in the implementation of action plans, especially in the field of energy efficiency. an inventory of local energy efficiency operations in france confirmed a significant expansion of these activities, but also highlighted how rare their evaluation is, although a rich methodological evaluation material is available. the research question for this study was then how to fill the gap between theory and practice. this was addressed through studying the issue of evaluation use. the first step was to find in the evaluation literature the key components of evaluation use and the success factors to overcome the barriers to evaluation practice previously identified. this was used to adjust our evaluation methods and approach, and then to apply this to a particular case study. key success factors for evaluation use were highlighted, such as the constructive and regular contacts between evaluators and program partners, and presenting the evaluation as a win-win collaboration. finally, the main evaluation use was not to quantify the results of the operation, even if it was initially the most important stakeholder expectation, but to learn how to work together, how to supervise and use an evaluation, and how to improve the operation management and the operations themselves. this way, the evaluation really appears to be a learning-by-doing tool for all stakeholders involved in the implementation of local energy efficiency activities.
raising awareness for energy efficiency in the service sector: learning from success stories to disseminate good practices. energy efficiency in the service sector is a key issue because of the important growth of its energy consumption. the energy performance of buildings and equipment can be improved through technical investments, but this has to be linked with an efficient management and good practices in order to reach better energy efficiency levels in a cost-effective way. experience feedback concerning awareness activities in the service sector highlights the interesting opportunities of energy efficiency improvements they represent. this paper first draws a synthesis of the available feedback in this area to detect factors of success for this kind of activities. more than twenty operations from europe and north america were analyzed looking at items such as the stakeholders involved, the actions implemented, the communication means, and the evaluation performed. then a case study describes an edf pilot operation in south east of france. an awareness campaign was led in four particular edf buildings to inform the employees of the best practices and to involve them to apply these advice. different action packages were used to compare their efficiency. the evaluation emphasizes the success of the operation, with around 10% of energy savings (i.e. more than 270 mwh/a). more than 80% of the employees said they changed their energy behavior and other indicators show their commitment and satisfaction towards the campaign. finally, suggestions are made to disseminate good practices at a broader scale, especially out of the "initiated" circle. building up a know-how from the evaluation of past experiences makes easier the development of process such as networking, experience sharing, and including these activities in energy services offers and in white certificates systems.
a process to develop operational bottom-up evaluation methods – from reference guidebooks to a practical culture of evaluation. needs for evaluating energy efficiency (ee) activities are increasing, for the accounting of results and for understanding their success/failures. indeed evaluation results should be used for both reporting past activities and improving future operations. lack of easy to use methods is pointed out by local stakeholders as a major barrier to evaluation. another issue is the frequent negative perception of evaluation, experienced as a control and/or a waste of time. this paper presents a systematic process to develop bottom-up evaluation methods designed to fit to stakeholders needs: directly operational, easy to appropriate, providing useful conclusions to improve operations and to communicate about their results. our approach relies on the principle of experience capitalisation and on an organisation with two levels, central and on-field. it aims to create conditions for continuous improvement. moreover it should insure involved stakeholders do actually take part in and take advantage of the evaluation process. this methodology handles both impact and process evaluation. for the impacts, focus is on calculations transparency, data quality and reliability of the results. regarding operation process, main issues are analysing causality between actions and results, and detecting the success and failure factors. this work was first developed for the evaluation of local operations in france[1]. the resulting methodology was tested on two case studies from the eco energy plan, a local ee programme implemented in south-east of france. [1] within a partnership between armines, the wuppertal institute for climate environment and energy, and edf r&amp;d (electricité de france).
extended h2 output feedback control for continuous descriptor systems. this paper investigates the design of an “extended” h2 output feedback controller for continuous descriptor systems. in such problem, the controller, which is described within descriptor framework, has to satisfy simultaneously two conditions: 1-the closed-loop is admissible and has minimum h2 norm. 2-only the internal stability of a part of the generalized closed-loop is sought. in such case, the standard h2 output feedback control problem for descriptor systems cannot be solved. an explicit characterization of the optimal solution, based on two generalized algebraic riccati equations and two structured generalized sylvester-type equations, is given. a numerical example is given to illustrate the applicability of the proposed results.
extended stabilizing controllers for continuous-time descriptor systems. this paper investigates the so-called extended stabilization control problem for continuous-time descriptor systems. in such nonstandard stabilization problem, the generalized plant and its unstable and/or nonproper weights are all described within the descriptor framework. the extended stabilizing controller (esc) is found such that: -the closed-loop is admissible; - only the internal stability of a part of the generalized closed-loop is sought. first, the necessary and sufficient conditions for the existence of a solution are explicitly given in terms of two structured generalized sylvester equations. moreover, the parametrization of a class of controllers satisfying such stability is formulated. finally, numerical examples are presented to show the effectiveness of the proposed results.
in-target radioactive nuclei production rates with eurisol single-stage target configuration. null
a comparison of model migration tools. modelling languages and thus their metamodels are subject to change. when a metamodel evolves, existing models may no longer conform to the evolved metamodel. to avoid rebuilding them from scratch, existing models must be migrated to conform to the evolved metamodel. manually migrating existing models is tedious and errorprone. to alleviate this, several tools have been proposed to build a migration strategy that automates the migration of existing models. little is known about the advantages and disadvantages of the tools in different situations. in this paper, we thus compare a representative sample of migration tools - aml, cope, ecore2ecore and epsilon flock - using common migration examples. the criteria used in the comparison aim to support users in selecting the most appropriate tool for their situation.
transverse momentum spectra of charged particles in proton-proton collisions at $\sqrt{s} = 900$~gev with alice at the lhc. the inclusive charged particle transverse momentum distribution is measured in proton-proton collisions at $\sqrt{s} = 900$~gev at the lhc using the alice detector. the measurement is performed in the central pseudorapidity region $(|\eta|&lt;0.8)$ over the transverse momentum range $0.15.
two-pion bose-einstein correlations in pp collisions at sqrt(s)=900 gev. we report on the measurement of two-pion correlation functions from pp collisions at sqrt(s)=900 gev performed by the alice experiment at the large hadron collider. our analysis shows an increase of the hbt radius with increasing event multiplicity, in line with other measurements done in particle- and nuclear collisions. conversely, the strong decrease of the radius with increasing transverse momentum, as observed at rhic and at tevatron, is not manifest in our data.
trends in yield and azimuthal shape modification in dihadron correlations in relativistic heavy ion collisions. fast parton probes produced by hard scattering and embedded within collisions of large nuclei have shown that partons suffer large energy loss and that the produced medium may respond collectively to the lost energy. we present measurements of neutral pion trigger particles at transverse momenta p^t_t = 4-12 gev/c and associated charged hadrons (p^a_t = 0.5-7 gev/c) as a function of relative azimuthal angle delta phi at midrapidity in au+au and p+p collisions at sqrt(s_nn) = 200 gev. these data lead to two major observations. first, the relative angular distribution of low momentum hadrons, whose shape modification has been interpreted as a medium response to parton energy loss, is found to be modified only for p^t_t &lt; 7 gev/c. at higher p^t_t, the data are consistent with unmodified or very weakly modified shapes, even for the lowest measured p^a_t. this observation presents a quantitative challenge to medium response scenarios. second, the associated yield of hadrons opposite to the trigger particle in au+au relative to that in p+p (i_aa) is found to be suppressed at large momentum (iaa ~ 0.35-0.5), but less than the single particle nuclear modification factor (r_aa ~0.2).
crystal chemistry, rietveld refinements and raman spectroscopy studies of the new solid solution series: ba3−xsrx(vo4)2 (0 ≤ x ≤ 3). the new solid solution series ba3−xsrx(vo4)2 (0 ≤ x ≤3) has been synthesized and studied by a combination of x-ray powder diffraction and raman vibrational spectroscopy. this continuous solid solution crystallise in the hexagonal system with view the mathml source space group. the structure has been determined at room temperature from x-ray diffraction by the rietveld method analysis. it is formed by a 3d network of (ba/sr)(1)(vo4)24− layers linked into a crystal network by (ba/sr)2+(2) cations. the vibrational spectra of this crystalline orthovanadate solid solution series are interpreted by means of factor group analysis in terms of space group view the mathml source (view the mathml source). assignments of the v–o vibrational stretching and bending modes, as well as some of the external modes, have been made. while all the modes show a monotonous shift as a function of the composition x, a break in the curves of intensities, full width at half maximum and band areas as a function of x is observed and attributed to the statistical distribution of ba and sr ions in the same crystallographic sites.
behaviour of a glass ceramic in contact with various media and solvents. null
neutron correlations in 6he viewed through nuclear break-up reactions. null
midrapidity antiproton-to-proton ratio in pp collisions at $\sqrt{s} = 0.9$ and $7$~tev measured by the alice experiment. the ratio of the yields of antiprotons to protons in pp collisions has been measured by the alice experiment at $\sqrt{s} = 0.9$ and $7$~tev during the initial running periods of the large hadron collider(lhc). the measurement covers the transverse momentum interval $0.45 &lt; p_{\rm{t}} &lt; 1.05$~gev/$c$ and rapidity $|y| &lt; 0.5$. the ratio is measured to be $r_{|y| &lt; 0.5} = 0.957 \pm 0.006 (stat.) \pm 0.014 (syst.)$ at $0.9$~tev and $r_{|y| &lt; 0.5} = 0.991 \pm 0.005 (stat.) \pm 0.014 (syst.)$ at $7$~tev and it is independent of both rapidity and transverse momentum. the results are consistent with the conventional model of baryon-number transport and set stringent limits on any additional contributions to baryon-number transfer over very large rapidity intervals in pp collisions.
cosmic ray properties from the electric fields measured by the codalema experiment. this thesis is dedicated to the study of high energy cosmic rays that penetrating the atmosphere and dissipating their energy by producing a very large amount of secondary particles called an air shower. the codalema experiment is measuring the radio counterpart associated to these air showers by using antennas in the 1-100 mhz frequency domain. from the characteristics of the air shower some questions have been studied in order to determine the main effects giving rise to the radio signal. in a second part, we have develop a linear prediction method to identify transients associated to air shower in the noise environment of the experiment. finally, possible signatures of the radio emission mechanism have been searched for in the experimental signals. this last study is linking the two previous analysis.
on the liquid drop model mass formulas and alpha decay of the heaviest nuclei. the coefficients of different macro-microscopic liquid drop model mass formulas have been determined by a least square fitting procedure to 2027 experimental atomic masses. a rms deviation of 0.54 mev can be reached. the remaining differences come mainly from the determination of the shell and pairing energies. extrapolations are compared to 161 new experimental masses and to 656 mass evaluations. the different fits lead to a surface energy coefficient of around 17-18 mev. finally, alpha decay potential barriers are revisited and predictions of alpha decay half-lives of still unknown superheavy elements are given from previously proposed analytical formulas and from extrapolated qalpha values.
declarative events for object-oriented programming. in object-oriented designs inversion of control is achieved by an event-driven programming style based on imperatively triggered events. an alternative approach can be found in aspect-oriented programming, which defines events as declarative queries over implicitly available events. this helps to localize definition of events and avoid preplanning, but lacks a clean integration with object-oriented features and principles. the contribution of this work is a concept of object-oriented events that combines imperative, declarative and implicit events and provides their seamless integration with object-oriented features, preserving encapsulation and modular reasoning. we present an efficient and type-safe implementation of the concept as an extension to the scala language.
architecture for the next generation system management tools for distributed computing platforms. in order to get more results or greater accuracy, computational scientists execute mainly parallel or distributed applications, and try to scale these applications up. accordingly, they use more and more distributed resources, using local large-scale hpc systems, grids or even clouds. however, in most of cases, the use and management of such platforms is static. indeed generally, the application has to be adapted to the environment rather than adapting the environment to the applications' needs. in addition, platforms are managed through the concept of time and space partitioning mainly via the use of batch schedulers: time partitioning enables the execution of several applications on a same resources, and space partitioning enables the execution of applications across several distributed resources. this leads to some usage limitations, where applications can only be executed on a subset of the available resources. therefore, scientists have to manage technical details related to the execution of their applications on each target hpc platforms, which could result in application modifications, rather than focusing on the science. in this article, we advocate for a system management tool enabling the transparent configuration of the hpc platform and the customization of the execution environment for large-scale hpc systems (such as clusters or mpps), grids, and clouds. we propose a new approach to manage these systems in a more dynamic way, where the resources can be configured and reconfigured automatically and transparently. the proposed solution is not removing the benefit of resource management systems such as batch system (they still provide a well-known interface for job submission), but rather redefine the underlying system capabilities. our approach is based on a refinement of the concept of emulation and virtualization introduced by goldberg. furthermore, the proposed approach leads to the definition of a method that provides a unique interface to scientists for the deployment and management of their applications on hpc platforms. this method is based on two concepts: (i) the virtual system environment (vse), and (ii) the virtual platforms (vps).
self-duality and supersymmetry. we observe that the hamiltonian view the mathml source, where view the mathml source is the flat 4d dirac operator in a self-dual gauge background, is supersymmetric, admitting 4 different real supercharges. a generalization of this model to the motion on a curved conformally flat 4d manifold exists. for an abelian self-dual background, the corresponding lagrangian can be derived from known harmonic superspace expressions.
producing hard processes regarding the complete event: the epos event generator. jet cross sections can be in principle compared to simple pqcd calculations, based on the hypothesis of factorization. but often it is useful or even necessary to not only compute the production rate of the very high pt jets, but in addition the "rest of the event". the proposed talk is based on recent work, where we try to construct an event generator fully compatible with pqcd which allows to compute complete events, consisting of high pt jets plus all the other low pt particles produced at the same time. whereas in "generators of inclusive spectra" like pythia one may easily trigger on high pt phenomena, this is not so obvious for "generators of physical events", where in principle one has to generate a very large number of events in order to finally obtain rare events (like those with a very high pt jet). we recently developped an independnat block method which allow us ta have a direct access to dedicated variables 1. we will present latest results concerning this approach.
higher moments of net-proton multiplicity distributions at rhic. we report the first measurements of the kurtosis (\kappa), skewness (s) and variance (\sigma^2) of net-proton multiplicity (n_p - n_pbar) distributions at midrapidity for au+au collisions at \sqrt(s_nn) = 19.6, 62.4, and 200 gev corresponding to baryon chemical potentials (\mu_b) between 200 - 20 mev. our measurements of the products \kappa \sigma^2 and s \sigma, which can be related to theoretical calculations sensitive to baryon number susceptibilities and long range correlations, are constant as functions of collision centrality. we compare these products with results from lattice qcd and various models without a critical point and study the \sqrt(s_nn) dependence of \kappa \sigma^2. from the measurements at the three beam energies, we find no evidence for a critical point in the qcd phase diagram for \mu_b below 200 mev.
sqm with non-abelian self-dual fields: harmonic superspace description. we present a lagrangian formulation for n=4 supersymmetric quantum-mechanical systems describing the motion in external non-abelian self-dual gauge fields. for any such system, one can write a component supersymmetric lagrangian by introducing extra bosonic variables with topological chern-simons type interaction. for a special class of such system when the fields are expressed in the `t hooft ansatz form, it is possible to give a superfield description using harmonic superspace formalism. as a new explicit example, the n=4 mechanics with yang monopole is constructed.
neutron production in neutron-induced reactions at 96 mev on iron and lead. null
emission patterns of neutral pions in 40 a mev ta+au reactions. differential cross sections of neutral pions emitted in 181ta + 197au collisions at a beam energy of 39.5a mev have been measured with the photon spectrometer taps. the kinetic energy and transverse momentum spectra of neutral pions cannot be properly described in the framework of the thermal model, nor when the reabsorption of pions is accounted for in a phenomenological model. however, high energy and high momentum tails of the pion spectra can be well fitted through thermal distributions with unexpectedly soft temperature parameters below 10 mev.
spectra of identified high-pt π± and p(p¯) in cu+cu collisions at √snn=200 gev. we report new results on identified (anti)proton and charged pion spectra at large transverse momenta (3.
adequacy between autosar os specification and real-time scheduling theory. null
schedulability analysis of osek/vdx applications. null
improvement in mechanical properties of aluminium polypropylene composite fiber. null
high p_t direct photon and pi^0 triggered azimuthal jet correlations in sqrt(s)=200 gev p+p collisions. correlations of charged hadrons of 1 &lt; pt &lt; 10 gev/c with high pt direct photons and pi^ 0 mesons in the range 5.
energy dependence of pion interferometry scales in ultra-relativistic heavy ion collisions. a study of energy behavior of the pion spectra and interferometry scales is carried out for the top sps, rhic and lhc energies within the hydrokinetic approach. the latter allows one to describe evolution of quark–gluon and hadron matter as well as continuous particle emission from the fluid in agreement with the underlying kinetic equations. the main mechanisms that lead to the paradoxical, at first sight, behavior of the interferometry scales, are exposed. in particular, a slow decrease and apparent saturation of rout/rside ratio with an energy growth happens due to a strengthening of positive correlations between space and time positions of pions emitted at the radial periphery of the system. such en effect is a consequence of the two factors: a developing of the pre-thermal collective transverse flows and an increase of the initial energy density in the fireball.
on the compared expressiveness of arc, place and transition time petri nets. in this paper, we consider safe time petri nets where time intervals (strict and large) are associated with places (tppn), arcs (tapn) or transitions (ttpn). we give the formal strong and weak semantics of these models in terms of timed transition systems. we compare the expressiveness of the six models w.r.t. (weak) timed bisimilarity (behavioral semantics). the main results of the paper are : (i) with strong semantics, tapn is strictly more expressive than tppn and ttpn ; (ii) with strong semantics tppn and ttpn are incomparable ; (iii) ttpn with strong semantics and ttpn with weak semantics are incomparable. moreover, we give a complete classification by a set of 9 relations explained in a figure.
inclusive pi^0, eta, and direct photon production at high transverse momentum in p+p and d+au collisions at sqrt(s_nn) = 200 gev. we report a measurement of high-p_t inclusive pi^0, eta, and direct photon production in p+p and d+au collisions at sqrt(s_nn) = 200 gev at midrapidity (0 &lt; eta &lt; 1). photons from the decay pi^0 -&gt; gamma gamma were detected in the barrel electromagnetic calorimeter of the star experiment at the relativistic heavy ion collider. the eta -&gt; gamma gamma decay was also observed and constituted the first eta measurement by star. the first direct photon cross section measurement by star is also presented, the signal was extracted statistically by subtracting the pi^0, eta, and omega(782) decay background from the inclusive photon distribution observed in the calorimeter. the analysis is described in detail, and the results are found to be in good agreement with earlier measurements and with next-to-leading order perturbative qcd calculations.
abnormal combustions in internal combustion engines: review on the latest patents eliminating knock conditions. abnormal combustions occurred since the invention of internal combustion engines. knock was intensively studied by researchers and engineers since this date. first patents were taken out on fuel quality (defined by octane index nowadays). after patents on sensors itself, engine settings were piloted to eliminate knock conditions in 1979-1980. many companies followed this pathway. latest patents on knock are reviewed. a highlight on a preventive protection (open loop control) is proposed for stationary engines fuelled natural gas, see le corre et al. [1]: this patent is a new concept in rupture with curative protection (closed loop control). preventive protection is based on a gas quality sensor.
towards expressive, well-founded and correct aspect-oriented programming. null
explicitly distributed aop using awed. distribution-related concerns, such as data replication, often crosscut the business code of a distributed application. currently such crosscutting concerns are frequently realized on top of distributed frameworks, such as ejbs, and initial ao support for the modularization of such crosscutting concerns, e.g., jboss aop and spring aop, has been proposed. based on an investigation of the implementation of replicated caches using jboss cache, we motivate that crosscutting concerns of distributed applications benefit from an aspect language for explicit distributed programming. we propose awed , a new aspect language with explicit distributed programming mechanisms, which provides three contributions. first, remote pointcut constructors which are more general than those of previous related approaches, in particular, supporting remote sequences. second, a notion of distributed advice with support for asynchronous and synchronous execution. third, a notion of distributed aspects including models for the deployment, instantiation and state sharing of aspects. we show several concrete examples how awed can be used to modularly implement and extend replicated cache implementations. finally, we present a prototype implementation of awed , which we have realized by extending jasco, a system providing dynamic aspects for java.
traceability for model driven, software product line engineering. null
a study on human-human interactions for common frame of reference development within collaborative virtual environments. collaborative virtual environments are 3d spaces that allow multiple users to work together on a common task. to design such environments to support human-human interactions, it is important to study how people develop a common frame of reference during collaboration. the concept of common frame of reference is central to all collaborative activities. it allows partners to understand each other through a continuous exchange of information (explicit and implicit). the ultimate goal of this research is to facilitate and enrich the construction of common frame of reference to accommodate specific collaborative virtual environments characteristics. indeed, the design elements related to the common frame of reference (i.e. communication modes, environment's construction and interactions) are essential for successful collaborative activity. two experimental studies were conducted using different collaborative virtual environments conditions. the first study shows that adding fixed landmarks can improve the development of common frame of reference within an objects manipulation task. the second study shows that haptic communication can improve the construction of the common frame of reference in a technical gesture learning task. these results are used to provide recommendations for collaborative virtual environments design. it represents a first step towards the development of a standardized collaborative virtual environments design methodology.
vpa-based aspects: better support for aop over protocols. null
haptic communication to enhance collaboration in virtual environments. motivation – to study haptic communication in collaborative virtual environments. research approach – an experimental study was conducted, in which 60 students were asked to perform in dyads a shared manual task after a training period. findings/design – the results show that haptic communication can influence the common frame of reference development in a shared manual task. research limitations/implications – deeper verbalization analyses are needed to evaluate the common frame of reference development. originality/value – this study highlights haptic interactions importance when designing virtual environment that support shared manual tasks. take away message – haptic communication, combined with visual and verbal communication, enriches interactions in virtual environments.
safe dynamic reconfigurations of fractal architectures with fscript. null
a language for quality of service requirements specification in web services orchestrations. service oriented architectures industry aims to deliver agile service infrastructures. in this context, solutions to specify service compositions (mostly bpel language) and quality of service (qos) of individual services have emerged. however, architects still lack adapted means to specify and implement qos in service compositions. typically, they use ad-hoc technical solutions that significantly reduce flexibility and require cost-effective development. our approach aims to overcome this shortcoming by introducing both a new language and tool for qos specification and implementation in service compositions. more specifically, our language is a declarative domain-specific language that allows the architect to specify qos constraints and mechanisms in web service orchestrations. our tool is responsible for the qos constraints processing and for qos mechanisms injection into the orchestration. a key property of our approach is to preserve compatibility with existing languages and standards. in this paper, we present our language and tool, as well as an illustrative scenario dealing with multiple qos concerns.
a declarative approach for qos-aware web service compositions. while bpel language has emerged to allow the specification of web service compositions from a functional point of view, it is still left to the architects to find proper means to handle the quality of service (qos) concerns of their compositions. typically, they use ad-hoc technical solutions, at the message level, that significantly reduce flexibility and require costly developments. in this paper, we propose a policy-based language aiming to provide expressivity for qos behavioural logic specification in web service orchestrations, as well as a non-intrusive platform in charge of its execution both at pre-deployment time and at runtime.
a hierarchical control scheme based on prediction and preview: an application to the cruise control problem. null
global constraint catalog website. null
reliability of dynamic reconfigurations in component-based architectures. software engineering must cope with a more and more increasing need for evolutivity of software systems in order to make their maintenance and more generally their administration easier. however, evolution and especially dynamic evolution in a system must not be done at the expense of its reliability, that is to say its ability to deliver the expected functionalities. actually modifications in a given system may let it in an inconsistent state and so it can have an impact on its reliability. the aim of this thesis is to guarantee reliability of dynamic reconfigurations used to make systems evolve at runtime while preserving their availibility, i.e. their continuity of service. we are especially interested in component based and distributed systems. the system architecture can be used as a support for dynamic, non-anticipated (also called ad-hoc) and concurrent reconfigurations. we propose a definition of consistency for configurations and reconfigurations in the fractal component model with a model based on integrity constraints like for example structural invariants. reliability of reconfigurations is ensured thanks to a transactional approach which allows both to deal with error recovery and to manage reconfiguration concurrency in systems. finally, we propose a modular component-based architecture so as to implement transactional mechanisms adapted to dynamic reconfigurations in fractal applications.
summary of the third workshop on domain-specific aspect languages. null
fiesta: an approach for fine-grained scope definition, configuration and derivation of model-driven software product lines. we present an approach based on model-driven development ideas to create software product lines(spls). in model-driven spl approaches, the derivation of a product starts from a domain application model. this model is transformed through several stages reusing model transformation rules until a product is obtained. transformations rules are selected according to variants included in configurations created by product designers. configurations include variants from variation points, which are relevant characteristics representing the variability of a product line. our approach (1) provides mechanisms to improve the expression of variability of model-driven spls by allowing designers to create fine-grained configurations of products, and (2) integrates a product derivation process which uses decision models and aspect-oriented programming facilitating the reuse, adaptation and composition of model transformation rules. we introduce constraint models which make it possible for product line architects to capture the scope of product lines using the concepts of constraint, cardinality property and structural dependency property. to configure products, we create domain models and binding models, which are sets of bindings between model elements and variants and satisfy the constraint models. we define a decision model as a set of aspects. an aspect maintains information of what and when transformations rules that generate commonalities of products must be intercepted (joinpoints) and what transformation rules (advices) that generate variable structures must be executed instead. our strategy maintains uncoupled variants from model transformation rules. this solves problems related to modularization, coupling, flexibility and maintainability of transformations rules because they are completely separated from variants; thus, they can evolve independently.
distributed aspects: better separation of crosscutting concerns in distributed software systems. this thesis shows that abstractions provided by current mainstream object oriented (oo) languages are not enough to address the modularization of distributed and concurrent algo- rithms, protocols, or architectures. in particular, we show that code implementing concurrent and distributed algorithms is scattered and tangled in the main implementation of jboss cache, a real industrial middleware application. we also show that not only code is tangled, but also conceptual algorithms are hidden behind object-based structures (i.e., they are not visible in the code). additionally, we show that such code is resilient to modularization. thus, we show that several cycles of re-engineering (we study the evolution of three diﬀerent version of jboss cache) using the same set of oo abstractions do not improve on the modularization of distributed and concurrent code. from these ﬁndings we propose a novel aspect oriented programming language with explicit support for distribution and concurrency (awed). the language uses aspects as main abstractions and propose a model for distributed aspects and remote pointcuts, extending sequential approaches with support for regular sequences of distributed events. the language also proposes advanced support for the manipulation of groups of host, and the ﬁne-grained deterministic ordering of distributed events. to evaluate the proposal we perform several experiments in diﬀerent domains: refactoring and evolution of replicated caches, development of automatic toll systems, and debugging and testing of distributed applications. finally, using this general model for distribution we provide two additional contributions. first, we introduce invasive patterns, an extension to traditional communication patterns for distributed applications. invasive patterns present an aspect-based language to express protocols over distributed topologies considering diﬀerent coordination strategies (architec- tural programming). the implementation of this approach is leveraged by the distributed features of awed and is realized by means of a transformation into it. second, we add the deterministic manipulation of distributed messages to our model by means of causally ordered protocols.
designing open-ended languages: an historical perspective. null
graph partitioning constraints. combinatorial problems based on graph partitioning enable to represent many practical applications. examples based on phylogenetic supertree problem, mission planning, or the routing problems in logistic, perfectly illustrate such applications. nevertheless, these problems are not based on the same partitioning pattern : generally, patterns like cycles, paths, or trees are distinguished. moreover, the practical applications are not often limited to theoretical problems like hamiltonian path problem, or k-node disjoint paths problems. indeed, they usually combine the graph partitioning problem with several restrictions related to the topology of nodes and arcs. the diversity of implied constraints in real-life applications is a practical limit to the resolution of such problems by approaches considering the partitioning problem independently from each additional restriction. this thesis focuses on constraint satisfaction problems related to tree partitioning problems enriched by several additional constraints that restrict the possible partitions topology. on the one hand, our study focuses the structural properties of tree partitioning constraints. on the ohter hand, it is dedicated to the interactions between the tree partitioning problem and classical restrictions (such as precedence relations or incomparability relations between nodes) involved in practical applications. precisely, we show how to globally take into account several restrictions within one single tree partitioning constraint. another interesting aspect of this thesis is related to the implementation of such a constraint. in the context of graph-based global constraints, we show how a fully dynamic management of data structures makes the runtime of filtering algorithms independent of the graph density.
radio emission in a toy model with point-charge-like air showers. null
geometric model of a narrow tilting car using robotics formalism. the use of an electrical narrow tilting car instead of a large gasoline car should dramatically decrease traffic congestion, pollution and parking problem. the aim of this paper is to give a unique presentation of the geometric modeling issue of a new narrow tilting car. the modeling is based on the modified denavit hartenberg geometric description, which is commonly used in robotics. also, we describe the special kinematic of the vehicle and give a method to analyze the tilting mechanism of it. primarily experimental results on the validation of the geometrical model of a real tilting car are given.
observation of pi^+pi^-pi^+pi^- photoproduction in ultra-peripheral heavy ion collisions at sqrt{s_nn} = 200 gev at the star detector. we present a measurement of pi^+pi^-pi^+pi^- photonuclear production in ultra-peripheral au-au collisions at sqrt(s_{nn}) = 200 gev from the star experiment. the pi^+pi^-pi^+pi^- final states are observed at low transverse momentum and are accompanied by mutual nuclear excitation of the beam particles. the strong enhancement of the production cross section at low transverse momentum is consistent with coherent photoproduction. the pi^+pi^-pi^+pi^- invariant mass spectrum of the coherent events exhibits a broad peak around 1540 pm 40 mev/c^2 with a width of 570 pm 60 mev/c^2, in agreement with the photoproduction data for the rho^0(1700). we do not observe a corresponding peak in the pi^+pi^- final state and measure an upper limit for the ratio of the branching fractions of the rho^0(1700) to pi^+pi^- and pi^+pi^-pi^+pi^- of 2.5 % at 90 % confidence level. the ratio of rho^0(1700) and rho^0(770) coherent production cross sections is measured to be 13.4 pm 0.8 (stat.) pm 4.4 (syst.) %.
upsilon cross section in p+p collisions at sqrt(s) = 200 gev. we report on a measurement of the upsilon(1s+2s+3s) -&gt; e+e- cross section at midrapidity in p+p collisions at sqrt(s)=200 gev. we find the cross section to be 114 +/- 38 (stat.) +23,-24 (syst.) pb. perturbative qcd calculations at next-to-leading order in the color evaporation model are in agreement with our measurement, while calculations in the color singlet model underestimate it by 2 sigma. our result is consistent with the trend seen in world data as a function of the center-of-mass energy of the collision and extends the availability of upsilon data to rhic energies. the dielectron continuum in the invariant mass range near the upsilon is also studied to obtain a combined cross section of drell-yan plus (b b-bar) -&gt; e+e-.
competition of heavy quark radiative and collisional energy loss in deconfined matter. we extend our recently advanced model on collisional energy loss of heavy quarks in a quark gluon plasma (qgp) by including radiative energy loss. we discuss the approach and present first preliminary results. we show that present data on nuclear modification factor of non photonic single electrons hardly permit to distinguish between those 2 energy loss mechanisms.
isospin effects on the energy of vanishing flow in heavy-ion collisions. using the isospin-dependent quantum molecular dynamics model we study the isospin effects on the disappearance of flow for the reactions of $^{58}ni$+$^{58}ni$ and $^{58}fe$+$^{58}fe$ as a function of impact parameter. we found good agreement between our calculations and experimentally measured energy of vanishing flow at all colliding geometries. our calculations reproduce the experimental data within 5\%(10\%) at central (peripheral) geometries.
combustion of syngas in internal combustion engines. the combustion of synthesis gas will play an important role in advanced power systems based on the gasification of fuel feedstocks and combined cycle power production. while the most commonly discussed option is to burn syngas in gas turbine engines, another possibility is to burn the syngas in stationary reciprocating engines. whether spark ignited or compression ignited, syngas could serve to power large bore stationary engines, such as those presently operated on natural gas. to date, however, there has been little published on the combustion of syngas in reciprocating engines. one area that has received attention is dual-fueled diesel combustion, using a combination of diesel pilot injection and syngas fumigation in the intake air. in this article, we survey some of the relevant published work on the use of synthesis gas in ic engines, highlighting recent work on dual-fuel (syngas + diesel) combustion.
atl: a model transformation tool. null
solving allocation problems of hard real-time systems with dynamic constraint programming. in this paper, we present an original approach (cprta for ”constraint programming for solving real-time allocation”) based on constraint programming to solve an allocation problem of hard real-time tasks in the context of fixed priority preemptive scheduling. cprta is built on dynamic constraint programming together with a learning method to find a feasible processor allocation under constraints. it is a new approach which produce in its current version as acceptable performances as classical algorithms do. some experimental results are given to show it. moreover, cprta shows very interesting properties. it is complete —i.e., if a problem has no solution, the algorithm is able to prove it—, and it is non-parametric —i.e., it does not require specific initializations—. thanks to its capacity to explain failures, it offers attractive perspectives for guiding the architectural design process.
an inventory pick-up and delivery problem in the reverse logistics context: optimization using a grasp and hybrid approach. we consider the multiperiodic planning and optimization of transport activities (direct and reverse), and inventory management in a two level distribution network. the goal is to satisfy the customers demands while minimizing the routing and storage costs. we use the grasp and a hybrid approach to solve this problem.
choco: an open source java constraint programming library. choco is a java library for constraint satisfaction problems (csp), constraint programming (cp) and explanation-based constraint solving (e-cp). it is built on a event-based propagation mechanism with backtrackable structures.
learning from the past to dynamically improve search: a case study on the mosp problem. this paper presents a study conducted on the minimum number of open stacks problem (mosp) which occurs in various production environments where an efficient simultaneous utilization of resources (stacks) is needed to achieve a set of tasks. we investigate through this problem how classical look-back reasonings based on explanations could be used to prune the search space and design a new solving technique. explanations have often been used to design intelligent backtracking mechanisms in constraint programming whereas their use in nogood recording schemes has been less investigated. in this paper, we introduce a generalized nogood (embedding explanation mechanisms) for the mosp that leads to a new solving technique and can provide explanations.
explanation-based algorithms in constraint programming. constraint programming is a search paradigm for solving combinatorial optimization pro- blems, that has been used to design generic solvers. numerous researches are conducted to deal with over-constrained and dynamic problems. one of those, is based on the concept of explanations. explanations provide a trace of the behavior of the solver and have been initially introduce to improve backtracking based algorithms. they have been used to design clever but costly ways of exploring the search space since that day. this phd thesis study explanation based algorithms on industrial as well as academical problems. we study the interest of explanation within generic decomposition techniques and imple- ment such an algorithm for a hard real time task allocation problem. this approach outlines the role of explanations within the cooperation of di®erent solving techniques. we also show that the explanation network is a relevant information to analyse the struc- tures of a problem and understand the relationships between its di®erent parts (variables and constraints). this information, used to improve the search heuristic, is another step toward generic search techniques. finally, explanations have been often used for look-back but are still under-exploited for look-ahead in cp. nogood recording techniques have never been successful contrary to what happended in the sat community. we implement in this thesis such a nogood recording in the case of the minimum open stack problem.
krivine realizability for compiler correctness. we propose a semantic type soundness result, formalized in the coq proof assistant, for a compiler from a simple functional language to secd machine code. our result is quite independent from the source language as it uses krivine's realizability to give a denotational semantics to secd machine code using only the type system of the source language. we use realizability to prove the correctness of both a call-by-name (cbn) and a call-by-value (cbv) compiler with the same notion of orthogonality. we abstract over the notion of observation (e.g. divergence or termination) and derive an operational correctness result that relates the reduction of a term with the execution of its compiled secd machine code.
on-line resource allocation for atm networks with rerouting. null
method for protecting a gas engine and associated device. the invention relates to a device for protecting (24) a gas engine (12), especially a stationary gas engine, for regulating at least one parameter (pj) of the engine (12), said device comprising means (26) for obtaining an index (imi; img) which represents the combustion behaviour of a gas to be potentially supplied to the engine (12). the invention is characterised in that the device also comprises means (28) for determining an optimum regulating value of the parameter in view of the index obtained (imi; img).
device for protecting a gas engine. null
stationary gas engine protecting device for stationary power generating facility of vehicle, has determining unit determining power value, and changing unit changing set power value such that supplied power is equal to pre-determined value. the device (24) has a measuring unit i.e. sensor (26), measuring a quantity i.e. methane index (im), representing combustion behavior of gas supplied to a stationary gas engine (12). a determining unit (28) determines an optimum electrical power value (peopt) to be achieved based on the measured quantity representing the combustion behavior of the measured combustion gas. a changing unit (30) changes a set power value (pc) such that supplied electrical power (pe) is equal to the pre-determined optimum electrical power value. independent claims are also included for the following: (1) a method of protecting a stationary gas engine (2) a method of generating a digital drift cartography.
experiments with resolution search. null
lower bounds computation for rcpsp. null
mathematical programming formulations and lower bounds for the rcpsp. null
resource-constrained project scheduling: models, algorithms, extensions and applications. this title presents a large variety of models and algorithms dedicated to the resource-constrained project scheduling problem (rcpsp), which aims at scheduling at minimal duration a set of activities subject to precedence constraints and limited resource availabilities. in the first part, the standard variant of rcpsp is presented and analyzed as a combinatorial optimization problem. constraint programming and integer linear programming formulations are given. relaxations based on these formulations and also on related scheduling problems are presented. exact methods and heuristics are surveyed. computational experiments, aiming at providing an empirical insight on the difficulty of the problem, are provided. the second part of the book focuses on several other variants of the rcpsp and on their solution methods. each variant takes account of real-life characteristics which are not considered in the standard version, such as possible interruptions of activities, production and consumption of resources, cost-based approaches and uncertainty considerations. the last part presents industrial case studies where the rcpsp plays a central part. applications are presented in various domains such as assembly shop and rolling ingots production scheduling, project management in information technology companies and instruction scheduling for vliw processor architectures.
potential physics measurement with alice electromagnetic calorimeters. we present the two electromagnetic calorimeters of the alice (a large ion collider experiment) experiment at lhc (large hadron collider). one is the high-resolution photon spectrometer (phos) made of lead tungsten crystals and the other is the electromagnetic calorimeter (emcal), a lead-scintillator sampling calorimeter. they are dedicated to the measurement and identification of direct photons, light neutral mesons such as π0, η and ω(782), and jets emitted in proton-proton and heavy-ion collisions at the lhc energies. the phos is capable of precisely detecting photons with momentum range between 0.1 gev/c and 100 gev/c and the emcal can extend the prompt photon and light neutral meson momentum measurement beyond 200 gev/c. the objective of the study is to explore the physics of strongly interacting qcd matter under extreme conditions of energy density.
co-combustion of pulverised coal/biomass for steam production: emergy approach. emergy, also called embodied energy, has been firstly defined by odum. it is the energy required to make a service or product expressed in energy of one form (here solar energy). emergy per mass or transformity (expressed as solar energy joule per gram [sej/g]) varies from 0.88 109 sej/g for wood to a high of 1.1 1015 sej/g for coal. this concept of emergy is particularly relevant to compare on the same basis different fuels especially to analyse co-combustion of renewable and non-renewable combustibles. the cost of co-combustion, in term of emergy, makes it relevant if collection of biomass remains below the threshold distance of 400 km from the power plant. that threshold can be raised to 700 km when the calorific value of biomass feed stock is higher.
a combustionless determination method for combustion properties of natural gases. this paper presents a methodology for the determination and measurement of important natural gas combustion properties (higher heating value, wobbe index and stoichiometric air-fuel ratio). a pseudo-gas formulation is used to determine an equivalent gas composition to the real natural gas tested. the pseudo-composition, made up of the most influent constituents of the natural gas, is determined by solving a system of non-linear equations. the input parameters to the procedure are: the thermal conductivity of the natural gas at 333 k and 383 k, the speed of sound at 303 k and the concentration of co2. a combustion properties measurement sensor has been developed and tested for many natural gas compositions. the tested natural gases are chosen to represent typical european gases as well as to account for large variations of individual components (heavy hydrocarbons, inert gases). with the developed sensor, combustion properties measured at standard conditions agree with gas chromatographic analysis, to within 0.5%.
safe operating conditions determination for stationary si gas engines. knock is a major problem when running combined heat and power (chp) gas engines because of the variation in the network natural gas composition. a curative solution is widely applied, using an accelerometer to detect knock when it occurs. the engine load is then reduced until knock disappears. the present paper deals with a knock preventive device. it is based on the knock prediction following the engine operating conditions and the fuel gas methane number, and it acts on the engine load before knock happens. a state of the art about knock prediction models is carried out. the maximum of the knock criterion is selected as knock risk estimator, and a limit value above which knock may occur is defined. the estimator is calculated using a two-zone thermodynamic model. this model is specifically based on existing formulas for the calculation of the combustion progress, modified to integrate the effect of the methane number. a chemical kinetic model with 53 species and 325 equilibrium reactions is used to calculate unburned and burned gases composition. the different parameters of the model are fitted with a least squares method from an experimental data base. errors less than 8% are achieved. the knock risks predicted for various natural gases and operating conditions are in agreement with previous work. nevertheless, the knock risk estimator is overestimated for natural gases with high concentrations of inert gases such as nitrogen and carbon dioxide. the definition of a methane number limit based on the engine manufacturer's recommendation is then required to eliminate unwarranted alerts. safe operating conditions are thus calculated and gathered in the form of a map. this map, combined with the real time measurement of the fuel gas methane number, can be integrated to the control device of the chp engine in order to guarantee a safe running towards fuel gas quality variation.
core-corona separation in heavy ion collision at rhic and sps. null
nuclear surface effects in auau@rhic. null
light algorithms for maintaining max-rpc during search. this article presents two new algorithms whose purpose is to maintain the max-rpc domain filtering consistency during search with a minimal memory footprint and implementation effort. both are sub-optimal algorithms that make use of support residues, a backtrack-stable and highly efficient data structure which was successfully used to develop the state-of-the-art ac-3rm algorithm. the two proposed algorithms, max-rpcrm and l-max-rpcrm are competitive with best, optimal max-rpcalgorithms while being considerably simplier to implement. l-max-rpcrm computes an approximation of the max-rpc consistency, which is garanteed to be strictly stronger than ac with the same space complexity and better worst-case time complexity than max-rpcrm. in practice, the difference in filtering power between l-max-rpcrm and standard max-rpc is nearly indistinguisable on random problems. max-rpcrm and l-max-rpcrm are implemented into the choco constraint solver through a strong consistency global constraint. this work opens new perspective upon the development of strong consistency algorithms into constraint solvers.
preventive knock protection technique for stationary si engines fuelled by natural gas. in a combined heat and power (chp) plant, spark ignition engines must operate at their maximum power to reduce the pay back time. because of environmental and economic concerns, engines are set with high compression ratios. consequently, optimal operating conditions are generally very close to those of knock occurrence and heavy knock can severely damage the engine piston. there are two main protection techniques: the curative one commonly used by engine manufacturers and well documented in the literature and the preventive one based on a knock prediction according to the quality of the supplied gas. the indicator used to describe gas quality is the methane number (mn). the methane number requirement (mnr) of the engine is defined, for an engine set (spark advance, air-fuel ratio, and load), as the minimum value of mn above which knock free operation is ensured. to prevent knock occurrence, it is necessary to adapt the engine tuning according to variable gas composition. the objective of the present work is to validate the concept of knock preventive protection. first, a prediction of mnr according to engine settings (es) is computed through a combustion simulator composed of a thermodynamic 2-zone model. predicted mnr are compared to experimental results performed on a single-cylinder si gas engine and show good agreement with numerical results (uncertainty below 1 point). then, the combustion simulator is used to generate a protection mapping. at last, the knock preventive protection was successfully tested.
global propagation of practicability constraints. null
a generic scheme for integrating strong consistencies into constraint solvers. this article presents a generic scheme for adding strong local consistencies to the set of features of constraint solvers, which is notably applicable to event-based constraint solvers. we encapsulate a subset of constraints into a global constraint. this approach allows a solver to use diﬀerent levels of consistency for diﬀerent subsets of constraints in the same model. moreover, we show how strong consistencies can be applied with diﬀerent kinds of constraints, including user-deﬁned constraints. we experiment our technique with a coarse-grained algorithm for max-rpc, called max-rpcrm and a variant of it, l-max-rpcrm. experiments conﬁrm the interest of strong consistencies for constraint programming tools.
trends in constraint programming. constraint programming is a constantly evolving field, something which is explored at the annual international conference on principles and practice of constraint programming. this conference provides papers and workshops which produce new insights, concepts and results which those involved in this area can then use to develop their own work. this title provides an accessible overview of this by bringing together the best papers on a range of topics within this subject area, thus allowing those involved in constraint programming to benefit from the new innovations and results created as a result of the conference.
superconformal higher-dimensional field theories and the theory of everything. null
heavy quark energy loss in finite size qgp. null
performance and power management for cloud infrastructures. a key issue for cloud computing data-centers is to maximize their proﬁts by minimizing power consumption and sla violations of hosted applications. in this paper, we propose a resource management framework combining a utility-based dynamic virtual machine provisioning manager and a dynamic vm placement manager. both problems are modeled as constraint satisfaction problems. the vm provisioning process aims at maximizing a global utility capturing both the performance of the hosted applications with regard to their slas and the energy- related operational cost of the cloud computing infrastructure. we show several experiments how our system can be controlled through high level handles to make different trade-off between application performance and energy consumption or to arbitrate resource allocations in case of contention.
the search of pentaquark. null
the pentaquark case: state of the art. null
exotic particle search in star. null
the nuclear equation of state from k+. null
description of low energy heavy ion collision within the isospin quantum molecular dynamics model. null
nuclear waste package performance in european geological disposal concepts. null
experimental research on improved antineutrino spectra from reactors. null
global constraints: introduction and graph-based representation. second international summer school of the association for constraint programming. samos, greece. global constraints: introduction and graph-based representation.
graph-properties based filtering. sics technical report t2006-10. graph-properties based filtering.
combining tree partitioning, precedence, incomparability, and degree constraints, with an application to phylogenetic and ordered-path problems, technical report 2006-20 of uppsala university. null
undirected forest constraints. null
graph-based filtering. graph-based filtering.
bounds of parameters for global constraints. bounds of parameters for global constraints.
sweep synchronisation as a global propagation mechanism. sweep synchronisation as a global propagation mechanism.
a generic geometrical constraint kernel in space and time for handling polymorphic k-dimensional objects. sics technical report t2007-08. a generic geometrical constraint kernel in space and time for handling polymorphic k-dimensional objects.
necessary condition for path partitioning constraints. necessary condition for path partitioning constraints.
a continuous multi-resources cumulative constraint with positive-negative resource consumption-production. a continuous multi-resources cumulative constraint with positive-negative resource consumption-production.
a generic geometrical constraint kernel in space and time for handling polymorphic k-dimensional objects. a generic geometrical constraint kernel in space and time for handling polymorphic k-dimensional objects.
special issue on global constraints. special issue on global constraints.
filtering for a continuous multi-resources cumulative constraint with resource consumption and production. filtering for a continuous multi-resources cumulative constraint with resource consumption and production.
new filtering for the cumulative constraint in the context of non-overlapping rectangles. new filtering for the cumulative constraint in the context of non-overlapping rectangles.
a geometric constraint over k-dimensional objects and shapes subject to business rules. a geometric constraint over k-dimensional objects and shapes subject to business rules.
combining tree partitioning, precedence, and incomparability constraints. combining tree partitioning, precedence, and incomparability constraints.
six ways of integrating symmetries within non-overlapping constraints, sics technical report t2009-01. six ways of integrating symmetries within non-overlapping constraints.
compiling business rules in a geometric constraint over k-dimensional objects and shapes. sics technical report t2009-02. compiling business rules in a geometric constraint over k-dimensional objects and shapes.
six ways of integrating symmetries within non-overlapping constraints. six ways of integrating symmetries within non-overlapping constraints.
on the difference of bremsstrahlung in qed and qcd processes. null
challenges for transport theories to describe high density systems. null
bimodality a smoking gun signal for a first order phase transition. null
tomography of the quark gluon plasma by charmed mesons. null
transport theories in the fermi energy domain. null
the nuclear equation of state in nuclear and astrophysics. null
strange particles in hadron matter. null
improving inter-block backtracking with interval newton. inter-block backtracking (ibb) computes all the solutions of sparse systems of nonlinear equations over the reals. this algorithm, introduced by bliek et al. (1998) handles a system of equations previously decomposed into a set of (small) k × k sub-systems, called blocks. partial solutions are computed in the different blocks in a certain order and combined together to obtain the set of global solutions. when solutions inside blocks are computed with interval-based techniques, ibb can be viewed as a new interval-based algorithm for solving decomposed systems of non-linear equations. previous implementations used ilog solver and its ilcinterval library as a black box, which implied several strong limitations. new versions come from the integration of ibb with the interval-based library ibex. ibb is now reliable (no solution is lost) while still gaining at least one order of magnitude w.r.t. solving the entire system. on a sample of benchmarks, we have compared several variants of ibb that differ in the way the contraction/filtering is performed inside blocks and is shared between blocks. we have observed that the use of interval newton inside blocks has the most positive impact on the robustness and performance of ibb. this modifies the influence of other features, such as intelligent backtracking. also, an incremental variant of inter-block filtering makes this feature more often fruitful.
a generalized interval lu decomposition for the solution of interval linear systems. null
on the approximation of linear ae-solution sets. null
observation of charge-dependent azimuthal correlations and possible local strong parity violation in heavy ion collisions. parity-odd domains, corresponding to non-trivial topological solutions of the qcd vacuum, might be created during relativistic heavy-ion collisions. these domains are predicted to lead to charge separation of quarks along the orbital momentum of the system created in non-central collisions. to study this effect, we investigate a three particle mixed harmonics azimuthal correlator which is a \p-even observable, but directly sensitive to the charge separation effect. we report measurements of this observable using the star detector in au+au and cu+cu collisions at $\sqrt{s_{nn}}$=200 and 62~gev. the results are presented as a function of collision centrality, particle separation in rapidity, and particle transverse momentum. a signal consistent with several of the theoretical expectations is detected in all four data sets. we compare our results to the predictions of existing event generators, and discuss in detail possible contributions from other effects that are not related to parity violation.
detailed measurement of the e+e- pair continuum in p+p and au+au collisions at √snn=200 gev and implications for direct photon production. in 2002, an innovative neutron time-of-flight facility started operation at cern: n_tof. the main characteristics that make the new facility unique are the high instantaneous neutron flux, high resolution and wide energy range. combined with state-of-the-art detectors and data acquisition system, these features have allowed to collect high accuracy neutron cross-section data on a variety of isotopes, many of which radioactive, of interest for nuclear astrophysics and for applications to advanced reactor technologies. a review of the most important results on capture and fission reactions obtained so far at n_tof is presented, together with plans for new measurements related to nuclear industry.
self consistent investigation of structural transitions in neutron star crusts. null
heavy ion collisions in a wavelet representation. null
advance waste forms. null
study of various options for final disposal of htr coated particles. null
leaching behaviour of spent htr fuel kernels. null
new synthesis route and characterisation of siderite (feco3) and coprecipitation of tc99. null
non proliferation studies with the double chooz detector. null
j/psi analysis in the alice muon spectrometer. null
electroweak boson detection in the alice muon spectrometer. null
on qualitative properties of linear neutral type control systems. null
online management of jobs in clusters using virtual machines. resources management systems relying on a dynamic management of jobs can efficiently use resources in clusters. indeed they provide mechanisms to manipulate online the state of the jobs and their assignment on the nodes. in practice, these scheduling strategies are hard to deploy on clusters as they can not necessarily handle the manipulation of the jobs and may have specific scheduling constraints to consider. in this thesis, we try to ease the development of resources management systems relying on a dynamic management of jobs. we based our environment on the use of virtual machines to execute the jobs in their legacy environments while providing the mechanisms to manipulate them in a non-intrusive way. moreover, we propose an autonomous environment to continuously optimize the scheduling of jobs. scheduling strategies are implemented using constraints programming which aims to model and solve combinatory problems. we validate our approach with the development of our prototype entropy, which has been used to implement various scheduling strategies. the evaluation of these strategies show their capability to to solve present problems.
cluster-wide context switch of virtualized jobs. clusters are massively used through resource management systems with a static allocation of resources for a bounded amount of time. such an approach leads to a coarse-grain exploitation of the architecture and an increase of the job completion times since most of the scheduling policies rely on users estimates and do no consider the real needs of applications in terms of both resources and times. encapsulating jobs into vms enables to implement finer scheduling policies through cluster-wide context switches: a permutation between vms present in the cluster. it results a more flexible use of cluster resources and relieve end-users of the burden of dealing with time estimates. leveraging the entropy framework, this paper introduces a new infrastructure enabling cluster-wide context switches of virtualized jobs to improve resource management. as an example, we propose a scheduling policy to execute a maximum number of jobs simultaneously, and uses vm operations such as migrations, suspends and resumes to resolve underused and overloaded situations. we show through experiments that such an approach improves resource usage and reduces the overall duration of jobs. moreover, as the cost of each action and the dependencies between them is considered, entropy reduces, the duration of each cluster-wide context switch by performing a minimum number of actions, in the most efficient way.
charged-particle multiplicity measurement in proton-proton collisions at sqrt(s) = 7 tev with alice at lhc. the pseudorapidity density and multiplicity distribution of charged particles produced in proton-proton collisions at the lhc, at a centre-of-mass energy sqrt(s) = 7 tev, were measured in the central pseudorapidity region |eta| &lt; 1. comparisons are made with previous measurements at sqrt(s) = 0.9 tev and 2.36 tev. at sqrt(s) = 7 tev, for events with at least one charged particle in |eta| &lt; 1, we obtain dnch/deta = 6.01 +- 0.01 (stat.) +0.20 -0.12 (syst.). this corresponds to an increase of 57.6% +- 0.4% (stat.) +3.6 -1.8% (syst.) relative to collisions at 0.9 tev, significantly higher than calculations from commonly used models. the multiplicity distribution at 7 tev is described fairly well by the negative binomial distribution.
linear logic as a foundation for service-oriented computing. we present a calculus that provides formal and uniﬁed foundations to service- oriented computing. service-oriented computing allows network-based software applica- tions to be developed by resorting to services as primitive components. to date, there are two popular – and often antagonistic – models for service-oriented computing. on the ﬁrst hand, the computation-oriented model, illustrated by ws* web services, considers services as sets of operations. on the other hand, the resource-oriented model, illustrated by restful web services, considers services as interfaces to resources. the lack of uniﬁed models leads to adaptation, integration and coordination problems, three major concerns in this ﬁeld. our calculus restores unity to service-oriented computing, by reconciling both points of view. we give the operational semantics of the calculus using chemical solutions, and illustrate its expressive power not only as a query language over resources with sup- port for recursion and aggregation, but also as a concurrent process language. finally, we show that our calculus is also meaningful in logic programming since computation can be interpreted as proof search in focused linear logic with resource modalities: afﬁne, contractible and exponential.
reliable dynamic reconfiguration in a reflective component model. null
sla-aware virtual resource management for cloud infrastructures. cloud platforms host several independent applications on a shared resource pool with the ability to allocate computing power to applications on a per-demand basis. the use of server virtualization techniques for such platforms provide great flexibility with the ability to consolidate several virtual machines on the same physical server, to resize a virtual machine capacity and to migrate virtual machine across physical servers. a key challenge for cloud providers is to automate the management of virtual servers while taking into account both high-level qos requirements of hosted applications and resource management costs. this paper proposes an autonomic resource manager to control the virtualized environment which decouples the provisioning of resources from the dynamic placement of virtual machines. this manager aims to optimize a global utility function which integrates both the degree of sla fulfillment and the operating costs. we resort to a constraint programming approach to formulate and solve the optimization problem. results obtained through simulations validate our approach.
autonomic virtual resource management for service hosting platforms. cloud platforms host several independent applications on a shared resource pool with the ability to allocate com- puting power to applications on a per-demand basis. the use of server virtualization techniques for such platforms provide great ﬂexibility with the ability to consolidate sev- eral virtual machines on the same physical server, to resize a virtual machine capacity and to migrate virtual machine across physical servers. a key challenge for cloud providers is to automate the management of virtual servers while taking into account both high-level qos requirements of hosted applications and resource management costs. this paper proposes an autonomic resource manager to con- trol the virtualized environment which decouples the provi- sioning of resources from the dynamic placement of virtual machines. this manager aims to optimize a global utility function which integrates both the degree of sla fulﬁllment and the operating costs. we resort to a constraint pro- gramming approach to formulate and solve the optimization problem. results obtained through simulations validate our approach.
charged-particle multiplicity measurement in proton-proton collisions at sqrt(s) = 0.9 and 2.36 tev with alice at lhc. charged-particle production was studied in proton-proton collisions collected at the lhc with the alice detector at centre-of-mass energies 0.9 tev and 2.36 tev in the pseudorapidity range |eta| &lt; 1.4. in the central region (|eta| &lt; 0.5), at 0.9 tev, we measure charged-particle pseudorapidity density dnch/deta = 3.02 +- 0.01 (stat.) +0.08 -0.05 (syst.) for inelastic interactions, and dnch/deta = 3.58 +- 0.01 (stat.) +0.12 -0.12 (syst.) for non-single-diffractive interactions. at 2.36 tev, we find dnch/deta = 3.77 +- 0.01 (stat.) +0.25 -0.12 (syst.) for inelastic, and dnch/deta = 4.43 +- 0.01 (stat.) +0.17 -0.12 (syst.) for non-single-diffractive collisions. the relative increase in charged-particle multiplicity from the lower to higher energy is 24.7% +- 0.5% (stat.) +5.7% -2.8% (syst.) for inelastic and 23.7% +- 0.5% (stat.) +4.6% -1.1% (syst.) for non-single-diffractive interactions. this increase is consistent with that reported by the cms collaboration for non-single-diffractive events and larger than that found by a number of commonly used models. the multiplicity distribution was measured in different pseudorapidity intervals and studied in terms of kno variables at both energies. the results are compared to proton-antiproton data and to model predictions.
proceedings of the 5th int. symposium on software composition (sc'06). null
aop for systems software and middleware. null
a formal semantics of flexible and safe pointcut/advice bindings. null
towards correct evolution of components using vpa-based aspects. null
proc. of the 9th international symposium on distributed objects, middleware, and applications (doa'07). null
atoll: aspect-oriented toll system. null
on dissipativity of continuous-time singular systems. this paper addresses the dessipativity problem for linear time-invariant (lti) continuous-time singular systems. a new kalman-yakubovish-popov (kyp) lemma for dissipativity of singular systems is formulated in terms of a strict linear matrix inequality (lmi) condition. this lemma removes the equality constraints in the results recently reported in the literature. moreover, by means of the proposed lmi condition, feedback controller synthesis for rendering the closed-loop dissipative is also investigated.
extended lmi characterizations and multiobjective synthesis for discrete descriptor systems. this paper proposes new lmi characterizations for linear discrete-time descriptor systems. these formulations not only encompass the existing results for descriptor systems, but provide an alternative angle of view for reviewing the known extended lmis for conventional state-space systems as well. an application of these lmis with regard to the robust analysis and multiobjective synthesis is also investigated. numerical examples are given to illustrate the effectiveness of the proposed lmis.
necessary and sufficient condition for generalized estimation/control forms of continuous descriptor controllers. this paper deals with observer-based forms of arbitrary continuous-time descriptor controllers. the general form includes static control and estimation gains with or without an extra descriptor youla parameter that can be either static or dynamic. a necessary and sufficient condition for the derived controller to be input-output equivalent to the initial one, with a separated estimation-control structure, is given. the present method is consistent with the existing results in the conventional linear state-space case. some numerical examples illustrate the effectiveness of the proposed method.
ecoop 2006-object-oriented technology. workshop reader, workshops, nantes, france, july 3-7, 2006, final reports. null
acp4is'07: proceedings of the 6th workshop on aspects, components, and patterns for infrastructure software. null
strongaspectj: flexible and safe pointcut/advice bindings. null
proceedings of the int. workshop on aspects, dependencies and interactions (adi'07). null
proceedings of the 8th int. workshop on foundations of aspect-oriented languages (foal'09). null
elaboration of a common frame of reference in collaborative virtual environments. motivation – to design virtual environments that support collaborative activities. research approach – an experimental approach in which 44 students were asked to work in pairs to reconstruct five 3d figures. findings/design – the results show that including a contextual clue in virtual environments improves collaboration between operators. research limitations – further investigative work must be carried out to extract accurate female collaboration profiles. originality/value – the results enable three collaboration profiles to be identified. they also allow the extraction of some characteristics of a contextual clue which can be added to a virtual environment to improve collaboration. take away message – the contents of a collaborative virtual environment influences the way that users collaborate.
cross-document dependency analysis for system-of-system integration. null
aspect-oriented software development in practice: tales from aosd-europe. null
proceedings of the 4th workshop on domain-specific aspect languages (dsal 2009). it is our great pleasure to host the fourth edition of the domain-specific aspect languages workshop (dsal09), as part of the eight international conference on aspect-oriented software development (aosd09). the tendency to raise the abstraction level in programming languages towards a particular domain is also a major driving force in the research domain of aspect-oriented programming languages. the dsal workshop series aims to bring the research communities of domain-specific language engineering and domain-specific aspect design together. in the editions held at gpce06/oopsla06 and aosd07 we approached domain-specific aspect languages both from a design and a language implementation point of view. at aosd08 we explicitly invited contributions of work on adding domain-specific extensions (dsxs) to general-purpose aspect languages (gpals). we continue this trend for this edition as the focus on language embedding raises specific issues for language designers, such as proper symbiosis between, and composition of, dsxs.
proceedings of the 2008 aosd workshop on domain-specific aspect languages (dsal 2008). the workshop aims to bring the research communities of domain-specific language engineering and domain- specific aspect design together. in the previous successful editions held at gpce'06/oopsla'06 and aosd'07 we approached domain-specific aspect languages both from a design and a language implementation point of view. new for this edition is that we explicitly invited contributions of work on adding domain-specific extensions (dsxs) to general-purpose aspect languages (gpals).
proof of correctness of aspect transformations in the casb. null
towards a common aspect semantic base (casb), deliverable 54. null
lts-based semantics and property analysis of distributed aspects and invasive patterns. null
invasive patterns: aspect-based adaptation of distributed applications. null
relational aspects for context passing beyond stack inspection. null
technology-oriented optimization of the secondary design parameters of robots for high-speed machining applications. null
unifying runtime adaptation and design evolution. the increasing need for continuously available software systems has raised two key-issues: self-adaptation and design evolution. the former one requires software systems to monitor their execution platform and automatically adapt their configuration and/or architecture to adjust their quality of service (optimization, fault-handling). the later one requires new design decisions to be reflected on the fly on the running system to ensure the needed high availability (new requirements, corrective and preventive maintenance). however, design evolution and selfadaptation are not independent and reflecting a design evolution on a running self-adaptative system is not always safe. we propose to unify run-time adaptation and run-time evolution by monitoring both the run-time platform and the design models. thus, it becomes possible to correlate those heterogeneous events and to use pattern matching on events to elaborate a pertinent decision for run-time adaptation. a flood prediction system deployed along the ribble river (yorkshire, england) is used to illustrate how to unify design evolution and run-time adaptation and to safely perform runtime evolution on adaptive systems.
reliable dynamic reconfigurations in the fractal component model. this article is an analysis based on our experience with the fractal component model of the need of reliability for dynamic reconfigurations in component based systems. we make a proposal to ensure this reliability for concurrent and distributed fractal applications. we started from the definition of acid properties in the context of dynamic reconfigurations in component models and we propose to use integrity constraints to define system consistency and transactions for guaranteeing the respect of these constraints at runtime. moreover we manage concurrency between reconfigurations by avoiding potential conflicts between reconfiguration operations. finally, a recovery mechanism has been conceived to deal with failures.
fpath and fscript: language support for navigation and reliable reconfiguration of fractal architectures. component-based systems must support dynamic reconfigurations to adapt to their execution context, but not at the cost of reliability. fractal provides intrinsic support for dynamic reconfiguration, but its definition in terms of low-level apis makes it complex to write reconfigurations and to ensure their reliability. this article presents a language-based approach to solve these issues: direct and focused language support for architecture navigation and reconfiguration make it easier both to write the reconfigurations and to ensure their reliability. concretely, this article presents two languages: (1) fpath, a domain-specific language that provides a concise yet powerful notation to navigate inside and query fractal architectures, and (2) fscript, a scripting language that embeds fpath and supports the definition of complex reconfigurations. fscript ensures the reliability of these reconfigurations thanks to sophisticated run-time control, which provides transactional semantics (acid properties) to the reconfigurations.
qos policies for business processes in service oriented architectures. the advent of service oriented architectures tends to promote a new kind of software architecture where services, exposing features accessible through highly standardized protocols, are composed in a loose coupling way. in such a context, where services are likely to be replaced or used by a large number of clients, the notion of quality of service (qos), which focuses on the quality of the relationship between a service and its customers, becomes a key challenge. this paper aims to ease qos management in service compositions through a better separation of concerns. for this purpose, we designed qosl4bp, a domain-specific language which allows qos policies specification for business processes. more specifically, the qosl4bp language is designed to allow an architect to specify qos constraints and mechanisms over parts of bpel compositions. this language is executed by our orqos platform which cooperates in a non-intrusive way with orchestration engines. at pre-deployment time, orqos platform performs service planning depending on services qos offers and on the qos requirements in qosl4bp policies. at runtime, qosl4bp policies allow to react to qos variations and to enact qos management related mechanisms.
capacitated multi-item lot-sizing problems with time windows. this research concerns a new family of capacitated multi-item lot-sizing problems, namely, lot-sizing problems with time windows. two classes of the problem are analyzed and solved using different lagrangian heuristics. capacity constraints and a subset of time window constraints are relaxed resulting in particular single-item time window problems that are solved in polynomial time. other relaxations leading to the classical wagner-whitin problem are also tested. several smoothing heuristics are implemented and tested, and their results are compared. the gaps between lower and upper bounds for most problems are very small (less than 1%). moreover, the proposed algorithms are robust and do not seem to be too affected when different parameters of the problem are varied.
proceedings of the 2nd workshop on domain specific aspect languages. although the majority of work in the aosd community focuses on general-purpose aspect languages (e.g. aspectj), seminal work on aosd proposed a number of domainspecific aspect languages, such as cool for concurrency management and ridl for serialization, rg, aml, and others. a growing trend of research in the aosd community is returning to this seminal work, as witnessed by the high attendance rate at the dsal06 workshop, held as part of gpce06/oopsla06. the workshop aimed to bring the research communities of domain-specific language engineering and domainspecific aspect design together. in the previous successful edition we approached domain-specific aspect languages from a language implementation point of view, where advances in the field of domain-specific language engineering were investigated to answer the implementation challenges of aspect languages. in this second edition, we approached the design and implementation of new domain-specific aspect languages, as well as the composition at all levels (from design to implementation) of these languages or individual features.
proceedings of the first domain-specific aspect languages workshop - acm international conference on generative programming and component engineering (gpce 2006). although the majority of work in the aosd community focuses on general-purpose aspect languages (eg. aspectj), seminal work on aosd proposed a number of domain-specific aspect languages, such as cool for concurrency management and ridl for serialization. a growing trend of research in the aosd community is returning to this seminal work, motivated by the known advantages of domain-specific approaches, as argued by mitchell wand in his keynote at icfp 2003. this workshop is conceived for researchers who are further exploring the area of domain-specific aspect languages, including language design, enabling technologies and composition issues. this volume contains 6 papers and abstracts of invited talks. we hope the reader will find them useful for advancing their understanding of some issues in concerning the design of aspect languages.
a domain-specific language for coordinating concurrent aspects in java. aspect-oriented programming (aop) promises the modularisation of so-called cross-cutting functionality in large applications. currently, almost all approaches to aop provide means for the description of sequential aspects that are to be applied to a sequential base program. a recent approach, concurrent event-based aop (ceaop), has been introduced, which models the concurrent application of aspects to concurrent base programs. ceaop uses finite state processes (fsp) and their representation as labeled transition systems (lts) for modeling aspects, base programs and their concurrent composition, thus enabling the use of the labeled transition system analyzer (ltsa) for formal property verification. the initial work on ceaop does not provide an implementation of its concepts, restricting the study of concurrent aspects to the study of a model. the contribution of this paper is the provision of an implementation of ceaop as a small dsal (domain-specific aspect language), baton, which is very close to fsp, and can be compiled into java. as an intermediate layer, we have developed a java library which makes it possible to associate a java implementation to a finite state process. the compilation process consists of translating both the baton aspects and the java base program into java finite state processes. this translation relies on metaborg/sdf to extend java with baton and reflex to instrument the base program.
towards a model of concurrent aop. aspect-oriented programming (aop) is concerned with the modularization of crosscutting functionalities. such functionalities are problematic, in particular, for the development of concurrent applications, such as graphical user interfaces or multithreaded server applications. however, few approaches address this problem: there is, in particular, no general model for concurrent aop that enables coordination among concurrently-executing aspects as well as coordination of concurrent aspects and base applications. in this paper, we discuss general requirements for such models, briefly present a specific instance meeting these requirements, and propose a set of general composition operators for the construction of concurrent applications using concurrently executing advice.
a seamless extension of components with aspects using protocols. this paper shows how components and aspects can be seamlessly integrated using protocols. a simple component model equipped with protocols is extended with aspect compo- nents. the protocol of an aspect component observes the service requests and replies of plain components, and possibly internal component actions, and react to these actions (possibly preventing some base actions to happen as is standard with aop). a nice feature of the model is that an assembly of plain and aspect components can be transformed back into an assembly of components. all this is done without breaking the black-box nature of the components (dealing with internal actions requires to extend the component interface with an action interface).
declarative definition of contexts with polymorphic events. this paper introduces a new model of event handling combining explicitly triggered events with events intercepted with aspect-oriented features. the model supports event abstraction, polymorphic references to events, and declarative definition of events as expressions involving references to events from other objects. we show that this model makes it easy to define a declarative and compositional notion of event-based context. we illustrate these ideas with examples in ecaesarj, a language with concrete support for our model, and relate the events of ecaesarj to other event-handling and context-handling models.
concurrent aspects. aspect-oriented programming (aop) promises the modularization of so-called crosscutting functionalities in large applications. currently, almost all approaches to aop provide means for the description of sequential aspects that are to be applied to a sequential base program. in particular, there is no formally-defined concurrent approach to aop, with the result that coordination issues between aspects and base programs as well as between aspects cannot precisely be investigated. this paper presents concurrent event-based aop (ceaop), which addresses this issue. our contribution can be detailed as follows. first, we formally define a model for concurrent aspects which extends the sequential event-based aop approach. the definition is given as a translation into concurrent specifications using finite sequential processes (fsp), thus enabling use of the labelled transition system analyzer (ltsa) for formal property verification. further, we show how to compose concurrent aspects using a set of general composition operators and sketch a java prototype implementation for concurrent aspects we have realized.
invasive patterns for distributed programs. null
proceedings of the 9th int. conference on aspect-oriented software development. null
automatizing the evaluation of model matching systems. model-driven engineering (mde) and the semantic web are valuable paradigms. this paper sketches how to use a scientific and technical result around mde in the semantic web. the work proposes a mechanism to automatize the evaluation of model matching algorithms. the mechanism involves megamodeling and a domain specific language named aml (atlanmod matching language). aml allows to implement matching algorithms in a straightforward way. we present how to adapt the mechanism to the ontology context, for example, to the ontology alignment evaluation initiative (oaei).
a domain specific language for expressing model matching. a matching strategy computes mappings between two models by executing a set of heuristics. in this paper, we introduce the atlanmod matching language (aml), a domain specific language (dsl) for expressing matching strategies. aml is based on the model-driven paradigm, i.e., it implements model matching strategies as chains of model transformations. a matching model transformation takes a set of models as input, and yields a mapping model as output. we present a compiler that takes aml programs and generates atl (atlanmod transformation language) and apache ant code. the atl code instruments the matching model transformations, and the ant code orchestrates their execution. we evaluate this implementation on two strategies including robust matching transformations from the literature.
managing model adaptation by precise detection of metamodel changes. technological and business changes influence the evolution of software systems. when this happens, the software artifacts may need to be adapted to the changes. this need is rapidly increasing in systems built using the model-driven engineering (mde) paradigm. an mde system basically consists of metamodels, terminal models, and transformations. the evolution of a metamodel may render its related terminal models and transformations invalid. this paper proposes a three-step solution that automatically adapts terminal models to their evolving metamodels. the first step computes the equivalences and (simple and complex) changes between a given metamodel, and a former version of the same metamodel. the second step translates the equivalences and differences into an adaptation transformation. this transformation can then be executed in a third step to adapt to the new version any terminal model conforming to the former version. we validate our ideas by implementing a prototype based on the atlanmod model management architecture (amma) platform. we present the accuracy and performance that the prototype delivers on two concrete examples: a petri net metamodel from the research literature, and the netbeans java metamodel.
an improved upper bound for the railway infrastructure capacity problem on the pierrefitte-gonesse junction. null
multi-criteria optimization with the choquet integral in shortest path problems. the multi-criteria decision aid (mcda) research field focuses on building a preference model that helps the decision maker (dm) choosing a preferred solution among the set of efficient solutions. the multi-objective combinatorial optimization (moco) research field focuses on algorithms to find a set of efficient solutions. the straightforward approach to integrate preferences in a moco algorithm is first to compute a complete set of efficient solutions with a moco algorithm and then to use a mcda preference model. this approach allows to work on these two research fields independently but algorithms computing a complete set of efficient solutions have in general exponential execution times. the aim of our work is to use the preference model during the search of solutions to reduce the computation time.
common frame of reference in collaborative virtual environments and their impact on presence. virtual collaborative environment are 3d shared spaces in which people can work together. to collaborate through these systems, users must have a shared comprehension of the environment. the objective of this experimental study was to determine if visual stable landmarks improve the construction of a common representation of the virtual environment and thus facilitate collaboration. this seems to increase the awareness of the partner's presence.
the exact controllability property of neutral type systems by the moment problem approach revisited. in a recent paper authors gave an analysis of the exact controllability problem via the moment problem approach. namely, the steering conditions of controllable states are formulated as a vectorial moment problem using some riesz basis. one of the main difficulties was the choice of the basis as, in general, a basis of eigenvectors does not exist. in this contribution we use a change of control by a feedback law and modify the structure of the system in such a way that there exists a basis of eigenvectors which allows a simpler expression of the moment problem. hence, one obtains the result on exact controllability and on the time of exact controllability.
low momentum heavy-flavours measurements in the semi-muonic channel, and tracking efficiency of the alice's muon spectrometer. in accelerators as the one of lhc, collisions will help to reproduce the quark gluon plasma. with such a purpose, the alice detector is optimized to study the transition toward this hypothetic state of matter. the alice's muon spectrometer will measure the muon related probes (quarkonia, heavy flavours...). the first part of this thesis presents the method to calculate the tracking efficiency of this spectrometer. the results, obtained by simulation, are placed in the global efficiency context. they show the evolution of the efficiency as a function of likely electronics failures. they establish that a proper working of the detector involves less than 90% of channels failures and 85% of missing so-called manu cards. the second part presents the distance closest approach method which enhance the identification of muons from charm and beauty particle decays. for momentum as for pt &lt; 4 gev/c, the determination of heavy flavours contribution (charm in particular) in the single muon spectrum requires subtracting the decay part from pions and kaons. this discrimination is not possible track by track, an alternative method to subtract the light-hadrons has been developed. the dca method uses the distance between the extrapolated track in the primary vertex transverse plan and the vertex itself. the different shapes of the distributions in dca between the signal and the noise, arising from the different decay length path to the particle types, permit a better separation, and therefore a better estimation of the corresponding cross sections.
a theory of distributed aspects. over the last five years, several systems have been proposed to take distribution into account in aspect-oriented programming. while they appeared to be fruitful to develop or improve distributed component infrastructures or application servers, those systems are not underpinned with a formal semantics and so do not permit to establish properties on the code to be executed. this paper introduces the aspect join calculus -- an aspect-oriented and distributed language based on the join calculus, a member of the pi-calculus family of process calculi suitable as a programming language. it provides a first formal theory of distributed aop as well as a base language in which many features of previous distributed aop systems can be formalized. the semantics of the aspect join calculus is given by a (chemical) operational semantics and a type system is developed to ensure properties satisfied by aspects during the execution of a process. we also give a translation of the aspect join calculus into the core join calculus. the translation is proved to be correct by a bisimilarity argument. in this way, we provide a well-defined version of a weaving algorithm which constitutes the main step towards an implementation of the aspect join calculus directly in jocaml. we conclude this paper by showing that despite its minimal definition, the aspect join calculus is a convenient language in which existing distributed aop languages can be formalized. indeed, many features (such as remote pointcut, distributed advice, migration of aspects, asynchronous and synchronous aspects, re-routing of messages and distributed control flow) can be defined in this simple language.
radioelectric fields from cosmic-ray air showers at large impact parameters. we discuss electric fields generated by cosmic-ray air showers at large impact parameters b. an approximation relevant to this situation is given. the formulation makes explicit the relationship between the shower profile and the radio pulse shapes at large b, putting forward one important observational consequence, namely the decrease of the high-frequency cutoff νc∝1/b when the impact parameter increases. the approximation is also used to give a detailed comparison between two emission models, the geosynchrotron model and the transverse current model.
study of a treatment of siloxane by adsorption process into porous materials: treatment application to biogas. study of a treatment of siloxane by adsorption process into porous materials: treatment application to biogas biogases have strong content of methane used in the production of heat or electricity. they contain more or less important quantities of siloxanes, which are forbidden for numerous uses of biogases. the possibility of siloxanes elimination by adsorption process is studied. the study in batch reactors allows us to evaluate the adsorption capacities of different materials as activated carbons cloths and grains, zeolites and silica gel. the influence on the treatment capacities of siloxanes under the presence of ch4, co2, humidity, and other volatile organic compounds is studied. good adsorption capacities for some adsorbents were found. for the most disadvantageous conditions of adsorption, a reduction of 20 % on the adsorption capacities has been found. the adsorption capacity remains modest. the influence of the operational conditions of the process is studied in order to improve the treatment capacities and to reduce the quantity of the adsorbent used. the results allowed us to define, validate and design a reduced scale system in the treatment of siloxanes. the process consists of an adsorption into activated carbon cloth, alternating with thermal regeneration by joule effect. a pilot plant allowed us to accomplish and evaluate the aging of the process. all this work allows us to see the possibilities of industrial applications of the process, even if a trial stage on site in real conditions is still necessary.
generalized h2-preview control and its application to car lateral steering. . this paper is dedicated to studying the characteristics of the optimal preview control for lateral steering of a passenger vehicle. this synthesis of control law has proved through many applications its ability to guarantee improved performance. the success of this advanced control strategy lies mainly in its ability to include knowledge of a path to follow in the future on a finite horizon, and its ability to moderate the effect of potential delays in the control loop through the advanced information. h2-preview; lateral steering; optimal control.
flexible pointcut implementation: an interpreted approach. one of the main elements of an aspect-oriented programming (aop) language or framework is its pointcut language. a pointcut is a predicate which selects program execution points and determines at which points the execution should be affected by an aspect. experimenting with aspectj shows that two basic primitive pointcuts, call and execution, dealing with method invoca- tion from the caller and callee standpoints, respectively, lead to confusion. this is due to a subtle interplay between the use of static and dynamic types to select execution points, dynamic lookup, and the expectation to easily select the caller and callee execution points related to the same invocation. as a result, alternative semantics have been proposed but have remained paper design. in this article, we reconsider these various semantics in a practical way by implementing them using cali, our common aspect language interpreter. this framework reuses both java as a base language and aspectj as a way to select the program execution points of interest. an additional interpretation layer can then be used to prototype interesting aop variants in a full-blown environment. the paper illustrates the benefits of applying such a setting to the case of the call and execution pointcuts. we show that alternative semantics can be implemented very easily and exercised in the context of aspectj without resorting to complex compiler technology.
automatically discovering hidden transformation chaining constraints. model transformations operate on models conforming to precisely defined metamodels. consequently, it often seems relatively easy to chain them: the output of a transformation may be given as input to a second one if metamodels match. however, this simple rule has some obvious limitations. for instance, a transformation may only use a subset of a metamodel. therefore, chaining transformations appropriately requires more information. we present here an approach that automatically discovers more detailed information about actual chaining constraints by statically analyzing transformations. the objective is to provide developers who decide to chain transformations with more data on which to base their choices. this approach has been successfully applied to the case of a library of endogenous transformations. they all have the same source and target metamodel but have some hidden chaining constraints. in such a case, the simple metamodel matching rule given above does not provide any useful information.
how synchronization protects from noise. the functional role of synchronization has attracted much interest and debate: in particular, synchronization may allow distant sites in the brain to communicate and cooperate with each other, and therefore may play a role in temporal binding, in attention or in sensory-motor integration mechanisms. in this article, we study another role for synchronization: the so-called "collective enhancement of precision". we argue, in a full nonlinear dynamical context, that synchronization may help protect interconnected neurons from the influence of random perturbations-intrinsic neuronal noise-which affect all neurons in the nervous system. more precisely, our main contribution is a mathematical proof that, under specific, quantified conditions, the impact of noise on individual interconnected systems and on their spatial mean can essentially be cancelled through synchronization. this property then allows reliable computations to be carried out even in the presence of significant noise (as experimentally found e.g., in retinal ganglion cells in primates). this in turn is key to obtaining meaningful downstream signals, whether in terms of precisely-timed interaction (temporal coding), population coding, or frequency coding. similar concepts may be applicable to questions of noise and variability in systems biology.
correcting a buffer overflow vunerability at runtime with arachne. null
expression and composition of design patterns with aspectj. design patterns are well-known couples of problems-solutions for software engineer- ing. by nature, they often lack support from languages and this further complicates the study of their composition in the code. aspect-oriented languages provide new mechanisms for modula- rization, which can help to improve design patterns implementation. (hannemann et al., 2002) is the first extensive study of patterns aspectization with aspectj. we notice some aspectj idioms are needed in order to implement object relationships. we give a more reusable visitor pat- tern. we highlight a reusable composition of composite and visitor patterns and expressive interactions of the observer pattern with a tree structure. we thus show that modularization by aspects helps composition of design patterns.
understanding design patterns density with aspects: a case study in jhotdraw using aspectj. design patterns offer solutions to common engineering prob- lems in programs [1]. in particular, they shape the evolution of program elements. however, their implementations tend to vanish in the code: thus it is hard to spot them and to understand their impact. the prob- lem becomes even more difficult with a "high density of pattern": then the program becomes easy to evolve in the direction allowed by patterns but hard to change [2]. aspect languages offer new means to modular- ize elements. implementations of object-oriented design patterns with aspectj have been proposed [3]. we aim at testing the scalability of such solutions in the jhotdraw framework. we first explore the impact of density on pattern implementation. we show how aspectj helps to reduce this impact. this unveils the principles of aspects and aspectj to control pattern density.
a human-centred approach of steering control modelling. future driving assistances will be designed in such a way that their action blends into the perceptual and motor control loops of the driver without yielding negative interference. this objective cannot be reached without integrating into the design process a model that allows making valid predictions on the behaviour of the driver. in that context, this paper will deal with the understanding and modelling of the driver's steering behaviour, with an emphasis on the role of visual information and kinaesthetic feedback. this paper describes strategies of steering and speed control used by the driver and the combined cornering and braking condition is analyzed. for the driver speed control, the information of driver visual direction is used to determinate anticipatory speed control during curve negotiation.
radiolytic yield of uiv oxidation into uvi: a new mechanism for uv reactivity in acidic solution. the yields of the radiolytic oxidation of uiv and of the uvi formation, measured by spectrophotometry, are found to be the same (g(−uiv)n2o = g(uvi)n2o = 8.4 × 10−7 mol j−1) and almost double the h2 formation yield (g(h2) = 4.4 × 10−7 mol j−1) in the 60co γ radiolysis of n2o-aqueous solutions in the presence of 2 mol l−1 cl− at ph = 0 (hcl). according to the mechanism of uiv radiolytic oxidation, we show that under the conditions of our experiments the uv ions do not disproportionate, but undergo a stoichiometric oxidation into uvi by h+ with forming h2.
javacompext: extracting architectural elements from java source code. software architecture erosion is a general problem in legacy software. to fight this trend, component models and languages are designed to try to make explicit, and automatically enforceable, the architectural decisions in terms of components, interfaces, and allowed communication channels between component interfaces. to help maintainers work on existing object-oriented systems, we explore the possibility of extracting architectural elements (components, communications, services, ...) from the source code. we designed a tool based on some heuristics for extracting component information from java source code.
an aspect-oriented approach for developing self-adaptive fractal components. nowadays, application developers have to deal with increasingly variable execution contexts, requiring the creation of applications able to adapt themselves autonomously to the evolutions of this context. in this paper, we show how an aspect-oriented approach enables the development of self-adaptive applications where the adaptation code is well modularized, both spatially and temporally. concretely, we propose safran, an extension of the fractal component model for the development of the adaptation aspect as reactive adaptation policies. these policies detect the evolutions of the execution context and adapt the base program by reconfiguring it. this way, safran allows the development of the adaptation aspect in a modular way and its dynamic weaving into applications.
development of strategies for htr fuel waste management. in an effort to reduce the volume of nuclear wastes and to allow the reuse of remaining fissile materials, a strategy for management of high temperature reactors (htr) fuel was developed in this study. the volume reduction passes through the separation of highly radioactive triso particles with fuel kernels from the slightly radioactive graphite matrix (both combined in a fuel assembly called "compact") while the total recycling option requires the separation of the valuable kernel from the particle coating as ultimate waste. the separation methods must preserve the integrity of triso to prevent the release of radionuclides. a thermal shock treatment between liquid nitrogen and hot water allows for a partial destruction of the compact but only few particles are separated. alternatively, graphite erosion by a high pressure water jet presents the risk of fracturing the particles. better is the total combustion of carbon which releases all the particles. the treatment of compacts by ultrasounds in water erodes the graphite as function of the intensity, distance and direction of attack, temperature and gas saturation, and provides clean particles. the acid attack of compacts by a mixture h2o2 + h2so4 causes the intercalation of graphite by acid, which inflates the structure and releases the intact particles. the triso on one hand and coatings on the other hand were then vitrified by sintering to achieve a high density, up to a rate of 25% vol. finally, the leaching of composites in ultrapure water at 90°c shows strong confinement properties.
measurement of the energy spectrum of cosmic rays above 10^18 ev using the pierre auger observatory. we report a measurement of the flux of cosmic rays with unprecedented precision and statistics using the pierre auger observatory. based on fluorescence observations in coincidence with at least one surface detector we derive a spectrum for energies above 10^18 ev. we also update the previously published energy spectrum obtained with the surface detector array. the two spectra are combined addressing the systematic uncertainties and, in particular, the influence of the energy resolution on the spectral shape. the spectrum can be described by a broken power law e^-gamma with index gamma=3.3 below the ankle which is measured at log10(e/ev) = 18.6. above the ankle the spectrum is described by a power law with index 2.6 followed by a flux suppression, above about log10(e/ev) = 19.5, detected with high statistical significance.
combined effects of the filling ratio and the vapour space thickness on the performance of a flat plate heat pipe. null
[bevacizumab therapy for poems syndrome]. null
measurement of the depth of maximum of extensive air showers above 10^18 ev. we describe the measurement of the depth of maximum, xmax, of the longitudinal development of air showers induced by cosmic rays. almost four thousand events above 10^18 ev observed by the fluorescence detector of the pierre auger observatory in coincidence with at least one surface detector station are selected for the analysis. the average shower maximum was found to evolve with energy at a rate of (106 +35/-21) g/cm^2/decade below 10^(18.24 +/- 0.05) ev and (24 +/- 3) g/cm^2/decade above this energy. the measured shower-to-shower fluctuations decrease from about 55 to 26 g/cm^2. the interpretation of these results in terms of the cosmic ray mass composition is briefly discussed.
l-asparaginase-based treatment of 15 western patients with extranodal nk/t-cell lymphoma and leukemia and a review of the literature. background: extranodal natural killer (nk)/t-cell lymphoma, nasal type, and aggressive nk-cell leukemia are highly aggressive diseases with a poor outcome. patients and methods: we report a multicentric french retrospective study of 15 patients with relapsed, refractory, or disseminated disease, treated with l-asparaginase-containing regimens in seven french centers. thirteen patients were in relapse and/or refractory and 10 patients were at stage iv. results: all but two of the patients had an objective response to l-asparaginase-based treatment. seven patients reached complete remission and only two relapsed. conclusion: these data, although retrospective, confirm the excellent activity of l-asparaginase-containing regimens in refractory extranodal nk/t-cell lymphoma and aggressive nk-cell leukemia. therefore, l-asparaginase-based regimen should be considered as a salvage treatment, especially for patients with disseminated disease. first-line l-asparaginase combination therapy for extranodal nk/t-cell lymphoma and aggressive nk-cell leukemia should be tested in prospective trials.
investigation of deep inelastic reactions in 238u + 238u at coulomb barrier energies. we investigated deep inelastic reactions in 238u+238u collisions at 6.09, 6.49, 6.91, 7.1 and 7.35×amev at the vamos spectrometer (ganil). a large transfer of neutrons and protons was observed at all beam energies. for a transfer of more than 10 nucleons the total kinetic energy of the detected fragments becomes independent of the beam energy and reaches values far below the coulomb barrier for spherical fragments. this points to the formation of a di-nuclear system in the entrance channel which develops an elongated shape and a strong neck. for such reactions we expect an enhanced lifetime of the di-nuclear system which is significantly longer than the time scale for elastic and quasi-elastic reactions. different theoretical approaches predict delay times of more than 5 × 10−21 s for a subset of our data.
aqueous corrosion of the gese4 chalcogenide glass: surface properties and corrosion mechanism. the aqueous corrosion behavior of the gese4 glass composition has been studied over time under various conditions (temperature and ph). the evolution of the surface topography by atomic force microscopy and properties such as surface hardness and reduced modulus, as well as the optical transmission in the 1-16 μm window, have been measured as a function of time spent in the corrosive solution. it was found that even if the glass reacts at room temperature, its optical transparency was barely affected. nevertheless, the durability of gese4 was found to be drastically affected by an increase of both temperature and ph. furthermore, pure selenium nanoparticles were formed during the corrosion process, and the nature of these nanoparticles--amorphous or crystallized (hexagonal phase)--depends on temperature. a reaction mechanism was proposed, and the activation energy of the reaction of corrosion in deionized water (47 kj/mol) was determined from an original technique that relies on the temporal optical loss variation of a gese4 optical fiber placed in water at different temperatures.
optimal technology-oriented design of parallel robots for high-speed machining applications. null
characterization of new injectable biomaterials containing bisphosphonates. null
labelling of peptides and antibodies with bi-213 for targeted alpha-radionuclide therapy. null
how relativistic effects control elemental astatine chemistry. null
determination of complexation constants between inorganic ligands and at(i) and at(iii) species present at ultra-trace concentrations. null
novel phosphate – phosphonate hybrid materials, applied to biology. null
sorption of se(iv) on compacted clay materials. null
migration of uranium in the presence of natural organic matter colloids in a sandy aquifer. null
exploration of the metallic character of astatine in aqueous solution. alpha-radiotherapy is an innovative technique for the treatment of cancers complementary to current approaches. the principle is to use tumor-specific vectors labeled with alpha-radioisotopes. astatine 211 is a very promising candidate if one considers the energy of the α particles emitted as well as its physical half-life (7.2 h). one of the ways of labeling is to use astatine at a higher oxidation state as a “metal cation”. indeed, considering its location in the periodic table. astatine is supposed to present a more metallic character than the other halogens. this way of labeling has however never been explored. this can be explained by the fact that the astatine chemistry is nearly unknown. the purpose of this work was therefore to explore this property of astatine and to define its eh, ph diagram (or pourbaix diagram) in non complexing aqueous solution. to this end, both experimental and theoretical approaches were used. the experimental approach uses competition methods to identify the formed species and the thermodynamics constants of studied equilibria. the theoretical approach uses methods of computational chemistry and provides information at the molecular scale on the studied systems in order to predict thermodynamic data which are used as support and complement to the experimental approach. the important result of this work shows the presence of two stable cationic forms of astatine in aqueous solution, i.e., at+ and ato+.
spectroscopic study of the interaction of u(vi) with transferrin and albumin for speciation of u(vi) under blood serum conditions. the quantitative description of the interactions of uranium with blood serum components is of high relevance for a rational design of molecules suitable for in vivo chelation of uranium. we have determined the stability constants for the complexation of u(vi) with human serum transferrin and albumin by time-resolved laser-induced fluorescence spectroscopy and difference ultraviolet spectroscopy. both proteins interact strongly with u(vi), forming ternary complexes with carbonate acting as a synergistic anion. together with literature data describing the interaction of u(vi) with low molecular weight inorganic and organic serum components, the speciation of u(vi) in blood serum was calculated. in agreement with published experimental data, the model calculation shows that complexation with proteins and carbonate ion governs u(vi) speciation: 35% of u(vi) is bound to proteins and 65% to carbonate. among the protein pool, albumin is the main protein interacting with u(vi). in addition, the results show that ca(ii) must be considered in the model as a competitive metal ion with respect to u(vi) for binding to albumin surface sites. based on these findings several promising molecules for in vivo chelation of u-230 could be identified. (c) 2009 elsevier inc. all rights reserved.
porosities accessible to hto and iodide on water-saturated compacted clay materials and relation with the forms of water: a low field proton nmr study. the aim of the present work was to quantify accessible porosities for iodide and for a water tracer (hto) on water-saturated compacted clay samples (illite, montmorillonite and mx-80 bentonite) and to relate these macroscopic values to the forms of water in these porosities (surface/bulk water, external/internal water). low field proton nmr was used to characterize and quantify the forms of water. this enabled the three different populations (structural oh, external surface and internal surface water) to be differentiated on hydrated clays by considering the difference in proton mobility. an accurate description of the water forms within the different populations did not appear possible when water molecules of these populations were in contact because of the occurrence of rapid exchange reactions. for this reason, it was not possible to use the low resolution nmr method to quantify external surface and bulk water in fully water-saturated compacted clay media at room temperature. this latter information could however be estimated when analyzing the samples at -25°c. at this temperature, a distinction based on the difference in mobility could be made since surface water remained in a semi-liquid state whereas bulk water froze. in parallel, accessible porosities for anions and hto were determined by an isotopic dilution method using capillaries to confine the materials. hto was shown to probe the whole pore volume (i.e. the space made of surface and bulk water). when the surface water volume was mainly composed of interlayer water (case of montmorillonite and bentonite), iodide was shown to be located in the pore space made of bulk water. when the interlayer water was not present (case of illite), the results showed that iodide could access a small fraction of the surface water volume localized at the external surface of the clay particles.
investigation of para-sulfonatocalix[n]arenes [n = 6, 8] as potential chelates for 230u. literature reports of the efficacy of para-sulfonatocalix[6]- and calix[8]-arenes as u(vi) complexants indicated that they might be useful for in vivo chelation of the novel therapeutic alhpa-emitter 230u. we have studied the complexation of u(vi) with para-sulfonatocalix[6]arene and para-sulfonatocalix[8]arene by time resolved laser induced fluorescence spectroscopy and using competition methods with chelex resin and 4-(2-pyridylazo)resorcinol in simplified and in biological media. new thermodynamic parameters describing the stability of u(vi)-para-sulfonatocalix[n]arene [n = 6, 8] complexes were obtained. although the interactions are strong, the complexes do not exhibit sufficient stability to compete with carbonate ions and serum proteins for complexation of u(vi) under physiological conditions.
astatine standard redox potentials and speciation in acidic medium. a combined experimental and theoretical approach is used to define astatine (at) speciation in acidic aqueous solution and answer the two main questions raised from literature data: does at(0) exist in aqueous solution and what is the chemical form of at(iii), if it exists. the experimental approach considers that a given species is characterized by its distribution coefficient (d) experimentally determined in a biphasic system. the change in speciation arising from a change in experimental conditions is observed by a change in d value. the theoretical approach involves quasi-relativistic quantum chemistry calculations. the results show that at at the oxidation state 0 cannot exist in aqueous solution. the three oxidation states present in the range of water stability are at(-i), at(i) and at(iii) and exist as at-, at+ and ato+, respectively, in the 1 to 2 ph range. the standard redox potentials of the at+/at- and ato+/at+ couples have been determined, the respective values being 0.36 ± 0.01 and 0.74 ± 0.01 v vs. nhe.
multidisciplinary and multiobjective optimization: comparison of several methods. engineering design of complex systems is a decision making process that aims at choosing from among a set of options that implies an irrevocable allocation of resources. it is inherently a multidisciplinary and multi-objective process. the paper describes some classical multidisciplinary optimization (mdo) methods with their advantages and drawbacks. some new approaches combining genetic algorithms (moga) and collaborative optimization (co) are presented. they allow to: 1) increase the convergence rate when a design problem can be broken up regarding design variables, and 2) provide an optimal set of design variables in case of multi-level multi-objective design problem.
how to build web self-services by functional profiles?. companies, institutions, and local authorities seek to computerize services to be more profitable in terms of productivity, profitability, quality of service, traceability... the main issues are the identification of problems to solve and the choice of adapted solutions. to build these solutions, functional and technical profiles must communicate to understand precisely the domain, the problem, and the expected features... this paper provides a framework composed of three contributions, which aim to simplify the building of web self-services by functional profiles. the first part deals with a language to specify functional needs using the main concept of goal. this language must be as simple as possible but not limiting. the second part deals with the tool to use this language and, finally, the last section is a set of interpretations to simplify communication between all involved people. to automate a large part of recurrent tasks, all the proposed approach complies with model driven methodology.
combined air treatment: effect of composition of fibrous filters on toluene adsorption and particle filtration efficiency. null
detection and characterization of the radio emission from atmospheric showers induced by cosmic rays with energy above 10^16 ev by the codalema experiment. ultra high energy cosmic rays, extraterrestrial particles which nature and origin remain today uncertain, are ordinarily studied by using two major techniques of eas (extensive air shower) detection: ground particles detectors or fluorescence light telescopes. appeared for the first time in the 60', researches on eas radiodetection by measuring the electric field induced by shower's charged particles was first stopped because of technical difficulties. with the developpement of fast electronic, radiodetection technique became again potentially interesting for cosmic rays study. the codalema experiment, since 2002, use and improve the radiodetection method. these last years, the experimental setup was largely modified, original log-periodic antennas were replaced by active dipole dedicated to radiodetection and the trigger, realized by an array of 17 scintillators, allow now to estimate the primary cosmic ray energy. present objective of codalema is to characterize the electric signal induced by an eas, according to the physical parameters of the shower. in this thesis the main results obtain by codalema are presented. the evidences for a geomagnetic origin of eas radioelectric field is one of the more important. moreover a first study of electric field lateral distribution functions and the correlation between the primary particle energies with the amplitude of the eas electric field are also discussed.
views for aspectualizing component models. component based software development (cbsd) and aspect- oriented software development (aosd) are two complemen- tary approaches. however, existing proposals for integrating aspects into component models are direct transposition of object-oriented aosd techniques to components. in this article, we propose a new approach based on views. our proposal introduces crosscutting components quite naturally and can be integrated into different component models.
guiding architectural design process of hard real-time systems with constraint programming. null
optimal control theory : a method for the design of wind instruments. it has been asserted previously by the author that optimal control theory can be a valuable framework for theoretical studies about the shape that a wind instrument should have in order to satisfy some optimization criterion, inside a fairly general class. the purpose of the present work is to develop this new approach with a look at a specific criterion to be optimized. in this setting, the webster horn equation is regarded as a controlled dynamical equation in the space variable. pressure is the state, the control being made of two parts~: one variable part, the inside diameter of the duct and one constant part, the weights of the elementary time-harmonic components of the velocity potential. then one looks for a control that optimizes a criterion related to the definition of an {oscillation regime} as the cooperation of several natural modes of vibration with the excitation, the {playing frequency} being the one that maximizes the total generation of energy, as exposed by a.h. benade, following h. bouasse. at the same time the relevance of this criterion is questionned with the simulation results.
a study of the effect of molecular and aerosol conditions in the atmosphere on air fluorescence measurements at the pierre auger observatory. the air fluorescence detector of the pierreaugerobservatory is designed to perform calorimetric measurements of extensive air showers created by cosmic rays of above 10^18 ev. to correct these measurements for the effects introduced by atmospheric fluctuations, the observatory contains a group of monitoring instruments to record atmospheric conditions across the detector site, an area exceeding 3,000 km2. the atmospheric data are used extensively in the reconstruction of air showers, and are particularly important for the correct determination of shower energies and the depths of shower maxima. this paper contains a summary of the molecular and aerosol conditions measured at the pierreaugerobservatory since the start of regular operations in 2004, and includes a discussion of the impact of these measurements on air shower reconstructions. between 10^18 and 10^20 ev, the systematic uncertainties due to all atmospheric effects increase from 4% to 8% in measurements of shower energy, and 4 g cm^-2 to 8 g cm^-2 in measurements of the shower maximum.
positive invariance of polyhedrons and comparison of markov reward models with different state spaces. in this paper, we discuss the comparison of expected rewards for discrete-time reward markov chains with different state spaces. necessary and sufficient conditions for such a comparison are derived. due to the special nature of the introduced binary relation, a criterion may be formulated in terms of an inclusion of polyhedral sets. then, algebraic and geometric forms are easily obtained from haar's lemma. our results allow us to discuss some earlier results on the stochastic comparison of functions of markov chains.
nucleate boiling in flat grooved heat pipes. null
a model curriculum for aspect-oriented software development. as new software engineering techniques emerge, there's a cognitive shift in how developers approach a problem's analysis and how they design and implement its software-based solution. future software engineers must be appropriately and effectively trained in new techniques' fundamentals and applications. with techniques becoming more mature, such training moves beyond specialized industrial courses into postgraduate curricula (as advanced topics) and subsequently into undergraduate curricula. a model curriculum for aspect-oriented software development provides guidelines about fundamentals, a common framework, and a step toward developing a body of knowledge.
alignment of the alice inner tracking system with cosmic-ray tracks. alice (a large ion collider experiment) is the lhc (large hadron collider) experiment devoted to investigating the strongly interacting matter created in nucleus-nucleus collisions at the lhc energies. the alice its, inner tracking system, consists of six cylindrical layers of silicon detectors with three different technologies; in the outward direction: two layers of pixel detectors, two layers each of drift, and strip detectors. the number of parameters to be determined in the spatial alignment of the 2198 sensor modules of the its is about 13,000. the target alignment precision is well below 10 micron in some cases (pixels). the sources of alignment information include survey measurements, and the reconstructed tracks from cosmic rays and from proton-proton collisions. the main track-based alignment method uses the millepede global approach. an iterative local method was developed and used as well. we present the results obtained for the its alignment using about 10^5 charged tracks from cosmic rays that have been collected during summer 2008, with the alice solenoidal magnet switched off.
bounds of graph properties. null
dynamic adaptation of the squid web cache with arachne. null
generalized dynamic probes for the linux kernel and applications with arachne. finding the root cause of bugs and performance problems in large applications is a difficult task. the main reason of this difficulty is that the comprehension of such applications crosscuts the boundaries of a single process, indeed the concurrent nature of large applications requires insight of multiple threads and process and even sometimes of the kernel. in the meantime, most existing tools lacks support for simultaneous kernel and applications analysis. in this paper, we present arachne, a tool for runtime analysis of complex applications. while efficiency considerations have played an important role in the design of arachne, it allows safe and runtime injection of probes inside the linux kernel and user space applications on both function calls and variable access. it features an aspect- oriented language that allows to access context of execution and to compose primitive probes (for example sequence of function calls). we show how arachne allows to easily analyze problems such as race conditions which involves complex interactions between multiple process. and finally, we show arachne is fast enough to analyze high performance applications such as the squid web cache.
power management in grid computing with xen. while chip vendor still stick to moore's law, and the performance per dollar keeps going up, the performance per watt has been stagnant for the last few years. moreover energy prices continue to rise world-wide. this poses a major challenge to organisations running grids, indeed such architectures require cooling systems. indeed the one-year cost of a cooling system and of the power consumption may outfit the grid initial investement. we observe , however, that a grid does not constantly run at peak performance. in this paper, we propose a workload concentration strategy to reduce grid power consumption. using the xen virtual machine migration technology, our power management policy can dispatch transparently and dynamically any applications of the grid. our policy concentrates the workload to shutdown nodes that are unused with a neglectable impact on performance. we show through evaluations that this policy decreases the overall power consumption of the grid significantly.
the case for execution replay using a virtual machine. debugging grid systems is complex, mainly because of the probe effect and non reproducible execution. the probe effect arises when an attempt to monitor a system changes the behavior of that system. moreover, two executions of a distributed system with identical inputs may behave differ- ently due to non determinism. execution replay is a tech- nique developed to facilitate the debugging of distributed systems: a debugger first monitors the execution of a dis- tributed system and then replays it identically. existing approaches to execution replay only partially address the probe effect and irreproducibility problem. in this paper, we argue for execution replay of distributed sys- tems using a virtual machine approach. the vm approach addresses the irreproducibility problem, it does not com- pletely avoid the probe effect. nevertheless, we believe that the full control of the virtual hardware addresses the probe issue well enough to debug distributed system errors.
a reflexive extension to arachne's aspect language. aspect weaving at run time has proven to be an effective way of implementing software evolution. nevertheless, it is often hard to achieve adequate modularization and reusability in face of run time and implementation issues. arachne is an ao system that features a run time aspect weaver for c applications, and a language close to the c syntax. in this paper we present a reflexive extension of arachne's aspect language. we show through extracts of a deadlock detection aspect, how this extension improves the modularization of crosscutting concerns and the reusability of aspects.
performance of prototypes for the alice electromagnetic calorimeter. the performance of prototypes for the alice electromagnetic sampling calorimeter has been studied in test beam measurements at fnal and cern. a $4\times4$ array of final design modules showed an energy resolution of about 11% /$\sqrt{e(\mathrm{gev})}$ $\oplus$ 1.7 % with a uniformity of the response to electrons of 1% and a good linearity in the energy range from 10 to 100 gev. the electromagnetic shower position resolution was found to be described by 1.5 mm $\oplus$ 5.3 mm /$\sqrt{e \mathrm{(gev)}}$. for an electron identification efficiency of 90% a hadron rejection factor of $&gt;600$ was obtained.
nonlinear effects in stiffness modeling of robotic manipulators. the paper focuses on the enhanced stiffness modeling of robotic manipulators by taking into account influence of the external force/torque acting upon the end point. it implements the virtual joint technique that describes the compliance of manipulator elements by a set of localized six-dimensional springs separated by rigid links and perfect joints. in contrast to the conventional formulation, which is valid for the unloaded mode and small displacements, the proposed approach implicitly assumes that the loading leads to the non-negligible changes of the manipulator posture and corresponding amendment of the jacobian. the developed numerical technique allows computing the static equilibrium and relevant force/torque reaction of the manipulator for any given displacement of the end-effector. this enables designer detecting essentially nonlinear effects in elastic behavior of manipulator, similar to the buckling of beam elements. it is also proposed the linearization procedure that is based on the inversion of the dedicated matrix composed of the stiffness parameters of the virtual springs and the jacobians/hessians of the active and passive joints. the developed technique is illustrated by an application example that deals with the stiffness analysis of a parallel manipulator of the orthoglide family.
first proton--proton collisions at the lhc as observed with the alice detector: measurement of the charged particle pseudorapidity density at $\ sqrt(s)$= 900 gev. on 23rd november 2009, during the early commissioning of the cern large hadron collider (lhc), two counter-rotating proton bunches were circulated for the first time concurrently in the machine, at the lhc injection energy of 450 gev per beam. although the proton intensity was very low, with only one pilot bunch per beam, and no systematic attempt was made to optimize the collision optics, all lhc experiments reported a number of collision candidates. in the alice experiment, the collision region was centred very well in both the longitudinal and transverse directions and 284 events were recorded in coincidence with the two passing proton bunches. the events were immediately reconstructed and analyzed both online and offline. we have used these events to measure the pseudorapidity density of charged primary particles in the central region. in the range |eta| &lt; 0.5, we obtain dnch/deta = 3.10 +- 0.13 (stat.) +- 0.22 (syst.) for all inelastic interactions, and dnch/deta = 3.51 +- 0.15 (stat.) +- 0.25 (syst.) for non-single diffractive interactions. these results are consistent with previous measurements in proton--antiproton interactions at the same centre-of-mass energy at the cern spps collider. they also illustrate the excellent functioning and rapid progress of the lhc accelerator, and of both the hardware and software of the alice experiment, in this early start-up phase.
sorption of eu(iii) on attapulgite studied by batch, xps and exafs techniques. the effects of ph, ionic strength and temperature on sorption of eu(iii) on attapulgite were investigated in the presence and absence of fulvic acid (fa) and humic acid (ha). the results indicated that the sorption of eu(iii) on attapulgite was strongly dependent on ph and ionic strength, and independent of temperature. in the presence of fa/ha, eu(iii) sorption was enhanced at ph &lt; 4, decreased at ph range of 4 - 6, and then increased again at ph &gt; 7. the x-ray photoelectron spectroscopy (xps) analysis suggested that the sorption of eu(iii) might be expressed as ≡x3eu0 ≡swoheu3+ and ≡soeu-ooc-/ha in the ternary eu/ha/attapulgite system. the extended x-ray absorption fine structure (exafs) analysis of eu-ha complexes indicated that the distances of d(eu-o) decreased from 2.451 to 2.360 å with increasing ph from 1.76 to 9.50, whereas the coordination number (n) decreased from ~9.94 to ~8.56. different complexation species were also found for the different addition sequences of ha and eu(iii) to attapulgite suspension. the results are important to understand the influence of humic substances on eu(iii) behavior in the natural environment.
proceedings of the workshop: hera and the lhc workshop series on the implications of hera for lhc physics. 2nd workshop on the implications of hera for lhc physics. working groups: parton density functions multi-jet final states and energy flows heavy quarks (charm and beauty) diffraction cosmic rays monte carlos and tools.
study of hard processes in proton-proton and nucleus-nucleus collisions. collisions of high energy particles are useful to probe the elementary structure of matter: partons (quarks and gluons). ultra relativistic heavy ion collisions produce in the same event a so-called quark-gluon plasma (qgp) as well as elementary parton-parton interaction. those interactions, called hard processes, originate from the very first moment of the collision, and produce high transverse momentum partons which then decay into observable hadrons (jets). before this “hadronization”, the partons may interact with the qgp, which modifies the hadron properties. studying hard processes will be an important issue at lhc. jet production cross sections can be computed within the pqcd framework. the epos formalism tends to compute full events which are directly comparable to experimental events, reproducing the soft part (qgp) as well as the hard part in a coherent model. in this manuscript, i will motivate the realization of hard processes in a complete event. the hard part should be compatible with the results from pqcd. doing so, as in experiment, production of rare processes needs large statistics. i will therefore introduce method allowing to trigger on high transverse momentum events, to easily produce rare event in the context of soft particles.
on the liquid drop model mass formulae and charge radii. an adjustment to 782 ground state nuclear charge radii for nuclei with n,z $\ge$ 8 leads to $r_0=1.2257~a^{1/3}$~fm and $\sigma =0.124$~fm for the charge radius. assuming such a coulomb energy $e_c=\frac {3}{5} {e^2z^2}/{1.2257~a^{\frac {1}{3}}}$, the coefficients of different possible mass formulae derived from the liquid drop model and including the shell and pairing energies have been determined from 2027 masses verifying n,z $\ge$ 8 and a mass uncertainty $\le $ 150 kev. these formulae take into account or not the diffuseness correction ($z^2/a$ term), the charge exchange correction term ($z^{4/3}/a^{1/3}$ term), the curvature energy, the wigner terms and different powers of $i=(n-z)/a$. the coulomb diffuseness correction or the charge exchange correction term plays the main role to improve the accuracy of the mass formulae. the different fits lead to a surface energy coefficient of around 17-18~mev. a possible more precise formula for the coulomb radius is $r_0=1.2332a^{1/3}+{2.8961}/{a^{2/3}}-0.18688a^{1/3}i$~fm with $\sigma =0.052$~fm.
changes in fire regime since the last glacial maximum: an assessment based on a global synthesis and analysis of charcoal data. null
reducing kernel development complexity in distributed environments. setting up generic and fully transparent distributed services for clusters implies complex and tedious kernel developments. more flexible approaches such as user-space libraries are usually preferred with the drawback of requiring application recompilation. a second approach consists in using specific kernel modules (such as fuse in gnu/linux system) to transfer kernel complexity into user space. in this paper, we present a new way to design and implement kernel distributed services for clusters by using a cluster wide consistent data management service. this system, entitled kddm for "kernel distributed data management", offers flexible kernel mechanisms to transparently manage remote accesses, cache and coherency. we show how kddm simplifies distributed kernel developments by presenting the design and the implementation of a service as complex as a fully symmetric distributed file system. the innovative approach of kddm has the potential to boost the development of distributed kernel services because it relieves the developers of the burden of dealing with distributed protocols and explicit data transfers. instead, it allows focusing on the implementation of services in a manner very similar to that of parallel programming on smp systems. more generally, the use of kddm could be exploited in almost all local kernel services to extend them to cluster scale. cluster wide ipc, distributed namespaces (such as /proc) or process migration are some potential examples.
the effect of high power ultrasound on an aqueous suspension of graphite. ultrasound treatment was used to study the decrease of the granulometry of graphite, due to the cavitation, which allows the erosion by separating grains. at a smaller scale, cavitation bubble implosion tears apart graphite sheets as shown by hrtem, while ho and h radicals produced from water sonolysis, generate oxidative and reductive reactions on these sheet fragments. such reactions form smaller species, e.g. dissolved organic matter. the methodology proposed is very sensitive to unambiguously identifying the in situ composition of organic compounds in water. the use of the atmospheric pressure chemical ionization (apci) fourier transform mass spectrometry (ftms) technique minimizes the perturbation of the organic composition and does not require chemical treatment for analysis. the structural features observed in the narrow range (m/z &lt; 300) were mainly aromatic compounds (phenol, benzene, toluene, xylene, benzenediazonium, etc.), c4-c6 alkenes and c2-c10 carboxylic acids. synthesis of small compounds from graphite sonication has never been reported and will probably be helpful to understand the mechanisms involved in high energy radical reactions.
special issue : "robotics and factory of the future, new trends and challenges in mechatronics" from incom 2006. null
trigger and aperture of the surface detector array of the pierre auger observatory. null
erratum to "atmospheric effects on extensive air showers observed with the surface detector of the pierre auger observatory" [astroparticle physics 32(2) (2009), 89-99]. null
aspect-based patterns for grid programming. the development of grid algorithms is frequently ham- pered by limited means to describe topologies and lack of support for the invasive composition of legacy components in order to pass data between them. in this paper we present a solution to overcome these limitations using the notion of invasive patterns for the construction of distributed algo- rithms, a recent extension of well-known computation and communication patterns. concretely, we present two con- tributions. first, based on a study of how patterns are in- stantiated in nas grid, a well-known benchmark used for evaluating performance of computational grids, we show how invasive patterns can be used for the declarative defini- tion of large-scale grid topologies and checkpointing algo- rithms. second, we qualitatively and quantitatively evaluate how our approach can be used to implement the checkpoint- ing on top of grid applications.
extension of hansen-bliek's method to right-quantified linear systems. null
on pole assignment and stabilizability of neutral type systems. in this note we present a systematic approach to the stabilizability problem of linear infinite-dimensional dynamical systems whose infinitesimal generator has an infinite number of instable eigenvalues. we are interested in strong non-exponential stabilizability by a linear feed-back control. the study is based on our recent results on the riesz basis property and a careful selection of the control laws which preserve this property. the investigation may be applied to wave equations and neutral type delay equations.
stability analysis of mixed retarded-neutral type systems. the strong stability property of mixed retarded-neutral type systems is studied. we combine two technics: the existence of a riesz basis of invariant infinite dimensional subspaces and the boundedness of the resolvent on some invariant subspace.
proceedings of the 38th international symposium on multiparticle dynamics (ismd08). proceedings of ismd08.
analytic expressions for alpha particle preformation in heavy nuclei. the experimental alpha decay energies and half-lives are investigated systematically to extract the alpha particle preformation in heavy nuclei. formulas for the preformation factors are proposed. they can be used to guide the microscopic studies on preformation factors and perform accurate calculations of the alpha decay half-lives. there is little evidence for the existence of an island of long stability of superheavy nuclei (shn).
dynamic aspectj. this paper considers the diﬃculties linked to the static scheduling strategy of aspectj and shows how to overcome them by turning to a more dynamic strategy, making it possible to order, cancel, and deploy aspects at runtime. we show that this more dynamic strategy can be obtained by a minor update of the semantics of aspectj introducing the notion of current aspect group, that is, the aspects scheduled for the current join point. we show how to reﬂect this change at the language level and present a prototype of the resulting aspectj variant, dynamic aspectj. this prototype reuses aspectj to perform a ﬁrst step of static weaving, which we complement by a second step of dynamic weaving, implemented through a thin interpretation layer. this can be seen as an interesting example of reconciling interpreters and compilers, the dynamic and the static world.
hull consistency under monotonicity. we prove that hull consistency for a system of equations or inequalities can be achieved in polynomial time providing that the underlying functions are monotone with respect to each variable. this result holds including when variables have multiple occurrences in the expressions of the functions, which is usually a pitfall for interval-based contractors. for a given constraint, an optimal contractor can thus be enforced quickly under monotonicity and the practical significance of this theoretical result is illustrated on a simple example.
a constraint on the number of distinct vectors with application to localization. this paper introduces a generalization of the "nvalue" constraint that bounds the number of distinct values taken by a set of variables.the generalized constraint (called "nvector") bounds the number of distinct (multi-dimensional) vectors. the first contribution of this paper is to show that this global constraint has a significant role to play with continuous domains, by taking the example of simultaneous localization and map building (slam). this type of problem arises in the context of mobile robotics. the second contribution is to prove that enforcing bound consistency on this constraint is np-complete. a simple contractor (or propagator) is proposed and applied on a real application.
contractor programming. this paper describes a solver programming method, called "contractor programming", that copes with two issues related to constraint processing over the reals. first, continuous constraints involve an inevitable step of solver design. existing softwares provide an insufficient answer by restricting users to choose among a list of fixed strategies. our first contribution is to give more freedom in solver design by introducing programming concepts where only configuration parameters were previously available. programming consists in applying operators (intersection, composition, etc.) on algorithms called "contractors" that are somehow similar to propagators. second, many problems with real variables cannot be cast as the search for vectors simultaneously satisfying the set of constraints, but a large variety of different outputs may be demanded from a set of constraints (e.g., a paving with boxes inside and outside of the solution set). these outputs can actually be viewed as the result of different "contractors" working concurrently on the same search space, with a bisection procedure intervening in case of deadlock. such algorithms (which are not strictly speaking solvers) will be made easy to build thanks to a new branch &amp; prune system, called "paver". thus, this paper gives a way to deal harmoniously with a larger set of problems while giving a fine control on the solving mechanisms. the contractor formalism and the paver system are the two contributions. the approach is motivated and justified through different cases of study. an implementation of this framework named quimper is also presented.
a priori error analysis and spring arithmetic. error analysis is defined by the following concern: bounding the output variation of a (nonlinear) function with respect to a given variation of the input variables. this paper investigates this issue in the framework of interval analysis. the classical way of analyzing the error is to linearize the function around the point corresponding to the actual input, but this method is local and not reliable. both drawbacks can be easily circumvented by a combined use of interval arithmetic and domain splitting. however, because of the underlying linearization, a standard interval algorithm leads to a pessimistic bound, and even simply fails (i.e., returns an infinite error) in case of singularity. we propose an original nonlinear approach where intervals are used in a more sophisticated way through the so-called "springs". this new structure allows to represent an (infinite) set of intervals constrained by their midpoints and their radius. the output error is then calculated with a spring arithmetic in the same way as the image of a function is calculated with interval arithmetic. our method is illustrated on two examples, including an application of geopositioning.
calibration of 3-d.o.f. translational parallel manipulators using leg observations. the paper proposes a novel approach for the geometrical model calibration of quasi-isotropic parallel kinematic mechanisms of the orthoglide family. it is based on the observations of the manipulator leg parallelism during motions between the specific test postures and employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. they are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the tcp along the cartesian axes. using the measured differences, the developed algorithm estimates the joint offsets and the leg lengths that are treated as the most essential parameters. validity of the proposed calibration technique is confirmed by the experimental results.
functional analysis approach for delay systems of neutral type. linear delay systems of neutral type are presented using the hilbert space analysis approach. we consider in particular the problem of asymptotic non-exponential stability and the corresponding property of stabilizability for controlled systems. the main tool is the riesz basis property of eigenspaces. this property is also used for the exact controllability via the moment problem approach.
on a vector moment problem appearing in the analysis of controllability of neutral type systems. we consider the solvability of a vector moment problem associating it to the analysis of controllability for a certain delayed system of neutral type. in this way we succeeded to determine exactly the minimal interval on which the moment problem is solvable.
stability, stabilizability and exact controllability of a class of linear neutral type systems. linear systems of neutral type are considered using the infinite dimensional approach. the main problems are asymptotic, non-exponential stability, exact controllability and regular asymptotic stabilizability. the main tools are the moment problem approach, the riesz basis of invariant subspaces and the riesz basis of family of exponentials.
dealing with constraints during a feature configuration process in a model-driven software product line. null
implementing an mda approach for managing variability in product line construction using the gmf and gme frameworks. null
components with symbolic transition systems: a java implementation of rendez-vous. component-based software engineering is becoming an important approach for system development. a crucial issue is to fill the gap between high-level models, needed for design and verification, and implementation. this paper introduces first a component model with explicit protocols based on symbolic transition systems. it then presents a java implementation for it that relies on a rendezvous mechanism to synchronize events between component protocols. this paper shows how to get a correct implementation of a complex rendezvous in presence of full data types, guarded transitions and, possibly, guarded receipts.
identification of replication-competent hsv-1 cgal+ strain signaling targets in human hepatoma cells by functional organelle proteomics. in the present work, we have attempted a comprehensive analysis of cytosolic and microsomal proteomes to elucidate the signaling pathways impaired in human hepatoma (huh7) cells upon herpes simplex virus type 1 (hsv-1; cgal(+)) infection. using a combination of differential in-gel electrophoresis and nano liquid chromatography/tandem mass spectrometry, 18 spots corresponding to 16 unique deregulated cellular proteins were unambiguously identified, which were involved in the regulation of essential processes such as apoptosis, mrna processing, cellular structure and integrity, signal transduction, and endoplasmic-reticulum-associated degradation pathway. based on our proteomic data and additional functional studies target proteins were identified indicating a late activation of apoptotic pathways in huh7 cells upon hsv-1 cgal(+) infection. additionally to changes on ruvb-like 2 and bif-1, down-regulation of erlin-2 suggests stimulation of ca(2+)-dependent apoptosis. moreover, activation of the mitochondrial apoptotic pathway results from a time-dependent multi-factorial impairment as inferred from the stepwise characterization of constitutive pro- and anti-apoptotic factors. activation of serine-threonine protein phosphatase 2a (pp2a) was also found in huh7 cells upon hsv-1 cgal(+) infection. in addition, pp2a activation paralleled dephosphorylation and inactivation of downstream mitogen-activated protein (map) kinase pathway (mek(1/2), erk(1/2)) critical to cell survival and activation of proapoptotic bad by dephosphorylation of ser-112. taken together, our results provide novel molecular information that contributes to define in detail the apoptotic mechanisms triggered by hsv-1 cgal(+) in the host cell and lead to the implication of pp2a in the transduction of cell death signals and cell survival pathway arrest.
identification of replication-competent hsv-1 cgal(+) strain targets in a mouse model of human hepatocarcinoma xenograft. recent studies based on animal models have shown the advantages and potential of oncolytic viral therapy using hsv-1 -based replication-competent vectors in the treatment of liver tumors, but little is known about the cellular targets that are modulated during viral infection. in the present work, we have studied the effects of intratumoral injections of hsv-1 cgal(+) strain in a murine model of human hepatoma xenografts. viral replication was assessed for more than 1month, leading to a significant reduction of tumor growth rate mediated, in part, by a cyclin b dependent cell proliferation arrest. early events resulting in this effect were analyzed using a proteomic approach. protein extracts from xenografted human hepatomas treated with saline or hsv-1 cgal(+) strain during 24h were compared by 2-d dige and differential spots were identified by nanolc-esi-ms/ms. alterations on glutathione s transferase 1 omega, and erp29 suggest novel hsv-1 cgal(+) targets in solid liver tumors. additionally, erp29 showed a complex differential isoform pattern upon hsv-1 cgal(+) infection, suggesting regulatory mechanisms based on post-translational modification events.
handling persistent states in process checkpoint/restart mechanisms for hpc systems. computer clusters are today the reference architecture for high-performance computing. the large number of nodes in these systems induces a high failure rate. this makes fault tolerance mechanisms, e.g. process checkpoint/restart, a required technology to eﬀectively exploit clusters. most of the process checkpoint/restart implementations only handle volatile states and do not take into account persistent states of applications, which can lead to incoherent application restarts. in this paper, we introduce an eﬃcient persistent state checkpoint/restoration approach that can be interconnected with a large number of ﬁle systems. to avoid the performance issues of a stable support relying on synchronous replication mechanisms, we present a failure resilience scheme optimized for such persistent state checkpointing techniques in a distributed environment. first evaluations of our implementation in the kdfs distributed ﬁle system show the negligible performance impact of our proposal.
a multi-stage approach for reliable dynamic reconfigurations of component-based systems. in this paper we present an end-to-end solution to define and execute reliable dynamic reconfigurations of open component-based systems while guaranteeing their continuity of service. it uses a multi-stage approach in order to deal with the different kinds of possible errors in the most appropriate way; in particular, the goal is to detect errors as early as possible to minimize their impact on the target system. reconfigurations are expressed in a restricted, domain-specific language in order to allow different levels of static and dynamic validation, thus detecting errors before executing the reconfiguration where possible. for errors that can not be detected early (including software and hardware faults), a runtime environment provides transactional semantics to the reconfigurations.
thermal properties of phi mesons and the qcd equation of state. null
turning on the charm. i argue that the strong jet quenching of heavy flavors observed in heavy-ion collisions is to a large extent due to binary scatterings in the quark-gluon plasma. it can be understood from first principles: the charm collision probability beyond logarithmic accuracy and markov evolution.
the micro pattern gas detector pim : a multi-modalities solution for novel investigations in functional imaging. null
a heavy water converter concept for spiral ii. null
nuclear reactor simulations for unveiling diversion scenarios : capabilities of the antineutrino probe. nuclear reactors emit a huge amount of electronic antineutrinos, arising from the fission product decay. reactor antineutrinos thus posess unique features that place them as a potential new safeguards tool for the international atomic energy agency (iaea). indeed, they carry outside the core the direct picture of its isotopic fission rates, thus opening the possibility of a remote, non-intrusive and tamperproof reactor monitoring. sophisticated simulations of reactors and their associated antineutrino flux and energy spectrum have been developed to predict the neutrino signature of the fuel burnup and of a diversion. the only user-defined inputs driving the time evolution of the isotopic composition of the core are the initial fuel composition, the refueling scheme, and the thermal power. the evolution of the antineutrino flux and energy spectrum with the fuel burnup, as well as the effect of neutron capture on various nuclei are taken into account. non-proliferation scenarios and burnup monitoring with antineutrinos have been studied using these tools for pwr and candu reactors. a full core simulation of an n4-pwr will be presented in a first part. gross unveiling diversion scenarios using a pwr have been simulated in order to test the ability of the antineutrino probe. a channel of a heavy water reactor (candu 600) loaded with natural uranium, has been simulated also in order to provide a first hint of what antineutrino detection would bring to the monitoring of such on-line refueled reactor which are maintained in a steady state through quasi-continuous refueling. very simple proliferation scenario studies with candu reactors, based on several channel calculations, made at various fuel dwell-times, will be shown in a second part. in both cases, the response of a nucifer-like detector placed at 25m from the core to these scenarios has been studied.
hydrodynamic evolution : on the role of initial conditions and freeze out (thermal or not). null
gribov-regge theorie, partons, strings – and the epos model for hadronic interactions. null
epos and other. null
hydrodynamics evolution in epos. null
macroscopic of radio emission based on shower simulations and a realistic index of refraction. null
energy losses of ultrarelativistic particles in hot plasmas revisited. null
quark condensate in strong magnetic fields. null
heavy flavour in alice. null
hadronic collisions at the lhc and qcd at high energy loss in a qgp. null
collisional energy loss of a fast parton in a qgp. null
results of the radio-detection experiment codalema. measurements of the radio transients associated to extensive air showers could provide a timely new effective method of detection of ultra high energy cosmic rays with interesting performances in term of techniques and of accessible observables. the codalema experiment at the radio observatory of nançay, france, explores this possibility using an array of broadband active dipole antennas triggered by an array of ground particle detectors. the implemented detection techniques, the methods of analysis developed and the principal results achieved during the 5 years of upgrading will be recalled. this cursory glance will provide an opportunity to point out some new issues and to suggest some future challenges in this domain.
objectives of the carbonwaste project “treatment and disposal of irradiated graphite and other carboneous waste. null
carbonwaste: new euratom project on treatment and disposal of irradiated graphite and other carboneus waste. null
an overview on the current plan of eu for the radioactive wastes management- the role of the french national evaluation commission, the public interactions and the european framework. null
nuclear waste glasses. null
description of heavy ion collisions within the isospin quantum molecular dynamics model. null
radio detection of high energy cosmic ray showers-a short review. null
tomography of the quark gluon plasma. null
interaction of heavy quark with the quark gluon plasma. null
on running coupling constants and infared regulators. null
critical phenomena in heavy ion reactions. null
collisional energy loss of heavy quarks in a quark gluon plasma. null
gas production and activation calculation in megapie. null
high pt and photon physics at lhc with alice. null
the qcd phase transition and the thermal properties of the phi mesons. null
radiodetection of cosmic air showers with autonomous radio detectors installed at the pierre auger observatory. null
generation of complete event containing very high pt jets. null
astrophysical sources of cosmic rays and related measurements with the pierre auger observatory. studies of the correlations of ultra-high energy cosmic ray directions with extra-galactic objects, of general anisotropy, of photons and neutrinos, and of other astrophysical effects, with the pierre auger observatory. contributions to the 31st icrc, lodz, poland, july 2009.
calibration and monitoring of the pierre auger observatory. reports on the atmospheric monitoring, calibration, and other operating systems of the pierre auger observatory. contributions to the 31st international cosmic ray conference, lodz, poland, july 2009.
operations of and future plans for the pierre auger observatory. technical reports on operations and features of the pierre auger observatory, including ongoing and planned enhancements and the status of the future northern hemisphere portion of the observatory. contributions to the 31st international cosmic ray conference, lodz, poland, july 2009.
expressive scoping of distributed aspects. dynamic deployment of aspects brings greater flexibility and reuse potential, but requires proper means for scoping aspects. scoping issues are particularly crucial in a distributed context: adequate treatment of distributed scoping is necessary to enable the propagation of aspect instances across host boundaries and to avoid inconsistencies due to unintentional spreading of data and computations in a distributed system. we motivate the need for expressive scoping of dynamically-deployed distributed aspects by an analysis of the deficiencies of current approaches for distributed aspects. extending recent work on deployment strategies for non-distributed aspects, we then introduce a set of high-level strategies for specifying locality of aspect propagation and activation, and illustrate the corresponding gain in expressiveness. we present the operational semantics of our proposal using scheme interpreters, first introducing a model of distributed aspects that covers the range of current proposals, and then extending it with dynamic aspect deployment. this work shows that, given some extensions to their original execution model, deployment strategies are directly applicable to the expressive scoping of distributed aspects.
studies of cosmic ray composition and air shower structure with the pierre auger observatory. studies of the composition of the highest energy cosmic rays with the pierre auger observatory, including examination of hadronic physics effects on the structure of extensive air showers. submissions to the 31st icrc, lodz, poland (july 2009).
the cosmic ray energy spectrum and related measurements with the pierre auger observatory. studies of the cosmic ray energy spectrum at the highest energies with the pierre auger observatory.
study of 2,9-dicarboxy-1, 10-phenanthroline as potential chelate for the synthesis of 230u-labeled radioconjugates. null
a simulation approach to describe bi-213 radiolabelling on chx-dtpa-igg conjugate. null
arronax, a high energy and high intensity cyclotron for nuclear medicine. null
xt-ads spallation target proof of feasibility. null
cyclotron arronax. null
preformation of clusters in heavy nuclei and cluster radioactivity. within the preformed cluster model approach, the values of the preformation factors have been deduced from the experimental cluster decay half-lives assuming that the decay constant of the heavy-ion emission is the product of the assault frequency, the preformation factor and the penetrability. the law according to which the preformation factors follow a simple dependence on the mass of the cluster was confirmed. then predictions for some most possible cluster decays are provided.
identified particle production, azimuthal anisotropy, and interferometry measurements in au+au collisions at $\sqrt{s_{nn}}$ = 9.2 gev. we present the first measurements of identified hadron production, azimuthal anisotropy, and pion interferometry from au+au collisions below the nominal injection energy at the relativistic heavy-ion collider (rhic) facility. the data were collected using the large acceptance star detector at $\sqrt{s_{nn}}$ = 9.2 gev from a test run of the collider in the year 2008. midrapidity results on multiplicity density (dn/dy) in rapidity (y), average transverse momentum (), particle ratios, elliptic flow, and hbt radii are consistent with the corresponding results at similar $\sqrt{s_{nn}}$ from fixed target experiments. directed flow measurements are presented for both midrapidity and forward rapidity regions. furthermore the collision centrality dependence of identified particle dn/dy, , and particle ratios are discussed. these results also demonstrate the readiness of the star detector to undertake the proposed qcd critical point search and the exploration of the qcd phase diagram at rhic.
refinement proposal of the goldberg's theory. virtual machines (vm) allow the execution of various operating systems and provide several functionalities which are nowadays strongly appreciated by developers and administrators (isolation between applications, flexibility of resource management, and so on). as a direct consequence, ``virtualization'' has become a buzz word and a lot of ``virtualization'' solutions have been proposed, each providing particular functionalities. goldberg proposed to classify virtualization techniques in two models (type-i and type-ii), which does not enable the classification of latest ``virtualizations'' technologies such abstraction, emulation, partitioning and so on. in this document, we propose an extension of the goldberg model in order to take into account and formaly define latest ``virtualization'' mechanisms. after giving general definitions, we show how our proposal enables to rigorously formalize the following terms: virtualization, emulation, abstraction, partitioning, and identity. we also demonstrate that a single virtualization solution is generally composed by several layers of virtualization capabilities, depending on the granularity of the analysis.
challenge of assessing long-term performance of nuclear matrices in repository near-field environment- insights from the nf-pro and micado projects. null
carbonwaste wp6 disposal behaviour of i-graphite and other carbonaceous waste. null
the backend of the fuel cycle of hrt/vhrt reactors. null
monte carlo simulations in the context of reactor monitoring with antineutrinos. null
nuclear reactor and spectra simulations for unveiling diversion scenarios. null
selected effects of the in-medium nn cross section on heavy-ion dynamics below 100 mev/u. null
bimodality - a general feature of heavy ion reactions. recently, is has been observed that events with the {\it same} total transverse energy of light charged particles (lcp) in the quasi target region, $e_{\perp 12}^{qt}$, show two quite distinct reaction scenarios in the projectile domain: multifragmentation and residue production. this phenomenon has been dubbed "bimodality". using quantum molecular dynamics calculations we demonstrate that this observation is very general. it appears in collisions of all symmetric systems larger than ca and at beam energies between 50 a.mev and 600 a.mev and is due to large fluctuations of the impact parameter for a given $e_{\perp 12}^{qt}$. investigating in detail the $e_{\perp 12}^{qt}$ bin in which both scenarios are present, we find that neither the average fragment momenta nor the average transverse and longitudinal energies of fragments show the behavior expected from a system in statistical equilibrium, in experiment as well as in qmd simulations. on the contrary, the experimental as well as the theoretical results point towards a fast process. this observation questions the conjecture that the observed bimodality is due to the coexistence of 2 phases at a given temperature in finite systems.
complementarity between virtualization and single system image technologies. nowadays, the use of clusters in research centers or industries is undeniable. since few years, the usage of virtual machines (vm) offers more advanced resource management capabilities, using features such as virtual machine live migration. because of the latest contributions in the domain, some may argue that single system image (ssi) technologies are now deprecated, without considering some complementarities between vms and ssi technologies are possible. after evaluating different configurations, we show that combining both approaches allows us to better address cluster challenges such as flexibility for the usage of available resources and simplicity of use. in other terms, the study shows that vms add a level of management flexibility between the hardware and the application, whereas, ssis give an abstraction of the distributed resources. the simultaneous usage of both technologies could improve the overall platform resources utilization, the cluster productivity and the efficiency of the running applications.
debugging and testing middleware with aspect-based control-flow and causal patterns. many tasks that involve the dynamic manipulation of mid- dleware and large-scale distributed applications, such as debugging and testing, require the monitoring of intricate relationships of execution events that trigger modiﬁcations to the executing system. furthermore, events often are of interest only if they occur as part of speciﬁc execu- tion traces and not all possible non-deterministic interleavings of events in these traces. current techniques and tools for the deﬁnition of such manipulations provide only very limited support for such event relation- ships and do not allow to concisely deﬁne restrictions on the interleaving of events. in this paper, we argue for the use of high-level programming abstractions for the deﬁnition of relationships between execution events of distributed systems and the control of non-deterministic interleavings of events. con- cretely, we provide the following contributions: we (i) motivate that such abstractions improve on current debugging and testing methods for mid- dleware, (ii) introduce corresponding language mechanisms as well as corresponding implementation support by extending an existing aspect- oriented system for the dynamic manipulation of distributed systems, and (iii) evaluate our approach in the context of the debugging and test- ing of jboss cache, a java-based middleware for replicated caching.
specialized aspect languages preserving classes of properties. null
aspect preserving properties. null
kinematic calibration of orthoglide-type mechanisms from observation of parallel leg motions. the paper proposes a new calibration method for parallel manipulators that allows efficient identification of the joint offsets using observations of the manipulator leg parallelism with respect to the base surface. the method employs a simple and low-cost measuring system, which evaluates deviation of the leg location during motions that are assumed to preserve the leg parallelism for the nominal values of the manipulator parameters. using the measured deviations, the developed algorithm estimates the joint offsets that are treated as the most essential parameters to be identified. the validity of the proposed calibration method and efficiency of the developed numerical algorithms are confirmed by experimental results. the sensitivity of the measurement methods and the calibration accuracy are also studied.
stiffness analysis of multi-chain parallel robotic systems. the paper presents a new stiffness modelling method for multi-chain parallel robotic manipulators with flexible links and compliant actuating joints. in contrast to other works, the method involves a fea-based link stiffness evaluation and employs a new solution strategy of the kinetostatic equations, which allows computing the stiffness matrix for singular postures and to take into account influence of the internal forces. the advantages of the developed technique are confirmed by application examples, which deal with stiffness analysis of the orthoglide manipulator.
final results of the tests on the resistive plate chambers for the alice muon arm. the trigger for the alice muon spectrometer will be issued by single-gap, low resistivity bakelite resistive plate chambers (rpcs). the trigger system consists of four 5.5x6.5 m2 rpc planes arranged in two stations, for a total of 72 detectors. one hundred and sixteen detectors have been assembled and tested in torino. the tests have been performed with the streamer mixture developed for heavy ion data-taking. the tests include: the detection of gas leaks and parasitic currents; the measurement of the efficiency with cosmic rays, with particular regard to the uniformity of the efficiency throughout the whole active surface, with a granularity of about 2x2 cm2; the measurement of the dark current and of the mean and localised noise rate. all the rpcs produced have been characterised. among them, the detectors to be finally installed in alice and some spare have been selected; 17% of all the produced detectors have been discarded. a short description of the test set-up is given. the results of the tests are presented, with particular regard to the performance of the selected detectors.
ample : supporting product line engineering through synthesis of aspect-oriented and model-driven development. null
event strictness for components with complex bindings. null
product derivation in a model-driven software product line using decision models. null
dealing with fine-grained configurations in model-driven spls. null
the stslib project: towards a formal component model based on sts. null
the ample project, traceability of software product line development: models and uncertainty. null
an event-based coordination model for context-aware applications. context-aware applications adapt their behavior depending on changes in their environment context. programming such applications in a modular way requires to modularize the global context into more speciﬁc contexts and attach speciﬁc behavior to these contexts. this is reminiscent of aspects and has led to the notion of context-aware aspects. this paper revisits this notion of context-aware aspects in the light of previous work on concurrent event-based aspect-oriented programming (ceaop). it shows how ceaop can be extended in a seamless way in order to deﬁne a model for the coordination of concurrent adaptation rules with explicit contexts. this makes it possible to reason about the compositions of such rules. the model is concretized into a prototypical modeling language.
exact controllability of linear neutral type systems by the moment problem approach. the problem of exact null-controllability is considered for a wide class of linear neutral type systems with distributed delay. the main tool of the analysis is the application of the moment problem approach and the theory of the basis property of exponential families. a complete characterization of this problem is given. the minimal time of controllability is specified. the results are based on the analysis of the riesz basis property of eigenspaces of the neutral type systems in hilbert space.
design optimization of parallel manipulators for high-speed precision machining applications. the paper proposes an integrated approach to the design optimization of parallel manipulators, which is based on the concept of the workspace grid and utilizes the goal-attainment formulation for the global optimization. to combine the non-homogenous design specification, the developed optimization technique transforms all constraints and objectives into similar performance indices related to the maximum size of the prescribed shape workspace. this transformation is based on the dedicated dynamic programming procedures that satisfy computational requirements of modern cad. efficiency of the developed technique is demonstrated via two case studies that deal with optimization of the kinematical and stiffness performances for parallel manipulators of the orthoglide family.
accuracy improvement for stiffness modeling of parallel manipulators. the paper focuses on the accuracy improvement of stiffness models for parallel manipulators, which are employed in high-speed precision machining. it is based on the integrated methodology that combines analytical and numerical techniques and deals with multidimensional lumped-parameter models of the links. the latter replace the link flexibility by localized 6-dof virtual springs describing both translational/rotational compliance and the coupling between them. there is presented detailed accuracy analysis of the stiffness identification procedures employed in the commercial cad systems (including statistical analysis of round-off errors, evaluating the confidence intervals for stiffness matrices). the efficiency of the developed technique is confirmed by application examples, which deal with stiffness analysis of translational parallel manipulators.
structured lft representation of digital lti filters/controllers implementation as a graphic tool. as a powerful method in modeling parametric uncertainties, the linear fractional transformation (lft) scheme in this paper is extended to digital implementation of lti filters/controllers. the convenient coordinate transformation and modification of digital operators can be easily represented under the proposed lft interpretation. moreover, a specialized descriptor system is deduced from this graphic representation, and a systematic procedure of implementation is presented. an example is also provided to illustrate this approach.
a practical strategy of an efficient and simple fwl implementation of lti filters. the problem of finite word length implementation is discussed in this paper. alternatively to the rhodfiit recently proposed by g. li et al., and leaning on the specialized implicit form for a unified analysis, a new effective and sparse structure, named rho-modal realization, is developed. this realization meets simultaneously accuracy (low sensitivity, round-off noise gain and overflow risk), few and flexible computational efforts with a good readability (owing to sparsity), and simplicity (no tricky optimization is involved) as well. two numerical examples are presented to confirm the theoretical results and illustrate the rho-modal realization interest.
k/$\pi$ fluctuations at relativistic energies. we report k/$\pi$ fluctuations from au+au collisions at $\sqrt{s_{nn}}$=19.6, 62.4, 130, and 200 gev using the star detector at the relativistic heavy ion collider. k/$\pi$ fluctuations in central collisions show little dependence on incident energy and are on the same order as those from na49 at the super proton synchrotron in central pb+pb collisions at $\sqrt{s_{nn}}$=12.3 and 17.3 gev. we report results for the collision centrality dependence of k/$\pi$ fluctuations and results for charge-separated fluctuations. we observe that the k/$\pi$ fluctuations scale with the charged particle multiplicity density.
using transformation-aspects in model-driven software product lines. model-driven software product lines (md-spl) are configured by using configuration models and problem space metamodels that capture product line scope. products are derived by means of successive model transformations, starting from problem space models and based on the configuration models. fine-variations of md-spls correspond to characteristics that afect particular elements of models involved in the model transformations. in this paper, we present an approach to create md-spl including fine-variations. we configure products creating fine-feature configurations. then, based on such configurations, we create md-spls using principles of aspects oriented development. thus, our approach allows to derive products including fine-grained details of configuration.
on the optimal design of parallel robots taking into account their deformations and natural frequencies. this paper discusses the utility of using simple stiffness and vibrations models, based on the jacobian matrix of a manipulator and only the rigidity of the actuators, whenever its geometry is optimised. in many works, these simplified models are used to propose optimal design of robots. however, the elasticity of the drive system is often negligible in comparison with the elasticity of the elements, especially in applications where high dynamic performances are needed. therefore, the use of such a simplified model may lead to the creation of robots with long legs, which will be submitted to large bending and twisting deformations. this paper presents an example of manipulator for which it is preferable to use a complete stiffness or vibration model to obtain the most suitable design and shows that the use of simplified models can lead to mechanisms with poorer rigidity.
the quest for the nuclear equation of state. the present status of the efforts to determine the nuclear equation of state from results of heavy ion reactions and from astrophysical observations is reviewed.
elliptic flow of thermal photons at midrapidity in au+au collisions at $\sqrt{s_{nn}}=200$ gev. the elliptic flow $v_{2}$ of thermal photons at midrapidity in au+au collisions at $\sqrt{s_{nn}}=200$gev is predicted, based on three-dimensional ideal hydrodynamics. because of the interplay between the asymmetry and the strength of the transverse flow, the thermal photon $v_{2}$ reaches a maximum at $\pt \sim $ 2gev/$c$ and the $\pt$-integrated $v_{2}$ reaches a maximum at about 50% centrality. the $\pt$-integrated $v_{2}$ is very sensitive to the lower limit of the integral but not sensitive to the upper limit due to the rapid decrease in the spectrum of the transverse momentum.
on the role of initial conditions and final state interactions in ultrarelativistic heavy ion collisions. we investigate the rapidity dependence of the elliptical flow in heavy ion collisions at 200 gev (cms), by employing a three-dimensional hydrodynamic evolution, based on different initial conditions, and different freeze-out scenarios. it will be shown that the form of pseudo-rapidity ($\eta$) dependence of the elliptical flow is almost identical to space-time-rapidity ($\eta_{s}$) dependence of the initial energy distribution, independent of the freeze-out prescriptions.
energy loss of heavy quarks in a qgp with a running coupling constant approach. we show that the effective running coupling constant, $\alpha_{\rm eff}$, and the effective regulator, $\kappa \tilde{m}_{d}^2$, which we used recently to calculate the energy loss, $\frac{de}{dx}$, and the elliptic flow, $v_2$, of heavy quarks in an expanding quark gluon plasma plasma (qgp) are compatible with lattice results and with recently advanced analytical pqcd calculation.
the fluorescence detector of the pierre auger observatory. the pierre auger observatory is a hybrid detector for ultra-high energy cosmic rays. it combines a surface array to measure secondary particles at ground level together with a fluorescence detector to measure the development of air showers in the atmosphere above the array. the fluorescence detector comprises 24 large telescopes specialized for measuring the nitrogen fluorescence caused by charged particles of cosmic ray air showers. in this paper we describe the components of the fluorescence detector including its optical system, the design of the camera, the electronics, and the systems for relative and absolute calibration. we also discuss the operation and the monitoring of the detector. finally, we evaluate the detector performance and precision of shower reconstructions.
a memetic algorithm for the multi-compartment vehicle routing problem with stochastic demands. null
alpha-decay half-lives of new superheavy elements through quasimolecular shapes. null
search for a long-lived di-nuclear system in u+u reactions near the coulomb barrier. null
measurements of phi meson production in relativistic heavy-ion collisions at the bnl relativistic heavy ion collider (rhic). we present results for the measurement of $\phi$ meson production via its charged kaon decay channel $\phi$ -&gt; k+k- in au+au collisions at $\sqrt{s_{nn}}$=62.4,130, and 200 gev, and in p+p and d+au collisions at $\sqrt{s_{nn}}$=200 gev from the star experiment at the bnl relativistic heavy ion collider (rhic). the midrapidity (|y|&lt;0.5)$\phi$ meson transverse momentum (pt) spectra in central au+au collisions are found to be well described by a single exponential distribution. on the other hand, the pt spectra from p+p, d+au, and peripheral au+au collisions show power-law tails at intermediate and high pt and are described better by levy distributions. the constant $\phi$/k- yield ratio vs beam species, collision centrality, and colliding energy is in contradiction with expectations from models having kaon coalescence as the dominant mechanism for $\phi$ production at rhic. the $\omega/\phi$ yield ratio as a function of pt is consistent with a model based on the recombination of thermal s quarks up to pt~4 gev/c, but disagrees at higher transverse momenta. the measured nuclear modification factor, rdau, for the $\phi$ meson increases above unity at intermediate pt, similar to that for pions and protons, while raa is suppressed due to the energy loss effect in central au+au collisions. number of constituent quark scaling of both rcp and v2 for the $\phi$ meson with respect to other hadrons in au+au collisions at $\sqrt{s_{nn}}$=200 gev at intermediate pt is observed. these observations support quark coalescence as being the dominant mechanism of hadronization in the intermediate pt region at rhic.
development of a radio detection array for the observation of showers induced by uhe tau neutrinos. development of a radio detection array for the observation of showers induced by uhe tau neutrinos.
lazy composition of representations in java. abstract. the separation of concerns has been a core idiom of software engineering for decades. in general, software can be decomposed properly only according to a single concern, other concerns crosscut the prevailing one. this problem is well known as “the tyranny of the dominant decomposition”. similarly, at the programming level, the choice of a representation drives the implementation of the algorithms. this article explores an alternative approach with no dominant representation. instead, each algorithm is developed in its “natural” representation and a representation is converted into another one only when it is required. to support this approach, we designed a laziness framework for java, that performs partial conversions and dynamic optimizations while preserving the execution soundness. performance evaluations over graph theory examples demonstrates this approach provides a practicable alternative to a naive one.
a new lower bound for bin packing problem with general conflicts graph. null
a model-driven engineering framework for constrained model search. this document describes a formalization, a solver-independant methodology and implementation alternatives for realizing constrained model search in a model-driven engineering framework. the proposed approach combines model-driven engineering tools ((meta)model transformations, models to text, text to models) and constraint programming techniques. based on previous research, motivations to model search are first introduced together with objectives and background context. a theory of model search is then presented, and a methodology is proposed that details the different involved tasks. concerning implementation, three constraint programming paradigms are envisionned and discussed. an open-source implementation based on the relationnal language alloy is described and available for download.
atmospheric effects on extensive air showers observed with the surface detector of the pierre auger observatory. atmospheric parameters, such as pressure (p), temperature (t) and density, affect the development of extensive air showers initiated by energetic cosmic rays. we have studied the impact of atmospheric variations on extensive air showers by means of the surface detector of the pierre auger observatory. the rate of events shows a ~10% seasonal modulation and ~2% diurnal one. we find that the observed behaviour is explained by a model including the effects associated with the variations of pressure and density. the former affects the longitudinal development of air showers while the latter influences the moliere radius and hence the lateral distribution of the shower particles. the model is validated with full simulations of extensive air showers using atmospheric profiles measured at the site of the pierre auger observatory.
comments on thermodynamics of supersymmetric matrix models. we present arguments that the structure of the spectrum of the supersymmetric matrix model with 16 real supercharges in the large n limit is rather nontrivial, involving besides the natural energy scale ~\lambda^{1/3} = (g^2 n)^{1/3} also a lower scale ~\lambda^{1/3}n^{-5/9}. this allows one to understand a nontrivial behaviour of the mean internal energy of the system, e proportional to t^{14/5}, predicted by ads duality arguments.
measurement of d* mesons in jets from p+p collisions at $\sqrt{s}$=200 gev. we report the measurement of charged d* mesons in inclusive jets produced in proton-proton collisions at a center-of-mass energy sqrt(s)=200 gev with the star experiment at the relativistic heavy ion collider. for d* mesons with fractional momenta 0.2.
sequencing and counting with the multicost-regular constraint. this paper introduces a global constraint encapsulating a regular constraint together with several cumulative costs. it is motivated in the context of personnel scheduling problems, where a schedule meets patterns and occurrence requirements which are intricately bound. the optimization problem underlying the multicost-regular constraint is np-hard but it admits an efficient lagrangian relaxation. hence, we propose a filtering based on this relaxation. the expressiveness and the efficiency of this new constraint is experimented on personnel scheduling benchmark instances with standard work regulations. the comparative empirical results show how multicost-regular can significantly outperform a decomposed model with regular and global-cardinality constraints.
the qcd equation of state with thermal properties of phi mesons. in this work a first attempt is made to extract the equation of state (eos) using experimental results of the meson produced in nuclear collisions at ags, sps and rhic energies. the data are confronted to simple thermodynamic expectations and lattice results. the experimental data indicate a first-order phase transition, with a mixed phase stretching energy density between ~1and3.2gevfm−3.
proton radioactivity within a generalized liquid drop model. the proton radioactivity half-lives of spherical proton emitters are investigated theoretically. the potential barriers preventing the emission of protons are determined in the quasimolecular shape path within a generalized liquid drop model (gldm) including the proximity effects between nuclei in a neck and the mass and charge asymmetry. the penetrability is calculated with the wkb approximation. the spectroscopic factor has been taken into account in half-life calculation, which is obtained by employing the relativistic mean field (rmf) theory combined with the bcs method with the force nl3. the half-lives within the gldm are compared with the experimental data and other theoretical values. the gldm works quite well for spherical proton emitters when the spectroscopic factors are considered, indicating the necessity of introducing the spectroscopic factor and the success of the gldm for proton emission. finally, we present two formulas for proton emission half-life calculation similar to the viola-seaborg formulas and royer's formulas of alpha decay.
heavy-quark energy loss observed via muon spectra in pb-pb collisions at root s(nn)=5.5 tev. based on conesa del valle et al (2008 phys. lett. b 663 202), we present the results for the nuclear modification factors raa and rcp of the high transverse momentum (5 &lt; pt &lt; 60 gev/c) distribution of muons in pb–pb collisions at lhc energies, in which two pseudo-rapidity ranges covered by the lhc experiments, |η| &lt; 2.5 and 2.5 &lt; η &lt; 4, are investigated. heavy-quark energy loss is taken into account by mass-dependent bdmps quenching weights. muons from semi-leptonic decays of heavy quarks (c and b) and from leptonic decays of weak gauge bosons (w and z) are the main contributions to the muon pt distribution above a few gev/c. the muons from w and z, that dominates the high pt, can be used as a medium-blind reference to observe the medium-induced suppression of beauty quarks.
tomography of the quark gluon plasma by heavy quarks. using the recently published model \cite{gossiaux:2008jv,goss2} for the collisional energy loss of heavy quarks in a quark gluon plasma (qgp), based on perturbative qcd (pqcd), we study the centrality dependence of $r_{aa}$ and $r_{aa}(p_t^{min})$, %= \frac{dn_{aa}/dp_t}{ dn_{pp}/dp_t}$ measured by the phenix collaboration, and compare our model with other approaches based on pqcd and on anti de sitter/ conformal field theory (ads/cft).
kaon interferometric probes of space-time evolution in au+au collisions at sqrt(s_nn) = 200 gev. bose-einstein correlations of charged kaons are measured for au+au collisions at sqrt(s_nn) = 200 gev and are compared to charged pion probes, which have a larger hadronic scattering cross section. three dimensional gaussian source radii are extracted, along with a one-dimensional kaon emission source function. the centrality dependences of the three gaussian radii are well described by a single linear function if n_part^1/3 with zero intercept. imaging analysis shows a deviation from a gaussian tail at r &gt;~ 10 fm, although the bulk emission at lower radius is well-described by a gaussian. the presence of a non-gaussian tail in the kaon source reaffirms that the particle emission region in a heavy ion collision is extended, and that similar measurements with pions are not solely due to the decay of long-lived resonances.
eu(iii) sorption to tio2 (anatase and rutile): batch, xps, and exafs studies. the sorption of eu(iii) on anatase and rutile was studied as a function of ionic strength, humic acid (ha, 7.5 mg/l), and electrolyte anions over a large range of ph (2−12). the presence of ha significantly affected eu(iii) sorption to anatase and rutile. the sorption of eu(iii) on anatase and rutile was independent of ionic strength. results of an x-ray photoelectron spectroscopy (xps) analysis showed that eu(iii) was chemically present within the near-surface of tio2 due to the formation of soeu and sohaeu complexes. an extended x-ray absorption fine structure (exafs) technique was applied to characterize the local structural environment of the adsorbed eu(iii), and the results indicated that eu(iii) was bound to about seven or eight o atoms at a distance of about 2.40 å. the functional groups of surface-bound ha were expected to be involved in the sorption process. the measured eu−ti distance confirmed the formation of inner-sphere sorption complexes on a tio2 surface.
kinematic control of a robot-positioner system for arc welding application. null
combined routing and staff scheduling model for home health care. the combined routing and scheduling problem has become a relevant issue in health care logistics as demand for cost effective and quality health services rises. the main challenge of this problem is to combine aspects of routing and staff scheduling. both composing problems are well known as combinatorial optimization problems and there is an extensive literature of exact and heuristic methods for each of them. although combined routing and scheduling problems arise in many applications, only few works have been developed in the field of home health care logistics. we consider a set of patients with different geographical locations who require different medical treatments composed by a list of medical procedures. for each patient, the duration, frequency and hours in which the medical treatment must be applied are known. each medical procedure must be performed by a medical staff who has the adequate qualifications. medical staff professionals are also located in different geographical points, and each staff member is assigned to work shifts while respecting work legal guidelines. the problem that arises in this system is scheduling the activities of each staff member and sequencing visits to patients such that overall logistic costs are minimized and a high quality service is performed. desired conditions that must be satisfied in the system include the accomplishment of customer medical treatments in terms of time windows, and precedence and synchronization constraints of medical procedures. in terms of medical staff, work shifts must be respected, geographical location must be considered, assignment of visits must respect required qualifications, and balanced work loads are expected. the overall objective of this work is to propose a formulation of the combined routing and scheduling problem in the context of the home health care services. we propose a mixed integer linear program and demonstrate the high complexity of the resulting problem through computational experiments. research and improvement opportunities in the field are also detailed.
a tabu search based heuristic for the transportation of mentally disabled people. transportation of mentally disabled persons is subject to many specific constraints due to medical or organisational reasons. we consider the problem of the daily transportation of persons from their home to specialized medical or social institutions. this concerns either mentally disabled children who attend specialised schools, or adults working in vocational rehabilitation centres. most of them are unable to travel on their own, so that a dedicated transportation system must be managed by the centres. daily inbound and return trips for dozens of persons are generally performed by costly taxis or minibuses. we model the underlying optimisation problem as a multi-periodic vehicle routing problem with time windows, heterogeneous fleet and additional constraints. the main characteristic of the model is to mix closed routes (that form loops from the centre) and open routes from the driver's home to the centre. we propose two objective functions aimed at minimising the transportation cost or finding a trade-off between the cost and the quality of service respectively. our approach for solving this problem efficiently and rapidly is a three phase heuristic. the first phase addresses a so called standard problem where every person is located at their main address with the most classical time window. initial routes are built through a best insertion procedure based on a regret calculation. the second phase is a tabu search that combines several classical moves of the literature: customer relocation, customer exchange, cross-exchange, string exchange etc. the third step considers the real daily demand of every person. this step integrates possible daily variations in the demand: possible absence, variations in the time windows, daily changes in the addresses (pick-up or delivery at the parents, home family, nurse, etc.). the algorithm has been embedded in a decision support system called marika. this decision support system seeks both technical efficiency and the satisfaction of human concerns. we present numerical results obtained from both test instances from the vrp literature and real instances. we show that the average of the individual travelling times as well as the total travelling time of the vehicles can be reduced by around 15% for real instances. we finally enumerate some research perspectives. a first improvement is to obtain consistent schedules for every person. the idea is that daily variations in the demand of some transported persons should have limited impact on the schedules of other persons. this issue is important when applied to a fragile population. a second objective is to look for a trade-off between the cost, the quality and the consistency of service through a multi-objective optimization. the last project research concerns the possibility for distinct centres to organize common transportation services in order to reduce the costs.
binary reaction decays from $^{24}$mg+$^{12}c. charged particle and gamma decays in 24mg* are investigated for excitation energies where quasimolecular resonances appear in 12c+12c collisions. various theoretical predictions for the occurence of superdeformed and hyperdeformed bands associated with resonance structures with low spin are discussed within the measured 24mg* excitation energy region. the inverse kinematics reaction 24mg+12c is studied at e_lab(24mg) = 130 mev, an energy which enables the population of 24mg states decaying into 12c+12c resonant break-up states. exclusive data were collected with the binary reaction spectrometer in coincidence with euroball iv installed at the vivitron tandem facility at strasbourg. specific structures with large deformation were selectively populated in binary reactions and their associated gamma decays studied. coincident events associated with inelastic and alpha-transfer channels have been selected by choosing the excitation energy or the entry point via the two-body q-values. the analysis of the binary reaction channels is presented with a particular emphasis on 24mg-gamma, 20ne-gamma and 16o-gamma coincidences. new information (spin and branching ratios) is deduced on high-energy states in 24mg and 16o, respectively.
neutron correlations in 6he viewed through nuclear break-up. the nuclear break-up of 6he on a 208pb target was studied at 20amev using a secondary beam of 6he produced by the spiral facility at ganil. α-particles were detected in coincidence with two neutrons with a large angular coverage and the reaction mechanism was identified. from the distribution of the relative angles between the two neutrons the correlation function was extracted. it shows a strong correlation at small relative angles attributed to the contribution of the di-neutron configuration of 6he.
epos model and ultra high energy cosmic rays. interpretation of extensive air showers (eas) experiments results is strongly based on air shower simulations. the latter being based on hadronic interaction models, any new model can help for the understanding of the nature of cosmic rays. the epos model reproducing all major results of existing accelerator data (including detailed data of rhic experiments) has been introduced in air shower simulation programs corsika and conex few years ago. the new epos 1.99 has recently been updated taking into account the problem seen in eas development using epos 1.61. we will show in details the relationship between some epos hadronic properties and eas development, as well as the consequences on the model and finally on cosmic ray analysis.
systematic studies of elliptic flow measurements in au+au collisions at sqrt(s_nn) = 200 gev. we present inclusive charged hadron elliptic flow v_2 measured over the pseudorapidity range |\eta| &lt; 0.35 in au+au collisions at sqrt(s_nn) = 200 gev. results for v_2 are presented over a broad range of transverse momentum (p_t = 0.2-8.0 gev/c) and centrality (0-60%). in order to study non-flow effects that are not correlated with the reaction plane, as well as the fluctuations of v_2, we compare two different analysis methods: (1) event plane method from two independent sub-detectors at forward (|\eta| = 3.1-3.9) and beam (|\eta| &gt; 6.5) pseudorapidities and (2) two-particle cumulant method extracted using correlations between particles detected at midrapidity. the two event-plane results are consistent within systematic uncertainties over the measured p_t and in centrality 0-40%. there is at most 20% difference of the v_2 between the two event plane methods in peripheral (40-60%) collisions. the comparisons between the two-particle cumulant results and the standard event plane measurements are discussed.
determination of stability constants between complexing agents and at(i) and at(iii) species present at ultra-trace concentrations. a competition method is proposed to determine the complexation constants between at(i) and at(iii) species and complexing agents. the method, tested with an inorganic ligand, thiocyanate ion (scn−), and an organic macromolecule, thiacalix[4]arenetetrasulfonate (lh4) is based on solid/liquid separation or liquid/liquid extraction. for the solid/liquid separation, the cationic exchanger dowex 50x8 was used. the interaction of at(i) and at(iii) with the cationic exchanger is specific but could not be described by the expected cation exchange process. most probably, at(i, iii) interacts with a “strong” site (in weak amount) to form a surface complex at the surface of the resin organic skeleton. for the liquid/liquid separation, chloroform, toluene and hexane were used. all solvents extract astatine species with distribution coefficients varying between 0.7 and 120. the extraction process was shown to be independent of aqueous phase characteristics (ph, ionic strength) and was explained by the solvation of astatine species by the organic solvent. the effect of the addition of the thiacalix[4]arenetetrasulfonate on the solid/liquid or liquid/liquid distribution coefficients could be well described by the formation of a 1:1 complex with stability constants of log β1 = 4.5 ± 0.4 and 3.3 ± 0.3 for at(i) and at(iii), respectively. for the thiocyanate ion, the data measured in the presence of the organic solvents could be explained by the formation of both 1:1 and 1:2 at:scn complexes. in the case of the solid/liquid separation, data analysis was hampered by the probable formation of a ternary complex between at(i, iii), scn− and the functional groups of the resin. as for the calixarene, the interaction strength appeared slightly higher for at(i) (log β2 = 5.9 ± 0.3 and log β1 = 3.8 ± 0.2 for 1:2 and 1:1 complexed species, respectively) than for at(iii) (log β2 = 5.3 ± 0.2 and log β1 = 2.8 ± 0.2 for 1:2 and 1:1 complexed species, respectively).
extraction of astatine-211 in diisopropylether (dipe). the extraction mechanism of astatine-211 in diisopropylether (dipe) is studied by analyzing the effect of the nature of the aqueous solution and organic solvent on the distribution coefficients characterizing the ratio of organic and aqueous concentration of astatine (d). on the one hand, at a given ph value, d was shown to be independant of the nature and concentration of the counter-ion present in the aqueous solution (cl-, clo4-, no3-). on the other hand, the nature of the organic solvent had a strong effect. d increases as the polarity of the organic solvent increases. these experimental observations are explained by the solvation of astatine by the organic solvent. the back-extraction of astatine from the organic to the aqueous phase is efficient in basic conditions. this is explained by the formation of a hydrolyzed species of astatine presenting no affinity for dipe. hydrolysis constants of astatine are deduced from experimental data (logβ1=-3.0±0.1 and logβ2=-14.2±0.1).
making bound consistency as effective as arc consistency. we study under what conditions bound consistency (bc) and arc consistency (ac), two forms of propagation used in constraint solvers, are equivalent to each other. we show that they prune exactly the same values when the propagated constraint is connected row convex / closed under median and its complement is row convex. this characterization is exact for binary constraints. since row convexity depends on the order of the values in the domains, we give polynomial algorithms for computing orders under which bc and ac are equivalent, if any.
growth of long range forward-backward multiplicity correlations with centrality in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. forward-backward multiplicity correlation strengths have been measured for the first time with the star detector for au+au and $\textit{p+p}$ collisions at $\sqrt{s_{nn}}$ = 200 gev. strong short and long range correlations are seen in central (0-10%) au+au collisions. the magnitude of these correlations decrease with decreasing centrality until only short range correlations are observed in 40-50% au+au collisions. the results are in agreement with predictions from the dual parton and color glass condensate models.
in-target yields forradioactive ion beam (rib) production with eurisol. eurisol ds (europe isotope separation on-line design study) project is the european common effort in planning a next generation rib factory able to deliver secondary beams up to 1013 pps at energies up to 150 mev u-1. the proposed schematic layout of the facility is based on four target stations, three direct targets of 100 kw of beam power and one multi-mw target two stages assembly. being produced via spallation the ribs produced in the direct targets are mainly proton rich. while in the multi-mw target high intensity ribs of neutron rich isotopes are produced by fission in actinide targets placed in the fast neutron spectrum given by a liquid metal spallation source. the purpose of this paper is to summarize the work carried out within task 11 “beam intensity calculations” with special emphasis to the estimation of the in-target yield intensities produced in the various target configurations. benchmark studies were performed initially in order to verify the accurate description of the spallation models used by the mcnpx2.5.0 code and to choose the best options to be used for the present work requirements. the predictions of the code tested against measured data are presented. several calculations using mcnpx2.5.0 combined with the evolution code cinder'90 were carried out to assess the performance of the direct targets. a complex analysis was performed to study the in-target production rib intensities varying with various parameters: target dimensions, materials and incident proton beam energies. the optimized configurations for the targets together with the corresponding quantitative estimates of the production rates for all interested nuclei resulted from this investigation are discussed.
the nucifer experiment : reactor antineutrino detection for reactor monitoring. during the last decades, tremendous progresses have been achieved on the fundamental knowledge and detection of neutrinos which give new opportunities of applied neutrino physics. among them, antineutrinos could be exploited for two nuclear reactor monitoring applications: the thermal power measurement and the control of the isotopic composition of the reactor fuel. this application arouses the international atomic energy agency (iaea) interest as a potential new safeguard tool. the nucifer detector, under development in france, will be dedicated to applied neutrino physics. the design of the detector takes advantage of the technical improvements performed for fundamental neutrino experiments such as double chooz. nucifer will be tested within the next two years at the osiris (saclay-france) and the ill (grenoble-france) research reactors. after an brief overview on the worldwide effort in the field of reactor monitoring with antineutrinos, the nucifer experiment will be presented, as well as monte-carlo pwr and candu reactor simulations and the method to compute the antineutrino energy spectrum using nuclear databases. the expected response of the nucifer detector to diversion scenarios in pwr and candu reactors will be shown.
pion interferometry in au+au and cu+cu collisions at $\sqrt{s_{nn}}$ = 62.4 and 200gev. we present a systematic analysis of two-pion interferometry in au+au collisions at $\sqrt{s_{\rm{nn}}}$ = 62.4 gev and cu+cu collisions at $\sqrt{s_{\rm{nn}}}$ = 62.4 and 200 gev using the star detector at rhic. the multiplicity and transverse momentum dependences of the extracted femtoscopic radii are studied. the scaling of the apparent freeze-out volume with charged particle multiplicity is studied for the rhic energy domain. the multiplicity scaling of the measured radii is found to be independent of colliding system and collision energy.
j/psi production at high transverse momentum in p+p and cu+cu collisions at \snn=200gev. the star collaboration at rhic presents measurements of \jpsi$\to{e^+e^-}$ at mid-rapidity and high transverse momentum ($p_t&gt;5$ gev/$c$) in \pp and central \cucu collisions at \snn = 200 gev. the inclusive \jpsi production cross section for \cucu collisions is found to be consistent at high $p_t$ with the binary collision-scaled cross section for \pp collisions, in contrast to previous measurements at lower $p_t$, where a suppression of \jpsi production is observed relative to the expectation from binary scaling. azimuthal correlations of $j/\psi$ with charged hadrons in \pp collisions provide an estimate of the contribution of $b$-meson decays to \jpsi production of $13% \pm 5%$.
cut generation for an integrated employee timetabling and production scheduling problem. this paper investigates the integration of the employee timetabling and production scheduling problems. at the first level, we manage a classical employee timetabling problem. at the second level, we aim at supplying a feasible production schedule for a set of interruptible tasks with qualification requirements and time-windows. instead of hierarchically solving these two problems as in the current practice, we try here to integrate them and propose two exact methods to solve the resulting problem. the former is based on a benders decomposition while the latter relies on a specific decomposition and a cut generation process. the relevance of these different approaches is discussed here through experimental results.
upper limit on the cosmic-ray photon fraction at eev energies from the pierre auger observatory. from direct observations of the longitudinal development of ultra-high energy air showers performed with the pierre auger observatory, upper limits of 3.8%, 2.4%, 3.5% and 11.7% (at 95% c.l.) are obtained on the fraction of cosmic-ray photons above 2, 3, 5 and 10 eev (1 eev = 10^18 ev) respectively. these are the first experimental limits on ultra-high energy photons at energies below 10 eev. the results complement previous constraints on top-down models from array data and they reduce systematic uncertainties in the interpretation of shower data in terms of primary flux, nuclear composition and proton-air cross-section.
system size dependence of associated yields in hadron-triggered jets. we present results on the system size dependence of high transverse momentum di-hadron correlations at $\sqrt{s_{nn}}$ = 200 gev as measured by star at rhic. measurements in d+au, cu+cu and au+au collisions reveal similar jet-like correlation yields at small angular separation ($\delta\phi\sim0$, $\delta\eta\sim0$) for all systems and centralities. previous measurements have shown that the away-side yield is suppressed in heavy-ion collisions. we present measurements of the away-side suppression as a function of transverse momentum and centrality in cu+cu and au+au collisions. the suppression is found to be similar in cu+cu and au+au collisions at a similar number of participants. the results are compared to theoretical calculations based on the parton quenching model and the modified fragmentation model. the observed differences between data and theory indicate that the correlated yields presented here will provide important constraints on medium density profile and energy loss model parameters.
the fréchet contingency array problem is max-plus linear. in this paper we show that the so-called array fréchet problem in probability/statistics is (max, +)-linear. the upper bound of fréchet is obtained using simple arguments from residuation theory and lattice distributivity. the lower bound is obtained as a loop invariant of a greedy algorithm. the algorithm is based on the max-plus linearity of the fréchet problem and the monge property of bivariate distribution.
stiffness analysis of overconstrained parallel manipulators. the paper presents a new stiffness modeling method for overconstrained parallel manipulators with flexible links and compliant actuating joints. it is based on a multidimensional lumped-parameter model that replaces the link flexibility by localized 6-dof virtual springs that describe both translational/rotational compliance and the coupling between them. in contrast to other works, the method involves a fea-based link stiffness evaluation and employs a new solution strategy of the kinetostatic equations for the unloaded manipulator configuration, which allows computing the stiffness matrix for the overconstrained architectures, including singular manipulator postures. the advantages of the developed technique are confirmed by application examples, which deal with comparative stiffness analysis of two translational parallel manipulators of 3-puu and 3-prpar architectures. accuracy of the proposed approach was evaluated for a case study, which focuses on stiffness analysis of orthoglide parallel manipulator.
introduction to the experimental study of hadronic matter in heavy ion collisions. the quark gluon plasma. in the last 20 years, heavy ion collisions have been an unique way to study the hadronic matter in the laboratory. the phase diagram of hadronic matter remains unknown, although many experimental and theoretical studies have been done in the last decennia, aiming at studying its phase transitions. after a general introduction, two phases transition of the hadronic matter, liquid-gas and the transition to the quark gluon plasma, are addressed. a general view about the experimental methods to study these phase transitions is presented in chapter three. the most important results of the heavy ion program in the rhic collider at bnl (upton, ny, usa) are presented in chapter four. the last three chapters are devoted to the heavy ion program in the future lhc collider at cern (geneva, switzerland). in particular, the unique lhc experiment specially designed for heavy ion physics, alice and its muon spectrometer are presented.
interfacial tension for studying asphaltene flocculation under pressure. null
an experimental investigation of liquid-solid phase transition in crude oils under high pressure conditions. null
limit on the diffuse flux of ultra-high energy tau neutrinos with the surface detector of the pierre auger observatory. data collected at the pierre auger observatory are used to establish an upper limit on the diffuse flux of tau neutrinos in the cosmic radiation. earth-skimming ντ may interact in the earth's crust and produce a τ lepton by means of charged-current interactions. the τ lepton may emerge from the earth and decay in the atmosphere to produce a nearly horizontal shower with a typical signature, a persistent electromagnetic component even at very large atmospheric depths. the search procedure to select events induced by τ decays against the background of normal showers induced by cosmic rays is described. the method used to compute the exposure for a detector continuously growing with time is detailed. systematic uncertainties in the exposure from the detector, the analysis, and the involved physics are discussed. no τ neutrino candidates have been found. for neutrinos in the energy range 2×1017ev.
mobile fission and activation products in nuclear waste disposal. when disposing nuclear waste in clay formations it is expected that the most radiotoxic elements like pu, np or am move only a few centimetres to meters before they decay. only a few radionuclides are able to reach the biosphere and contribute to their long-term exposure risks, mainly anionic species like i129, cl36, se79 and in some cases c14 and tc99, whatever the scenario considered. the recent oecd/nea cosponsored international mofap workshop focussed on transport and chemical behaviour of these less toxic radionuclides. new research themes have been addressed, such as how to make use of molecular level information for the understanding of the problem of migration at large distances. diffusion studies need to face mineralogical heterogeneities over tens to hundreds of meters. diffusion rates are very low since the clay rock pores are so small (few nm) that electrostatic repulsion limits the space available for anion diffusion (anion exclusion). the large volume of traversed rock will provide so many retention sites that despite weak retention, even certain of these “mobile” nuclides may show significant retardation. however, the question how to measure reliably very low retention parameters has been posed. an important issue is whether redox states or organic/inorganic speciation change from their initial state at the moment of release from the waste during long term contact with surfaces, hydrogen saturated environments, etc.
interaction of selenite with mx-80 bentonite: effect of minor phases, ph, selenite loading, solution composition and compaction. we propose a simple model describing the retention of selenite, se(iv), by the mx-80 bentonite in a synthetic groundwater (sgw) in dispersed and compacted states. the model was calibrated on a pure montmorillonite from data obtained for 4 &lt; ph &lt; 10 and total selenite concentrations between 10−7 and 5 × 10−3 mol/l. furthermore, the matrix solution covers a wide range of conditions regarding the ionic strength and the nature of the background electrolyte. three selenite surface species had to be considered. below ph 5, sorption is governed by a ligand exchange reaction with h2seo3. between ph 5 and 7, the experimental data are well described considering the formation of a surface complex implying hseo3−. finally, at ph above 7, we propose ternary surface complexes involving ca2+ and mg2+. the model, which we consider as operational, appears in agreement with spectroscopic data available in the literature and predicts surprisingly well selenite sorption on mx-80 bentonite even in the compacted state at a dry density of 1100 kg/m3. based on the model, the solid phase montmorillonite is responsible for selenite retention. minor solid phases containing iron (pyrite, hematite) had not to be considered in the modelling. interestingly, calcite (an important phase in mx-80 bentonite) has an indirect effect via the release of calcium into solution and its subsequent contribution to se(iv) sorption through a ternary surface complex.
self-consistent dynamical mean-field investigation of exotic structures in isospin-asymetric nuclear matter. the exotic structures expected in the outermost layer of neutron stars are investigated in a new approach. it is based on the dynamical wavelets in nuclei (dywan) model of nuclear collisions. this microscopic dynamical approach is an extended time-dependent hartree-fock description based on a wavelet representation. the model addresses the dynamical exploration of complex nuclear structures, beyond the wigner-seitz (ws) approximation and without any assumption on their final shapes. the present study focuses on exotic phases of cold matter evidenced dynamically at sub-saturation densities, currently within a pure mean field framework, before tackling the effects of the multi-particle correlations in a forthcoming study. starting from inhomogeneous initial conditions provided by nuclei located on an initial crystalline lattice, the exotic structures result from a dynamical self-consistent treatment where, in principle, the nuclear system can freely self-organize, modify the lattice structure or even break the lattice and the initial matter distribution symmetries. in this work nuclei are initially slightly excited with low-lying collective modes. the system can then explore geometrical configurations with similar energies, without being trapped in the vicinity of a local minimum. in this quantum framework, different effects are analyzed, among them the sensitivity to the equation of state and to the proton fraction.
photoproduction of j/psi and of high mass e+e- in ultra-peripheral au+au collisions at sqrt(s_nn) = 200 gev. we present the first measurement of photoproduction of j/psi and of two-photon production of high-mass e+e- pairs in electromagnetic (or ultra-peripheral) nucleus-nucleus interactions, using au+au data at sqrt(s_nn) = 200 gev. the events are tagged with forward neutrons emitted following coulomb excitation of one or both au^{star} nuclei. the event sample consists of 28 events with m_{e+e-} &gt; 2 gev/c^2 with zero like-sign background. the measured cross sections at midrapidity of d\sigma / dy (j/psi + xn, y=0) = 76 +/- 33 (stat) +/- 11 (syst) micro b and d^2\sigma/dm dy (e^+e^- + xn, y=0) = 86 +/- 23 (stat) +/- 16 (syst) micro b/(gev/c^2) for m_{e+e-} \in [2.0,2.8] gev/c^2 are consistent with various theoretical predictions.
high spatial resolution in beta-imaging with a pim device. the autoradiography is an imaging technique able to locate in 2 dimensions molecules labeled with p radioactive emitters. the usual technique in use is based on solid device, mostly films or phosphor screens. we report on tests performed with pim (parallel ionization multiplier) on microscope slides demonstrating that mpgd incorporating micromegas micromeshes have to be considered for a next generation of beta-imager. the main advantages come from the high spatial resolution measured (25 gin fwhm in two dimensions) and from the event by event reconstruction position measurement resulting in "on- line" imaging availability in which no "after development" have to be performed.
energy and system size dependence of $\phi$ meson production in cu+cu and au+au collisions. we study the beam-energy and system-size dependence of $\phi$ meson production (using the hadronic decay mode $\phi$→k+k−) by comparing the new results from cu+cu collisions and previously reported au+au collisions at $\sqrt{s_{nn}}$ = 62.4 and 200 gev measured in the star experiment at rhic. data presented in this letter are from mid-rapidity (|y|&lt;0.5) for 0.4.
design and performances of a fully autonomous antenna for radio detection of extensive air showers. the use of the radio-detection technique in a wide area cosmic-ray detector requires autonomous antenna stations, in terms of power feeding, triggering and data transmission. a prototype has been tested at the nançay radio observatory (france). it uses the broadband (1-200 mhz) active dipoles installed on the codalema experiment (see other contributions in this conference), together with a solar power supply, an independent trigger electronics and a dedicated communication system. we present here the complete setup and the performances of this new kind of detector.
the alice electromagnetic calorimeter. null
pi0 measurement with the alice photon spectrometer in first p+p collisions at the lhc. null
generation of complete events containing very high pt jets. the study of very high transverse-momentum jets will be an important issue at the lhc, in particular since the corresponding cross sections will be considerably larger than at rhic energies. jets are expected to provide information on qgp formation, due to the energy loss of fast partons in the medium. jet cross sections can in principle be compared to simple pqcd calculations, based on the hypothesis of factorization. but often it is useful or even necessary to not only compute the production rate of the very high-p t jets, but in addition the “rest of the event”. the proposed talk is based on recent work, where we try to construct an event generator—fully compatible with pqcd—which allows one to compute complete events, consisting of high-p t jets plus all the other low p t particles produced at the same time. whereas in “generators of inclusive spectra” like pythia one may easily trigger on high-p t phenomena, this is not so obvious for “generators of physical events”, where in principle one has to generate a very large number of events in order to finally obtain rare events (like those with a very high-p t jet). we shall discuss how we overcome these difficulties in the framework of the epos model.
extensions of logical analysis of data for medical applications. logical analysis of data (lad) has already been applied on several biomedical applications [bonates and hammer, 2006]. in this talk we shall present two developments designed to cope with new requirements. for a problem of height-loss due to irradiation during childhood, we shall show how combinatorial regression - an extension of lad to numerical outputs (instead of the classical binary ones) - allowed to precisely characterize the influences of several variables for a problem of height-loss due to irradiation during childhood. for the diagnosis of growth hormone deficiency, there is a need for very simple (e.g., straightforwardly usable by a gp) and accurate classifiers. we shall show an the concept of functional patterns allowed to meet this requirement by aggregating many boolean patterns into one single (non-binary) condition.
transaction activation scheduling support for transactional memory. transactional memory (tm) is considered as one of the most promising paradigms for developing concurrent applications. tm has been shown to scale well on multiple cores when the data access pattern behaves “well,” i.e., when few conflicts are induced. in contrast, data patterns with frequent write sharing, with long transactions, or when many threads contend for a smaller number of cores, produce numerous aborts. these problems are traditionally addressed by application-level contention managers, but they suffer from a lack of precision and provide unpredictable benefits on many workloads. in this paper, we propose a system approach where the scheduler tries to avoid aborts by preventing conflicting transactions from running simultaneously. we use a combination of several techniques to help reduce the odds of conflicts, by (1) avoiding preempting threads running a transaction until the transaction completes, (2) keeping track of conflicts and delaying the restart of a transaction until conflicting transactions have committed, and (3) keeping track of conflicts and only allowing a thread with conflicts to run at low priority. our approach has been implemented in linux for software transactional memory (stm) using a shared memory segment to allow fast communication between the stm library and the scheduler. it only requires small and contained modifications to the operating system. experimental evaluation demonstrates that our approach significantly reduces the number of aborts while improving transaction throughput on various workloads.
condition-based maintenance models for deteriorating systems in stressful environments. null
a predictive maintenance policy based on two explicative variables. null
integration of environmental stress factors in maintenance optimization. null
a predictive maintenance policy combining statistical process control and condition-based approaches. null
condition-based maintenance approaches for deteriorating system influenced by environmental conditions. null
comparison of health monitoring strategies for a gradually deteriorating system in a stressfull envirnment. null
maintenance policy for non-stationary deteriorating system. null
combining statistical process control and condition-based maintenance for gradually deteriorating systems subject to stress. null
a constraint programming approach to integrated routing and packing problems. null
two-dimensional pickup and delivery routing problem with loading constraints. null
reactive approaches. null
a memetic algorithm for a pick-up and delivery problem by helicopter. null
a branch-and-price algorithm to minimize the maximum lateness on a batch processing machine. null
vehicle routing in a public utility: a decision support system with hybrid techniques. null
a genetic algorithm for the multi-compartment vehicle routing problem with stochastic demands. null
design and implementation of a decision support system in a public utility. null
construction heuristics for the multi-compartment vehicle routing problem with stochastic demands. null
jcw: an object-oriented framework for the rapid development of vehicle routing heuristics based on savings. null
integration and propagation of a multicriteria decision model in constraint programming. in this paper we propose a general integration scheme for a multi-criteria decision making model of the multi-attribute utility theory in constraint programming . we introduce the choquet integral as a general aggregation function for multi-criteria optimization problems and define the choquet global constraint that propagates this function during the search. finally the benefit of the propagation of the choquet constraint are evaluated on the examination timetabling problem.
new methods for htr fuel waste management. considering the need to reduce waste production and greenhouse emissions by still keeping high energy efficiency, various 4th generation nuclear energy systems have been proposed. as far as graphite moderated reactors are concerned, one of the key issues is the large volumes of irradiated graphite encountered (1770 m3 for fuel elements and 840 m3 for reflector elements during the lifetime (60 years) of a single reactor module [1]). with the objective to reduce volume of waste in the htr concept, it is very important to be able to separate the fuel from low level activity graphite. this requires to separate triso particles from the graphite matrix with the sine qua non condition to not break triso particles in case of future embedding of particles in a matrix for disposal. according to national regulatory systems, in case of limited graphite waste production or of short duration htr projects (e.g. in germany), direct disposal without separation is acceptable. nevertheless, in case of large scale deployment of htr technology, such approach is not economical and sustainable. previous attempts in graphite management (furnace, fluidised bed and laser incinerations and encapsulation matrices) dealt with graphite matrix only. these are the reasons why we studied the management of irradiated compact-type fuel element. we simulated the presence of fuel in the particles by using zro2 kernels. compacts with zro2 triso particles were manufactured by areva np. two original methods have been studied. first, we tested high pressure jet to erode graphite and clean triso particles. best erosion rate reached about 0.18 kg/h for a single nose ending. examination of treated graphite showed a mixture of undamaged triso particles, particles that have lost the outer pyrolytic carbon layer and zro2 kernels. secondly, we studied the thermal shock method by immerging successively graphite into liquid nitrogen and hot water to cause fracturing of the compact. this produced particles and graphite fragments with diameter ranging from several centimetres to less than 500 µm. this relatively simple and economic method may potentially be considered as a pre-treatment step and be coupled with other method(s) before reprocessing and recycling for example.
maintenance policy for deteriorating system evolving in a stressful environment. this paper deals with the maintenance optimization of a system subject to a stressful environment. the system deterioration behaviour can be modified by the environment; the degradation mode can change due to the random evolution of the stressful environment. reciprocally, the environment conditions can be influenced by the system state and as a consequence, a change in the environment can be an indicator of the system state. this paper describes a condition-based maintenance decision framework to tackle the potential variations in the system deterioration, and especially in the deterioration rate, and the new information on the system state given by the evolution of the environmental variables.
predictive maintenance policy for a gradually deteriorating system subject to stress. this paper deals with a predictive maintenance policy for a continuously deteriorating system subject to stress. we consider a system with two failure mechanisms which are, respectively, due to an excessive deterioration level and a shock. to optimize the maintenance policy of the system, an approach combining statistical process control (spc) and condition-based maintenance (cbm) is proposed. cbm policy is used to inspect and replace the system according to the observed deterioration level. spc is used to monitor the stress covariate. in order to assess the performance of the proposed maintenance policy and to minimize the long-run expected maintenance cost per unit of time, a mathematical model for the maintained system cost is derived. analysis based on numerical results are conducted to highlight the properties of the proposed maintenance policy in respect to the different maintenance parameters.
adaptation of models to evolving metamodels. the problem of automatic or semi-automatic adaptation of models to their evolving metamodels is gaining importance in the model-driven community. recent approaches propose to adapt models using predefined information (i.e., a trace of changes). unfortunately, this information is not always available in practice. in many situations metamodels evolve without keeping track of the applied changes. we propose a more general two step solution. first step computes equivalences and differences between the metamodels and saves these into a ``weaving model''. this weaving model acts as a high-level specification of adaptation transformation. second step translates this model into an executable transformation. this technical report shows the results obtained in applying the approach on two concrete scenarios: a petri net metamodel, and the netbeans java metamodel.
development of a radio-detection method array for the observation of ultra-high energy neutrino induced showers. the recent demonstration by the codalema collaboration of the ability of the radio-detection technique for the characterization of uhe cosmic-rays calls for the use of this powerful method for the observation of uhe neutrinos. for this purpose, an adaptation of the existing 21cm array (china) is presently under achievment. in an exceptionally low electromagnetic noise level, 10160 log-periodic 50-200 mhz antennas sit along two high valleys, surrounded by mountain chains. this lay-out results in 30-60 km effective rock thicnesses for neutrino interactions with low incidence trajectories along the direction of two 4-6 km baselines. we will present first in-situ radio measurements demonstrating that this environment shows particularly favourable conditions for the observation of electromagnetic decay signals of taus originating from the interaction of 10^17-20 ev tau neutrinos.
branching ratios of $\alpha$-decay to excited states of even-even nuclei. branching ratios of $\alpha $-decay to members of the ground state rotational band and excited 0$^{+}$ states of even-even nuclei are calculated in the framework of the generalized liquid drop model (gldm) by taking into account the angular momentum of the $\alpha$-particle and the excitation probability of the daughter nucleus. the calculation covers isotopic chains from hg to fm in the mass regions $180&lt; a &lt;202$ and a$\geq 224$. the calculated branching ratios of the $\alpha $-transitions are in good agreement with the experimental data and some useful predictions are provided for future experiments.
a language-based approach for robust and efficient network application protocol implementations. null
selenide retention onto pyrite under reducing conditions. pyrite (fes2) is a mineral phase often present as inclusions in temperate soils. moreover, it turns out to be a sorption sink for certain radionuclides in deep geological disposals. the present study was thus initiated to determine the capacity of pyrite to immobilize selenide (se(-ii)). due to the fact that pyrite surface oxidizes readily, potentials were applied in order to minimise its surface evolution, and ensure the reducing conditions necessary for stabilizing se(-ii). the sorption experiments were carried out in nacl electrolyte and were amperometrically controlled. after only several minutes of reaction, at least 97% of se(-ii) initially present in solution was disappeared. the kd values vary from 7–65 l/g and the isotherm curve shows site saturation at higher initial selenide concentrations and no ph-dependence. by means of several spectroscopic techniques, the reaction mechanism was investigated. the xrd and in situ xanes results indicate the presence of se(0) on pyrite surface, which explain the rapid disappearance of se observed in the sorption experiments. moreover, xps results obtained from se-reacted pyrite particles reveal cleavage of s–s bonding which resulted in formation of s2− on pyrite surface. thus, we conclude that se(-ii) can be immobilized by pyrite via surface redox reaction: ≡fes2 + hse− ⇔ ≡fes + se(0) + hs−.
search for a long living giant system in $^{238}$u+$^{238}$u collisions near the coulomb barrier. we searched for a long-living component in the collision of 238u+238u between 6.09 a and 7.35 a mev. the experiment was performed at ganil using the spectrometer vamos, tuned for observing reactions with kinematics similar to quasi-fission events. theoretical calculations indicate that reactions with strong energy dissipation and a large number of transferred nucleons are correlated to a time delay in the decay of the giant system. we detected events of such type in the focal plane of vamos. these events present an excitation function increasing with bombarding energy.
alpha decay potential barriers and half-lives and analytical formula predictions for superheavy nuclei. the synthesis of superheavy elements has advanced strongly recently and their main observed decay mode is alpha emission. predictions of alpha decay half-lives of other possible superheavy nuclei are needed. the alpha decay potential barrier is often described using a finite square well for the one-body shapes plus an hyperbola for the coulomb repulsion between the alpha particle and its daughter. an arbitrary adjustment of the parameters allows to reproduce roughly the experimental data. here the alpha decay potential barriers are determined in the cluster-like shape path within a generalized liquid drop model including the proximity effects between the alpha particle and the daughter nucleus and adjusted to reproduce the experimental qalpha. the ? emission half-lives are deduced within the wkb penetration probability through these barriers.
ttomography of quark gluon plasma at energies available at the bnl relativistic heavy ion collider (rhic) and the cern large hadron collider (lhc). using the recently published model for the collisional energy loss of heavy quarks (q) in a quark gluon plasma (qgp), based on perturbative qcd (pqcd) with a running coupling constant, we study the interaction between heavy quarks and plasma particles in detail. we discuss correlations between the simultaneously produced $c$ and $\bar{c}$ quarks, study how central collisions can be experimentally selected, predict observable correlations and extend our model to the energy domain of the large hadron collider (lhc). we finally compare the predictions of our model with that of other approaches like ads/cft.
gamma-jet physics with emcal calorimeter in alice experiment at lhc. heavy ion collisions at lhc will produce a new state of matter : the quark-gluon plasma (qgp). photons are not sensible to the strong interaction which dominates the nuclear medium, and hence are a valuable tool to explore qgp. gamma-jets are rare hard processes : a photon and a parton are emitted back-to-back. the parton hadronises and produces a jet of particles. these jets are quenched due to the strong interaction of the parton with the qgp. this quenching, or more precisely the re-distribution of the energy in the jet, can be measured by the modification of the distribution of the particle energy in the jet, comparing p-p and pb-pb collisions (fragmentation functions or hump-backed plateau distributions). for this porpose, jet energy is needed, and can be provided precisely by gamma-jet measurement.&lt;br /&gt;our goal is to use emcal to detect a photon correlated with a jet reconstructed in alice tracking system. then, the jet energy distribution are compared for p-p an pb-pb collisions.&lt;br /&gt;gamma-jet physics is first addressed, the particle identification with emcal is introduced to isolate the direct photon, i.e. a photon and a jet emitted back-to-back. methods of jet identification and reconstruction are developed to determine hump-backed plateau distributions. finally, these methods are tested to evaluate alice and particularly emcal capabilities for gamma-jet study at lhc and to quantify the sensibility of this probe to explore the qgp.
beta neutrino correlation measurement with trapped radioactive ions. null
a language-based approach for improving the robustness of network application protocol implementations. the secure and robust functioning of a network relies on the defect-free implementation of network applications. as network protocols have become increasingly complex, however, hand-writing network message processing code has become increasingly error-prone. in this paper, we present a domain-specific language, zebu, for describing protocol message formats and related processing constraints. from a zebu specification, a compiler automatically generates stubs to be used by an application to parse network messages. zebu is easy to use, as it builds on notations used in rfcs to describe protocol grammars. zebu is also efficient, as the memory usage is tailored to application needs and message fragments can be specified to be processed on demand. finally, zebu-based applications are robust, as the zebu compiler automatically checks specification consistency and generates parsing stubs that include validation of the message structure. using a mutation analysis in the context of sip and rtsp, we show that zebu significantly improves application robustness.
observation of two-source interference in the photoproduction reaction $au au \to au au \rho^0$. in ultra-peripheral relativistic heavy-ion collisions, a photon from the electromagnetic field of one nucleus can fluctuate to a quark-antiquark pair and scatter from the other nucleus, emerging as a $\rho^0$. the $\rho^0$ production occurs in two well-separated (median impact parameters of 20 and 40 fermi for the cases considered here) nuclei, so the system forms a 2-source interferometer. at low transverse momenta, the two amplitudes interfere destructively, suppressing $\rho^0$ production. since the $\rho^0$ decay before the production amplitudes from the two sources can overlap, the two-pion system can only be described with an entangled non-local wave function, and is thus an example of the einstein-podolsky-rosen paradox. we observe this suppression in 200 gev per nucleon-pair gold-gold collisions. the interference is $87% \pm 5% {\rm (stat.)}\pm 8%$ (syst.) of the expected level. this translates into a limit on decoherence due to wave function collapse or other factors, of 23% at the 90% confidence level.
development and data analysis of a radiodetection of ultra high energy cosmic rays experiment. radiodetection of cosmic rays was first attempt in the early 60's. unfortunately, at that time, results suffered of poor reproducibility and the technique was abandoned in favour of direct particle and fluorescence detection. &lt;br /&gt;&lt;br /&gt;nowadays, the ultra high energy cosmic rays constitute an area of scientific interest of first plan and the extreme rarity of those particles implies development of huge detectors. in this context and taking advantage of recent technologic improvements, the radiodetection of ultra high cosmic rays is being reinvestigated in order to enhance the statistic of experiments besides already existing methods.&lt;br /&gt;&lt;br /&gt;in this document, we first remind to the reader the global problematic of cosmic rays. then, the several mechanisms involved in the emission of electric field associated with extensive air showers are discussed. the codalema experiment that aims to demonstrate the feasibility of cosmic rays radiodetection, is extensively described along with the obtained results. and finally, a radiodetection test experiment implanted at the giant detector pierre auger is presented. it should provide inputs to design the future detector using this technique at extreme energies.
on the compact formulation of the derivation of a transfer matrix with respect to another matrix. a new operator is considered, allowing compact formulae and proofs in the context of the derivation of a transfer matrix with respect to another matrix. the problem of the parametric sensitivity matrix calculation is chosen for illustration. it consists in deriving a multiple input multiple output transfer function with respect to a parametric matrix and is central in robust control theory. efficient algorithms may be straightforwardly got from the compact analytic formulae using the operator introduced.
condition-based maintenance policies for a deteriorating system subject to a stressful environment. one of the challenges of maintenance optimisation is the development of decision-making models combining performance at the strategic level and at the operational level. a classic hypothesis is to consider that the degradation level of the system can be modelled by a stochastic process characterized in stationary state without taking into account the effects of the operating environment on the system. this hypothesis can be seen as one of the factors leading to gaps between expected performance and measured performance of maintenance policies. however, many works have been developed in the reliability field for the integration of the impact of these environmental conditions. the main objective of this manuscript is to develop maintenance decision tools for gradually deteriorating systems evolving in a stressful environment. first, we propose different ways to model the environment and its impact since it can directly influence the system failure or the degradation process. we express mutual relationships between environment and the degradation processes. next, we propose and compare different adaptive maintenance policies which are based not only on the degradation level, but also on the stressful environment level. in addition, the proposed policies can be based either on an a priori knowledge of the system, or integrate the available online information on the environment. we will try throughout this manuscript to propose new maintenance approaches which combine theoretical expected performance on one side and operational reality and pragmatism on the other.
structural investigation of coprecipitation of technetium-99 with iron phases. technetium is a long-lived product of nuclear fission which commonly exhibits two oxidation states (iv and vii). siderite (feco3), suspected to be formed as a container corrosion product in geological radioactive waste repositories, may concentrate by coprecipitation more than 90% of technetium-99, present as tc(iv) in surrounding aqueous fluids. x-ray diffraction and transmission electron microscopy measurements indicate that technetium can be incorporated within the siderite structure, even if we note that technetium-bearing green rust phase may also be observed. these results suggest that siderite might play a beneficial role in limiting tc diffusion to the next environment of nuclear waste repositories.
thermodynamic interpretation of neptunium coprecipitation in uranophane for application to the yucca mountain repository. interpretation and modeling of recent experimental data [1] yield thermodynamic constants for the distribution of trace np(v) between aqueous solutions and uranophane. these data indicate that neptunyl is relatively excluded from the uranyl mineral structure, but the interpretation depends on uncertain aqueous speciation and thermodynamic properties as a function of temperature. despite np exclusion, the low calculated solubility of uranophane at 25 °c under conditions relevant to the proposed nuclear waste repository at yucca mountain, nevada, leads to np concentrations at equilibrium with a np-bearing uranophane solid solution that are low compared to concentrations invoked as solubility limits in yucca mountain performance assessments.
precision measurements in $\beta decay trapped and cooled radiocative ions. null
neutron correlations in $^{6}$he viewed through nuclear break-up reactions. null
service level in scheduling. null
industrial-strength rule interoperability using model driven engineering. model driven engineering (mde) is rapidly maturing and is being deployed in several situations. we report here on an experiment conducted in the context of ilog, a leader in the development of business rule management systems (brms). brmss aim at enabling business users automating their business policies. there is a growing number of brms supporting different languages, but also a lack of tools for bridging them. in this paper, we present an approach based on mde techniques for bridging rule languages; the solution has been fully implemented and tested on different brms. the success of the experiment has led to the development of a significant number of model transformations. at the same time, this deployment has shown new problems arising from the management of a high number of artifacts. we discuss the positive assessment of mde in this field, but also the need to address the complexity generated.
zero degree cherenkov calorimeters for the alice experiment. the collision centrality in the alice experiment will be determined by the zero degree calorimeters (zdcs) that will measure the spectator nucleons energy in heavy ion collisions. the zdcs detect the cherenkov light produced by the fast particles in the shower that cross the quartz fibers, acting as the active material embedded in a dense absorber matrix. test beam results of the calorimeters are presented.
bounded analysis and decomposition for behavioural descriptions of components. explicit behavioural interfaces are now accepted as a mandatory feature of components to address architectural analysis. behavioural interface description languages should be able to deal with data types and with rich communication means. symbolic transition systems (sts) support the definition of component models which take into account control, concurrency, communication and data types. however, verification of components described with protocol modelled by sts, especially model-checking, is difficult since they possibly involve different sources of infinity. in this paper, we propose the notions of bounded analysis and bounded decomposition. they can be used to test boundedness of systems and to generate finite simulations for them so that standard model-checking techniques may be applied for verification purposes.
a formal architectural description language based on symbolic transition systems and modal logic. component based software engineering has now emerged as a discipline for system development. after years of battle between component platforms, the need for means to abstract away from specific implementation details is now recognized. this paves the way for model driven approaches (such as mde) but also for the more older architectural description language (adl) paradigm. in this paper we present kadl, an adl based on the korrigan formal language which supports the following features: integration of fully formal behaviours and data types, expressive component composition mechanisms through the use of modal logic, specification readability through graphical notations, and dedicated architectural analysis techniques. key words: architectural description language, component based software engineering, mixed formal specifications, symbolic transition systems, abstract data types, modal logic glue, graphical notations, verification.
forward neutral-pion transverse single-spin asymmetries in p+p collisions at $\sqrt{s}$=200 gev. we report precision measurements of the feynman x (xf) dependence, and first measurements of the transverse momentum (pt) dependence, of transverse single-spin asymmetries for the production of $\pi$0 mesons from polarized proton collisions at $\sqrt{s}$=200 gev. the xf dependence of the results is in fair agreement with perturbative qcd model calculations that identify orbital motion of quarks and gluons within the proton as the origin of the spin effects. results for the pt dependence at fixed xf are not consistent with these same perturbative qcd-based calculations.
consequences of lambda c/d enhancement on the non-photonic electron nuclear modification factor in central heavy ion collisions at rhic energies. null
climatic and tectonic controls on weathering in south china and indochina peninsula: clay mineralogical and geochemical investigations from the pearl, red, and mekong drainage basins. null
clay minerals in surface sediments of the pearl river drainage basin and their contribution to the south china sea. null
major element geochemistry of glass shards and minerals of the youngest toba tephra in the southwestern south china sea. null
a new contribution to the nuclear modification factor of non-photonic electrons in au+au collisions at sqrt(s) = 200 gev. we investigate the effect of the so-called anomalous baryon/meson enhancement to the nuclear modification factor of non-photonic electrons in au+au collisions at sqrt(s) = 200 gev. it is demonstrated that an enhancement of the charm baryon/meson ratio, as it is observed for non-strange and strange hadrons, can be responsible for part of the amplitude of the nuclear modification factor of non-photonic electrons. about half of the measured suppression of non-photonic electrons in the 2-4 pt range can be explained by a charm baryon/meson enhancement of 5. this contribution to the non-photonic electron nuclear modification factor has nothing to do with heavy quark energy loss.
consequences of a lambda c/d enhancement effect on the non-photonic electron nuclear modification factor in central heavy-ion collisions at rhic energy. null
quantitative description and local structures of trivalent metal ions eu(iii) and cm(iii) complexed with polyacrylic acid. the trivalent metal ion (m(iii) = cm, eu)/polyacrylic acid (paa) system was studied in the ph range between 3 and 5.5 for a molar paa-to-metal ratio above 1. the interaction was studied for a wide range of paa (0.05 mg l−1–50 g l−1) and metal ion concentrations (2×10−9–10−3 m). this work aimed at 3 goals (i) to determine the stoichiometry of m(iii)–paa complexes, (ii) to determine the number of complexed species and the local environment of the metal ion, and (iii) to quantify the reaction processes. asymmetric flow-field-flow fractionation (asflfff) coupled to icp-ms evidenced that size distributions of eu–paa complexes and paa were identical, suggesting that eu bound to only one paa chain. time-resolved laser fluorescence spectroscopy (trlfs) measurements performed with eu and cm showed a continuous shift of the spectra with increasing ph. the environment of complexed metal ions obviously changes with ph. most probably, spectral variations arose from conformational changes within the m(iii)–paa complex due to ph variation. complexation data describing the distribution of complexed and free metal ion were measured with cm by trlfs. they could be quantitatively described in the whole ph-range studied by considering the existence of only a single complexed species. this indicates that the slight changes in m(iii) speciation with ph observed at the molecular level do not significantly affect the intrinsic binding constant. the interaction constant obtained from the modelling must be considered as a mean interaction constant.
surface site density, silicic acid retention and transport properties of compacted magnetite powder. in france, within the framework of investigations of the feasibility of deep geological disposal of high-level radioactive waste, studies on corrosion products of steel over packs are ongoing. such studies concern silica and radionuclide retention. the objective of the present work is to study sorption of silicic acid on compacted magnetite in percolation cells to attempt to simulate confined site conditions. potentiometric titration of commercial magnetite was carried out with both dispersed and compacted magnetite. the titration of the magnetite suspension has been made with two different methods: a batch method (several suspensions) and a direct fast method (one suspension). the gran's function gave 1.7 (±0.4) and 2.4 (±0.5) sorption sites nm−2 with these respective methods but site densities as high as 20/nm2 could be obtained by modelling. the titration of magnetite compacted at 120 bars showed that the evolution of charge density on magnetite surfaces is similar for compacted and dispersed magnetite. silicic acid sorption onto dispersed and compacted magnetite was similar with sorption site densities ranging between 2.2 and 4.4/nm2.
inclusive cross section and double helicity asymmetry for pi^0 production in p+p collisions at sqrt(s) = 62.4 gev. the phenix experiment presents results from the rhic 2006 run with polarized proton collisions at sqrt(s) = 62.4 gev for inclusive pi^0 production at mid-rapidity. unpolarized cross section results are measured for transverse momenta p_t = 0.5 to 7 gev/c. next-to-leading order perturbative quantum chromodynamics calculations are compared with the data, and while the calculations are consistent with the measurements, next-to-leading logarithmic corrections improve the agreement. double helicity asymmetries a_ll are presented for p_t = 1 to 4 gev/c and probe the higher range of bjorken_x of the gluon (x_g) with better statistical precision than our previous measurements at sqrt(s)=200 gev. these measurements are sensitive to the gluon polarization in the proton for 0.06 &lt; x_g &lt; 0.4.
the gluon spin contribution to the proton spin from the double helicity asymmetry in inclusive pi^0 production in polarized p+p collisions at sqrt(s)=200 gev. the double helicity asymmetry in neutral pion production for p_t = 1 to 12 gev/c has been measured with the phenix experiment in order to access the gluon spin contribution, delta-g, to the proton spin. measured asymmetries are consistent with zero, and at a theory scale of \mu^2 = 4 gev^2 give delta-g^[0.02,0.3] = 0.1 to 0.2, with a constraint of -0.7 &lt; delta-g^[0.02,0.3] &lt; 0.5 at delta-chi^2 = 9 (~3 sigma) for our sampled gluon momentum fraction (x) range, 0.02 to 0.3. the results are obtained using predictions for our measured asymmetries generated from four representative fits to polarized deep inelastic scattering data. we also consider the dependence of the delta-g constraint on the choice of theoretical scale, a dominant uncertainty in these predictions.
reaction mechanisms in 24mg+12c and 32s+24mg. the occurence of "exotic'' shapes in light n=z alpha-like nuclei is investigated for 24mg+12c and 32s+24mg. various approaches of superdeformed and hyperdeformed bands associated with quasimolecular resonant structures with low spin are presented. for both reactions, exclusive data were collected with the binary reaction spectrometer in coincidence with euroball iv installed at the vivitron tandem facility of strasbourg. specific structures with large deformation were selectively populated in binary reactions and their associated $\gamma$-decays studied. the analysis of the binary and ternary reaction channels is discussed.
hadronic resonance production in d+au collisions at $\sqrt{s_{nn}}$=200 gev measured at the bnl relativistic heavy ion collider. we present the first measurements of the $\rho$(770)0,k*(892),$\delta$(1232)++,$\sigma$(1385), and $\lambda$(1520) resonances in d+au collisions at $\sqrt{s_{nn}}$=200 gev, reconstructed via their hadronic decay channels using the star detector (the solenoidal tracker at the bnl relativistic heavy ion collider). the masses and widths of these resonances are studied as a function of transverse momentum pt. we observe that the resonance spectra follow a generalized scaling law with the transverse mass mt. the pt of resonances in minimum bias collisions are compared with the pt of $\pi$,k, and $\bar{p}$. the $\rho$0/$\pi$-,k*/k-,$\delta$++/p,$\sigma$(1385)/$\lambda, and $\lambda$(1520)/$\lambda$ ratios in d+au collisions are compared with the measurements in minimum bias p+p interactions, where we observe that both measurements are comparable. the nuclear modification factors (rdau) of the $\rho$0,k*, and $\sigma$* scale with the number of binary collisions (nbin) for pt&gt; 1.2 gev/c.
fission decay of n = z nuclei at high angular momentum: $^{60}$zn. using a unique two-arm detector system for heavy ions (the brs, binary reaction spectrometer) coincident fission events have been measured from the decay of $^{60}$zn compound nuclei formed at 88mev excitation energy in the reactions with $^{36}$ar beams on a $^{24}$mg target at $e_{lab}(^{36}$ar) = 195 mev. the detectors consisted of two large area position sensitive (x,y) gas telescopes with bragg-ionization chambers. from the binary coincidences in the two detectors inclusive and exclusive cross sections for fission channels with differing losses of charge were obtained. narrow out-of-plane correlations corresponding to coplanar decay are observed for two fragments emitted in binary events, and in the data for ternary decay with missing charges from 4 up to 8. after subtraction of broad components these narrow correlations are interpreted as a ternary fission process at high angular momentum through an elongated shape. the lighter mass in the neck region consists dominantly of two or three-particles. differential cross sections for the different mass splits for binary and ternary fission are presented. the relative yields of the binary and ternary events are explained using the statistical model based on the extended hauser-feshbach formalism for compound nucleus decay. the ternary fission process can be described by the decay of hyper-deformed states with angular momentum around 45-52 $hbar$.
spacetime scales and initial conditions in relativistic a+a collisions. the hydro-kinetic model accounting for continuous decoupling of expanding thermal systems and deviations from local equilibrium is applied for a description of the pion spectra and interferometry radii in cental au+au collisions at the rhic energies. we use the initial conditions inspired by the color glass condensate (cgc) approach and equation of state which incorporates the results of lattice qcd and accounts for gradual decays of the resonances during the expansion of hadron gas. the initial transverse flows which are developed at the pre-thermal glasma stage are important for simultaneous description of the spectra and spacetime scales.
reduced mean model for controlling a three-dimensional eel-like robot. null
improved screening for growth hormone deficiency using logical analysis data. null
neutronic characterization of the megapie target. null
sedimentary responses to the pleistocene climatic variations recorded in the south china sea. null
consequences of a lambda_c/d enhancement effect on the non-photonic electron nuclear modification factor in central heavy ion collisions at rhic energy. the rhic experiments have measured the nuclear modification factor r_aa of non-photonic electrons in au+au collisions at sqrt(s_nn) = 200gev. this r_aa exhibits a large suppression for p_t&gt; 2gev/c which is commonly attributed to heavy-quark energy loss. it is expected that the heavy-quark radiative energy loss is smaller than the light quark one because of the so-called dead-cone effect. an enhancement of the charm baryon yield with respect to the charm meson yield, as it is observed for light and strange hadrons, can explain part of the suppression. this phenomenon has been put forward in a previous work. we present in this paper a more complete study based on a detailed simulation which includes electrons from charm and bottom decay, charm and bottom quark realistic energy loss as well as a more realistic modeling of the lambda_c/d enhancement. we show that a lambda_c/d ratio close to unity, as observed for light and strange quarks, could explain 20-25% of the suppression of non-photonic electrons in central au+au collisions. this effect remains significant at relatively high non-photonic electron transverse momenta of 8-9gev/c.
physics perspectives with alice emcal. null
toward an understanding of the single electron data measured at the bnl relativistic heavy ion collider (rhic). high transverse momentum (pt) single nonphotonic electrons which have been measured in the rhic experiments come dominantly from heavy meson decay. the ratio of their pt spectra in pp and aa collisions [raa(pt)] reveals the energy loss of heavy quarks in the environment created by aa collisions. using a fixed coupling constant and the debye mass (md[approximate]gt) as the infrared regulator, perturbative qcd (pqcd) calculations are not able to reproduce the data, neither the energy loss nor the azimuthal (v2) distribution. employing a running coupling constant and replacing the debye mass by a more realistic hard thermal loop (htl) calculation, we find a substantial increase in the collisional energy loss, which brings the v2(pt) distribution as well as raa(pt) to values close to the experimental ones without excluding a contribution from radiative energy loss.
global properties of nucleus-nucleus collisions. in this lecture note, we discuss the global properties of nucleus-nucleus collisions. after a brief introduction to heavy-ion collisions, we introduce useful kinematics and then discuss the bulk hadron production in a+a collisions. at the end we discuss the hadronization and hadronic freeze-out in a+a collisions. we have tried to cover the topic from very fundamental arguments especially for the beginners in the field. we also give very useful formulae frequently used by experimentalists, from a first principle derivation.
heavy ion collisions at the lhc - last call for predictions. this writeup is a compilation of the predictions for the forthcoming heavy ion program at the large hadron collider, as presented at the cern theory institute 'heavy ion collisions at the lhc - last call for predictions', held from may 14th to june 10th 2007.
systematic measurements of identified particle spectra in pp, d+au and au+au collisions from star. identified charged particle spectra of $\pi^{\pm}$, $k^{\pm}$, $p$ and $\pbar$ at mid-rapidity ($|y|&lt;0.1$) measured by the $\dedx$ method in the star-tpc are reported for $pp$ and d+au collisions at $\snn = 200$ gev and for au+au collisions at 62.4 gev, 130 gev, and 200 gev. ... [shortened for arxiv list. full abstract in manuscript.].
alpha-cluster states populated in 24mg+12c. charged particle and gamma decays in light alpha-like nuclei are investigated for 24mg+12c. various theoretical predictions for the occurence of superdeformed and hyperdeformed bands associated with resonance structures with low spin are presented. the inverse kinematics reaction 24mg+12c is studied at elab(24mg) = 130 mev. exclusive data were collected with the binary reaction spectrometer in coincidence with euroball iv installed at the vivitron tandem facility at strasbourg. specific structures with large deformation were selectively populated in binary reactions and their associated gamma decays studied. coincident events from $\alpha$-transfer channels were selected by choosing the excitation energy or the entry point via the two-body q-values. the analysis of the binary reaction channels is presented with a particular emphasis on 20ne-gamma and 16o-gamma coincidences.
the alice experiment at the cern lhc. alice (a large ion collider experiment) is a general-purpose, heavy-ion detector at the cern lhc which focuses on qcd, the strong-interaction sector of the standard model. it is designed to address the physics of strongly interacting matter and the quark-gluon plasma at extreme values of energy density and temperature in nucleus-nucleus collisions. besides running with pb ions, the physics programme includes collisions with lighter ions, lower energy running and dedicated proton-nucleus runs. alice will also take data with proton beams at the top lhc energy to collect reference data for the heavy-ion programme and to address several qcd topics for which alice is complementary to the other lhc detectors. the alice detector has been built by a collaboration including currently over 1000 physicists and engineers from 105 institutes in 30 countries. its overall dimensions are 161626 m3 with a total weight of approximately 10 000 t. the experiment consists of 18 different detector systems each with its own specific technology choice and design constraints, driven both by the physics requirements and the experimental conditions expected at lhc. the most stringent design constraint is to cope with the extreme particle multiplicity anticipated in central pb-pb collisions. the different subsystems were optimized to provide high-momentum resolution as well as excellent particle identification (pid) over a broad range in momentum, up to the highest multiplicities predicted for lhc. this will allow for comprehensive studies of hadrons, electrons, muons, and photons produced in the collision of heavy nuclei. most detector systems are scheduled to be installed and ready for data taking by mid-2008 when the lhc is scheduled to start operation, with the exception of parts of the photon spectrometer (phos), transition radiation detector (trd) and electro magnetic calorimeter (emcal). these detectors will be completed for the high-luminosity ion run expected in 2010. this paper describes in detail the detector components as installed for the first data taking in the summer of 2008.
natural organic matter nom: a challenge for assessing nuclear waste disposal safety in clay. null
prompt photon production in p-a collisions at lhc and the extraction of gluon shadowing. a report is given on the study of using prompt photon production at the lhc to probe the gluon nuclear density, and more specifically the shadowing ratio ga/gp that one could access in foreseen p-a runs.
scavenging of $e_s^-$ and oh° radicals in concentrated hcl and nacl aqueous solutions. null
comments on the dynamics of the pais-uhlenbeck oscillator. we discuss the quantum dynamics of the pais-uhlenbeck oscillator. the lagrangian of this higher-derivative model depends on two frequencies. when the frequencies are different, the free pu oscillator has a pure point spectrum that is dense everywhere. when the frequencies are equal, the spectrum is continuous. it is not bounded from below, running from minus to plus infinity, but this is not disastrous as the hamiltonian is still hermitian and the evolution operator is still unitary. generically, the inclusion of interaction terms break unitarity, but in some special cases unitarity is preserved. we discuss also the nonstandard realization of the pu oscillator suggested by bender and mannheim, where the spectrum of the free hamiltonian is positive definite, but wave functions grow exponentially for large real values of canonical coordinates. the free nonstandard pu oscillator is unitary when the frequencies are different, but unitarity is broken in the equal frequencies limit.
coinductive big-step operational semantics. using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. we formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. we then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. a methodological originality of this paper is that all results have been proved using the coq proof assistant. we explain the proof-theoretic presentation of coinductive definitions and proofs offered by coq, and show that it facilitates the discovery and the presentation of the results.
a dynamic model for facility location in the design of complex supply chains. this paper proposes a mixed integer linear program (milp) for the design and planning of a production-distribution system. this study aims to help strategic and tactical decisions: opening, closing or enlargement of facilities, supplier selection, flows along the supply chain. these decisions are dynamic, i.e. the value of the decision variables may change within the planning horizon. the model considers a multi-echelon, multi-commodity production-distribution network with deterministic demands. we present one application: how to plan the expansion of a company that has to face increasing demands. we report numerical experiments with an milp solver.
study of processes involving selenite immobilization in a soil-plant-microorganisms system. null
studies of neutron-induced light-ion production with the medley facility. the growing interest in applications involving high-energy neutrons (e &gt; 20 mev) demands high-quality experimental data on neutron-induced reactions. such data have been measured with the medley setup at the the svedberg laboratory (tsl), uppsala, sweden. it has been used to measure differential cross sections for elastic nd scattering and double-differential cross sections for light-ion production (a ≤ 4) with targets ranging from c to u and at incident neutron energies around 96mev. we summarize the experimental results obtained so far and compared with theoretical reaction model calculations. a new method for correcting charged-particle spectra for thick target effects has been used for data obtained with the medley facility. the new quasi-monoenergetic neutron beam facility of tsl offers the possibility to extend these measurements up to neutron energies of 175mev. in january 2007, the neutron beam facility at tsl has been equipped with improved shielding and pre-collimator to reduce the background observed with medley during the first experimental campaigns at 175mev to an acceptable level. we present the current status of the medley facility after the shielding upgrade. we summarize also our ongoing projects including both measurements of light-ion production at 175mev from c to u targets and fission studies of u-238 in the energy region of 11 to 175mev.
beam-energy and system-size dependence of dynamical net charge fluctuations. we present measurements of net charge fluctuations in $au + au$ collisions at $\sqrt{s_{nn}} = $ 19.6, 62.4, 130, and 200 gev, $cu + cu$ collisions at $\sqrt{s_{nn}} = $ 62.4, 200 gev, and $p + p$ collisions at $\sqrt{s} = $ 200 gev using the dynamical net charge fluctuations measure $\nu_{+-{\rm,dyn}}$. we observe that the dynamical fluctuations are non-zero at all energies and exhibit a modest dependence on beam energy. a weak system size dependence is also observed. we examine the collision centrality dependence of the net charge fluctuations and find that dynamical net charge fluctuations violate $1/n_{ch}$ scaling, but display approximate $1/n_{part}$ scaling. we also study the azimuthal and rapidity dependence of the net charge correlation strength and observe strong dependence on the azimuthal angular range and pseudorapidity widths integrated to measure the correlation.
underwater robotic: localization with electrolocation for collision avoidance. this paper proposes and compares two observers designed to calculate the location of an obstacle. the two methods are bio-inspired with a sense used by electric fishes of equatorial forests: the electrolocation. firstly, this study presents the electrolocation and then develops two models of emitter-sensors inspired by the electrical sense. secondly, the two models are used in different observers for detection and localisation of wall obstacles. the estimation methods are based on an extended kalman filter algorithm. observers are tested on simulations in order to assess their potentials and to analyze observability.
heavy quarks thermalization in heavy-ion ultrarelativistic collisions: elastic or radiative?. we present a dynamical model of heavy quark evolution in the quark–gluon plasma (qgp) based on the fokker–planck equation. we then apply this model to the case of ultrarelativistic nucleus–nucleus collisions performed at rhic in order to investigate which experimental observables might help to discriminate the fundamental process leading to thermalization.
eas radio detection at large impact parameter: the inverse problem and the design of a giant array. extensive air shower radio electric fields can be evaluated at large impact parameter with analytical expressions. such a theoretical tool is most valuable in the present phase where the capabilities of the radio detection of extensive air shower are under investigations. it can help shaping strategies for the analysis of radio detection data. it can also be used to perform non trivial test of much more detailed numerical approaches which are currently under development. the approximation leading to such a formulation will be presented and two applications will be discussed: the "inverse" problem of how to go from a sampling of the radio electric field on a few antennas to the main characteristics of the extensive air shower, and the question of the antenna spacing of a giant array for ultra high energy cosmic rays.
detection of photons and electrons in emcal. null
error band in heavy quark thermalization: the viewpoint of theory. null
collisional energy loss at rhic and predictions for the lhc. null
toward an understanding of the rhic single electron data. null
open and hidden charm from rhic to fair. null
open and hidden heavy flavors, separately and together. null
a foundation for flow-based program matching using temporal logic and model checking. reasoning about program control-flow paths is an important functionality of a number of recent program matching languages and associated searching and transformation tools. temporal logic provides a well-defined means of expressing properties of control-flow paths in programs, and indeed an extension of the temporal logic ctl has been applied to the problem of specifying and verifying the transformations commonly performed by optimizing compilers. nevertheless, in developing the coccinelle program transformation tool for performing linux collateral evolutions in systems code, we have found that existing variants of ctl do not adequately support rules that transform subterms other than the ones matching an entire formula. being able to transform any of the subterms of a matched term seems essential in the domain targeted by coccinelle. in this paper, we propose an extension to ctl named ctl-vw (ctl with variables and witnesses) that is a suitable basis for the semantics and implementation of the coccinelle's program matching language. our extension to ctl includes existential quantification over program fragments, which allows metavariables in the program matching language to range over different values within different control-flow paths, and a notion of witnesses that record such existential bindings for use in the subsequent program transformation process. we formalize ctl-vw and describe its use in the context of coccinelle. we then assess the performance of the approach in practice, using a transformation rule that fixes several reference count bugs in linux code.
system-size independence of directed flow at the relativistic heavy-ion collider. we measure directed flow ($v_1$) for charged particles in au+au and cu+cu collisions at $\sqrt{s_{nn}} =$ 200 gev and 62.4 gev, as a function of pseudorapidity ($\eta$), transverse momentum ($p_t$) and collision centrality, based on data from the star experiment. we find that the directed flow depends on the incident energy but, contrary to all existing models, not on the size of the colliding system at a given centrality. we extend the validity of the limiting fragmentation concept to different collision systems, and investigate possible explanations for the observed sign change in $v_1(p_t)$.
wysiwib: a declarative approach to finding protocols and bugs in linux code. although a number of approaches to finding bugs in systems code have been proposed, bugs still remain to be found. current approaches have emphasized scalability more than usability, and as a result it is difficult to relate the results to particular patterns found in the source code and to control the tools to be able to find specific kinds of bugs. in this paper, we propose a declarative approach based on a control-flow based program search engine. our approach is wysiwib (what you see is where it bugs), since the programmer is able to express specifications for protocol and bug finding using a syntax that is close to that of ordinary c code. search specifications, called semantic matches, can be easily tailored so as to either eliminate false positives or catch more potential bugs. we introduce our approach by describing three case studies which have allowed us to find 395 bugs.
solving a real-time allocation problem with constraint programming. in this paper, we present an original approach (cprta for "constraint programming for solving real-time allocation") based on constraint programming to solve a static allocation problem of hard real-time tasks. this problem consists in assigning periodic tasks to distributed processors in the context of fixed priority preemptive scheduling. cprta is built on dynamic constraint programming together with a learning method to find a feasible processor allocation under constraints. two efficient new approaches are proposed and validated with experimental results. moreover, cprta exhibits very interesting properties. it is complete (if a problem has no solution, the algorithm is able to prove it); it is non-parametric (it does not require specific tuning) thus allowing a large diversity of models to be easily considered. finally, thanks to its capacity to explain failures, it offers attractive perspectives for guiding the architectural design process.
a cost-regular based hybrid column generation approach. constraint programming (cp) offers a rich modeling language of constraints embedding efficient algorithms to handle complex and heterogeneous combinatorial problems. to solve hard combinatorial optimization problems using cp alone or hybrid cp-ilp decomposition methods, costs also have to be taken into account within the propagation process. optimization constraints, with their cost-based filtering algorithms, aim to apply inference based on optimality rather than feasibility. this paper introduces a new optimization constraint: cost-regular. its filtering algorithm is based on the computation of shortest and longest paths in a layered directed graph. the support information is also used to guide the search for solutions. we believe this constraint to be particularly useful in modeling and solving column generation subproblems and evaluate its behaviour on complex employee timetabling problems through a flexible cp-based column generation approach. computational results on generated benchmark sets and on a complex real-world instance are given.
influence of process operating parameters of fibrous filter media on end-use properties. null
influence of complex fibrous media composition on their performances for voc and particle removal. null
complexation study of trivalent metal ions with polyacrylic acid using mass spectrometry and fluorescence spectroscopy as speciation tools. null
antineutrinos and non-proliferation via the double-chooz experiment. null
residue production in 136xe + p spallation reaction. a research program on spallation reactions in inverse kinematics has been performed at gsi, darmstadt, taking advantage of the relativistic heavy-ions beams available from gsi accelerators and the high-resolution magnetic spectrometer, used to identify the reactions products in-flight and to determine their kinematical properties. in this paper, we report the results obtained up to now on the spallation reaction 136xe on protons, focusing on 500 and 200 amev energies.
towards reactor neutrino applied physics. nuclear power plants are intense sources of antineutrinos. their energy spectrum and emitted flux depend on the composition of the nuclear fuel and on the thermal power of the reactor. these properties led to potential applications of neutrino physics: they could be used to non intrusively monitor a nuclear reactor. to this purpose, a better knowledge of the antineutrino energy spectra arising from uranium and plutonium isotope fission is necessary. in these proceedings we relate about on-going simulation efforts aiming at reducing the errors associated to these spectra. the generic tools under development and presented below will allow to perform scenario studies on the feasibility of using antineutrinos to measure thermal power and to test to which precision the fuel composition can be deduced using these particles.
towards reactor monitoring with antineutrinos. null
interaction of eu(iii)/cm(iii) with polyacrylic acids : effect of ph. null
scandium-dota complexe for a new pet/3gamma camera for medical applications and radio labelling studies. null
towards better understanding of neutrino spectrum. null
modeling metal–particle interactions with an emphasis on natural organic matter. modeling the binding of metal ions and actinides to natural particles of various origins remains a challenging task. the use of models to understand the speciation and distribution of metals in the environment is essential for predicting metal bioavailability. in this article, we discuss the primary metal–particle interaction models and their limits, with an emphasis on natural organic matter (nom). these models have varying levels of complexity, but we discuss only those based on parameters that are independent of environmental conditions. we also discuss the application of such models to field predictions.
discrepancies in thorium oxide solubility values: a new experimental approach to improve understanding of oxide surface at solid/solution interface. the solubility of tho2(cr) was studied since many years but a large discrepancy in solubility values is noticed in the literature. the present work suggests that this discrepancy is related to differences in the surface properties of thorium oxide. to understand the role of surface properties on solubility values, we conducted experiments with tho2(cr) spheres with reproducable surface properties. batch dissolution experiments were conducted in 0.01 m nacl solution at ph = 3.0 and 4.0 for periods of time up to 270 days. the solutions were spiked with 229th to determine precipitation (sorption) rates of thorium, while dissolution rates were determined by measuring 232th released from tho2(cr) spheres. we assume that 229th atoms are exchanged only with active sites involved in th-dissolution. using 229th as local sensor of attachement and detachment processes at the tho2(cr) surface under close-to-equilibrium conditions, allows to assess surface reactivity of the solid during solubility experiments.
a neutron beam facility at spiral-2. the future spiral-2 facility is mainly composed of a high-power superconducting driver linac, delivering a high-intensity deuteron, proton and heavy ions beams. the first two beams are particularly well suited to the construction of a neutron beam and irradiation facility called neutrons for science (nfs). thick c and be target-converters with incident deuteron beam will produce an intense white neutron spectrum, while thin 7li target and incident proton beam allows generating quasi-monoenergetic neutrons. the 1–40mev neutron energy range will be covered and characterized by very intense fluxes available. the primary ion beam characteristics (energy, time resolution, intensity, etc.) are adequate to create a neutron time-of-flight facility. irradiation stations for neutron, proton and deuteron induced reactions could also be built in order to perform cross-sections measurements by activation techniques. in this paper we will discuss the potential of this new installation to investigate numerous topics, both in fundamental and applied physics. in particular, cross section measurements could be performed for different purposes like nuclear data evaluation, fission and fusion technology, accelerator driven systems, nuclear medicine, astrophysics, etc.
dm1 design: development of a detailed design of xt-ads and of a conceptual design of efit with heavy liquid metal cooling. the domain dm1_design of the fp6 integrated project eurotrans has the mission to launch the conceptual design of a lead-alloy-cooled european facility for industrial transmutation (efit) loaded with transmutation-dedicated minor actinide fuel to “prove” transmutation at the industrial level. efit would represent a modular unit of a large power system able to handle european high-level waste. in parallel to the efit design, dm1_design partners are developing a detailed design of the short-term xt-ads (experimental demonstration of transmutation in an accelerator driven system) that would serve the full-scale demonstration of the ads concept and be able to accept up to a full ma fuel assembly. xt-ads is also intended to serve as a test banc for some components of efit. the work plan of the domain dm1_design addresses the following main issues of efit and xt-ads: definition of the design parameters for the optimisation of the transmutation objectives in an economical way for efit and the short-term realisation for xt-ads including the most essential features of efit. development and assessment of the reference pb design of efit and the back-up option based on gas. in parallel and in a synergetic way xt-ads with only pb-bi reference design is developed in a more detailed manner. further development of the high-power proton accelerator (hppa) needed for both efit and xt-ads and, in particular, qualification of the beam reliability, the development of the beam transport line and the demonstration of the prototypical components of the chosen hppa. proof of feasibility windowless spallation targets making maximum use of the existing work for defining the needed complementary experimental work for xt-ads spallation targets. evaluation of the safety performance and licensability of the three designs developed in the domain dm1. cost estimates and planning issues for the deployment of the transmutation facilities. in this paper we give the progress accomplished in these different work packages and present the first results obtained after 18 months of activity.
design and supporting r&amp;d of the xt-ads spallation target. the xt-ads is an experimental accelerator-driven system (ads) that is being developed within the framework of the european fp6 eurotrans project that runs from 2005 to 2009. in this paper the current level of the design of the xt-ads spallation target and the status of corresponding r&amp;d topics with respect to lbe handling, thermal-hydraulics and spallation product confinement are discussed.
megapie spallation target: design implementation and preliminary tests of the first prototypical spallation target for future ads. the megapie target has been designed, manufactured, set-up and fitted with all the ancillary systems on an integral test stand in paul scherrer institute for off-beam tests dedicated to thermo-hydraulic and operability tests, carried out during the last months of 2005. it was then moved for final implementation to the sinq facility, with the ancillary systems, for irradiation that is foreseen to be carried out from july to december 2006. the results obtained during the integral tests have shown that the target was well designed for a safe operation and allowed to validate the main procedures related to fill and drain, steady-state operation, and transients due to beam trips. a start-up procedure has been developed, and the operating and control parameters have been defined. the already performed steps, conceptual and engineering design, manufacturing and assembly, safety and reliability assessment, integral off-beam tests, start-up of irradiation at sinq psi, then later decommissioning, post-irradiation experiments, and waste management will provide the ads community a uniquely relevant design and operational feedback.
arronax, a high intensity cyclotron in nantes. a cyclotron named arronax is being built in nantes (france). it is mainly devoted to radiochemistry and nuclear medicine research and will be operational the last quarter of 2008. this machine will accelerate both protons and α-particles at high energy (up to 70 mev) and high intensity (2 simultaneous proton beams with intensity up to 350 µa). in nuclear medicine, these characteristics will allow the cyclotron to produce a large variety of radionuclides on a regular schedule and in sufficient amount to perform clinical trials. a priority list of 12 radioisotopes, which contains isotopes for therapeutic use as well as for pet imaging, has been established by an international scientific committee. in radiochemistry, a vertical pulsed α-beam will allow fundamental studies of radiolysis in aqueous media, which is of great interest for radiobiology and for nuclear waste management.
volatile elements production rates in a proton-irradiated molten lead-bismuth target. the is419 experiment at the isolde facility at cern dedicated to the measurement of production and release rates of volatile elements from an irradiated pb/bi target by a proton beam of 1/1.4 gev has been completed. the release of he, ne, ar, br, kr, cd, i, xe, hg, po and at isotopes was investigated at different target temperatures, ranging from 250 °c to 600 ° c. three experimental methods were used for the mass-separated, ionized beams: i) implantation of short- and medium-lived isotopes in a tape and on-line detection with a hpge γ detector; ii) implantation of longer-lived isotopes in al foils and off-line detection with a hpge detector; iii) a faraday cup used mainly for stable nuclides. the results were compared with predictions from the fluka and mcnpx codes using different options for the intra-nuclear cascades and evaporation/fission models. results show good agreement with calculations for hg and for noble gases. for other elements such as iodine it is apparent that only a fraction of the produced isotopes is released. the results from fluka and mcnpx with the incl4/abla models are in general more satisfactory than those obtained using mcnpx with the standard bertini/dresner model combination. interestingly also significant yields of 204-210at isotopes were observed. at isotopes are produced either by (p, π-xn) charge exchange reactions on 209bi or by secondary reactions involving 3he and 4he. despite the non-release of polonium from pb/bi targets at typical operation temperatures, a smaller amount of highly radiotoxic po isotopes can actually be liberated indirectly as decay daughters of the released astatine.
spin alignment measurements of the $k^{*0}(892)$ and $\phi(1020)$ vector mesons in heavy ion collisions at $\sqrt{^{s}nn}$=200 gev. we present the first spin alignment measurements for the $k^{*0}(892)$ and $\phi(1020)$ vector mesons produced at mid-rapidity with transverse momenta up to 5 gev/c at $\sqrt{s_{nn}}$ = 200 gev at rhic. the diagonal spin density matrix elements with respect to the reaction plane in au+au collisions are $\rho_{00}$ = 0.32 $\pm$ 0.04 (stat) $\pm$ 0.09 (syst) for the $k^{*0}$ ($0.8.
complexation of trivalent metal ions with polyacrylic acid : effect of ph. null
complexation properties of natural organic matter. null
arronax, a high-energy and high-intensity cyclotron for nuclear medicine. purpose this study was aimed at establishing a list of radionuclides of interest for nuclear medicine that can be produced in a high-intensity and high-energy cyclotron. methods we have considered both therapeutic and positron emission tomography radionuclides that can be produced using a high-energy and a high-intensity cyclotron such as arronax, which will be operating in nantes (france) by the end of 2008. novel radionuclides or radionuclides of current limited availability have been selected according to the following criteria: emission of positrons, low-energy beta or alpha particles, stable or short half-life daughters, half-life between 3 h and 10 days or generator-produced, favourable dosimetry, production from stable isotopes with reasonable cross sections. results three radionuclides appear well suited to targeted radionuclide therapy using beta (67cu, 47sc) or alpha (211at) particles. positron emitters allowing dosimetry studies prior to radionuclide therapy (64cu, 124i, 44sc), or that can be generator-produced (82rb, 68ga) or providing the opportunity of a new imaging modality (44sc) are considered to have a great interest at short term whereas 86y, 52fe, 55co, 76br or 89zr are considered to have a potential interest at middle term. conclusions several radionuclides not currently used in routine nuclear medicine or not available in sufficient amount for clinical research have been selected for future production. high-energy, high-intensity cyclotrons are necessary to produce some of the selected radionuclides and make possible future clinical developments in nuclear medicine. associated with appropriate carriers, these radionuclides will respond to a maximum of unmet clinical needs.
design of a low noise, wide band, active dipole antenna for a cosmic ray radiodetection experiment. an active dipole antenna has been designed to measure transient electric field induced by ultra high energy cosmic rays for the codalema experiment. the main requirements for this detector, composed of a low noise preamplifier placed close to a dipole antenna, are a wide bandwidth ranging from 100 khz to 100 mhz and a good sensitivity on the whole spectrum.
crypto-hermitian quantum systems. null
crypto-hermiticity and crypto-supersymmetry of non-anticommuting hamiltonians. null
hd field theory as a theory of everything. null
transverse energy measurement in au+au collisions by the star experiment. transverse energy ($e_t$) has been measured with both of its components, namely hadronic ($e_t^{had}$) and electromagnetic ($e_t^{em}$) in a common phase space at mid-rapidity for 62.4 gev au+au collisions by the star experiment. $e_t$ production with centrality and $\sqrt{s_{nn}}$ is studied with similar measurements from sps to rhic and is compared with a final state gluon saturation model (ekrt). the most striking feature is the observation of a nearly constant value of $e_t/n_{ch} \sim 0.8$ gev from ags, sps to rhic. the initial energy density estimated by the boost-invariant bjorken hydrodynamic model, is well above the critical density for a deconfined matter of quarks and gluons predicted by lattice qcd calculations.
cryptogauge symmetry and cryptoghosts for crypto-hermitian hamiltonians. we discuss the hamiltonian h = p^2/2 - (ix)^{2n+1} and the mixed hamiltonian h = (p^2 + x^2)/2 - g(ix)^{2n+1}, which are crypto-hermitian in a sense that, in spite of apparent complexity of the potential, a quantum spectral problem can be formulated such that the spectrum is real. we note first that the corresponding classical hamiltonian system can be treated as a gauge system, with imaginary part of the hamiltonian playing the role of the first class constraint. we observe then that, on the basis of this classical problem, several different nontrivial quantum problems can be formulated. we formulate and solve some such problems. we find in particular that the spectrum of the mixed hamiltonian undergoes in certain cases rather amazing transformation when the coupling g is sent to zero. there is an infinite set of phase transitions in g when a couple of eigenstates of h coalesce and disappear from the spectrum. when quantization is done in the most natural way such that gauge constraints are imposed on quantum states, the spectrum should not be positive definite, but must involve the negative energy states (ghosts). we speculate that, in spite of the appearance of ghost states, unitarity might still be preserved.
development of a liquid xenon compton telescope dedicated to functional medical imaging. functional imaging is a technique used to locate in three dimensions the position of a radiotracer previously injected to a patient. the two main modalities used for a clinical application to detect tumors, the spect and the pet, use solid scintillators as a detection medium. &lt;br /&gt;the objective of this thesis was to investigate the possibility of using liquid xenon in order to benefit from the intrinsic properties of this medium in functional imaging. the feasibility study of such a device has been performed by taking into account the technical difficulties specific to the liquid xenon.&lt;br /&gt;first of all, simulations of a liquid xenon pet has been performed using monte-carlo methods. the results obtained with a large liquid xenon volume are promising: we can expect a reduction of the injected activity of radiotracer, an improvement of the spatial resolution of the image and a parallax free camera.&lt;br /&gt;the second part of the thesis was focused on the development of a new concept of medical imaging, the three gamma imaging, based on the use of a new emitter: the 44 scandium. associated to a classical pet camera, the compton telescope is used to infer the incoming direction of the third gamma ray by triangulation. therefore, it is possible to reconstruct the position of each emitter in three dimensions. &lt;br /&gt;this work convinced the scientific community to support the construction and characterization of a liquid xenon compton telescope. the first camera dedicated to small animal imaging should then be operational in 2009.
study of ion emission from a germanium crystal surface under impact of fast pb ions in channeling conditions. a thin germanium crystal has been irradiated at ganil by pb beams of 29 mev/a (charge state qin = 56 and 72) and of 5.6 mev/a (qin = 28). the induced ion emission from the sample entrance surface was studied, impact per impact, as a function of qin, velocity vin and energy loss de in the crystal. the pb ions transmitted through the crystal were analyzed in charge (qout) and energy using the speg spectrometer. the emitted ionized species were detected and analyzed in mass by a time of flight multianode detector (lag). channeling was used to select peculiar de in ge and hence peculiar pb ion trajectories close to the emitting surface. the experiment was performed in standard vacuum. no ge emission was found. the dominating emitted species are h+ and hydrocarbon ions originating from the contamination layer on top of the crystal. the mean value  of the number of detected species per incoming pb ion (multiplicity) varies as (qin/vin)^p, with p values in agreement with previous results. we have clearly observed an influence of the energy deposition de in ge on the emission from the top contamination layer. when selecting increasing values of de, we observed a rather slow increase of . on the contrary, the probabilities of high multiplicity values, that are essentially connected to fragmentation after emission, strongly increase with de.
multi-variable constrained control approach for a three-dimensional eel-like robot. in this paper, a multi-variable feedback design for the 3d movement of an eel-like robot is presented. such a robot is under construction in the context of a national french robotic project. the proposed feedback enables the tracking of a desired 3d position of the eel's head as well as the stabilization of the rolling angle. the control design is based on a recently developed reduced model that have been validated using a 3d complete continuous model. several scenarios are proposed to assess the efficiency of the proposed feedback law.
charged hadron multiplicity fluctuations in au+au and cu+cu collisions from sqrt(s_nn) = 22.5 to 200 gev. a comprehensive survey of event-by-event fluctuations of charged hadron multiplicity in relativistic heavy ions is presented. the survey covers au+au collisions at sqrt(s_nn) = 62.4 and 200 gev, and cu+cu collisions sqrt(s_nn) = 22.5, 62.4, and 200 gev. fluctuations are measured as a function of collision centrality, transverse momentum range, and charge sign. after correcting for non-dynamical fluctuations due to fluctuations in the collision geometry within a centrality bin, the remaining dynamical fluctuations expressed as the variance normalized by the mean tend to decrease with increasing centrality. the dynamical fluctuations are consistent with or below the expectation from a superposition of participant nucleon-nucleon collisions based upon p+p data, indicating that this dataset does not exhibit evidence of critical behavior in terms of the compressibility of the system. an analysis of negative binomial distribution fits to the multiplicity distributions demonstrates that the heavy ion data exhibit weak clustering properties.
probing gluon shadowing with forward photons at rhic. there is a major need to better constrain nuclear parton densities in order to provide reliable perturbative qcd predictions at the lhc as well as to probe possible non-linear evolution at small values of x. in these proceedings, we discuss how the production of prompt photons at large rapidity in p-p and d-au collisions at rhic (sqrt(s_nn)=200 gev) is sensitive to the nuclear modifications of gluon distributions at x~0.001 and at rather low scales, q^2~10 gev^2. the nuclear production ratio, r_dau=sigma(d+a-&gt;gamma+x)/(2a sigma(p+p-&gt;gamma x), is computed for isolated prompt photons at nlo using the ndsg nuclear parton densities, in order to assess the visibility of the signal. we also emphasise that the expected counting rates in a year of running at rhic are large, indicating that r_dau could be measured with a high statistical accuracy.
$\alpha$ particle preformation in heavy nuclei and penetration probability. the $\alpha$ particle preformation in the even-even nuclei from $^{108}$te to $^{294}$118 and the penetration probability have been studied. the isotopes from pb to u have been firstly investigated since the experimental data allow us to extract the microscopic features for each element. the assault frequency has been estimated using classical methods and the penetration probability from tunneling through the generalized liquid drop model (gldm) potential barrier. the preformation factor has been extracted from experimental $\alpha$ decay energies and half-lives. the shell closure effects play the key role in the $\alpha$ preformation. the more the nucleon number is close to the magic numbers, the more the formation of $\alpha$ cluster is difficult inside the mother nucleus. the penetration probabilities reflect that 126 is a neutron magic number. the penetration probability range is very large compared to that of the preformation factor. the penetration probability determines mainly the $\alpha$ decay half-life while the preformation factor allows us to obtain information on the nuclear structure. the study has been extended to the newly observed heaviest nuclei.
ternary cluster decay within the liquid drop model. longitudinal ternary and binary fission barriers of $^{36}$ar, $^{56}$ni and $^{252}$cf nuclei have been determined within a rotational liquid drop model taking into account the nuclear proximity energy. for the light nuclei the heights of the ternary fission barriers become competitive with the binary ones at high angular momenta since the maximum lies at an outer position and has a much higher moment of inertia.
coplanar ternary cluster decay of hyper-deformed $^{56}$ni and $^{60}$zn at high angular momentum. using a unique two-arm detector system for heavy ions (brs, binary reaction spectrometer) coincident fission events have been measured from the decay of 60zn and 56ni compound nuclei formed at 83–88 mev excitation energy. from the binary coincidences inclusive and exclusive cross sections for fission channels with differing losses of charge were obtained. narrow out-of-plane correlations corresponding to coplanar decay are observed for two fragments emitted in binary events, and in the data for ternary decay with missing charges from 4 up to 8. differential cross sections for the different binary and ternary fission channels are obtained. the ternary cluster fission can be explained by the statistical decay from equilibrated compound nuclei with hyper-deformed shapes with angular momenta around 45–52 h.
indications of conical emission of charged hadrons at rhic. three-particle azimuthal correlation measurements with a high transverse momentum trigger particle are reported for pp, d+au, and au+au collisions at 200 gev by the star experiment. the acoplanarities in pp and d+au indicate initial state kt broadening. larger acoplanarity is observed in au+au collisions. the central au+au data show an additional effect signaling conical emission of correlated charged hadrons.
enhanced production of direct photons in au+au collisions at sqrt(s_nn)=200 gev and implications for the initial temperature. the production of low mass e+e- pairs for m_{e+e-} &lt; 300 mev/c^2 and 1 &lt; p_t &lt;5 gev/c is measured in p+p and au+au collisions at sqrt(s_nn)=200 gev. enhanced e+e- pair yield above hadronic sources is observed in au+au collisions. treating the excess as internal conversion of direct photons, the invariant yield of direct photons is deduced. in central au+au collisions, the excess over p+p is exponential in p_t}, with inverse slope t = 221 +/- 23 (stat) +/- 18 (syst) mev. hydrodynamical models with initial temperatures t_init ~ 300-600 mev at times of 0.6 - 0.15 fm/c after the collision are in qualitative agreement with the data. lattice qcd predicts a phase transition at ~ 170 mev.
charmed hadron production at low transverse momentum in au+au collisions at rhic. we report measurements of charmed hadron production from hadronic ($d\to k\pi$) and semileptonic ($\mu$ and $e$) decays in 200 gev au+au collisions at rhic. analysis of the spectra indicates that charmed hadrons have a different radial flow pattern from light or multi-strange hadrons. charm cross sections at mid-rapidity are extracted by combining the three independent measurements, covering the transverse momentum range that contributes to $\sim$90% of the integrated cross section. the cross sections scale with number of binary collisions of the initial nucleons, a signature of charm production exclusively at the initial impact of colliding heavy ions. the implications for charm quark interaction and thermalization in the strongly interacting matter are discussed.
selected effects of the in-medium nucleon-nucleon cross section on heavy-ion dynamics below 100 mev/u. the role of the value of the in-medium nucleon-nucleon cross section on nuclear dynamics at intermediate energies is studied within the framework of a semiclassical transport model. particular attention is payed to the early reaction phase and the effects produced on the energy transformed into heat and compression as well as on the prompt dynamical emission.
neutron-induced light-ion production from fe, pb and u at 96 mev. double-differential cross sections for light-ion production (up to a=4) induced by 96 mev neutrons have been measured for $^{nat}$fe, $^{nat}$pb and $^{nat}$u. the experiments have been performed at the the svedberg laboratory in uppsala, using two independent devices, medley and scandal. the recorded data cover a wide angular range (20º - 160º) with low energy thresholds. the work was performed within the hindas collaboration studying three of the most important nuclei for incineration of nuclear waste with accelerator-driven systems (ads). the obtained cross section data are of particular interest for the understanding of the so-called pre-equilibrium stage in a nuclear reaction and are compared with model calculations performed with the gnash, talys and preeq codes.
radio-detection of uhecr by the codalema experiment. the principle of the codalema experiment is based on an original approach of the de-tection of radio transients associated with extensive air showers induced by ultra high-energy cosmic rays. since september 2006, codalema has been under operation with a new setup at the nançay radio observatory, france. it uses 16 broadband dipole antennas associated with 13 particle detectors generating the trigger and allowing the primary cosmic ray energy estimation. we will present evi-dence for the radio detection of cosmic rays above $10^{17}$ ev, based on an event-by-event analysis and we will discuss the radio characteristics of these showers.
fission and ternary cluster decay of hyper-deformed $^{56}$n. coincidences between two heavy fragments have been measured from fission of 56ni compoud nuclei, formed in the 32s + 24mg reaction at elab(32s) = 165.4 mev. a unique experimental set-up consistiong of two large area position sensitive (x, y) gas-detector telescopes has been used allowing the complete determination of the observed fragments, and their momentum vectors. in addition to binary fission events with subsequent particle evaporation, narrow out-of-plane correlations are observed for two fragments emitted in purely binary events and in events with a missing charge consisting of 2$\alpha$ and 3$\alpha$-particles(12c). these events are interpreted as ternary cluster decay from 56ni-nuclei at high angular momenta through hyper-deformed shapes.
effect of heavy-quark energy loss on the muon differential production cross section in pb-pb collisions at $\sqrt{s_{nn}}$=5.5 tev. we study the nuclear modification factors raa and rcp of the high transverse momentum 5.
the hadronic interaction model epos. null
measuring gluon shadowing with prompt photons at rhic and lhc. the possibility to observe the nuclear modification of the gluon distribution at small-x (gluon shadowing) using high-p_t prompt photon production at rhic and at lhc is discussed. the per-nucleon ratio, sigma(p+a -&gt; gamma+x) / a sigma(p+p -&gt; gamma+x), is computed for both inclusive and isolated prompt photons in perturbative qcd at nlo using different parametrizations of nuclear parton densities, in order to assess the visibility of the shadowing signal. the production of isolated photons turns out to be a promising channel which allows for a reliable extraction of the gluon density, g^a/g^p, and the structure function, f_2^a/f_2^p, in a nucleus over that in a proton. moreover, the production ratio of prompt photons at forward-over-backward rapidity in p-a collisions provides an estimate of g^a/g^p (at small x) over f_2^a/f_2^p (at large x), without the need of p-p reference data at the same energy.
on the coefficients of the liquid drop model mass formulae and nuclear radii. the coefficients of different mass formulae derived from the liquid drop model and including or not the curvature energy, the diffuseness correction to the coulomb energy, the charge exchange correction term, different forms of the wigner term and different powers of the relative neutron excess $i=(n-z)/a$ have been determined by a least square fitting procedure to 2027 experimental atomic masses. the coulomb diffuseness correction $z^2/a$ term or the charge exchange correction $z^{4/3}/a^{1/3}$ term plays the main role to improve the accuracy of the mass formula. the wigner term and the curvature energy can also be used separately for the same purpose. the introduction of an $|i|$ dependence in the surface and volume energies improves slightly the efficiency of the expansion and is more effective than an $i^4$ dependence. different expressions reproducing the experimental nuclear charge radius are provided. the different fits lead to a surface energy coefficient of around 17-18~mev and a relative equivalent rms charge radius r$_0$ of 1.22-1.23~fm.
200 and 300 mev/nucleon nuclear reactions responsible for single-event effects in microelectronics. an experimental study of nuclear reactions between 28si nuclei at 200 and 300 mev/nucleon and hydrogen or deuterium target nuclei was performed at the celsius storage ring in uppsala, sweden, to collect information about the reactions responsible for single-event effects in microelectronics. inclusive data on 28si fragmentation, as well as data on correlations between recoils and spectator protons or particles are compared to predictions from the dubna cascade model and the japan atomic energy research institute version of the quantum molecular dynamics model. the comparison shows satisfactory agreement for inclusive data except for he fragments where low-energy sub-barrier fragments and recoiling fragments with very large momenta are produced much more frequently than predicted. the yield of exclusive data are also severely underestimated by the models whereas the charge distributions of recoils in these correlations compare well. the observed enhancement in he emission, which may well be important for the description of single-event effects, is most likely to be attributed to clustering in 28si nuclei.
a macroscopic description of coherent geo-magnetic radiation from cosmic-ray air showers. we have developed a macroscopic description of coherent electromagnetic radiation from air showers initiated by ultra-high-energy cosmic rays due to the presence of the geo-magnetic field. this description offers a simple and direct insight in the relation between the properties of the air shower and the time structure of the radio pulse.
macroscopic treatment of radio emission from cosmic ray air showers based on shower simulations. we present a macroscopic calculation of coherent electro-magnetic radiation from air showers initiated by ultra-high energy cosmic rays, based on currents obtained from monte carlo simulations of air showers in a realistic geo-magnetic field. we can clearly relate the time signal to the time dependence of the currents. we find that the the most important contribution to the pulse is related to the time variation of the currents. for showers forming a sufficiently large angle with the magnetic field, the contribution due to the currents induced by the geo-magnetic field is dominant, but neither the charge excess nor the dipole contribution can be neglected. we find a characteristic bipolar signal. in our calculations, we take into account a realistic index of refraction, whose importance depends on the impact parameter and the inclination. also very important is the role of the positive ions.
$\rho^0$ photoproduction in ultraperipheral relativistic heavy ion collisions at $\sqrt{s_{nn}}$=200 gev. photoproduction reactions occur when the electromagnetic field of a relativistic heavy ion interacts with another heavy ion. the star collaboration presents a measurement of rho^0 and direct pi^+pi^- photoproduction in ultra-peripheral relativistic heavy ion collisions at sqrt(s_{nn})=200 gev. we observe both exclusive photoproduction and photoproduction accompanied by mutual coulomb excitation. we find a coherent cross-section of sigma(auau) -&gt; au^*au^*rho^0 = 530 pm 19 (stat.) pm 57 (syst.) mb, in accord with theoretical calculations based on a glauber approach, but considerably below the predictions of a color dipole model. the rho^0 transverse momentum spectrum (p_{t}^2 ) is fit by a double exponential curve including both coherent and incoherent coupling to the target nucleus; we find sigma_{inc}/sigma_{coh} = 0.29 pm 0.03 (stat.) pm 0.08 (syst.). the ratio of direct pi^+pi^- to rho^0 production is comparable to that observed in gamma p collisions at hera, and appears to be independent of photon energy. finally, the measured rho^0 spin helicity matrix elements agree within errors with the expected s-channel helicity conservation.
saturation of $e_t/n_{ch}$ and freeze-out criteria in heavy-ion collisions. the pseudorapidity densities of transverse energy, the charged particle multiplicity and their ratios, $e_t/n_{ch}$, are estimated at mid-rapidity, in a statistical-thermal model based on chemical freeze-out criteria, for a wide range of energies from gsi-ags-sps to rhic. it has been observed that in nucleus-nucleus collisions, $e_t/n_{ch}$ increases rapidly with beam energy and remains approximately constant at about a value of 800 mev for beam energies from sps to rhic. $e_t/n_{ch}$ has been observed to be almost independent of centrality at all measured energies. the statistical-thermal model describes the energy dependence as well as the centrality independence, qualitatively well. the values of $e_t/n_{ch}$ are related to the chemical freeze-out criterium, $e/n \approx 1 gev$ valid for primordial hadrons. we have studied the variation of the average mass $(), n_{decays}/n_{primordial}, n_{ch}/n_{decays}$ and $e_t/n_{ch}$ with $\sqrt{s_{nn}}$ for all freeze-out criteria discussed in literature. these observables show saturation around sps and higher $\sqrt{s_{nn}}$, like the chemical freeze-out temperature ($t_{ch}$).
supersymmetry vs ghosts. we consider the simplest nontrivial supersymmetric quantum mechanical system involving higher derivatives. we unravel the existence of additional bosonic and fermionic integrals of motion forming a nontrivial algebra. this allows one to obtain the exact solution both in the classical and quantum cases. the supercharges $q, \bar q$ are not anymore hermitially conjugate to each other, which allows for the presence of negative energies in the spectrum. we show that the spectrum of the hamiltonian is unbounded from below. it is discrete and infinitely degenerate in the free oscillator-like case and becomes continuous running from $-\infty$ to $\infty$ when interactions are added. notwithstanding the absence of the ground state, there is no collapse, which suggests that a unitary evolution operator may be defined.
consequences of a $\lambda_c/d$ enhancement effect on the non-photonic electron nuclear modification factor in central heavy ion collisions at rhic energy. the rhic experiments have measured the nuclear modification factor r_aa of non-photonic electrons in au+au collisions at sqrt(s) = 200 gev. this r_aa exhibits a large suppression for pt &gt; 2 gev/c which is commonly attributed to heavy-quark energy loss. in order to reproduce satisfactorily the data, energy-loss models assume a heavy-quark energy loss similar to that of light quarks. however, it is expected that the heavy-quark radiative energy loss is smaller than the light quark one because of the so-called dead-cone effect. an enhancement of the charm baryon yield with respect to the charm meson yield, as it is observed for light and strange hadrons, can explain part of the suppression. this phenomenon has been put forward in a previous work. we present in this paper a more complete study based on a detailed simulation which includes electrons from charm and bottom decay, charm and bottom quark realistic energy loss as well as a more realistic modeling of the lambda_c/d enhancement. we show that a lambda_c/d ratio close to unity, as observed for light and strange quarks, could explain 20-25% of the suppression of non-photonic electrons in central au+au collisions. this effect remains significant at relatively high non-photonic electron transverse momenta of 8-9 gev/c.
qcd running coupling and collisional jet quenching. i point out that the existing results for the qcd collisional energy loss of a fast parton in the quark–gluon plasma need redressing. relating the corrected result to the running coupling (at t = 0) and to the debye mass, i argue that collisional energy loss is pertinent for the understanding of jet quenching.
recent $\alpha$ decay half-lives and analytic expression predictions including superheavy nuclei. new recent experimental $\alpha$ decay half-lives have been compared with the results obtained from previously proposed formulas depending only on the mass and charge numbers of the $\alpha$ emitter and the q$\alpha$ value. for the heaviest nuclei they are also compared with calculations using the density-dependent m3y (ddm3y) effective interaction and the viola-seaborg-sobiczewski (vss) formulas. the correct agreement allows us to make predictions for the $\alpha$ decay half-lives of other still unknown superheavy nuclei from these analytic formulas using the extrapolated q$\alpha$ of g. audi, a. h. wapstra, and c. thibault [nucl. phys. a729, 337 (2003)].
some boolean successes in medical research. null
collisional energy loss of a fast heavy quark in a quark-gluon plasma. we discuss the average collisional energy loss de/dx of a heavy quark crossing a quark-gluon plasma, in the limit of high quark energy e &gt;&gt; m^2/t, where m is the quark mass and t &gt;&gt; m is the plasma temperature. in the fixed coupling approximation, at leading order de/dx \propto \alpha_s^2, with a coefficient which is logarithmically enhanced. the soft logarithm arising from t-channel scattering off thermal partons is well-known, but a collinear logarithm from u-channel exchange had previously been overlooked. we also determine the constant beyond those leading logarithms. we then generalize our calculation of de/dx to the case of running coupling. we estimate the remaining theoretical uncertainty of de/dx, which turns out to be quite large under rhic conditions. finally, we point out an approximate relation between de/dx and the qcd debye mass, from which we derive an upper bound to de/dx for all quark energies.
stiffness analysis of 3-d.o.f. overconstrained translational parallel manipulators. the paper presents a new stiffness modelling method for overconstrained parallel manipulators, which is applied to 3-d.o.f. translational mechanisms. it is based on a multidimensional lumped-parameter model that replaces the link flexibility by localized 6-d.o.f. virtual springs. in contrast to other works, the method includes a fea-based link stiffness evaluation and employs a new solution strategy of the kinetostatic equations, which allows computing the stiffness matrix for the overconstrained architectures and for the singular manipulator postures. the advantages of the developed technique are confirmed by application examples, which deal with comparative stiffness analysis of two translational parallel manipulators.
towards an understanding of the rhic single electron data. the present theoretical approaches for the single electron spectra from heavy meson decay, measured at rhic by the star and the phenix collaborations, which are based on perturbative qcd (pqcd) underpredict the collisional energy loss by a large factor and are not able to reproduce the azimuthal distribution. they suffer in addition from the fact that neither the value of the (fixed) coupling constant nor that for the infrared (ir) regulator are well determined. we investigate the consequences of a physical running coupling constant and of a ir regulator, determined by a hard thermal loop (htl) approach. both together increase the collisional energy loss substantially and bring the out-of-plane $(v_2)$ anisotropy as well as the quenching to values close to the experimental values without excluding a contribution from radiative energy loss.
effects of ionizing radiation on the hollandite structure-type: $ba_{0.85}cs_{0.26}al_{1.35}fe_{0.77}ti_{5.90}o_{16}$. the hollandite structure-type has received considerable attention as a nuclear waste form for the incorporation of radioactive 135cs and 137cs, both of which are important fission product radionuclides in the high-level nuclear waste generated by the reprocessing of used nuclear fuel. a critical concern has been the effects of high doses of ionizing radiation from incorporated cs on the long-term structural stability of the hollandite structure. optimization of the synthesis conditions has resulted in the hollandite stoichiometry of ba0.85cs0.26al1.35fe0.77ti5.90o16. to evaluate the effect of cs-beta-decay on this stoichiometry, we have simulated the ionizing radiation using 200 kv electron beam using transmission electron microscopy (tem) at 298 and 573 k. complete amorphization was achieved at doses of 1.1 x 1014 and 1.8 x 1014 gy at temperatures of 298 and 573 k, respectively. electron energyloss spectroscopy (eels) of the cs m-edge revealed the selective loss of cs at the maximum doses. hollandite irradiated using gamma rays, ~106 gy, which has defects associated with the formation of ti3+ and o2 – had a dissolution rate similar to that of the pristine hollandite, suggesting that the initial stage of defect formation does not influence chemical durability. because the accumulated dose in the hollandite with 5 wt% of radioactive 137cs2o is estimated to be ~2.0 x 1010 gy after 500 years, the hollandite structure should be stable under the conditions anticipated for geologic disposal.
elastic scattering of 96 mev neutrons from iron, yttrium, and lead. data on elastic scattering of 96 mev neutrons from 56fe, 89y, and 208pb in the angular interval 10-70° are reported. the previously published data on 208pb have been extended, as a new method has been developed to obtain more information from data, namely to increase the number of angular bins at the most forward angles. a study of the deviation of the zero-degree cross section from wick's limit has been performed. it was shown that the data on 208pb are in agreement with wick's limit while those on the lighter nuclei overshoot the limit significantly. the results are compared with modern optical model predictions, based on phenomenology and microscopic nuclear theory. the data on 56fe, 89y, and 208pb are in general in good agreement with the model predictions.
dilepton mass spectra in p+p collisions at $\sqrt{s}$= 200 gev and the contribution from open charm. the phenix experiement has measured the electron-positron pair mass spectrum from 0 to 8 gev/c^2 in p+p collisions at sqrt(s)=200 gev. the contributions from light meson decays to e^+e^- pairs have been determined based on measurements of hadron production cross sections by phenix. they account for nearly all e^+e^- pairs in the mass region below 1 gev/c^2. the e^+e^- pair yield remaining after subtracting these contributions is dominated by semileptonic decays of charmed hadrons correlated through flavor conservation. using the spectral shape predicted by pythia, we estimate the charm production cross section to be 544 +/- 39(stat) +/- 142(syst) +/- 200(model) \mu b, which is consistent with qcd calculations and measurements of single leptons by phenix.
pion freeze-out time in pb+pb collisions at 158 a gev/c studied via $\pi^-/\pi^+$ and $k^-/k^+$ ratios. the effect of the final state coulomb interaction on particles produced in pb+pb collisions at 158 a gev/c has been investigated in the wa98 experiment through the study of the pi-/pi+ and k-/k+ ratios measured as a function of transverse mass. while the ratio for kaons shows no significant transverse mass dependence, the pi-/pi+ ratio is enhanced at small transverse mass values with an enhancement that increases with centrality. a silicon pad detector located near the target is used to estimate the contribution of hyperon decays to the pi-/pi+ ratio. the comparison of results with predictions of the rqmd model in which the coulomb interaction has been incorporated allows to place constraints on the time of the pion freeze-out.
dihadron azimuthal correlations in au+au collisions at $\sqrt{s_{nn}}$=200 gev. azimuthal angle (delta phi) correlations are presented for a broad range of transverse momentum (0.4 &lt; pt &lt; 10 gev/c) and centrality (0-92%) selections for charged hadrons from di-jets in au+au collisions at sqrt(s_nn) = 200 gev. with increasing pt, the away-side delta phi distribution evolves from a broad and relatively flat shape to a concave shape, then to a convex shape. comparisons to p+p data suggest that the away-side distribution can be divided into a partially suppressed head region centered at delta phi ~ \pi, and an enhanced shoulder region centered at delta phi ~ \pi \pm 1:1. the pt spectrum for the associated hadrons in the head region softens toward central collisions. the spectral slope for the shoulder region is independent of centrality and trigger pt . the properties of the near-side distributions are also modified relative to those in p + p collisions, reflected by the broadening of the jet shape in delta phi and delta eta, and an enhancement of the per-trigger yield. however, these modifications seem to be limited to pt &lt; 4 gev/c, above which both the dihadron pair shape and per-trigger yield become similar to p + p collisions. these observations suggest that both the away- and near-side distributions contain a jet fragmentation component which dominates for pt \ge 5gev and a medium-induced component which is important for pt \le 4 gev/c. we also quantify the role of jets at intermediate and low pt through the yield of jet-induced pairs in comparison to binary scaled p + p pair yield. the yield of jet-induced pairs is suppressed at high pair proxy energy (sum of the pt magnitudes of the two hadrons) and is enhanced at low pair proxy energy. the former is consistent with jet quenching; the latter is consistent with the enhancement of soft hadron pairs due to transport of lost energy to lower pt.
energy dependence of $\pi^0$ production in cu+cu collisions at $\sqrt{s_{nn}}$ = 22.4, 62.4, and 200 gev. neutral pion transverse momentum (pt) spectra at mid-rapidity (|y| &lt; 0.35) were measured in cu+cu collisions at \sqrt s_nn = 22.4, 62.4, and 200 gev. relative to pi -zero yields in p+p collisions scaled by the number of inelastic nucleon-nucleon collisions (ncoll) at the respective energies, the pi-zero yields for pt \ge 2 gev/c in central cu+cu collisions at 62.4 and 200 gev are suppressed, whereas an enhancement is observed at 22.4 gev. a comparison with a jet quenching model suggests that final state parton energy loss dominates in central cu+cu collisions at 62.4 gev and 200 gev, while the enhancement at 22.4 gev is consistent with nuclear modifications in the initial state alone.
theoretical analysis of singleton arc consistency and its extensions. null
heavy flavor physics with alice. null
centrality dependence of charged hadron and strange hadron elliptic flow from $\sqrt{s_{nn}}$ = 200 gev au+au collisions. we present star results on the elliptic flow v_2 of charged hadrons, strange and multi-strange particles from sqrt(s_nn) = 200 gev au+au collisions at rhic. the detailed study of the centrality dependence of v_2 over a broad transverse momentum range is presented. comparison of different analysis methods are made in order to estimate systematic uncertainties. in order to discuss the non-flow effect, we have performed the first analysis of v_2 with the lee-yang zero method for k_s^0 and lambda. in the relatively low p_t region, p_t &lt;= 2 gev/c, a scaling with m_t - m is observed for identified hadrons in each centrality bin studied. we do not observe v_2(p_t) scaled by the participant eccentricity to be independent of centrality. at higher p_t, 2 gev/c &lt;= p_t &lt;= 6 gev/c, v_2 scales with quark number for all hadrons studied. for the multi-strange hadron omega, which does not suffer appreciable hadronic interactions, the values of v_2 are consistent with both m_t - m scaling at low p_t and number-of-quark scaling at intermediate p_t. as a function of collision centrality, an increase of p_t-integrated v_2 scaled by the participant eccentricity has been observed, indicating a stronger collective flow in more central au+au collisions.
suppression pattern of neutral pions at high transverse momentum in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev and constraints on medium transport coefficients. for au + au collisions at 200 gev we measure neutral pion production with good statistics for transverse momentum, p_t, up to 20 gev/c. a fivefold suppression is found, which is essentially constant for 5 &lt; p_t &lt; 20 gev/c. experimental uncertainties are small enough to constrain any model-dependent parameterization for the transport coefficient of the medium, e.g. \mean(q^hat) in the parton quenching model. the spectral shape is similar for all collision classes, and the suppression does not saturate in au+au collisions; instead, it increases proportional to the number of participating nucleons, as n_part^2/3.
a process to develop operational bottom up evaluation methods - from reference guidebooks to a practical culture of evaluation. null
a systematic study on direct photon production from central heavy ion collisions. we investigate the production of direct photons in central au-au collisions at the relativistic heavy-ion collider (rhic) at 200 gev per nucleon, considering all possible sources. we treat thermal photons emitted from a quark-gluon plasma and from a hadron gas, based on a realistic thermodynamic expansion. hard photons from elementary nucleon-nucleon scatterings are included: primordial elementary scatterings are certainly dominant at large transverse momenta, but also secondary photons from jet fragmentation and jet-photon conversion cannot be ignored. in both cases we study the effect of energy loss, and we also consider photons emitted from bremsstrahlung gluons via fragmentation.
crypto-hermiticity of nonanticommutative theories. we note that, though nonanticommutative deformations of minkowski supersymmetric theories do not respect the reality condition and seem to lead to non-hermitian hamiltonians h, the latter belong to the class of crypto-hermitian (or quasi-hermitian) hamiltonians having attracted recently a considerable attention. they can be made manifestly hermitian via the similarity transformation h -&gt; e^r h e^{-r} with a properly chosen r. the deformed model enjoys the same supersymmetry algebra as the undeformed one though it is difficult in some cases to write explicit expressions for a half of supercharges. the deformed sqm models make perfect sense. it is not clear whether it is also the case for nac minkowski field theories -- the conventionally defined s--matrix is not unitary there.
high-order sliding-mode controllers of an electropneumatic actuator: application to an aeronautic benchmark. this paper presents the control of an electropneumatic system used for moving steering mechanism. this aeronautic application needs a high-precision position control and high bandwidth. the structure of the experimental setup and the benchmark on which controllers are evaluated have been designed in order to precisely check the use of such actuator in aeronautics. two kinds of controllers are designed: a linear one based on gain scheduling feedback, and two high order sliding mode controllers ensuring finite-time convergence, high accuracy and robustness. experimental results display feasibility and high performance of each controller and a comparison study is done.
source breakup dynamics in au+au collisions at $sqrt{s_{nn}}$=200 gev via three-dimensional two-pion source imaging. a three-dimensional (3d) correlation function obtained from mid-rapidity, low pt pion pairs in central au+au collisions at sqrt(s_nn)=200 gev is studied. the extracted model-independent source function indicates a long range tail in the directions of the pion pair transverse momentum (out) and the beam (long). model comparisons to these distensions indicate a proper breakup time \tau_0 ~ 9 fm/c and a mean proper emission duration \delta\tau ~ 2 fm/c, leading to sizable emission time differences (&lt;|\delta \tau_lcm |&gt; ~ 12 fm/c), partly due to resonance decays. they also suggest an outside-in "burning" of the emission source reminiscent of many hydrodynamical models.
performances of the silicon strip detector (ssd) of the star experiment at rhic. the silicon strip detector (ssd) is the fourth layer of detector using a double-sided microstrip technology of the star experiment at rhic, thus completes its inner tracking device. the goal of star is to study heavy ions collisions in order to probe the existence of the qgp, a deconfined state of nuclear matter. strangeness enhancement, such as k0-short, λ, ξ, ω particles production, has been proposed to sign the formation of the qgp. then precise measurement of secondary vertices is needed.&lt;br /&gt;the ssd will also permit an attempt to use the inner tracking device to measure charm and beauty with direct topological identification. it was proposed to enhance the star tracking capabilities by providing a better connexion between reconstructed tracks in the main tracking device (tpc) and the initial vertex detector (svt).&lt;br /&gt;in this thesis, we will present the intrinsic performances of the ssd and its impact on the inner tracking system performances by studying cu-cu collisions occurred at rhic in 2005. study of simulated data will also permit a better comprehension of these results.
motion control of a three-dimensional eel-like robot without pectoral fins. in this paper, recent advances in the design of feedback laws for the 3d movement of an eel-like robot are presented. such a robot is under construction in the context of a national french robotic project. the proposed feedback enables the tracking of a desired 3d position of the eel head as well as the stabilization of the rolling angle without using pectoral fins. we build on a previous work in which we proposed a complete control scheme for robot's 3d movement using its pectoral fins. the controller is tested on a recently developed complete 3d model in order to assess its efficiency in tackling 3d manoeuvres.
performance of the alice muon spectrometer. &lt;br /&gt;weak boson production and measurement in heavy-ion collisions at lhc. lattice qcd predicts a transition from a hadronic phase to a quark gluon plasma phase, qgp, for temperatures above 10^{13} k. heavy-ion collisions are proposed to recreate it in the laboratory. with such a purpose, the lhc will provide pb-pb collisions at 5.5 tev/u, and the alice experiment will permit to explore them. in particular, the alice muon spectrometer will permit to investigate the muon related probes (quarkonia, open beauty,...). the expected apparatus performances to measure muons and dimuons are discussed. a factorization technique is employed to unravel the different contributions to the global efficiency. results indicate that the detector should be able to measure muons up to pt~100 gev/c with a resolution of about 10%. we show that weak bosons production could be measured for the first time in heavy-ion collisions. single muon pt and dimuons invariant mass distributions will probe w and z production. as mainly muons from b- and c-quarks decays will populate the intermediate-pt of 5-25 gev/c, heavy quark in-medium energy loss calculations indicate that the single muon spectra would be suppressed by a factor 2-4 in the most central 0-10% pb-pb collisions at 5.5 tev. however, for pt &gt; 35 gev/c the weak boson decays are predominant, and no suppression is expected. estimations indicate that the b- and w-muons crossing point shifts down in transverse momenta by 5 to 7 gev/c in the most central 0-10% pb-pb collisions at 5.5 tev.
learning implied global constraints. finding a constraint network that will be efficiently solved by a constraint solver requires a strong expertise in constraint programming. hence, there is an increasing interest in automatic reformulation. this paper presents a general framework for learning implied global constraints in a constraint network assumed to be provided by a non-expert user. the learned global constraints can then be added to the network to improve the solving process. we apply our technique to global cardinality constraints. experiments show the significance of the approach.
radio pulses from cosmic ray air showers - boosted coulomb and cherenkov fields. high-energy cosmic rays passing through the earth's atmosphere produce extensive showers whose charges emit radio frequency pulses. despite the low density of the earth's atmosphere, this emission should be affected by the air refractive index because the bulk of the shower particles move roughly at the speed of radio waves, so that the retarded altitude of emission, the relativistic boost and the emission pattern are modified. we consider in this paper the contribution of the boosted coulomb and the cherenkov fields and calculate analytically the spectrum using a very simplified model in order to highlight the main properties. we find that typically the lower half of the shower charge energy distribution produces a boosted coulomb field, of amplitude comparable to the levels measured and to those calculated previously for synchrotron emission. higher energy particles produce instead a cherenkov-like field, whose amplitude may be smaller because both the negative charge excess and the separation between charges of opposite signs are small at these energies.
perspectives for heavy flavour physics in the alice detector. null
the physics of alice. null
hard probes in experiments. null
the muon spectrometer of alice and quarkonia performances. null
kaon production at subthreshold energies - what do we learn about the nuclear medium ?. the isospin quantum molecular dynamics model is used to compare spectra and elliptic flow of kaons produced at subthreshold energies with data taken at the sis accelerator at gsi. we find that temperatures of the spectra are dominated by the rescattering of the kaons. the study of elliptic flow observables indicates the influence of rescattering as well as of the optical potential of the kaons with increasing dominance of the optical potential at lower incident energies.
influence of process operating conditions on nonwoven filter properties : application to voc and particles combined treatment. null
cold nuclear matter effects on $j/\psi$ production as constrained by deuteron-gold measurements at $\sqrt{s_{nn}}$ = 200 gev. we present a new analysis of j/psi production yields in deuteron-gold collisions at sqrt(s_nn) = 200 gev using data taken by the phenix experiment in 2003 and previously published in [s.s. adler et al., phys. rev. lett 96, 012304 (2006)]. the high statistics proton-proton j/psi data taken in 2005 is used to improve the baseline measurement and thus construct updated cold nuclear matter modification factors r_dau. a suppression of j/psi in cold nuclear matter is observed as one goes forward in rapidity (in the deuteron-going direction), corresponding to a region more sensitive to initial state low-x gluons in the gold nucleus. the measured nuclear modification factors are compared to theoretical calculations of nuclear shadowing to which a j/psi (or precursor) break-up cross-section is added. breakup cross sections of sigma_breakup = 2.8^[+1.7_-1.4] (2.2^[+1.6_-1.5]) mb are obtained by fitting these calculations to the data using two different models of nuclear shadowing. these breakup cross section values are consistent within large uncertainties with the 4.2 +/- 0.5 mb determined at lower collision energies. projecting this range of cold nuclear matter effects to copper-copper and gold-gold collisions reveals that the current constraints are not sufficient to firmly quantify the additional hot nuclear matter effect.
charge injectors of alice silicon drift detector. null
 : the search for flexibility in training : conceptions and practices of self-learning. the author first of all proposes a rapide look back to the past in order to clarify the major socio-economic challenges involved in the search for flexilility in training. she shows that this search for flexibility has close links with the increasing place occupied by self-learning in educational institutions. she goes on to demonstrate that the growth of individualisation in training and of media-based pedagogical environments has contributed to the wide acceptance of two dominant conceptions of self-learning and to practices still in use to day. finally, the author gives a new interpretational key to flexibility in training and its links to self-learning. training.
research and development of a gaseous detector pim (parallel ionization multiplier) dedicated to particle tracking under high hadron rates. pim (parallel ionization multiplier) is a multi-stage micropattern gaseous detector using micromeshes technology. this new device, based on micromegas (micro-mesh gaseous structure) detector principle of operation, offers good characteristics for minimum ionizing particle tracking. however, this kind of detectors placed in hadron environment suffers discharges which degrade sensibly the detection efficiency and account for hazard to the front-end electronics. in order to minimize these strong events, it is convenient to perform charges multiplication by several successive steps. &lt;br /&gt;within the framework of the european hadronphysics project (eu-i3hp-jra4), we have investigated the multi-stage pim detector for high hadrons flux application. &lt;br /&gt;for this purpose, a systematic study for many geometrical configurations of a two amplification stages separated with a transfer space operated with the gaseous mixture ne+10%co2 has been performed. beam tests realised with high energy hadrons at cern facility have given that discharges probability could be strongly reduced with a suitable pim device. a discharges rate lower to 10-9 by incident hadron and a spatial resolution of 51 µm have been measured at the beginning efficiency plateau (&gt;96 %) operating point.
how can we explore the onset of deconfinement by experiment?. there is little doubt that quantumchromodynamics (qcd) is the theory which describes strong interaction physics. lattice gauge simulations of qcd predict that in the $\mu,t$ plane there is a line where a transition from confined hadronic matter to deconfined quarks takes place. the transition is either a cross over (at low $\mu$) or of first order (at high $\mu$). it is the goal of the present and future heavy ion experiment at rhic and fair to study this phase transition at different locations in the $\mu,t$ plane and to explore the properties of the deconfined phase. it is the purpose of this contribution to discuss some of the observables which are considered as useful for this purpose.
measurement of single muons at forward rapidity in p+p collisions at $\sqrt{s}$ = 200 gev and implications for charm production. muon production at forward rapidity (1.5 &lt; |\eta| &lt; 1.8) has been measured by the phenix experiment over the transverse momentum range 1 &lt; p_t \le 3 gev/c in sqrt(s) = 200 gev p+p collisions at the relativistic heavy ion collider. after statistically subtracting contributions from light hadron decays an excess remains which is attributed to the semileptonic decays of hadrons carrying heavy flavor, i.e. charm quarks or, at high p_t, bottom quarks. the resulting muon spectrum from heavy flavor decays is compared to pythia and a next-to-leading order perturbative qcd calculation. pythia is used to determine the charm quark spectrum that would produce the observed muon excess. the corresponding differential cross section for charm quark production at forward rapidity is determined to be d\sigma_(c c^bar)/dy|_(y=1.6)=0.243 +/- 0.013 (stat.) +/- 0.105 (data syst.) ^(+0.049)_(-0.087) (pythia syst.) mb.
identification of photon-tagged jets in the alice experiment. the alice experiment at lhc will detect and identify prompt photons and light neutral-mesons with the phos detector and the additional emcal electromagnetic calorimeter. charged particles will be detected and identified by the central tracking system. in this article, the possibility of studying the interaction of jets with the nuclear medium, using prompt photons as a tool to tag jets, is investigated by simulations. new methods to identify prompt photon-jet events and to distinguish them from the jet-jet background are presented.
rapidity and species dependence of particle production at large transverse momentum for $d$+au collisions at $\sqrt{s_{nn}}$ = 200 gev. we determine rapidity asymmetry in the production of charged pions, protons and anti-protons for large transverse momentum (pt) for d+au collisions at \sqrt s_nn = 200 gev. the identified hadrons are measured in the rapidity regions |y| &lt; 0.5 and 0.5 &lt; |y| &lt; 1.0 for the pt range 2.5 &lt; pt &lt; 10 gev/c. we observe significant rapidity asymmetry for charged pion and proton+anti-proton production in both rapidity regions. the asymmetry is larger for 0.5 &lt; |y| &lt; 1.0 than for |y|&lt; 0.5 and is almost independent of particle type. the measurements are compared to various model predictions employing multiple scattering, energy loss, nuclear shadowing, saturation effects, and recombination, and also to a phenomenological parton model. we find that asymmetries are sensitive to model parameters and show model-preference. the rapidity dependence of \pi^{-}/\pi^{+} and \bar{p}/p ratios in peripheral d+au and forward neutron-tagged events are used to study the contributions of valence quarks and gluons to particle production at high pt. the results are compared to calculations based on nlo pqcd and other measurements of quark fragmentation functions.
suppression of high-p_t neutral pion production in central pb+pb collisions at sqrt{s_nn} = 17.3 gev relative to p+c and p+pb collisions. neutral pion transverse momentum spectra were measured in p+c and p+pb collisions at sqrt{s_nn} = 17.4 gev at mid-rapidity (2.3.
centrality dependence of charged hadron production in deuteron+gold and nucleon+gold collisions at $\sqrt{s_{nn}}$=200 gev. we present transverse momentum (p_t) spectra of charged hadrons measured in deuteron-gold and nucleon-gold collisions at \sqrts = 200 gev for four centrality classes. nucleon-gold collisions were selected by tagging events in which a spectator nucleon was observed in one of two forward rapidity detectors. the spectra and yields were investigated as a function of the number of binary nucleon-nucleon collisions, \nu, suffered by deuteron nucleons. a comparison of charged particle yields to those in p+p collisions show that the yield per nucleon-nucleon collision saturates with \nu for high momentum particles. we also present the charged hadron to neutral pion ratios as a function of p_t.
physics of crypto-hermitian and/or cryptosupersymmetric field theories. we discuss non-hermitian field theories where the spectrum of the hamiltonian involves only real energies. we make three observations. (i) the theories obtained from supersymmetric theories by nonanticommutative deformations belong in many cases to this class. (ii) when the deformation parameter is small, the deformed theory enjoys the same supersymmetry algebra as the undeformed one. half of the supersymmetries are manifest and the existence of another half can be deduced from the structure of the spectrum. (iii) generically, the conventionally defined s--matrix is not unitary for such theories.
disappearance of transverse flow in central collisions for heavier nuclei. for the first time, mass dependence of balance energy only for heavier systems has been studied. our results are in excellent agreement with the data which allow us to predict the balance energy of u+u, for the first time, around 37-39 mev/nucleon. also our results indicate a hard equation of state along with nucleon-nucleon cross-section around 40 mb.
how sensitive are di-leptons from rho mesons to the high baryon density region?. we show that the measurement of di-leptons might provide only a restricted view into the most dense stages of heavy ion reactions. thus, possible studies of meson and baryon properties at high baryon densities, as e.g. done at gsi-hades and envisioned for fair-cbm, might observe weaker effects than currently expected in certain approaches. we argue that the strong absorption of resonances in the high baryon density region of the heavy ion collision masks information from the early hot and dense phase due to a strong increase of the total decay width because of collisional broadening. to obtain additional information, we also compare the currently used approaches to extract di-leptons from transport simulations - i.e. shining, only vector mesons from final baryon resonance decays and instant emission of di-leptons and find a strong sensitivity on the method employed in particular at fair and sps energies. it is shown explicitly that a restriction to rho meson (and therefore di-lepton) production only in final state baryon resonance decays provide a strong bias towards rather low baryon densities. the results presented are obtained from urqmd v2.3 calculations using the standard set-up.
longitudinal double-spin asymmetry for inclusive jet production in $\vec{p}+\vec{p}$ collisions at $\sqrt{s}$=200 gev. we report a new star measurement of the longitudinal double-spin asymmetry a_ll for inclusive jet production at mid-rapidity in polarized p+p collisions at a center-of-mass energy of sqrt(s) = 200 gev. the data, which cover jet transverse momenta 5 &lt; p_t &lt; 30 gev/c, are substantially more precise than previous measurements. they provide significant new constraints on the gluon spin contribution to the nucleon spin through the comparison to predictions derived from one global fit of polarized deep-inelastic scattering measurements.
on the mass formula and wigner and curvature energy terms. the efficiency of different mass formulas derived from the liquid drop model including or not the curvature energy, the wigner term and different powers of the relative neutron excess $i$ has been determined by a least square fitting procedure to the experimental atomic masses assuming a constant r$_{0,charge}$/a$^{1/3}$ ratio. the wigner term and the curvature energy can be used independently to improve the accuracy of the mass formula. the different fits lead to a surface energy coefficient of around 17-18 mev, a relative sharp charge radius r$_0$ of 1.22-1.23 fm and a proton form-factor correction to the coulomb energy of around 0.9 mev.
detailed study of high-pt neutral pion suppression and azimuthal anisotropy in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. measurements of neutral pion production at midrapidity in sqrt(s_nn) = 200 gev au+au collisions as a function of transverse momentum, p_t, collision centrality, and angle with respect to reaction plane are presented. the data represent the final pi^0 results from the phenix experiment for the first rhic au+au run at design center-of-mass-energy. they include additional data obtained using the phenix level-2 trigger with more than a factor of three increase in statistics over previously published results for p_t &gt; 6 gev/c. we evaluate the suppression in the yield of high-p_t pi^0's relative to point-like scaling expectations using the nuclear modification factor r_aa. we present the p_t dependence of r_aa for nine bins in collision centrality. we separately integrate r_aa over larger p_t bins to show more precisely the centrality dependence of the high-p_t suppression. we then evaluate the dependence of the high-p_t suppression on the emission angle \delta\phi of the pions with respect to event reaction plane for 7 bins in collision centrality. we show that the yields of high-p_t pi^0's vary strongly with \delta\phi, consistent with prior measurements. we show that this variation persists in the most peripheral bin accessible in this analysis. for the peripheral bins we observe no suppression for neutral pions produced aligned with the reaction plane while the yield of pi^0's produced perpendicular to the reaction plane is suppressed by more than a factor of 2. we analyze the combined centrality and \delta\phi dependence of the pi^0 suppression in different p_t bins using different possible descriptions of parton energy loss dependence on jet path-length averages to determine whether a single geometric picture can explain the observed suppression pattern.
theoretical and experimental $\alpha$ decay half-lives of the heaviest odd-z elements and general predictions. theoretical decay half-lives of the heaviest odd-z nuclei are calculated using the experimental q value. the barriers in the quasimolecular shape path are determined within a generalized liquid drop model (gldm) and the wkb approximation is used. the results are compared with calculations using the density-dependent m3y (ddm3y) effective interaction and the viola-seaborg-sobiczewski (vss) formulas. the calculations provide consistent estimates for the half-lives of the decay chains of these superheavy elements. the experimental data stand between the gldm calculations and vss ones in the most time. predictions are provided for the decay half-lives of other superheavy nuclei within the gldm and vss approaches using the recent extrapolated q of audi, wapstra, and thibault [nucl. phys. a729, 337 (2003)], which may be used for future experimental assignment and identification.
collisional energy loss of a fast muon in a hot qed plasma. we calculate the collisional energy loss of a muon of high energy $e$ in a hot qed plasma beyond logarithmic accuracy, i.e., we determine the constant terms of order o(1) in $-de/dx \propto \ln{e}+ o(1)$. considering first the $t$-channel contribution to $-de/dx$, we show that the terms $\sim o(1)$ are sensitive to the full kinematic region for the momentum exchange $q$ in elastic scattering, including large values $q \sim o(e)$. we thus redress a previous calculation by braaten and thoma, which assumed $q &lt;&lt; e$ and could not find the correct constant (in the large $e$ limit). the relevance of 'very hard' momentum transfers then requires, for consistency, that $s$ and $u$-channel contributions from compton scattering must be included, bringing a second modification to the braaten-thoma result. most importantly, compton scattering yields an additional large logarithm in $-de/dx$. our results might have implications in the qcd case of parton collisional energy loss in a quark gluon plasma.
uhecr observed by the radiodetection experiment codalema. the radiodetection experiment codalema allows to study, on an event-by-event basis, cosmic ray air showers through the detection of the radiated electric field. 2 major upgrades have been made in september 2006: - a new antenna array made up of 16 active dipole antennas, arranged in a cross shape. - 13 new particule detector providing more accurate informations on air showers: primary cosmic ray energy estimation, core shower position... results from 6 months of measurements: radiodetection efficiency versus energy, arrival direction distribution or showers lateral electric field dependence, will be discussed. full analysis of a high energy event will be presented.
coplanar ternary cluster decay of hyper-deformed $^{56}$ni. coincidences between two heavy fragments have been measured from fission of 56ni compound nuclei, formed in the 32s + 24mg reaction at elab(32s) = 163.5 mev. a unique experimental set-up consisting of two large area position sensitive (x, y) gas-detector telescopes has been used allowing the complete determination of the observed fragments, and their momentum vectors. in addition to binary fission events with subsequent particle evaporation, narrow out-of-plane correlations are observed for two fragments emitted in purely binary events and in events with a missing mass consisting of 2$\alpha$ and 3$\alpha$ particles(12c). these events are interpreted as ternary cluster decay from 56ni-nuclei at high angular momenta through hyper-deformed shapes.
bimodality: a sign of critical behavior in nuclear reactions. the recently discovered coexistence of multifragmentation and residue production for the same total transverse energy of light charged particles can be well reproduced in numerical simulations of the heavy ion reactions. a detailed analysis shows that fluctuations (introduced by elementary nucleon-nucleon collisions) determine which of the exit states is realized. thus we observe for the first time nonlinear dynamics in heavy ion reactions. also the scaling of the coexistence region with beam energy is well reproduced in these results from the qmd simulation program.
probing the nuclear matter isospin asymmetry by nucleon-induced reactions at fermi energies. despite the fact that valuable experimental measures are still lacking, available nuclear data on nucleon-induced reactions open new opportunities to address either reaction mechanisms or nuclear interaction characteristics. in this work single and double differential cross sections of emitted particles are analyzed and compared with the experiment. it will be evidenced that these cross sections follow a precise hierarchy. the preequilibrium components of the spectra are built up by the dynamics of the reaction as well as by the properties of the nuclear interaction.
mass, quark-number, and $\sqrt{s_{nn}}$ dependence of the second and fourth flow harmonics in ultrarelativistic nucleus-nucleus collisions. we present star measurements of the azimuthal anisotropy parameter v(2) for pions, kaons, protons, lambda,(lambda) over bar,xi+ (xi) over bar, and omega+ (omega) over bar, along with v(4) for pions, kaons, protons, and lambda+(lambda) over bar at midrapidity for au+au collisions at root s(nn)=62.4 and 200 gev. the v(2)(p(t)) values for all hadron species at 62.4 gev are similar to those observed in 130 and 200 gev collisions. for observed kinematic ranges, v(2) values at 62.4, 130, and 200 gev are as little as 10-15% larger than those in pb+pb collisions at root s(nn)=17.3 gev. at intermediate transverse momentum (p(t) from 1.5-5 gev/c), the 62.4 gev v(2)(p(t)) and v(4)(p(t)) values are consistent with the quark-number scaling first observed at 200 gev. a four-particle cumulant analysis is used to assess the nonflow contributions to pions and protons and some indications are found for a smaller nonflow contribution to protons than pions. baryon v(2) is larger than antibaryon v(2) at 62.4 and 200 gev, perhaps indicating either that the initial spatial net-baryon distribution is anisotropic, that the mechanism leading to transport of baryon number from beam- to midrapidity enhances v(2) or that antibaryon and baryon annihilation is larger in the in-plane direction.
alignment experience in star. the star experiment at rhic uses four layers of silicon strip and silicon drift detectors for secondary vertex reconstruction. an attempt for a direct charm meson measurement put stringent requirements on alignment and calibration. we report on recent alignment and drift velocity calibration work performed on the inner silicon tracking system.
ex-post evaluation of local energy efficiency and demand-side management operations - state of the art, bottom-up methods, applied examples and approach for the development of a practical culture of evaluation. energy end-use efficiency (ee) is a priority for energy policies to face resources exhaustion and to reduce pollutant emissions.&lt;br /&gt;at the same time, in france, local level is increasingly involved into the implementation of ee activities, whose frame is changing (energy market liberalisation, new policy instruments). needs for ex-post evaluation of the local ee activities are thus increasing, for regulation requirements and to support a necessary change of scale. our thesis focuses on the original issue of the ex-post evaluation of local ee operations in france.&lt;br /&gt;the state of the art, through the analysis of the american and european experiences and of the reference guidebooks, gives a substantial methodological material and emphasises the key evaluation issues. concurrently, local ee operations in france are characterized by an analysis of their environment and a work on their segmentation criteria. the combination of these criteria with the key evaluation issues provides an analysis framework used as the basis for the composition of evaluation methods.&lt;br /&gt;this also highlights the specific evaluation needs for local operations.&lt;br /&gt;a methodology is then developed to complete and adapt the existing material to design evaluation methods for local operations, so that stakeholders can easily appropriate. evaluation results thus feed a know-how building process with experience feedback. these methods are to meet two main goals: to determine the operation results, and to detect the success/failure factors.&lt;br /&gt;the methodology was validated on concrete cases, where these objectives were reached.
extended air shower simulations based on epos. we discuss air shower simulations based on the epos hadronic interaction model. a remarkable feature is the fact that the number of produced muons is considerably larger compared to other interaction models. we show that this is due to an improved treatment of baryon-antibaryon production.
kinematic and stiffness analysis of the orthoglide, a pkm with simple, regular workspace and homogeneous performances. the orthoglide is a delta-type pkm dedicated to 3-axis rapid machining applications that was originally developed at irccyn in 2000-2001 to meet the advantages of both serial 3-axis machines (regular workspace and homogeneous performances) and parallel kinematic architectures (good dynamic performances and stiffness). this machine has three fixed parallel linear joints that are mounted orthogonally. the geometric parameters of the orthoglide were defined as function of the size of a prescribed cubic cartesian workspace that is free of singularities and internal collision. the interesting features of the orthoglide are a regular cartesian workspace shape, uniform performances in all directions and good compactness. in this paper, a new method is proposed to analyze the stiffness of overconstrained delta-type manipulators, such as the orthoglide. the orthoglide is then benchmarked according to geometric, kinematic and stiffness criteria: workspace to footprint ratio, velocity and force transmission factors, sensitivity to geometric errors, torsional stiffness and translational stiffness.
inclusive cross section and double helicity asymmetry for \pi^0 production in p+p collisions at sqrt(s)=200 gev: implications for the polarized gluon distribution in the proton. the phenix experiment presents results from the rhic 2005 run with polarized proton collisions at sqrt(s)=200 gev, for inclusive \pi^0 production at mid-rapidity. unpolarized cross section results are given for transverse momenta p_t=0.5 to 20 gev/c, extending the range of published data to both lower and higher p_t. the cross section is described well for p_t &lt; 1 gev/c by an exponential in p_t, and, for p_t &gt; 2 gev/c, by perturbative qcd. double helicity asymmetries a_ll are presented based on a factor of five improvement in uncertainties as compared to previously published results, due to both an improved beam polarization of 50%, and to higher integrated luminosity. these measurements are sensitive to the gluon polarization in the proton, and exclude maximal values for the gluon polarization.
analysis of emission mechanisms in nucleon-induced reactions around the fermi energy. neutron-induced reactions around the fermi energy are investigated in the framework of the microscopic dywan model. comparisons with experimental data in particle spectra are performed. special attention has been devoted to pre-equilibrium emissions. the theoretical results show that the emission processes are sensitive to the nucleon–nucleon cross-section and to the characteristics of the self-consistent nuclear mean field, as the isospin dependence and the equation of state. accordingly, nucleon-induced reactions provide a precious probe of the nuclear interaction in the concerned energy domain.
(n,xn) measurements at 96 mev. double differential cross section for neutron production were measured in 96mev neutrons induced reactions at the tsl laboratory in uppsala (sweden). measurements for fe and pb targets were performed using simultaneously two independent setups: decoi-demon and clodia-scandal. the double differential cross section were measured for an angular range between 15 and 100 degrees and with low-energy thresholds (1–2 mev). elastic distribution, angular distribution, energy distribution and total inelastic cross section were derived from measured double differential cross section. results are compared with predictions given by several simulation codes and with other experimental data.
measurement of density correlations in pseudorapidity via charged particle multiplicity fluctuations in au+au collisions at $\sqrt{s_{nn}}$=200 gev. longitudinal density correlations of produced matter in au+au collisions at sqrt(s_nn)=200 gev have been measured from the inclusive charged particle distributions as a function of pseudorapidity window sizes. the extracted \alpha \xi parameter, related to the susceptibility of the density fluctuations in the long wavelength limit, exhibits a non-monotonic behavior as a function of the number of participant nucleons, n_part. a local maximum is seen at n_part ~ 90, with corresponding energy density based on the bjorken picture of \epsilon_bj \tau ~ 2.4 gev/(fm^2 c) with a transverse area size of 60 fm^2. this behavior may suggest a critical phase boundary based on the ginzburg-landau framework.
energy dependence of $\pi^{\pm}$, $p$ and $\bar{p}$ transverse momentum spectra for au+au collisions at $\sqrt{s_{nn}}$=62.4 and 200 gev. we study the energy dependence of the transverse momentum $(p_t)$ spectra for charged pions, protons and anti-protons for au+au collisions at $\sqrt{s_{nn}$=62.4 and 200 gev. data are presented at mid-rapidity (|y|&lt;0.5) for $0.2\le p_t\le12 $ gev/c . in the intermediate $p_t$ region $(0.2\le p_t\le 6 gev/c$ ), the nuclear modification factor is higher at 62.4 gev than at 200 gev, while at higher $p_t (p_t\ge 7 gev/c)$ the modification is similar for both energies. the p/$\pi+$ and $\bar{p} \pi^-$ ratios for central collisions at $\sqrt{s_{nn}}$ =62.4 gev/c peak at $p_t \appeq 2 gev/c$ . in the $p_t$ range where recombination is expected to dominate, the p/$\pi + ratios at 62.4 gev are larger than at 200 gev, while the $\bar{p}$/$\pi _-} ratios are smaller. for $p_t$, the $\bar{p} /\pi_-$ ratios at the two beam energies are independent of $p_t$ and centrality indicating that the dependence of the $\bar{p}:\pi_-$ ratio on $p_t$ does not change between 62.4 and 200 gev. these findings challenge various models incorporating jet quenching and/or constituent quark coalescence.
analysis of the dilepton invariant mass spectrum in c + c collisions at 2a and 1a gev. recently the hades collaboration has published the invariant mass spectrum of $e^+e^-$ pairs, $dn/dm_{e^+e^-}$, produced in c + c collisions at 2a gev. using electromagnetic probes, one hopes to get information from this experiment on hadron properties at high density and temperature. simulations show that firm conclusions on possible in-medium modifications of meson properties will only be possible when the elementary meson production cross sections, especially in the pn channel, as well as production cross sections of baryonic resonances are better known. presently one can conclude that (i) simulations overpredict by far the cross section at $me^+e^-\simm_{\omega}^0$ if free production cross sections are used and that (ii) the upper limit of the $\eta$ decay into $e^+e^-$ is smaller than the present upper limit of the particle data group. this is the result of simulations using the isospin quantum molecular dynamics approach.
scaling properties of azimuthal anisotropy in au+au and cu+cu collisions at $\sqrt{s_{nn}}$ = 200 gev. detailed differential measurements of the elliptic flow for particles produced in au+au and cu+cu collisions at sqrt(s_nn) = 200 gev are presented. predictions from perfect fluid hydrodynamics for the scaling of the elliptic flow coefficient v_2 with eccentricity, system size and transverse energy are tested and validated. for transverse kinetic energies ke_t ~ m_t-m up to ~1 gev, scaling compatible with the hydrodynamic expansion of a thermalized fluid is observed for all produced particles. for large values of ke_t, the mesons and baryons scale separately. a universal scaling for the flow of both mesons and baryons is observed for the full transverse kinetic energy range of the data when quark number scaling is employed. in both cases the scaling is more pronounced in terms of ke_t rather than transverse momentum.
forward $\lambda$ production and nuclear stopping power in d + au collisions at $\sqrt{s_{nn}}$ = 200 gev. we report the measurement of lamda and anti-lamda yields and inverse slope parameters in d + au collisions at sqrt(s_nn) = 200 gev at forward and backward rapidities (y = +- 2.75), using data from the star forward time projection chambers. the contributions of different processes to baryon transport and particle production are probed exploiting the inherent asymmetry of the d + au system. comparisons to model calculations show that the baryon transport on the deuteron side is consistent with multiple collisions of the deuteron nucleons with gold participants. on the gold side hijing based models do not describe the measured particle yields while models with initial state nuclear effects and/or hadronic rescattering do. the multi-chain model can provide a good description of the net baryon density in d + au collisions at rhic, and the derived parameters of the model agree with those from nuclear collisions at lower energies.
transverse momentum and centrality dependence of dihadron correlations in au+au collisions at $\sqrt{s_{nn}}$=200 gev: jet-quenching and the response of partonic matter. azimuthal angle \delta\phi correlations are presented for charged hadrons from dijets for 0.4 &lt; p_t &lt; 10 gev/c in au+au collisions at sqrt(s_nn) = 200 gev. with increasing p_t, the away-side distribution evolves from a broad to a concave shape, then to a convex shape. comparisons to p+p data suggest that the away-side can be divided into a partially suppressed "head" region centered at delta\phi ~ \pi, and an enhanced "shoulder" region centered at delta\phi ~ \pi +/- 1.1. the p_t spectrum for the "head" region softens toward central collisions, consistent with the onset of jet quenching. the spectral slope for the "shoulder" region is independent of centrality and trigger p_t, which offers constraints on energy transport mechanisms and suggests that the "shoulder" region contains the medium response to energetic jets.
enhanced strange baryon production in au+au collisions compared to p+p at $\sqrt{s_{nn}}$ = 200 gev. we report on the observed differences in production rates of strange and multi-strange baryons in au+au collisions at sqrts = 200 gev compared to p+p interactions at the same energy. the yields in au+au collisions, when scaled by the number of participants, are larger than those measured in the p+p data. the magnitudes of the differences grow with the strangeness of the baryon and with increasing centrality. the enhancements of the au+au yields are close to those measured in sqrts = 17.3 gev collisions. further, when the binary scaled p+p pt spectra are compared to those of au+au the heavy-ion yields exceed binary scaling in the pt range 1 &lt; pt&lt; 4 gev/c.
measurement of transverse single-spin asymmetries for di-jet production in proton-proton collisions at $\sqrt{s} = 200$ gev. we report the first measurement of the opening angle distribution between pairs of jets produced in high-energy collisions of transversely polarized protons. the measurement probes (sivers) correlations between the transverse spin orientation of a proton and the transverse momentum directions of its partons. with both beams polarized, the wide pseudorapidity ($-1 \leq \eta \leq +2$) coverage for jets permits separation of sivers functions for the valence and sea regions. the resulting asymmetries are all consistent with zero and considerably smaller than sivers effects observed in semi-inclusive deep inelastic scattering (sidis). we discuss theoretical attempts to reconcile the new results with the sizable transverse spin effects seen in sidis and forward hadron production in pp collisions.
cryptoreality of nonanticommutative hamiltonians. we note that, though nonanticommutative (nac) deformations of minkowski supersymmetric theories do not respect the reality condition and seem to lead to non-hermitian hamiltonians h, the latter belong to the class of "cryptoreal'' hamiltonians considered recently by bender and collaborators. they can be made manifestly hermitian via the similarity transformation h -&gt; exp{r} h exp{-r} with a properly chosen r. the deformed model enjoys the same supersymmetry algebra as the undeformed one, though being realized differently on the involved canonical variables. besides quantum-mechanical models, we treat, along similar lines, some nac deformed field models in 4d minkowski space.
energy loss of a heavy quark produced in a finite-size quark-gluon plasma. we study the energy loss of an energetic heavy quark produced in a high temperature quark-gluon plasma and travelling a finite distance before emerging in the vacuum. while the retardation time of purely collisional energy loss is found to be of the order of the debye screening length, we find that the contributions from transition radiation and the ter-mikayelian effect do not compensate, leading to a reduction of the zeroth order (in an opacity expansion) energy loss.
correlations of neutral and charged particles in $^{40}$ar- $^{58}$ni reaction at 77 mev/u. the measurement of the two-particle correlation function for different particle species allows to obtain information about the development of the particle emission process: the space-time properties of emitting sources and the emission time sequence of different particles. the single-particle characteristics and two-particle correlation functions for neutral and charged particles registered in forward direction are used to determine that the heavy fragments (deuterons and tritons) are emitted in the first stage of the reaction (pre-equilibrium source) while the majority of neutrons and protons originates from the long-lived quasi-projectile. the emission time sequence of protons, neutrons and deuterons has been obtained from the analysis of non-identical particle correlation functions.
evidence for a long-range component in the pion emission source in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. emission source functions are extracted from correlation functions constructed from charged pions produced at mid-rapidity in au+au collisions at sqrt(s_nn)=200 gev. the source parameters extracted from these functions at low k_t, give first indications of a long tail for the pion emission source. the source extension cannot be explained solely by simple kinematic considerations. the possible role of a halo of secondary pions from resonance emissions is explored.
nuclear medical imaging using $\beta+\gamma$ coincidences from $^{44}$sc radio-nuclide with liquid xenon as detection medium. we report on a new nuclear medical imaging technique based on the measurement of the emitter location in the three dimensions with a few mm spatial resolution using β+γ emitters. such measurement could be realized thanks to a new kind of radio-nuclides which emit a γ-ray quasi-simultaneously with the β+ decay. the most interesting radio-nuclide candidate, namely 44sc, will be potentially produced at the nantes cyclotron arronax. the principle is to reconstruct the intersection of the classical line of response (obtained with a standard pet camera) with the direction cone defined by the third γ-ray. the emission angle measurement of this additional γ-ray involves the use of a compton telescope for which a new generation of camera based on a liquid xenon (lxe) time projection chamber is considered. geant3 simulations of a large acceptance lxe compton telescope combined with a commercial micro-pet (lso crystals) have been performed and the obtained results will be presented. they demonstrate that a good image can be obtained from the accumulation of each three-dimensional measured position. a spatial resolution of 2.3 mm has been reached with an injected activity of 0.5 mbq for a 44sc point source emitter.
calibration of quasi-isotropic parallel kinematic machines: orthoglide. the paper proposes a novel approach for the geometrical model calibration of quasi-isotropic parallel kinematic mechanisms of the orthoglide family. it is based on the observations of the manipulator leg parallelism during motions between the specific test postures and employs a low-cost measuring system composed of standard comparator indicators attached to the universal magnetic stands. they are sequentially used for measuring the deviation of the relevant leg location while the manipulator moves the tcp along the cartesian axes. using the measured differences, the developed algorithm estimates the joint offsets and the leg lengths that are treated as the most essential parameters. validity of the proposed calibration technique is confirmed by the experimental results.
residual correlations between decay products of $\pi^0\pi^0$ and $p\sigma^0$ systems. residual correlations between decay products due to a combination of both correlations between parents at small relative velocities and small decay momenta are discussed. residual correlations between photons from pion decays are considered as a new possible source of information on direct photon fraction. residual correlations in $p\gamma$ and $p\lambda$ systems due to $p\sigma^0$ interaction in final state are predicted based on the $p\sigma^0$ low energy scattering parameters deduced from the spin-flavour su$_6$ model by fujiwara et al. including effective meson exchange potentials and explicit flavour symmetry breaking to reproduce the properties of the two-nucleon system and the low-energy hyperon-nucleon cross section data. the $p\gamma_{\sigma^0}$ residual correlation is concentrated at $k^* \approx 70$ mev/$c$ and its shape and intensity appears to be sensitive to the scattering parameters and space-time dimensions of the source. the $p\lambda_{\sigma^0}$ residual correlation recovers the negative parent $p\sigma^0$ correlation for $k^* &gt; 70$ mev/$c$. the neglect of this negative residual correlation would lead to the underestimation of the parent $p\lambda$ correlation effect and to an overestimation of the source size.
prompt photon identification in the alice experiment: the isolation cut method. the alice experiment at lhc will detect and identify prompt photons and light neutral mesons with the phos and emcal detectors. charged particles will be detected and identified by the central tracking system. in this paper, a method to identify prompt photons and to separate them from the background of hadrons and decay photons in phos with the help of isolation cuts is presented.
alice: physics performance report, volume ii. alice is a general-purpose heavy-ion experiment designed to study the physics of strongly interacting matter and the quark–gluon plasma in nucleus–nucleus collisions at the lhc. it currently involves more than 900 physicists and senior engineers, from both the nuclear and high-energy physics sectors, from over 90 institutions in about 30 countries. the alice detector is designed to cope with the highest particle multiplicities above those anticipated for pb–pb collisions (dnch/dy up to 8000) and it will be operational at the start-up of the lhc. in addition to heavy systems, the alice collaboration will study collisions of lower-mass ions, which are a means of varying the energy density, and protons (both pp and pa), which primarily provide reference data for the nucleus–nucleus collisions. in addition, the pp data will allow for a number of genuine pp physics studies. the detailed design of the different detector systems has been laid down in a number of technical design reports issued between mid-1998 and the end of 2004. the experiment is currently under construction and will be ready for data taking with both proton and heavy-ion beams at the start-up of the lhc. since the comprehensive information on detector and physics performance was last published in the alice technical proposal in 1996, the detector, as well as simulation, reconstruction and analysis software have undergone significant development. the physics performance report (ppr) provides an updated and comprehensive summary of the performance of the various alice subsystems, including updates to the technical design reports, as appropriate. the ppr is divided into two volumes. volume i, published in 2004 (cern/lhcc 2003-049, alice collaboration 2004 j. phys. g: nucl. part. phys. 30 1517–1763), contains in four chapters a short theoretical overview and an extensive reference list concerning the physics topics of interest to alice, the experimental conditions at the lhc, a short summary and update of the subsystem designs, and a description of the offline framework and monte carlo event generators. the present volume, volume ii, contains the majority of the information relevant to the physics performance in proton–proton, proton–nucleus, and nucleus–nucleus collisions. following an introductory overview, chapter 5 describes the combined detector performance and the event reconstruction procedures, based on detailed simulations of the individual subsystems. chapter 6 describes the analysis and physics reach for a representative sample of physics observables, from global event characteristics to hard processes.
partonic flow and $\phi$-meson production in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. we present first measurements of the $\phi$-meson elliptic flow ($v_{2}(p_{t})$) and high statistics $p_{t}$ distributions for different centralities from $\sqrt{s_{nn}}$ = 200 gev au+au collisions at rhic. in minimum bias collisions the $v_{2}$ of the $\phi$ meson is consistent with the trend observed for mesons. the ratio of the yields of the $\omega$ to those of the $\phi$ as a function of transverse momentum is consistent with a model based on the recombination of thermal $s$ quarks up to $p_{t}\sim 4$ gev/$c$, but disagrees at higher momenta. the nuclear modification factor ($r_{cp}$) of $\phi$ follows the trend observed in the $k^{0}_{s}$ mesons rather than in $\lambda$ baryons, supporting baryon-meson scaling. since $\phi$-mesons are made via coalescence of seemingly thermalized $s$ quarks in central au+au collisions, the observations imply hot and dense matter with partonic collectivity has been formed at rhic.
extraction of the high transverse momentum photons in proton+proton collisions at 200 gev in the phenix experiment at rhic. ultra-relativistic heavy ions collisions allow to reach an hot and dense matter. this new state, called quarks and gluons plasma (qgp), would exist at the first moment of our universe according to the big bang theory. the phenix experiment, one of the interaction point of the rhic collider at brookhaven national laboratory (usa), aims to study the qgp's signatures. photons don't interact strongly with the matter and so are an accurate tool to explore the phase of qgp. moreover photons are emitted during all the phases of the nuclear collision: from the initial state to the final hadronization. we will present a direct photon, produced by hard scattering process in the beginning of the collision, identification method (sica, spectroscopic isolation cut analysis) applied on p+p collisions at 200 gev. this method allows for a better discrimination between direct photons and the other contribution (mainly the electromagnetic decay of the neutral pion). one could find in this thesis the direct photon rate production obtained by sica and compared to other analysis. with the p+p collisions we have an important reference for the more heavier collisions (au+au) where we assume the qgp formation.
immobilization of inert triso-coated fuel in glass for geological disposal. vitrification of triso-coated gas reactor fuel particles was achieved via two methods: glass melting and sintering. inert triso-coated fuel particles and a borosilicate glass were used. with glass melting at 1200–1300 °c floatation and decomposition of carbon and silicon carbide occurred. thermal pre-treatment of the particles for oxidation of pyrocarbon did not improve the coating properties of the glass. during cooling most of the particles floated and sorbed on the crucible or mold walls. the sintered glass at 700 °c showed better coating properties of the triso-coated fuel particles despite higher porosity compared to glass made by melting. aqueous leaching properties of glass with particles are similar regardless the mode of fabrication, indicating the good chemical durability of the sintered glass. sintered glasses may constitute a good technique for triso-coated fuel particles immobilization for an eventual deep geological disposal.
the energy dependence of p$_t$ angular correlations inferred from mean-pt fluctuation scale dependence in heavy ion collisions at the sps and rhic. we present the first study of the energy dependence of p$_t$ angular correlations inferred from event-wise mean transverse momentum (p$_$t) fluctuations in heavy ion collisions. we compare our large-acceptance measurements at cm energies $\sqrt{^{s}nn}$=19.6, 62.4, 130 and 200 gev to sps measurements at 12.3 and 17.3 gev. p$_t$ angular correlation structure suggests that the principal source of pt correlations and fluctuations is minijets (minimum-bias parton fragments). we observe a dramatic increase in correlations and fluctuations from sps to rhic energies, increasing linearly with ln$\sqrt{^s}nn}$ from the onset of observable jet-related p$_t$ fluctuations near 10 gev.
charged particle distributions and nuclear modification at high rapidities in d+au collisions at $\sqrt{s_{nn}}$ = 200 gev. the measurements of the centrality dependence of $dn/d\eta$ and transverse momentum spectra from mid- to forward rapidity in d+au collisions at $\sqrt{s_{_{\rm nn}}} =$ 200 gev are reported. they provide a sensitive tool for understanding the dynamics of multi-particle production in the high parton-density regime. in particular, we observe strong suppression of the nuclear modification factor $r_{cp}$ at forward rapidities (d-side, $\eta$ = 3.1) and enhancement at backward rapidity ($\eta$ = $-$3.1). an empirical scaling is obtained for multiplicity and $r_{cp}$ when a shift of the center-of-mass in the asymmetric d+au collisions with respect to the nucleon-nucleon system is applied.
scaling properties of hyperon production in au+au collisions at $\sqrt{s_{nn}}$=200 gev. we present the scaling properties of lambda, xi, and omega in midrapidity au+au collisions at the brookhaven national laboratory relativistic heavy ion collider at root s(nn)=200 gev. the yield of multistrange baryons per participant nucleon increases from peripheral to central collisions more rapidly than that of lambda, indicating an increase of the strange-quark density of the matter produced. the strange phase-space occupancy factor gamma(s) approaches unity for the most central collisions. moreover, the nuclear modification factors of p, lambda, and xi are consistent with each other for 2 &lt; p(t) &lt; 5 gev/c in agreement with a scenario of hadron formation from constituent quark degrees of freedom.
upgrade of the codalema eas radio-detection experiment. in order to improve the performances of the eas radio-measurements, the codalema has been upgraded. the detection array has been widen and new types of antennas and particle detectors are used. first results and new possibilities given by this new configuration will be presented.
multiple-humped fission and fusion barriers of actinide and superheavy elements. the energy of a deformed nucleus has been determined within a generalized liquid drop model taking into account the proximity energy, the microscopic corrections and quasi-molecular shapes. in the potential barrier a third peak exists for actinides when one fragment is close to a magic spherical nucleus while the other one varies from oblate to prolate shapes. the barrier heights and half-lives agree with the experimental data. the different entrance channels leading possibly to superheavy elements are studied as well as their $\alpha$-decay.
binary fission and coplanar cluster decay of $^{60}$zn compound nuclei at high angular momentum. fission decays with two heavy fragments in coincidence have been measured from $^{60}$ni compound nuclei, formed in the $^{36}$ar + $^{24}$mg reaction at $e_{lab}(^{36}ar=195 mev$ (5.4 mev/a). the experiment was performed with a unique kinematic coincidence set-up consisting of two large area position sensitive (x,y) gas detector telescopes with bragg-ionisation chambers. very narrow out-of-plane correlations are observed for two heavy fragments emitted in either purely binary events or in events with a missing mass consisting of 2, 3 and 4 $\alpha$-particles. the broad out-of-plane distributions correspond to binary fission with the evaporation of $\alpha$-particles or nucleons from excited fragments. the narrow correlations are interpreted as ternary coplanar cluster decay from compound nuclei at high angular momenta through elongated shapes with large moments of inertia, and the lighter mass remains with very low momentum in the centre of mass frame. it is shown in calculations of shapes, that the large moments of inertia of the hyper-deformed configurations expected in these nuclei, will lower the ternary fission barrier so as to make a competition with binary fission possible.
measurement of direct photon production in p + p collisions at $\sqrt{s}$ = 200 gev. cross sections for mid-rapidity production of direct photons in p+p collisions at the relativistic heavy ion collider (rhic) are reported for 3 &lt; p_t &lt; 16 gev/c. next-to-leading order (nlo) perturbative qcd (pqcd) describes the data well for p_t &gt; 5 gev/c, where the uncertainties of the measurement and theory are comparable. we also report on the effect of requiring the photons to be isolated from parton jet energy. the observed fraction of isolated photons is well described by pqcd for p_t &gt; 7 gev/c.
alice potential for heavy flavour physics. heavy quarks will be abundantly produced in heavy ion collisions at lhc energies. both, the production of open heavy flavoured mesons and quarkonia will probe the strongly interacting medium created in these reactions. in particular, the alice detector will be able to measure heavy flavour production down to low transverse momentum, combining leptonic and hadronic channels, covering a large rapidity range $|eta|&lt;0.9$ and $-4&lt;-2.5$. in this talk we will present the main physics motivations for the study of heavy flavour production at lhc energies and some examples of physics analyses developed by the heavy flavour working group of alice.
proton-proton, anti-proton-anti-proton, proton-anti-proton correlations in au + au collisions measured by star at rhic. the analysis of two-particle correlations provides a powerful tool to study the properties of hot and dense matter created in heavy-ion collisions at ultra-relativistic energies. applied to identical and non-identical hadron pairs, it makes the study of space-time evolution of the source in femtoscopic scale possible. baryon femtoscopy allows extraction of the radii of produced sources which can be compared to those deduced from identical pion studies, providing complete information about the source characteristics. in this paper we present the correlation functions obtained for identical and non-identical baryon pairs of protons and anti-protons. the data were collected recently in au+au collisions at $\sqrt{s_{nn}}$ =62 gev and $\sqrt{s_{nn}}$ =200 gev by the star detector at the rhic accelerator. we introduce corrections to the baryon-baryon correlations taking into account: residual correlations from weak decays, particle identification probability and the fraction of primary baryons. finally we compare our results to theoretical predictions.
l'imagerie médicale 3 gammas - nuclear imagery 3 gammas. null
which radionuclides will nuclear oncology need tomorrow?. null
transverse momentum and centrality dependence of high-pt non-photonic electron suppression in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. the star collaboration at rhic reports measurements of the inclusive yield of non-photonic electrons, which arise dominantly from semi-leptonic decays of heavy flavor mesons, over a broad range of transverse momenta ($1.2 &lt; \pt &lt; 10$ \gevc) in \pp, \dau, and \auau collisions at \sqrtsnn = 200 gev. the non-photonic electron yield exhibits unexpectedly large suppression in central \auau collisions at high \pt, suggesting substantial heavy quark energy loss in hot qcd matter. the centrality and \pt dependences of the suppression provide stringent constraints on theoretical models of suppression.
measurement of high-$p_t$ single electrons from heavy-flavor decays in p+p collisions at $\sqrt{s}$ = 200 gev. the momentum distribution of electrons from decays of heavy flavor (charm and beauty) for midrapidity |y| &lt; 0.35 in p+p collisions at sqrt(s) = 200 gev has been measured by the phenix experiment at the relativistic heavy ion collider (rhic) over the transverse momentum range 0.3 &lt; p_t &lt; 9 gev/c. two independent methods have been used to determine the heavy flavor yields, and the results are in good agreement with each other. a fixed-order-plus-next-to-leading-log pqcd calculation agrees with the data within the theoretical and experimental uncertainties, with the data/theory ratio of 1.72 +/- 0.02^stat +/- 0.19^sys for 0.3 &lt; p_t &lt; 9 gev/c. the total charm production cross section at this energy has also been deduced to be sigma_(c c^bar) = 567 +/- 57^stat +/- 224^sys micro barns.
longitudinal double-spin asymmetry and cross section for inclusive jet production in polarized proton collisions at $\sqrt{s}$ =200 gev. we report a measurement of the longitudinal double-spin asymmetry a(ll) and the differential cross section for inclusive midrapidity jet production in polarized proton collisions at s=200 gev. the cross section data cover transverse momenta 5 &lt; p(t)&lt; 50 gev/c and agree with next-to-leading order perturbative qcd evaluations. the a(ll) data cover 5 &lt; p(t)&lt; 17 gev/c and disfavor at 98% c.l. maximal positive gluon polarization in the polarized nucleon.
recent astrophysical and accelerator based results on the hadronic equation of state. in astrophysics as well as in hadron physics progress has recently been made on the determination of the hadronic equation of state (eos) of compressed matter. the results are contradictory, however. simulations of heavy ion reactions are now sufficiently robust to predict the stiffness of the (eos) from (i) the energy dependence of the ratio of $k^+$ from au+au and c+c collisions and (ii) the centrality dependence of the $k^+$ multiplicities. the data are best described with a compressibility coefficient at normal nuclear matter density $\kappa$ around 200 mev, a value which is usually called ``soft'' the recent observation of a neutron star with a mass of twice the solar mass is only compatible with theoretical predictions if the eos is stiff. we review the present situation.
modeling the complexation properties of mineral-bound organic polyelectrolyte: an attempt at comprehension using the model system alumina/polyacrylic acid/m (m = eu, cm, gd). this paper contributes to the comprehension of kinetic and equilibrium phenomena governing metal ion sorption on organic-matter-coated mineral particles. sorption and desorption experiments were carried out with eu ion and polyacrylic acid (paa)-coated alumina colloids at ph 5 in 0.1 m naclo4 as a function of the metal ion loading. under these conditions, m interaction with the solid is governed by sorbed paa (paaads). the results were compared with spectroscopic data obtained by time-resolved laser-induced fluorescence spectroscopy (trlfs) with cm and gd. the interaction between m and paaads was characterized by a kinetically controlled process: after rapid metal adsorption within less than 1 min, the speciation of complexed m changed at the particle surface till an equilibrium was reached after about 4 days. at equilibrium, one part of complexed m was shown to be not exchangeable. this process was strongly dependent on the ligand-to-metal ratio. two models were tested to explain the data. in model 1, the kinetically controlled process was described through successive kinetically controlled reactions that follow the rapid metal ion adsorption. in model 2, the organic layer was considered as a porous medium: the kinetic process was explained by the diffusion of m from the surface into the organic layer. model 1 allowed a very good description of equilibrium and kinetic experimental data. model 2 could describe the data at equilibrium but could not explain the kinetic data accurately. in spite of this disagreement, model 2 appeared more realistic considering the results of the trlfs measurements.
analysis of dilepton invariant mass spectrum in c+c at 2 and 1 agev. recently the hades collaboration has published the invariant mass spectrum of $e^+e^-$ pairs, dn/dm$_{e^+e^-}$, produced in c+c collisions at 2 agev. using electromagnetic probes, one hopes to get in this experiment information on hadron properties at high density and temperature. simulations show that firm conclusions on possible in-medium modifications of meson properties will only be possible when the elementary meson production cross sections, especially in the pn channel, as well as production cross sections of baryonic resonances are better known. presently one can conclude that a) simulations overpredict by far the cross section at $m_{e^+e^-} \approx m_{\omega}{^0}$ if free production cross sections are used and that b) the upper limit of the $\eta$ decay into $e^+e^-$ is smaller than the present upper limit of the particle data group. this is the result of simulations using the isospin quantum molecular dynamics (iqmd) approach.
nuclear surface effects in heavy-ion collisions at rhic and sps. one may divide the collision zone in nuclear collisions into a central part ('core') with expected high energy densities, and a peripheral part ('corona') with smaller energy densities more like in pp or pa collisions. we discuss the consequences of the core–corona separation within the epos approach. the complicated centrality dependence of hadron yields observed at rhic and sps becomes almost trivial after subtracting the corona background.
electroweak boson detection in the alice muon spectrometer: w and z studies in hadron–hadron collisions at lhc. the alice capabilities for w and z detection at lhc are discussed. the contributions to single muon transverse momentum distribution are evaluated. the expected performance of the muon spectrometer for detecting w and z bosons via their muonic decay is presented. possible application for the study of in-medium effects in ultrarelativistic heavy ion collisions is discussed.
muon production in extended air shower simulations. whereas air shower simulations are very valuable tools for interpreting cosmic ray data, there is a long standing problem: is seems to be impossible to accommodate at the same time the longitudinal development of air showers and the number of muons measured at ground. using a new hadronic interaction model (epos) in air shower simulations produces considerably more muons, in agreement with results from the hires-mia experiment. we find that this is mainly due to a better description of baryon-antibaryon production in hadronic interactions. this is a new aspect of air shower physics which has never been considered so far.
multiple-humped fission and fusion barriers of the heaviest elements and ellipsoidal deformations. the deformation energy in the fusionlike deformation path has been determined from a generalized liquid drop model taking into account both the proximity energy, the asymmetry and the microscopic corrections. multiple-humped potential barriers appear in the exit and entrance channels of the heaviest elements. in the fission path of actinides, the second maximum corresponds to the transition from compact and creviced one-body shapes to two touching ellipsoids. a third peak appears in certain asymmetric exit channels where one fragment is almost a double magic nucleus with a quasi-spherical shape while the other one evolves from oblate to prolate shapes. the heights of the double and triple-humped fission barriers and the predicted half-lives of actinides follow the experimental results. in the fusion path leading possibly to superheavy elements, double-humped potential barriers appear for cold fusion but not for warm fusion. the $\alpha$ decay half-lives can be reproduced using the experimental $q\alpha$ value.
towards documenting and automating collateral evolutions in linux device drivers. collateral evolutions are a pervasive problem in linux device driver development, due to the frequent evolution of linux driver support libraries and apis. such evolutions are needed when an evolution in a driver support library affects the library's interface, entailing modifications in all dependent device-specific code. currently, collateral evolutions in linux are done nearly manually. the large number of linux drivers, however, implies that this approach is time-consuming and unreliable, leading to subtle errors when modifications are not done consistently. in this paper, we describe the development of a language-based infrastructure, coccinelle, with the goal of documenting and automating the kinds of collateral evolutions that occur in device driver code. because linux programmers are accustomed to manipulating program modifications in terms of patch files, we base our language on the patch syntax, extending patches to semantic patches.
on the efficiency of fermi acceleration at relativistic shocks. it is shown that fermi acceleration at an ultra-relativistic shock wave cannot operate on a particle for more than 1 1/2 fermi cycle (i.e., u -&gt; d -&gt; u -&gt; d) if the particle larmor radius is much smaller than the coherence length of the magnetic field on both sides of the shock, as is usually assumed. this conclusion is shown to be in excellent agreement with recent numerical simulations. we thus argue that efficient fermi acceleration at ultra-relativistic shock waves requires significant non-linear processing of the far upstream magnetic field with strong amplification of the small scale magnetic power. the streaming or transverse weibel instabilities are likely to play a key role in this respect.
jet properties from dihadron correlations in p+p collisions at $\sqrt{s}$=200 gev. the properties of jets produced in p+p collisions at sqrt(s)=200 gev are measured using the method of two particle correlations. the trigger particle is a leading particle from a large transverse momentum jet while the associated particle comes from either the same jet or the away-side jet. analysis of the angular width of the near-side peak in the correlation function determines the jet fragmentation transverse momentum j_t . the extracted value, sqrt()= 585 +/- 6(stat) +/- 15(sys) mev/c, is constant with respect to the trigger particle transverse momentum, and comparable to the previous lower sqrt(s) measurements. the width of the away-side peak is shown to be a convolution of j_t with the fragmentation variable, z, and the partonic transverse momentum, k_t . the is determined through a combined analysis of the measured pi^0 inclusive and associated spectra using jet fragmentation functions measured in e^+e^-. collisions. the final extracted values of k_t are then determined to also be independent of the trigger particle transverse momentum, over the range measured, with value of sqrt() = 2.68 +/- 0.07(stat) +/- 0.34(sys) gev/c.
an active dipole for cosmic ray radiodetection with codalema. the codalema experiment detects the electromagnetic pulses radiated during the development of extensive air showers (eas). since 2005, in addition to spiral log-periodic antennas, ultra broad bandwidth active dipoles have been designed to detect the full electric pulse shape of these signals. a few performances of these new detectors are presented.
search for a long lived component in the reaction u+u near the coulomb barrier. we performed an experiment to search for a signature of a long living component in the collision of $^{238}$u + $^{238}$u between 6.09 and 7.35a mev. the experiment was performed at ganil using the spectrometer vamos, tuned for observing reactions with kinematics similar to fusion-fission events. theoretical calculations indicate that if a long living component would exist for this reaction, the most probable fission channel of such a giant system would be via the emissionof quasi-lead nuclei. we detected events of such a category in the focal plane of vamos. these events present an excitation function growing as a function of the bombarding energy.
energy loss and flow of heavy quarks in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. the phenix experiment at the relativistic heavy ion collider (rhic) has measured electrons from heavy flavor (charm and bottom) decays for 0.3 &lt; p_t &lt; 9 gev/c at midrapidity (|y| &lt; 0.35) in au+au collisions at sqrt(s_nn) = 200 gev. the nuclear modification factor raa relative to p+p collisions shows a strong suppression in central au+au collisions, indicating substantial energy loss of heavy quarks in the medium produced at rhic. a large azimuthal anisotropy, v_2, with respect to the reaction plane is observed for 0.5 &lt; p_t &lt; 5 gev/c indicating non-zero heavy flavor elliptic flow. a simultaneous description of raa(p_t) and v2(p_t) constrains the existing models of heavy-quark rescattering in strongly interacting matter and provides information on the transport properties of the produced medium. in particular, a viscosity to entropy density ratio close to the conjectured quantum lower bound, i.e. near a perfect fluid, is suggested.
correlated production of p and $\bar{p}$ in au+au collisions at $\sqrt{s_{nn}}$ = 200 gev. correlations between p and pbar's at transverse momenta typical of enhanced baryon production in au+au collisions are reported. the phenix experiment measures same and opposite sign baryon pairs in au+au collisions at sqrt(s_nn) = 200 gev. correlated production of p and p^bar with the trigger particle from the range 2.5 &lt; p_t &lt; 4.0 gev/c and the associated particle with 1.8 &lt; p_t &lt; 2.5 gev/c is observed to be nearly independent of the centrality of the collisions. same sign pairs show no correlation at any centrality. the conditional yield of mesons triggered by baryons (and anti-baryons) and mesons in the same pt range rises with increasing centrality, except for the most central collisions, where baryons show a significantly smaller number of associated mesons. these data are consistent with a picture in which hard scattered partons produce correlated p and p^bar in the p_t region of the baryon excess.
$j/\psi$ production vs transverse momentum and rapidity in p+p collisions at $\sqrt{s}$ = 200 gev. j/psi production in p+p collisions at sqrt(s) = 200 gev has been measured in the phenix experiment at the relativistic heavy ion collider (rhic) over a rapidity range of -2.2 &lt; y &lt; 2.2 and a transverse momentum range of 0 &lt; pt &lt; 9 gev/c. the statistics available allow a detailed measurement of both the pt and rapidity distributions and are sufficient to constrain production models. the total cross section times branching ratio determined for j/psi production is b_{ll} sigma_pp^j/psi = 178 +/- 3(stat) +/- 53(syst) +/- 18(norm) nb.
characteristics of radioelectric fields from air showers induced by uhecr measured with codalema. the detection of radio transients associated with extensive air showers eas induced by ultra high energy cosmic rays with the apparatus codalema allows for the first time the characterisation and sampling of electric field features . core location together with the electric field profile are determined on an event-by-event basis.the possibility to discriminate an eas event using a self triggering network of antennas opens the possible deployment of complementary set-ups to existing large surface detectors in order to study the longitudinal development of eas as well as a contribution to the energy determination and nature of uhecr.
system size and energy dependence of jet-induced hadron pair correlation shapes in cu+cu and au+au collisions at $\sqrt{s_{nn}}$ = 200 and 62.4 gev. we present azimuthal angle correlations of intermediate transverse momentum (1-4 gev/c) hadrons from {dijets} in cu+cu and au+au collisions at sqrt(s_nn) = 62.4 and 200 gev. the away-side dijet induced azimuthal correlation is broadened, non-gaussian, and peaked away from \delta\phi=\pi in central and semi-central collisions in all the systems. the broadening and peak location are found to depend upon the number of participants in the collision, but not on the collision energy or beam nuclei. these results are consistent with sound or shock wave models, but pose challenges to cherenkov gluon radiation models.
production of $\omega$ mesons at large transverse momenta in p+p and d+au collisions at $\sqrt{s_{nn}}$ = 200 gev. the phenix experiment at rhic has measured the invariant cross section for omega-meson production at mid-rapidity in the transverse momentum range 2.5 &lt; p_t &lt; 9.25 gev/c in p+p and d+au collisions at sqrt(s_nn) = 200 gev. measurements in two decay channels (omega --&gt; pi^0 pi^+ pi^- and omega --&gt; pi^0 gamma) yield consistent results, and the reconstructed omega mass agrees with the accepted value within the p_t range of the measurements. the omega/pi^0 ratio is found to be 0.85 +/- 0.05(stat) +/- 0.09(sys) and 0.94 +/- 0.08(stat) +/- 0.12(sys) in p+p and d+au collisions respectively, independent of p_t . the nuclear modification factor r_da is 1.03 +/- 0.12(stat) +/- 0.21(sys) and 0.83 +/- 0.21(stat) +/- 0.17(sys) in minimum bias and central (0-20%) d+au collisions, respectively.
microscopic calculations of double and triple giant resonance excitation in heavy ion collisions. we perform microscopic calculations of the inelastic cross sections for the double and triple excitation of giant resonances induced by heavy ion probes within a semicalssical coupled channels formalism. the channels are defined as eigenstates of a bosonic quartic hamiltonian constructed in terms of collective rpa phonons. therefore, they are superpositions of several multiphonon states, also with different numbers of phonons and the spectrum is anharmonic. the inclusion of (n+1) phonon configurations affects the states whose main component is a n-phonon one and leads to an appreacible lowering of their energies. we check the effects of such further anharmonicities on the previous published results for the cross section for the double excitation of giant resonances. we find that the only effect is a shift of the peaks towards lower energies, the double gr cross section being not modified by the explicity inclusion of the three-phonon channels in the dynamical calculations. the latters give an important contribution to the cross section in the triple gr energy region which however is still smaller than the experimental available data. the inclusion of four phonon configurations in the structure calculations does not modify the results.
radioelectric field features of extensive air showers observed with codalema. based on a new approach to the detection of radio transients associated with extensive air showers induced by ultra high energy cosmic rays, the experimental apparatus codalema is in operation, measuring about 1 event per day corresponding to an energy threshold ~ 5. 10^16 ev. its performance makes possible for the first time the study of radio-signal features on an event-by-event basis. the sampling of the magnitude of the electric field along a 600 meters axis is analyzed. it shows that the electric field lateral spread is around 250 m (fwhm). the possibility to determine with radio both arrival directions and shower core positions is discussed.
forward neutral pion production in p+p and d+au collisions at $\sqrt{^{s}nn}$=200 gev. measurements of the production of forward pi(0) mesons from p+p and d+au collisions at root s(nn) = 200 gev are reported. the p+p yield generally agrees with next-to-leading order perturbative qcd calculations. the d+au yield per binary collision is suppressed as eta increases, decreasing to similar to 30% of the p+p yield at =4.00, well below shadowing expectations. exploratory measurements of azimuthal correlations of the forward pi(0) with charged hadrons at eta approximate to 0 show a recoil peak in p+p that is suppressed in d+au at low pion energy. these observations are qualitatively consistent with a saturation picture of the low-x gluon structure of heavy nuclei.
identified baryon and meson distributions at large transverse momenta from au plus au collisions at $\sqrt{^{s}nn}$=200 gev. transverse momentum spectra of pi(+/-), p, and (p) over bar p up to 12 gev/c at midrapidity in centrality selected au + au collisions at root s(nn) = 200 gev are presented. in central au + au collisions, both pi(+/-) and p((p) over bar) show significant suppression with respect to binary scaling at p(t) greater than or similar to 4 gev/c. protons and antiprotons are less suppressed than pi(+/-), in the range 1.5 less than or similar to p(t) less than or similar to 6 gev/c. the pi(-)/pi(+) and (p) over bar /p ratios show at most a weak pt dependence and no significant centrality dependence. the p/pi ratios in central au + au collisions approach the values in p + p and d + au collisions at p(t) greater than or similar to 5 gev/c. the results at high p(t) indicate that the partonic sources of pi(+/-), p, and (p) over bar have similar energy loss when traversing the nuclear medium.
physics process of cosmogenics isotopes production on muons interactions with carbon target in liquid scintillator. in the present paper we will focuse on the problematic of comogenics isotopes, in particular li-9 and he-8, which are produced by muons interactions with the carbon target contained in the liquid scintillator. the study of this kind of isotopes has a major role in reactor neutrino experiments because their ability to mimic the detection reaction. we will discuss the physics porcess which dominate the production of such isotopes, after we will show via comparisons with availiable data the reliability of this point of view.
one-dimensional hybrid approach to extensive air shower simulation. an efficient scheme for one-dimensional extensive air shower simulation and its implementation in the program conex are presented. explicit monte carlo simulation of the high-energy part of hadronic and electromagnetic cascades in the atmosphere is combined with a numeric solution of cascade equations for smaller energy sub-showers to obtain accurate shower predictions. the developed scheme allows us to calculate not only observables related to the number of particles (shower size) but also ionization energy deposit profiles which are needed for the interpretation of data of experiments employing the fluorescence light technique. we discuss in detail the basic algorithms developed and illustrate the power of the method. it is shown that monte carlo, numerical, and hybrid air shower calculations give consistent results which agree very well with those obtained within the corsika program.
chemical basis properties of 211-astatine, a potential radionuclide in alpha-immunotherapy. null
experimental investigations of colloids and eu(iii)/ni(ii) adsorption properties of the cavallova-oxfordian claystone from bure (france). null
comparison of complexed species of eu in alumina-bound and free polyacrylic acid: a spectroscopic study. the speciation of eu complexed with polyacrylic acid (paa) and alumina-bound paa (paaads) was studied at ph 5 in 0.1 m naclo4. structural parameters were obtained from 7f0 → 5d0 excitation spectra measured by laser-induced fluorescence spectroscopy as well as from eu liii-edge extended x-ray absorption fine structure (exafs) spectra. the coordination mode was also investigated by infrared spectroscopy. to elucidate the nature of the complexed species, eu–acetate complexes were used as references. the spectroscopic techniques show that two carboxylate groups with 2–3 (eupaa) and 4–5 (eupaaads) water molecules are coordinated to eu in the first coordination sphere. for eupaaads, the coordination between carboxylate groups and eu appears to be bidendate. a similar coordination is probable for eupaa but the exafs data indicate a slightly distorted coordination. the results show that the degree of freedom of carboxylate groups is not the same for free or adsorbed paa. for paa, the degree of freedom is constrained by the flexibility of the methylene chain. when paa is adsorbed on alumina, the polymer chains cannot any more be treated as independent chains. one may rather assume formation of aggregates that form an organic layer at the mineral surface presenting a complex arrangement of carboxylate groups.
experimental evidence for the influence of the excitation wavelength on the value of the equilibrium constant as determined by time-resolved emission spectroscopy (tres). in the case of a rapid photochemical process, a new theoretical result relating time-resolved emission spectroscopy data to three physical parameters of the chemical system has been recently proposed. this previous work, based on a simulation study, is experimentally evidenced in the present paper, using europium/acetate as a model system. the comparison of the emission spectra obtained upon direct excitation of europium (394 nm) and by use of the “antenna effect” (266 nm) evidences the occurring of a back-dissociation of excited europium complexes to form solvated excited free europium ions.
energy loss of a heavy quark produced in a finite size medium. we study the medium-induced energy loss $-\delta e_0(l_p)$ suffered by a heavy quark produced at initial time in a quark-gluon plasma, and escaping the plasma after travelling the distance $l_p$. the heavy quark is treated classically, and within the same framework $-\delta e_0(l_p)$ consistently includes: the loss from standard collisional processes, initial bremsstrahlung due to the sudden acceleration of the quark, and transition radiation. the radiative loss induced by rescatterings $-\delta e_{rad}(l_p)$ is not included in our study. for a ultrarelativistic heavy quark with momentum $p \gsim 10 \ {\rm gev}$, and for a finite plasma with $l_p \lsim 10 \ {\rm fm}$, the loss $-\delta e_0(l_p)$ is strongly suppressed compared to the standard stationary regime $-\delta e_0(l_p) \simeq -\delta e_{coll}(l_p) \propto l_p$ valid when $l_p$ becomes large. this suppression is a consequence of the retardation affecting collisional processes - the latter cannot start at initial time when the heavy quark has not built its asymptotic proper field yet. our results indicate that $-\delta e_{rad}$ should be the dominant contribution to the total heavy quark energy loss, $-\delta e_{tot} = -\delta e_0 -\delta e_{rad} \simeq -\delta e_{rad}$, as indeed assumed in most of jet-quenching analyses. however they might raise some question concerning the rhic data on large $p_{\perp}$ electron spectra.
perturbative gauge theory in a background. motivated by the gluon condensate in qcd we study the perturbative expansion of a gauge theory in the presence of gauge bosons of vanishing momentum, in the specific case of an abelian theory. the background is characterised by a dimensionful parameter $\lambda$ affecting only the on-shell prescription of the free (abelian) gluon propagator. when summed to all orders in $g\lambda$ the modification is equivalent to evaluating standard green functions in a pure gauge field with an imaginary gauge parameter $\propto \lambda$. we show how to calculate the corresponding dressed green functions, which are poincaré and gauge covariant. we evaluate the expressions for the dressed quark and $q \bar q$ propagators, imposing as boundary condition that they approach the standard perturbative form in the short-distance limit ($|p^2|\to\infty$). the on-shell ($p^2=m^2$) pole of the free quark propagator is removed for any $\lambda &gt; 0$, and replaced by a discontinuity which vanishes exponentially with $p^2$. the dressing introduces an effective interaction between quarks and antiquarks which is enhanced at low relative 3-momentum. further study should allow to identify the (bound) eigenstates of propagation and determine whether they define a unitary $s$-matrix.
nuclear effects on hadron production in d+au and p+p collisions at $\sqrt{s_{nn}}$=200 gev. phenix has measured the centrality dependence of mid-rapidity pion, kaon and proton transverse momentum distributions in d+au and p+p collisions at sqrt(s_nn) = 200 gev. the p+p data provide a reference for nuclear effects in d+au and previously measured au+au collisions. hadron production is enhanced in d+au, relative to independent nucleon-nucleon scattering, as was observed in lower energy collisions. the nuclear modification factor for (anti) protons is larger than that for pions. the difference increases with centrality, but is not sufficient to account for the abundance of baryon production observed in central au+au collisions at rhic. the centrality dependence in d+au shows that the nuclear modification factor increases gradually with the number of collisions suffered by each participant nucleon. we also present comparisons with lower energy data as well as with parton recombination and other theoretical models of nuclear effects on particle production.
study of the interaction of ni$^{2+}$ and cs$^+$ on mx-80 bentonite; effect of compaction using the "capillary method". the goal of the paper is to assess the applicability of sorption models to describe the retention of contaminants on clay materials, both in dispersed and compacted states. a batch method is used to characterize the sorption equilibria between cs, ni, and mx-80 bentonite for solid-to-liquid ratios varying from 0.5 to 4200 kg/m3. for compacted bentonite (dry density of 1100 kg/m3), a new method is presented where the material compaction is performed in peek capillaries. sorption edges and isotherms were measured in the presence of a synthetic groundwater. a model considering cation exchange reactions with interlayer cations and surface complexation reactions with edge sites was used for the dispersed state. montmorillonite was shown to be the dominant interacting phase in mx-80 bentonite. the applicability of the model to compacted bentonite was tested. the results indicate that under conditions where the cation exchange mechanism is dominant, there is no difference between the dispersed and compacted states. for the degree of compaction studied, all exchange sites are available for sorption. for ni, when surface complexation is the dominant sorption mechanism, a decrease of the kd values by a factor of about 3 was observed (ph 7-8, trace concentrations). this could be explained quantitatively by a diminution of the conditional interaction constant between ni and the edge surface site in the compacted state. one consequence of this decrease is that the contribution of the organic matter content of mx-80 bentonite to the total sorption becomes significant.
novel phosphate-phosphonate hybrid nanomaterials applied to biology. null
the multiplicity dependence of inclusive $p_t$ spectra from p-p collisions at $\sqrt{s}$ = 200 gev. we report measurements of transverse momentum $p_t$ spectra for ten event multiplicity classes of p-p collisions at $\sqrt{s} = 200$ gev. by analyzing the multiplicity dependence we find that the spectrum shape can be decomposed into a part with amplitude proportional to multiplicity and described by a lévy distribution on transverse mass $m_t$, and a part with amplitude proportional to multiplicity squared and described by a gaussian distribution on transverse rapidity $y_t$. the functional forms of the two parts are nearly independent of event multiplicity. the two parts can be identified with the soft and hard components of a two-component model of p-p collisions. this analysis then provides the first isolation of the hard component of the $p_t$ spectrum as a distribution of simple form on $y_t$.
azimuthal angle correlations for rapidity separated hadron pairs in d+au collisions at $\sqrt{s_{nn}}$=200 gev. deuteron-gold (d+au) collisions at the relativistic heavy ion collider provide ideal platforms for testing qcd theories in dense nuclear matter at high energy. in particular, models suggesting strong saturation effects for partons carrying small nucleon momentum fraction (x) predict modifications to jet production at forward rapidity (deuteron-going direction) in d+au collisions. we report on two-particle azimuthal angle correlations between charged hadrons at forward/backward (deuteron/gold going direction) rapidity and charged hadrons at midrapidity in d+au and p+p collisions at sqrt(s[sub nn])=200 gev. jet structures observed in the correlations are quantified in terms of the conditional yield and angular width of away-side partners. the kinematic region studied here samples partons in the gold nucleus with x~0.1 to ~0.01. within this range, we find no x dependence of the jet structure in d+au collisions.
water diffusion in the simulated french nuclear waste glass son 68 contacting silica rich solutions: experimental and modeling. to understand the role of water diffusion on the long-term nuclear waste glass alteration, dynamic experiments were conducted with the borosilicate son 68 glass in synthetic solutions enriched in si, na and b at 50 and 90 °c. the water entering the glass exists to 80% in the form of molecular h2o and to 20% in the form of sioh. the ratio of h/na was 2.6 ± 0.3, indicating a complex mechanism including water diffusion and ionic-exchange. it was in agreement with model calculations based on glass structural units such as reedmergnerite and b2o3. water diffusion coefficients in the glass, determined by modeling of the experimental data, were between 2 × 10−21 and 6 × 10−23 m2 s−1. finally, under hlw disposal conditions, where interaction of nuclear glass with groundwater is expected to maintain saturation conditions, it is likely that water diffusion will contribute to the control of the glass alteration and the release of radionuclides.
constraints on the time-scale of nuclear breakup from thermal hard-photon emission. measured hard photon multiplicities from second-chance nucleon-nucleon collisions are used in combination with a kinetic thermal model, to estimate the break-up times of excited nuclear systems produced in nucleus-nucleus reactions at intermediate energies. the obtained nuclear break-up time for the $^{129}${xe} + $^{nat}${sn} reaction at 50{\it a} mev is $\delta$$\tau$ $\approx$ 100 -- 300 fm/$c$ for all reaction centralities. the lifetime of the radiating sources produced in seven other different heavy-ion reactions studied by the taps experiment are consistent with $\delta$$\tau$ $\approx$ 100 fm/$c$, such relatively long thermal photon emission times do not support the interpretation of nuclear breakup as due to a fast spinodal process for the heavy nuclear systems studied.
strontium binding by calcium silicate hydrates. in the present study the binding of strontium with pure calcium silicate hydrates (c-s-h) has been investigated using batch-type experiments. synthetic c-s-h phases with varying cao:sio2 (c:s) mol ratios, relevant to non-degraded and degraded hardened cement paste, were prepared in the absence of alkalis (na(i), k(i)) and in an alkali-rich artificial cement pore water (acw). two types of experimental approaches have been employed, investigating sorption and co-precipitation processes, respectively. the sr(ii) sorption kinetics were determined as well as sorption isotherms, the effect of the solid to liquid ratio and the composition (c:s ratio) of the c-s-h phases. in addition, the reversibility of the sr(ii) sorption was tested. it was shown that both the sorption and co-precipitation tests resulted in sr(ii) distribution ratios which were similar in value, indicating that the same sites are involved in sr(ii) binding. in alkali-free solutions, the sr(ii) uptake by c-s-h phases was described in terms of a sr2+–ca2+ ion exchange model. the selectivity coefficient for the sr2+–ca2+ exchange was determined to be 1.2±0.3.
measurement and modeling of the surface potential evolution of hydrated cement pastes as a function of degradation. hydrated cement pastes (hcp) have a high affinity with a lot of (radio)toxic products and can be used as waste confining materials. in cementitious media, elements are removed from solution via (co)precipitation reactions or via sorption/diffusion mechanisms as surface complexation equilibria. in this study, to improve the knowledge of the surface charge evolution vs the degradation of the hcp particles, two cements have been studied: cem-i (ordinary portland cement, opc) and cem-v (blast furnace slag and fly ash added to opc). zeta potential measurements showed that two isoelectric points exist vs hcp leaching, i.e., ph. zeta potential increases from −17 to +20 mv for ph 13.3 to ph 12.65 (fresh hcp states) and decreases from 20 to −8 mv for ph 12.65 to 11 (degraded hcp states). the use of a simple surface complexation model of c-s-h, limited in comparison with the structural modeling of c-s-h in literature, allows a good prediction of the surface potential evolution of both hcp. using this operational modeling, the surface charge is controlled by the deprotonation of surface sites (&gt;so−) and by the sorption of calcium (&gt;soca+), which brings in addition a positive charge. the calcium concentration is controlled by portlandite or calcium silicate hydrate (c-s-h) solubilities.
coefficients and terms of the liquid drop model and mass formula. the coefficients of different combinations of terms of the liquid drop model have been determined by a least square fitting procedure to the experimental atomic masses. the nuclear masses can also be reproduced using a coulomb radius taking into account the increase of the ratio $r_0/a^{1/3}$ with increasing mass, the fitted surface energy coefficient remaining around 18 mev.
automatic verification of bossa scheduler properties. bossa is a development environment for operating-system process schedulers that provides numerous safety guarantees. in this paper, we show how to automate the checking of safety properties of a scheduling policy developed in this environment. we find that most of the relevant properties can be considered as invariant or refinement properties. in order to automate the related proof obligations, we use the ws1s logic for which a decision procedure is implemented by mona. the proof techniques are implemented using the fmona tool.
femtoscopy in hydrodynamics-inspired models with resonances. effects of the choice of the freeze-out hypersurface and resonance decays on the hbt interferometry in relativistic heavy-ion collisions are studied in detail within a class of models with single freeze-out. the monte-carlo method, as implemented in therminator, is used to generate hadronic events describing production of particles from a thermalized and expanding source. all well-established hadronic resonances are included in the analysis as their role is crucial at large freeze-out temperatures. we use the two-particle method to extract the correlation functions, which allows us to study the coulomb effects. we find that the pion hbt data from rhic are fully compatible with the single freeze-out scenario, pointing at the shape of the freeze-out hypersurface where the transverse radius is decreasing with time. results for the single-particle spectra for this situation are also presented. finally, we present predictions for the kaon femtoscopy.
alpha decay half-lives of new superheavy nuclei within a generalized liquid drop model. the alpha decay half-lives of the recently produced isotopes of the 112, 114, 116 and 118 nuclei and decay products have been calculated in the quasi-molecular shape path using the experimental qalpha value and a generalized liquid drop model including the proximity effects between nucleons in the neck or the gap between the nascent fragments. reasonable estimates are obtained for the observed alpha decay half-lives. the results are compared with calculations using the density-dependent m3y effective interaction and the viola-seaborg-sobiczewski formulae. generalized liquid drop model predictions are provided for the alpha decay half-lives of other superheavy nuclei using the finite range droplet model qalpha and compared with the values derived from the vss formulae.
double chooz, a search for the neutrino mixing angle $\theta$13. the double chooz reactor neutrino experiment in france plans to quickly measure the neutrino mixing angle theta-13, or limit it to sin^2 2-theta_13 less than 0.025. the physics reach, experimental site, detector structures, scintillator, photodetection, electronics, calibration and simulations are described. the possibility of using double chooz to explore the possible use of a antineutrino detector for non-proliferation goals is also presented.
on dynamics of 5d superconformal theories. 5d superconformal theories involve vacuum valleys characterized in the simplest case by the vacuum expectation value of a real scalar field. if it is nonzero, conformal invariance is spontaneously broken and the theory is not renormalizable. in the conformally invariant sector with zero scalar v.e.v., the theory is intrinsically nonperturbative. we study classical and quantum dynamics of this theory in the limit when field dependence of the spatial coordinates is disregarded. the classical trajectories ``fall'' on the singularity at the origin of scalar moduli space. the quantum spectrum involves ghost states with unbounded from below negative energies, but such states fail to form complete 16-plets as is dictated by the presence of four complex supercharges and should be rejected by that reason. physical excited states come in supermultiplets and have all positive energies. we conjecture that the spectrum of the complete field theory hamiltonian is nontrivial and has a similar nontrivial ghost-free structure and also speculate that the ghosts in higher-derivative supersymmetric field theories are exterminated by a similar mechanism.
chiral anomalies in higher-derivative supersymmetric 6d theories. we show that the recently constructed higher-derivative 6d sym theory involves an internal chiral anomaly breaking gauge invariance. the anomaly may be cancelled if adding to the theory five adjoint matter hypermultiplets.
identified hadron spectra at large transverse momentum in p+p and d+au collisions at $\sqrt{{^s}nn}$ = 200 gev. we present the transverse momentum (p$_t$) spectra for identified charged pions, protons and anti-protons from p+p and d+au collisions at $\sqrt{^s}nn}$ . the spectra are measured around midrapidity (|y|&lt;0.5) over the range of 0.3.
transition from participant to spectator fragmentation in au+au reactions between 60a and 150a mev. using the quantum molecular dynamics approach, we analyze the results of the recent indra au+au experiments at gsi in the energy range between 60 amev and 150 amev. it turns out that in this energy region the transition toward a participant-spectator scenario takes place. the large au+au system displays in the simulations as in the experiment simultaneously dynamical and statistical behavior which we analyze in detail: the composition of fragments close to midrapidity follows statistical laws and the system shows bi-modality, i.e. a sudden transition between different fragmentation pattern as a function of the centrality as expected for a phase transition. the fragment spectra at small and large rapidities, on the other hand, are determined by dynamics and the system as a whole does not come to equilibrium, an observation which is confirmed by fopi experiments for the same system.
further results on the accessibility of a satellite with two reaction wheels. null
re injecting the structure in nmpc schemes. application to the constrained stabilization of a snake board. in this paper, a constrained nonlinear predictive control scheme is proposed for a class of under-actuated nonholonomic systems. the scheme is based on fast generation of steering trajectories that inherently fulfill the contraints while showing a ""{\em translatability}"" property which is generally needed to derive stability results in receding-horizon schemes. the corresponding open-loop optimization problem can be solved very efficiently with a fixed and deterministic upper bound on the execution time making possible a real-time implementation on fast systems. the whole framework is shown to hold for the well known challenging problem of a snake board constrained stabilization. illustrative simulations are proposed to assess the efficiency of the proposed solution.
vehicle and personnel routing optimization in the service sector: application to water distribution and treatment. this research, financed by a research contract with générale des eaux,&lt;br /&gt;concerns multiperiod vehicle routing problemwith time windows and a limited fleet.&lt;br /&gt;we propose several solution approaches. the main approaches are a metaheuristic, an optimal method, and an optimal-based heuristic method. the metaheuristic is based on evolution strategies, and memetic&lt;br /&gt;algorithms. the optimal method is based on column generation. the very low rate of time windows makes the subproblem very hard to solve. this implies to find new ways to solve this subproblem, which lead us to design the optimal-based heuristic method.&lt;br /&gt;in a second stage, applying these methods to real-life inspired scenarios helps us providing decision aid methodology and information.
towards a concurrent model of event-based aspect-oriented programming. the event-based aspect-oriented programming model (eaop) makes it possible to define pointcuts in terms of sequences of events emitted by the base program. the current formalization of the model relies on a monolithic entity, the monitor, which observes the execution of the base program and executes the actions associated to the matching pointcut. this model is not intrinsically sequential but its current formalization favors a sequential point of view. in this paper, we present a new formalization of eaop as finite state processes. this new formalization paves the way to reasoning about aspects in a concurrent setting and to the definition and implementation of concurrent eaop languages.
condensation mechanisms of tetravalent technetium in chloride media. the condensation mechanisms of tetravalent technetium in chloride media were studied in the ph range 0–1.5. a new dimer complex of tc(iv) has thus been discovered, tc2ocl104−. spectroscopic and kinetics studies showed that the formation of this compound resulted from the condensation of tccl5(h2o)−. an exafs study indicates that the dimer displays a [tc-o-tc]6+ structure. as the ph increases, uv-visible measurements showed a cyclization of [tc-o-tc]6+ into [tc(μo)2tc]4+ leading, in fine, to the precipitation of tco2·x h2o. the aquation constant (kaq) of tccl62− into tccl5(h2o)− and the dimerisation constant (logkdim) of tccl5(h2o)− into tc2ocl104− were determined to be 2.20±0.26 and 4.68±0.09, respectively.
speciation of technetium and rhenium complexes by in situ xas-electrochemistry. a spectro-electrochemical cell was developed in order to study the speciation of radio-elements in thermodynamic unstable redox states using in situ xas spectroscopy. this cell was used for the speciation of re and tc complexes in chloride media. experiments on re were carried out with the aim to validate the functionality of the experimental set-up. during electro-reduction of re(vii) in hcl media, exafs and xanes studies were performed in order to reveal the formation of chloro-oxygenated compounds of re(iv). the speciation of technetium in aqueous solutions of deep geological deposits for radioactive waste is important to predict its mobility under reducing conditions. xanes spectra showed that electro-reduction of tc(vii) in chloride media leads to a position of k-edge absorption which agrees with a tc(iv)/tc(iii) mixture.
photochemical behaviour of tc$_2$ocl$_{10}^{4–}$ and tc$_no_y^{4n–2y+}$ in chloride media. the stabilities of the technetium polymers tc2ocl104− and tcnoy4n−2y+ have been studied under light irradiation in 3 m chloride media with a ph range from 0 to 1.3. it has been shown that under irradiation, tc2ocl104− is not stable and undergoes dissociation to tccl5(h2o)- at ph = 0 and ph = 0.3. at ph = 1, irradiation of tc2ocl104− leads to a stationary state involving tccl5(h2o)- and tc2ocl104−. at ph = 1.3, tcnoy4n−2 y+ remains stable under irradiation. under light irradiation, the predominance diagram of tc(iv) species obtained from tc2ocl104− aquation in a ph range from 0 to 1 is drawn. the chemical behaviour of tc2ocl104− and the influence of the light on the condensation of tc(iv) and solubility of tco2· x h2o are discussed.
directed flow in au+au collisions at $\sqrt{{^s}nn}$ =62.4 gev. we present the directed flow (v1) measured in au+au collisions at $\sqrt((^s)nn)$=62.4 gev in the midpseudorapidity region |\eta|&lt;1.3 and in the forward pseudorapidity region 2.5&lt;|\eta|&lt;4.0. the results are obtained using the three-particle cumulant method, the event plane method with mixed harmonics, and for the first time at the relativistic heavy ion collider, the standard method with the event plane reconstructed from spectator neutrons. results from all three methods are in good agreement. over the pseudorapidity range studied, charged particle directed flow is in the direction opposite to that of fragmentation neutrons.
multiplicity and pseudorapidity distributions of charged particles and photons at forward pseudorapidity in au+au collisons at $\sqrt{{^s}nn}$=62.4 gev. we present the centrality-dependent measurement of multiplicity and pseudorapidity distributions of charged particles and photons in au+au collisions at $\sqrt((^s)nn)$=62.4 gev. the charged particles and photons are measured in the pseudorapidity region $2.9 \leq \eta \leq 3.9$ and 2.3 \leq \eta \leq 3.7$, respectively. we have studied the scaling of particle production with the number of participating nucleons and the number of binary collisions. the photon and charged particle production in the measured pseudorapidity range has been shown to be consistent with energy-independent limiting fragmentation behavior. photons are observed to follow a centrality-independent limiting fragmentation behavior, while for charged particles it is centrality dependent. we have carried out a comparative study of the pseudorapidity distributions of positively charged hadrons, negatively charged hadrons, photons, pions, and net protons in nucleus-nucleus collisions and pseudorapidity distributions from p+p collisions. from these comparisons, we conclude that baryons in the inclusive charged particle distribution are responsible for the observed centrality dependence of limiting fragmentation. the mesons are found to follow an energy-independent behavior of limiting fragmentation, whereas the behavior of baryons is energy dependent.
hydrodynamic source with continuous emission in au+au collisions at $\sqrt{s}=200$ gev. we analyze single particle momentum spectra and interferometry radii in central au+au collisions at rhic within a hydro-inspired parametrization accounting for continuous hadron emission through the whole lifetime of hydrodynamically expanding fireball. we found that a satisfactory description of the data is achieved for a physically reasonable set of parameters when the emission from non space-like sectors of the enclosed freeze-out hypersurface is fairly long: $ 9$ fm/c. this protracted surface emission is compensated in outward interferometry radii by positive $r_{out} - t$ correlations that are the result of an intensive transverse expansion. the main features of the experimental data are reproduced: in particular, the obtained ratio of the outward to sideward interferometry radii is less than unity and decreases with increasing transverse momenta of pion pairs. the extracted value of the temperature of emission from the surface of hydrodynamic tube approximately coincides with one found at chemical freeze-out in rhic au+au collisions. a significant contribution of the surface emission to the spectra and to the correlation functions at relatively large transverse momenta should be taken into account in advanced hydrodynamic models of ultrarelativistic nucleus-nucleus collisions.
novel phosphate–phosphonate hybrid nanomaterials applied to biology. a new process for preparing oligonucleotide arrays is described that uses surface grafting chemistry which is fundamentally different from the electrostatic adsorption and organic covalent binding methods normally employed. solid supports are modified with a mixed organic/inorganic zirconium phosphonate monolayer film providing a stable, well-defined interface. oligonucleotide probes terminated with phosphate are spotted directly to the zirconated surface forming a covalent linkage. specific binding of terminal phosphate groups with minimal binding of the internal phosphate diesters has been demonstrated. on the other hand, the reaction of a bisphosphonate bone resorption inhibitor (zoledronate) with calcium deficient apatites (cdas) was studied as a potential route to local drug delivery systems active against bone resorption disorders. a simple mathematical model of the zoledronate/cda interaction was designed that correctly described the adsorption of zoledronate onto cdas. the resulting zoledronate-loaded materials were found to release the drug in different phosphate-containing media, with a satisfactory agreement between experimental data and the values predicted from the model.
on the multiple-humped fission barriers and half-lives of actinides. the energy of actinide nuclei has been determined within a generalized liquid drop model taking into account the proximity energy, the mass and charge asymmetry, an accurate nuclear radius in adding the shell and pairing energies. double and triple-humped potential barriers appear. the second maximum corresponds to the transition from compact and creviced one-body shapes to two touching ellipsoids. a third minimum and third peak appear in special asymmetric exit channels where one fragment is almost a magic nucleus with a quasi-spherical shape while the other one evolves from oblate to prolate shapes. the heights of the double and triple-humped fission barriers agree precisely with the experimental results in all the actinide region. the predicted half-lives follow the experimental data trend.
improved measurement of double helicity asymmetry in inclusive midrapidity $\pi^0$ production for polarized p+p collisions at $\sqrt s$=200 gev. we present an improved measurement of the double helicity asymmetry for $\pi^0$ production in polarized proton-proton scattering at $\sqrt s$= 200 gev employing the phenix detector at the relativistic heavy ion collider (rhic). the improvements to our previous measurement come from two main factors: inclusion of a new data set from the 2004 rhic run with higher beam polarizations than the earlier run and a recalibration of the beam polarization measurements, which resulted in reduced uncertainties and increased beam polarizations. the results are compared to a next to leading order (nlo) perturbative quantum chromodynamics (pqcd) calculation with a range of polarized gluon distributions.
ghost-free higher-derivative theory. we present an example of the quantum system with higher derivatives in the lagrangian, which is ghost-free: the spectrum of the hamiltonian is bounded from below and unitarity is preserved.
hadronic matter is soft. the stiffness of the hadronic equation of state has been extracted from the production rate of $k^+$ mesons in heavy ion collisions around 1 $a$ gev incident energy. the data are best described with a compressibility coefficient $\kappa$ around 200 mev, a value which is usually called ``soft''. this is concluded from a detailed comparison of the results of transport theories with the experimental data using two different procedures: (i) the energy dependence of the ratio of $k^+$ from au+au and c+c collisions and (ii) the centrality dependence of the $k^+$ multiplicities. it is demonstrated that input quantities of these transport theories which are not precisely known, like the kaon-nucleon potential, the $\delta n \to n k^+ \lambda$ cross section or the life time of the $\delta$ in matter do not modify this conclusion.
common suppression pattern of $\eta$ and $\pi^0$ mesons at high transverse momentum in au+au collisions at $\sqrt{s_nn}$ = 200 gev. inclusive transverse momentum spectra of eta mesons have been measured within p_t = 2-10 gev/c at mid-rapidity by the phenix experiment in au+au collisions at sqrt(s_nn) = 200 gev. in central au+au the eta yields are significantly suppressed compared to peripheral au+au, d+au and p+p yields scaled by the corresponding number of nucleon-nucleon collisions. the magnitude, centrality and p_t dependence of the suppression is common, within errors, for eta and pi^0. the ratio of eta to pi^0 spectra at high p_t amounts to 0.40 &lt; r_eta/pi^0 &lt; 0.48 for the three systems in agreement with the world average measured in hadronic and nuclear reactions and, at large scaled momentum, in e^+e^- collisions.
xps study of eu(iii) coordination compounds: core levels binding energies in solid mixed-oxo-compounds eu$_mx_xo_y$. literature is relatively sparse on xps studies of europium compounds: it is essentially restricted to metallic compounds (eum5, in which m is a transition metal) or to simple oxides. while particular interest have been devoted to understanding physical phenomenon in the beginning of “shake-down” and “shake-up” satellites evidenced on core-level regions of the lanthanides, few information on absolute binding energies (be) was available. this paper reports an xps binding energy data base for europium(iii) compounds, in which eu cation have various chemical environments: simple oxide eu2o3, eu mixed oxides with organic oxalate, acetylacetonate or inorganic sulfate, nitrate, carbonate ligands. the values of core-level be (o1s, eu3d and eu4d) and the characteristics of shake-down satellites of eu3d are reported, and their variations are attributed to ionicity/covalency changes. such interpretation was already published for group a mixed oxides and zeolites. these data are needed for determining eu(iii) species sorbed onto minerals in the presence of various ligands in the framework of retention studies for assessing the safety of future nuclear waste disposals.
nuclear modification of electron spectra and implications for heavy quark energy loss in au+au collisions at $\sqrt{s_{nn}}$=200 gev. the phenix experiment has measured mid-rapidity transverse momentum spectra (0.4 &lt; p_t &lt; 5.0 gev/c) of electrons as a function of centrality in au+au collisions at sqrt(s_nn)=200 gev. contributions from photon conversions and from light hadron decays, mainly dalitz decays of pi^0 and eta mesons, were removed. the resulting non-photonic electron spectra are primarily due to the semi-leptonic decays of hadrons carrying heavy quarks. nuclear modification factors were determined by comparison to non-photonic electrons in p+p collisions. a significant suppression of electrons at high p_t is observed in central au+au collisions, indicating substantial energy loss of heavy quarks.
the n=14 shell closure in $^{22}$o viewed through a neutron sensitive probe. to investigate the behavior of the n=14 neutron gap far from stability with a neutron-sensitive probe, proton elastic and 2+1 inelastic scattering angular distributions for the neutron-rich nucleus 22o were measured with a secondary beam intensity of only 1200 particles per second using the must silicon strip detector array at the ganil facility. a phenomenological analysis yields a deformation parameter bp;p' = 0.26 +- 0.04 for the 2+1 state, much lower than in 20o, showing a surprisingly weak neutron contribution to this state. a fully microscopic analysis was performed using optical potentials obtained from matter and transition densities generated by continuum skyrme-hfb and qrpa calculations, respectively. when the present results and those from a 22o + 197au scattering experiment are combined, the ratio of neutron to proton contributions to the 2+1 state is found close to the n/z ratio, demonstrating a strong n=14 shell closure in the vicinity of the neutron drip-line.
the next 700 krivine machines. the krivine machine is a simple and natural implementation of the normal weak-head reduction strategy for pure lambda-terms. while its original description has remained unpublished, this machine has served as a basis for many variants, extensions and theoretical studies. in this paper, we present the krivine machine and some well-known variants in a common framework. our framework consists of a hierarchy of intermediate languages that are subsets of the lambda-calculus. the whole implementation process (compiler + abstract machine) is described via a sequence of transformations all of which express an implementation choice. we characterize the essence of the krivine machine and locate it in the design space of functional language implementations. we show that, even within the particular class of krivine machines, hundreds of variants can be designed.
parton ladder splitting and the rapidity dependence of transverse momentum spectra in deuteron-gold collisions at rhic. we present a phenomenological approach (epos), based on the parton model, but going much beyond, and try to understand proton-proton and deuteron-gold collisions, in particular the transverse momentum results from all the four rhic experiments. it turns out that elastic and inelastic parton ladder splitting is the key issue. elastic splitting is in fact related to screening and saturation, but much more important is the inelastic contribution, being crucial to understand the data. we investigate in detail the rapidity dependence of nuclear effects, which is actually relatively weak in the model, in perfect agreement with the data, if the latter ones are interpreted correctly.
microcanonical pentaquark production in electron-positron annihilations. the existence of pentaquarks, namely baryonic states made up of four quarks and one antiquark, became questionable, because the candidates, i.e. the $\theta^+$ peak, are seen in certain reactions, i.e. p+p collisions, but not in others, i.e. $\ee$ annihilations. in this paper, we calculate the production of $\theta ^{+}(1540)$ and $\xi (1860)$ in $\ee$ annihilations at 91.2 gev with a microcanonical approach. a rather high production of pentaquark states is obtained. it is comparable with that from pp collisions at rhic energy, and higher than the yield from pp collisions at sps energy. if pentaquark states exist, the production is highly possible from high energy collisions, even without initial baryons.
retardation effect for collisional energy loss of hard partons produced in a qgp. we study the collisional energy loss suffered by an energetic parton travelling the distance l in a high temperature quark-gluon plasma and initially produced in the medium. we find that the medium-induced collisional loss -δe(l) is strongly suppressed compared to previous estimates which assumed the collisional energy loss rate -de/dx to be constant. the large l linear asymptotic behaviour of -δe(l) sets in only after a quite large retardation time. the suppression of -δe(l) is partly due to the fact that gluon bremsstrahlung arising from the initial acceleration of the energetic parton is reduced in the medium compared to vacuum. the latter radiation spectrum is sensitive to the plasmon modes of the quark-gluon plasma and has a rich angular structure.
single electrons from heavy flavor decays in p+p collisions at $\sqrt{s}$ = 200 gev. the invariant differential cross section for inclusive electron production in p+p collisions at sqrt(s) = 200 gev has been measured by the phenix experiment at the relativistic heavy ion collider over the transverse momentum range 0.4 &lt;= p_t &lt;= 5.0 gev/c at midrapidity (eta &lt;= 0.35). the contribution to the inclusive electron spectrum from semileptonic decays of hadrons carrying heavy flavor, i.e. charm quarks or, at high p_t, bottom quarks, is determined via three independent methods. the resulting electron spectrum from heavy flavor decays is compared to recent leading and next-to-leading order perturbative qcd calculations. the total cross section of charm quark-antiquark pair production is determined as sigma_(c c^bar) = 0.92 +/- 0.15 (stat.) +- 0.54 (sys.) mb.
measurement of identified pi^0 and inclusive photon v_2 and implication to the direct photon production in sqrt(s_nn) = 200 gev au+au collisions. the azimuthal distribution of identified pi^0 and inclusive photons has been measured in sqrt(s_nn) = 200 gev au+au collisions with the phenix experiment at the relativistic heavy ion collider (rhic). the second harmonic parameter (v_2) was measured to describe the observed anisotropy of the azimuthal distribution. the measured inclusive photon v_2 is consistent with the expected hadron decay and is also consistent with the lack of direct photon signal over the measured p_t range 1-6 gev/c. an attempt is made to extract v_2 of direct photons.
j/$\psi$ production and nuclear effects for $d+au$ and $p+p$ collisions at $\sqrt{s_nn}$ = 200 gev. j/psi production in d+au and p+p collisions at sqrt(s_nn) = 200 gev has been measured by the phenix experiment at rapidities -2.2 &lt; y &lt; +2.4. the cross sections and nuclear dependence of j/\psi production versus rapidity, transverse momentum, and centrality are obtained and compared to lower energy p+a results and to theoretical models. the observed nuclear dependence in d+au collisions is found to be modest, suggesting that the absorption in the final state is weak and the shadowing of the gluon distributions is small and consistent with dglap-based parameterizations that fit deep-inelastic scattering and drell-yan data at lower energies.
