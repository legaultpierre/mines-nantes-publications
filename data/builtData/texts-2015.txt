exact observability and controllability for linear neutral type systems. the problem of exact observability is analyzed for a wide class of neutral type systems by an infinite dimensional approach. the duality with the exact controllabil-ity problem is the main tool. it is based on an explicit expression of a neutral type system which corresponding to the abstract adjoint system. a nontrivial relation is obtained between the initial neutral system and the system obtained via the adjoint abstract state operator. the characterization of the duality between controllability and observability is deduced, and then observability conditions are obtained.
mean cross section prediction in pwr-mox using neural network. fuel depletion calculation codes require one-group mean cross sections of manynuclides to solve bateman's equations. in such codes, the mean cross sections are assed by themean of iterative calls of boltzmann equation solver thanks to neutron transport codes. this is atime consuming task, especially with monte carlo codes such as mcnp. this paper presents amethodology based on neural network for building a cross section predictor for a pwr reactorloaded with any mox fuel. this approach allows performing fuel depletion calculation in lessthan one minute with an excellent accuracy. a maximum deviation of 3% on actinides is obtainedat the end of cycle between inventories calculated from neural networks and from the referencecoupled neutron transport / fuel depletion calculation.
mgo effect on an ads neutronic parameters. accelerator driven systems (ads) are specifically studied for their capacity intransmuting minor actinides (ma). electronuclear scenarios involving ma transmutation in adsare widely researched. the dynamic fuel cycle simulation code class (core library foradvanced scenarios simulations) is used for predicting the inventory evolution induced by acomplex nuclear fleet. for managing reactors, the code class is based on physic models. a fuelloading model (flm) provides the fuel composition at beginning of cycle (boc) according tothe storages composition and the reactor requirements. a cross section predictor (csp) estimatesmean cross sections needed for solving evolution equations. physic models are built from reactorscalculation set ahead of the scenario calculation. an ads standard composition at boc is amixture of plutonium and ma oxide. the high number of fissile isotopes present in the sub-criticalcore leads to an issue for building an ads flm. a high number of isotopic vectors at boc isneeded to get an exhaustive simulation set. also, ads initial reactivity is adjusted with an inertmatrix that induces an additional degree of freedom. the building of an ads flm for classrequires two steps. for any heavy nuclide composition at beginning of cycle, the core reactivitymust be imposed at a sub-critical level. also, the reactivity coefficient evolution should bemaintained during the irradiation. for building a flm, a simulation set has been built. reactorsimulations are done with the transport code mcnp6 (monte carlo n particle transport code).the ads geometry is based on the efit (european facility for industrial-scale transmutation)concept. the simulation set is composed of more than 8000 randomized. a complete neutronicstudy is presented that highlight the effect on mgo on neutronic parameters.
the discovery initiative - overcoming major limitations of traditional server-centric clouds by operating massively distributed iaas facilities. instead of the current trend consisting of building larger and  larger data centers (dcs) in few strategic locations, the discovery initiative proposes to leverage any network point of presences (pop, i.e., a  small or medium-sized network center) available through the internet. the key idea is to demonstrate a widely distributed cloud platform that can better match the geographical dispersal of users. this involves radical changes in the way resources are managed, but leveraging computing resources around the end-users will enable to  deliver a new generation of highly efficient and sustainable utility computing (uc) platforms, thus providing a strong alternative to the actual cloud model based on mega dcs (i.e. dcs composed of tens of thousands resources).critical to the emergence of such distributed cloud platforms is the availability of appropriate operating mechanisms. although, some of protagonists of cloud federations would argue that it might be possible to federate a significant number of micro-clouds hosted on each pop, we emphasize that federated approaches aim at delivering a brokering service in charge of interacting with several cloud management systems, each of them being already deployed and operated independently by at least one administrator. in other words, current federated approaches do not target to operate, remotely, a significant amount of uc resources geographically distributed but  only to use them. the main objective of discovery is to design,  implement, demonstrate and promote a unified system in charge of turning a complex, extremely large-scale and widely distributed infrastructure into a collection of abstracted computing resources which is efficient, reliable, secure and friendly to operate and  use.  after presenting the discovery vision, we explain the different  choices we made, in particular the choice of revising the openstack solution leveraging p2p mechanisms. we believe that such a strategy is promising considering the architecture complexity of such systems and the velocity of open-source initiatives.
centrality dependence of the charged-particle multiplicity density at mid-rapidity in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev. the pseudorapidity density of charged particles ($\mathrm{d}n_\mathrm{ch}/\mathrm{d}\eta$) at mid-rapidity in pb-pb collisions has been measured at a center-of-mass energy per nucleon pair of $\sqrt{s_{\rm nn}}$ = 5.02 tev. it increases with centrality and reaches a value of $1943 \pm 54$ in $|\eta|&lt;0.5$ for the 5% most central collisions. a rise in $\mathrm{d}n_\mathrm{ch}/\mathrm{d}\eta$ as a function of $\sqrt{s_{\rm nn}}$ for the most central collisions is observed, steeper than that observed in proton-proton collisions and following the trend established by measurements at lower energy. the centrality dependence of $\mathrm{d}n_\mathrm{ch}/\mathrm{d}\eta$ as a function of the average number of participant nucleons, ${\langle n_\mathrm{part} \rangle}$, calculated in a glauber model, is compared with the previous measurement at lower energy. a constant factor of about 1.2 describes the increase in $\frac{2}{\langle n_\mathrm{part} \rangle}\langle \mathrm{d}n_\mathrm{ch}/\mathrm{d}\eta \rangle$ from $\sqrt{s_{\rm nn}}$ = 2.76 tev to $\sqrt{s_{\rm nn}}$ = 5.02 tev for all centrality intervals, within the measured range of 0-80% centrality. the results are also compared to models based on different mechanisms for particle production in nuclear collisions.
charge-dependent flow and the search for the chiral magnetic wave in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. we report on measurements of a charge-dependent flow using a novel three-particle correlator with alice in pb-pb collisions at the lhc, and discuss the implications for observation of local parity violation and the chiral magnetic wave (cmw) in heavy-ion collisions. charge-dependent flow is reported for different collision centralities as a function of the event charge asymmetry. while our results are in qualitative agreement with expectations based on the cmw, the nonzero signal observed in higher harmonics correlations indicates a possible significant background contribution. we also present results on a differential correlator, where the flow of positive and negative charges is reported as a function of the mean charge of the particles and their pseudorapidity separation. we argue that this differential correlator is better suited to distinguish the differences in positive and negative charges expected due to the cmw and the background effects, such as local charge conservation coupled with strong radial and anisotropic flow.
impact of reaction cross section on the unified description of fusion excitation function. a systematics of over 300 complete and incomplete fusion cross section data points covering energies beyond the barrier for fusion is presented. owing to a usual reduction of the fusion cross sections by the total reaction cross sections and an original scaling of energy, a fusion excitation function common to all the data points is established. a universal description of the fusion exci-tation function relying on basic nuclear concepts is proposed and its dependence on the reaction cross section used for the cross section normalization is discussed. the pioneering empirical model proposed by bass in 1974 to describe the complete fusion cross sections is rather successful for the incomplete fusion too and provides cross section predictions in satisfactory agreement with the observed universality of the fusion excitation function. the sophisticated microscopic transport dywan model not only reproduces the data but also predicts that fusion reaction mechanism disappears due to weakened nuclear stopping power around the fermi energy.
putting into practice another idea of cultural democratization : the “panier culture” case. new social structurations emerge so as to respond to the stakes of democratization which cross the cultural sector. they intend to revive democratic values through the use of the internet. the case of an art short-chain-value created in 2011 allows us to question the role of the internet in the working out of activities created both by artists and a public. the internet is a media which operates all along a continuum of places: public, open, closed, interpersonal. the description of the way this “culture-basket” functions induces that difficulties having democratization at stake are located at the interfaces of these different places.
energy of molecular structures in 12c, 16o, 20ne, 24mg, and 32s. the energy of the 12 c, 16 o, 20 ne, 24 mg and 32 s 4n-nuclei has been determined within a generalized liquid drop model and assuming different planar and three-dimensional shapes of α-molecules : linear chain, triangle, square, tetrahedron, pentagon, trigonal bipyramid, square pyramid, hexagon, octahedron, octogon and cube. the potential barriers governing the entrance and decay channels via α absorption or emission as well as more symmetric binary and ternary reactions have been compared. the rms radii of the linear chains differ from the experimental rms radii of the ground states. the binding energies of the three-dimensional shapes at the contact point are higher than the ones of the planar configurations. the alpha particle plus a-4 daughter configuration leads always to the lowest potential barrier. the binding energy can be reproduced within the sum of the binding energy of n α particles plus the number of bonds multiplied by 2.4 mev or by the sum of the binding energies of one alpha particle and the daughter nucleus plus the coulomb energy and the proximity energy.
nanosecond-level time synchronization of autonomous radio detector stations for extensive air showers. to exploit the full potential of radio measurements of cosmic-ray air showers at mhz frequencies, a detector timing synchronization within 1 ns is needed. large distributed radio detector arrays such as the auger engineering radio array (aera) rely on timing via the global positioning system (gps) for the synchronization of individual detector station clocks. unfortunately, gps timing is expected to have an accuracy no better than about 5 ns. in practice, in particular in aera, the gps clocks exhibit drifts on the order of tens of ns. we developed a technique to correct for the gps drifts, and an independent method used for cross-checks that indeed we reach nanosecond-scale timing accuracy by this correction. first, we operate a "beacon transmitter" which emits defined sine waves detected by aera antennas recorded within the physics data. the relative phasing of these sine waves can be used to correct for gps clock drifts. in addition to this, we observe radio pulses emitted by commercial airplanes, the position of which we determine in real time from automatic dependent surveillance broadcasts intercepted with a software-defined radio. from the known source location and the measured arrival times of the pulses we determine relative timing offsets between radio detector stations. we demonstrate with a combined analysis that the two methods give a consistent timing calibration with an accuracy of 2 ns or better. consequently, the beacon method alone can be used in the future to continuously determine and correct for gps clock drifts in each individual event measured by aera.
assessing both solution diversity and solution quality in constraint programming. techniques for solving optimization problems naturally prioritize the value of an objective function, typically producing a single solution. while this is essential from the optimality point of view, there are scenarios where it may be advantageous to consider multiple solutions, as distinct as possible from one another. unfortunately, constraint-based techniques that can be used to generate diverse solutions are designed for satisfaction problems. they do not assess objective quality. in this work, we tackle this issue with a generic paradigm for assessing both solution diversity and solution quality, that can be implemented in most existing solvers. as there is no requirement for the initial solution to be optimal, there is therefore no theoretical restriction on solving large problems, using for instance large neighborhood search. we show that our technique can be specialized to produce diverse solutions of high quality in the context of over-constrained problems. furthermore, our paradigm allows us to consider diversity from a different point of view, based on generic concepts expressed by global constraints. our experiments yield encouraging computational results.
opportunistic scheduling in clouds partially powered by green energy. the fast growth of demand for computing and storage resources in data centers has considerably increased their energy consumption.  improving the utilization of data center resources and integrating renewable energy, such as solar and wind, has been proposed to reduce both the brown energy consumption and carbon footprint of the data centers. in this paper, we propose a novel framework opportunistic scheduling broker infrastructure (pika) to save energy in small mono-site data centers. in order to reduce the brown energy consumption, pika integrates resource overcommit techniques that help to minimize the number of powered-on physical machines (pms). on the other hand,   pika dynamically schedules the jobs and adjusts the number of powered-on pms to match the variable renewable energy supply. our simulations with a real-world job workload and solar power traces demonstrate that pika saves brown energy consumption by up to 44.9% compared to a typical scheduling algorithm.
effect of the dynamic topology on the performance of pso-2s algorithm for continuous optimization. pso-2s is a multi-swarm pso algorithm using charged particles in a partitioned search space for continuous optimization problems. in order to improve the performance of pso-2s, this paper proposes a novel variant of this algorithm, called dpso-2s, which uses the dcluster neighborhood topologies to organize the communication networks between the particles. experiments were conducted on a set of classical benchmark functions. the obtained results prove the effectiveness of the proposed algorithm.
integrated strategic and tactical optimization of animal-waste sourced biopower supply chains. —many models have been recently developed for the optimization of biomass related supply chains. however, models for biopower supply chains powered by animal waste have not received much attention yet. in this paper, we propose a mixed integer linear programming model for supplier selection and procurement planning for a biopower plant. the model integrates time window constraints for the collection of animal waste as well as inventory constraints. we show that the model is intractable with a state-of-the art commercial solver and propose a heuristic approach based on the adaptive large neighbourhood search (alns) framework. we show the efficiency of this approach on a case study in central france.
a facility location problem for the design of a collaborative distribution network. null
rationalization of the solvation effects on the ato+ ground-state change. 211at radionuclide is of considerable interest as a radiotherapeutic agent for targeted alpha therapy in nuclear medicine, but major obstacles remain because the basic chemistry of astatine (at) is not well understood. the ato+ cationic form might be currently used for 211at-labeling protocols in aqueous solution and has proved to readily react with inorganic/organic ligands. but ato+ reactivity must be hindered at first glance by spin restriction quantum rules: the ground state of the free cation has a dominant triplet character. investigating ato+ clustered with an increasing number of water molecules and using various flavors of relativistic quantum methods, we found that ato+ adopts in solution a kramers restricted closed-shell configuration resembling a scalar-relativistic singlet. the ground-state change was traced back to strong interactions, namely, attractive electrostatic interactions and charge transfer, with water molecules of the first solvation shell that lift up the degeneracy of the frontier π* molecular orbitals (mos). this peculiarity brings an alternative explanation to the highly variable reproducibility reported for some astatine reactions: depending on the production protocols (with distillation in gas-phase or “wet chemistry” extraction), 211at may or may not readily react.
risks governance and risky operations helping each other to face unexpected events: a few contributions of pragmatist philosophy. this paper seeks to highlight a few contributions of pragmatist philosophy to the understanding of risks governance. the capacity to handle unexpected events is a necessity for high risks organizations, as known by the academic community for about thirty years (rochlin, la porte, &amp; roberts, 1987). in this communication, we study a french regulated process called “safety demonstration” which force nuclear operators to demonstrate the reliability of their technical solutions before implementing them. this process is settled since 2006 and is embedded in the french regulation of nuclear risks governance. it is currently facing its difficulties in grabbing the management of unexpected events, which until now were managed through rules. dismantling operations brings new situations, where unexpected events are more common and more significant.
from ventriloquism to high reliability: object of activity and figures’ significance. this paper presents the results of a communicational study of high reliability organizations (hro). starting from a gap in the hro literature, we seek to deepen the understanding of communicational nature of interactions within hros. in particular, we contribute to the question related to the track: what forms of talk do we find in organizational practice, and how do they differ in shaping and constituting organizational phenomena in our study of reliability? we draw on the concept of ventriloquism (cooren, 2010) and examine its impact and importance on the production of high reliability. we base our work on two empirical case studies. the first concerns heavy handling activity in a naval defense industry, and the second concerns the care provided to demented patients in a short-term geriatric ward. we use actor network theory, or ant (latour, 2005), to build an original analysis framework of ventriloquism that qualifies the figures in terms of “actor”, “actant” or “object.” through a comparative approach, we show that ventriloquism can serve or disserve the high reliability of an organization. specifically, we demonstrate that the nature of the object of activity (engeström, 1987) is a crucial element for the relevance of ventriloquism. we describe the impact of ventriloquism on hros and build a preliminary typology of talk practices that foster it. we conclude by discussing the theoretical, methodological and practical contributions of our research.
measurement of $\theta_{13}$ in double chooz using neutron captures on hydrogen with novel background rejection techniques. the double chooz collaboration presents a measurement of the neutrino mixing angle $\theta_{13}$ using reactor $\overline{\nu}_{e}$ observed via the inverse beta decay reaction in which the neutron is captured on hydrogen. this measurement is based on 462.72 live days data, approximately twice as much data as in the previous such analysis, collected with a detector positioned at an average distance of 1050m from two reactor cores. several novel techniques have been developed to achieve significant reductions of the backgrounds and systematic uncertainties. accidental coincidences, the dominant background in this analysis, are suppressed by more than an order of magnitude with respect to our previous publication by a multi-variate analysis. these improvements demonstrate the capability of precise measurement of reactor $\overline{\nu}_{e}$ without gadolinium loading. spectral distortions from the $\overline{\nu}_{e}$ reactor flux predictions previously reported with the neutron capture on gadolinium events are confirmed in the independent data sample presented here. a value of $\sin^{2}2\theta_{13} = 0.095^{+0.038}_{-0.039}$(stat+syst) is obtained from a fit to the observed event rate as a function of the reactor power, a method insensitive to the energy spectrum shape. a simultaneous fit of the hydrogen capture events and of the gadolinium capture events yields a measurement of $\sin^{2}2\theta_{13} = 0.088\pm0.033$(stat+syst).
radiolytic corrosion of uranium dioxide induced by he2+ localized irradiation of water: role of the produced h2o2 distance. null
dynamic packing with side constraints for datacenter resource management. resource management in datacenters involves assigning virtual machines with changing resource demands to physical machines with changing capacities. recurrently, the changes invalidate the assignment and the resource manager recomputes it at runtime. the assignment is also subject to changing restrictions expressing a variety of user requirements. the present chapter surveys this application of vector packing—called the vm reassignment problem—with an insight into its dynamic and heterogeneous nature. we advocate flexibility to answer these issues and present btrplace, a flexible and scalable heuristic solution based on constraint programming.
anisotropic emission of thermal dielectrons from au+au collisions at $\sqrt{s_{nn}}=200$~gev with epos3. dileptons, as an electromagnetic probe, are crucial to study the properties of quark-gluon plasma (qgp) created in heavy ion collisions. we calculated the invariant mass spectra and the anisotropic emission of thermal dielectrons from au+au collisions at the relativistic heavy ion collider (rhic) energy $\sqrt{s_{nn}}=200$~gev based on epos3. this approach provides a realistic (3+1)-dimensional event-by-event viscous hydrodynamic description of the expanding hot and dense matter with a very particular initial condition, and a large set of hadron data and direct photons (besides $v_{2}$ and $v_{3}$ !) can be successfully reproduced. thermal dilepton emission from both qgp phase and the hadronic gas are considered, with the emission rates based on lattice qcd and a vector meson model, respectively. we find that the computed invariant mass spectra (thermal contribution + star cocktail) can reproduce the measured ones from star at different centralities. different from other model predictions, the obtained elliptic flow of thermal dileptons is larger than the star measurement referring to all dileptons. a clear centrality dependence of thermal dilepton occurs not only to elliptic flow $v_{2}$ but also to high orders for centrality in 0-60\%. at a given centrality, $v_{n}$ of thermal dileptons decreases neatly with $n$ for $2 \leq n \leq 5$.
online monitoring of the osiris reactor with the nucifer neutrino detector. the detection of electron antineutrinos emitted in the decay chains of the fission products in nuclear reactors, combined with reactor core simulations, provides an efficient tool to assess both the thermal power and the fissile content of the whole nuclear core. this new information could be used by the international agency for atomic energy (iaea) to enhance the safeguards of civil nuclear reactors. we report the first results of the nucifer experiment demonstrating the concept of "neutrinometry" at the pre-industrialized stage. a novel detector has been designed to meet requirements discussed with the iaea for the last ten years as well as international nuclear safety standards. nucifer has been deployed at only 7.2m away from the osiris research reactor core (70 mw) operating at the saclay research center of the french alternative energies and atomic energy commission (cea). we describe the performances of the 1 m3 detector remotely operating at a shallow depth equivalent to 12m of water and under intense background radiation conditions due to the very short baseline. we present the first physics results, based on 145 (106) days of data with reactor on (off), leading to the detection of 40760 electron antineutrino candidates. the mean number of detected antineutrinos is 281 (7) neutrino/day, to be compared with the prediction 272 (23) neutrino/day. as a first societal application we quantify, on the basis of our data, how antineutrinos could be used for the plutonium management and disposition agreement.
ultraviolet behavior of 6d supersymmetric yang-mills theories and harmonic superspace. we revisit the issue of higher-dimensional counterterms for the n=(1,1) supersymmetric yang-mills (sym) theory in six dimensions using the off-shell n=(1,0) and on-shell n=(1,1) harmonic superspace approaches. the second approach is developed in full generality and used to solve, for the first time, the n=(1,1) sym constraints in terms of n=(1,0) superfields. this provides a convenient tool to write explicit expressions for the candidate counterterms and other n=(1,1) invariants and may be conducive to proving non-renormalization theorems needed to explain the absence of certain logarithmic divergences in higher-loop contributions to scattering amplitudes in n=(1,1) sym.
transport theory from the nambu-jona-lasinio lagrangian. starting from the (polyakov-) nambu-jona-lasinio lagrangian, (p)njl, we formulate a transport theory which allows for describing the expansion of a quark-antiquark plasma and the subsequent transition to the hadronic world --without adding any new parameter to the standard (p)njl approach, whose parameters are fixed to vacuum physics. this transport theory can be used to describe ultrarelativistic heavy-ion reaction data as well as to study the (first-order) phase transition during the expansion of the plasma. (p)njl predicts such a phase transition for finite chemical potentials. in this contribution we give an outline of the necessary steps to obtain such a transport theory and present first results.
prediction of mgo volume fraction in an ads fresh fuel for the scenario code class. subcritical reactors, also called accelerator driven systems (ads), are specifically studied for their capacity in transmuting minor actinides (ma). nuclear fuel cycle scenarios involving ma transmutation in ads are widely researched. the nuclear fuel cycle simulation tool code class (core library for advanced scenarios simulations) is dedicated to the inventory evolution calculation induced by a complex nuclear fleet. for managing reactors, the code class includes physic models. loading models aim to provide the fuel composition at beginning of cycle according to the stocks isotopic composition and the reactors requirements. a cross section predictor aims to provide mean cross sections needed for solving bateman equations. physic models are built from reactors calculation set ahead of the scenario calculation. an ads standard composition at boc is a mixture of plutonium and ma oxide. the high number of fissile isotopes present in the subcritical core leads to an issue for building an ads fuel loading model. a high number of isotopic vector at boc is needed to get an exhaustive simulation set. also, ads initial reactivity is adjusted with an inert matrix which induces an additional degree of freedom. the building of an ads fuel loading model for class requires two steps. for any heavy nuclide composition at beginning of cycle, the core reactivity must be imposed at a subcritical level. also, the reactivity coefficient evolution should be maintained during the irradiation. in this work, the mgo volume fraction is adjusted to reach the first requirement. the methodology based on a set of reactor simulations and neural network utilization to predict the mgo volume fraction needed to reach a wanted keff for any initial composition is presented. also, a complete neutronic study is done that highlight the effect on mgo on neutronic parameters. reactor simulations are done with the transport code mcnp6 (monte carlo n particle transport code). the ads geometry is based on the efit (european facility for industrial-scale transmutation) concept. the simulation set is composed of more than 8000 randomized runs from which a neural network has been built. the resulting mgo prediction method allows reaching a keff at 0.96 and the distribution standard deviation is around 200 pcm.
study of alternative routes for the production of innovative radionuclides for medical applications. nuclear medicine is a specialty that uses radioactive nuclei for therapy or diagnosis of diseases such as different types of cancer. these radionuclides are coupled to carrier molecules to target sick cells. currently, only few isotopes are used in clinical practice. however, many others may be of medical interest due to their emitted radiation and/or their half-life that can be adapted to the carrier molecule transit time and to the pathology. the aim of this phd thesis is to study the production of innovative radionuclides for therapy and diagnosis applications in collaboration with the giparronax, which possesses a multi-particle high energy cyclotron. a fundamental physical parameter to access the production rate of a radionuclide is the production cross section. experimental data were measured for a selection of radionuclides: photon emitter (tc-99m) and positron emitter (sc-44g) for diagnosis, as well as electron emitters (re-186, tb-155 and sn-117m) and α particles emitters (th-226, ra-223 and bi-213) for therapeutic applications. these acquired data are obtained using alternative production routes compared to the commonly used. data related to the contaminants produced during the irradiations were also extracted. the experimental cross section values arecompared with theoretical model predictions. the large set of data obtained contributes to the theoretical physicist studies allowing to constrain their models to improve and/or validate them.
the role of spin-orbit coupling on the chemical bonding in at2 and ato+: analysis via effective bond orders. null
effective bond orders from spin-orbit configuration interaction approaches. null
a heuristic approach to the water networks pumping scheduling issue. in order to improve the efficiency of drinking water networks, we develop a model predictive control method, whichoptimizes pumping scheduling by taking into account electricity tariffs and network constraints on a daily basis. weestimate a 10% discount in the energy bill, an amount which depends strongly on the characteristics of the networkunder study and the quality of the current strategy.
precision measurement of the mass difference between light nuclei and anti-nuclei. the measurement of the mass differences for systems bound by the strong force has reached a very high precision with protons and anti-protons. the extension of such measurement from (anti-)baryons to (anti-)nuclei allows one to probe any difference in the interactions between nucleons and anti-nucleons encoded in the (anti-)nuclei masses. this force is a remnant of the underlying strong interaction among quarks and gluons and can be described by effective theories, but cannot yet be directly derived from quantum chromodynamics. here we report a measurement of the difference between the ratios of the mass and charge of deuterons and anti-deuterons, and $^{3}{\rm he}$ and $^3\overline{\rm he}$ nuclei carried out with the alice (a large ion collider experiment) detector in pb-pb collisions at a centre-of-mass energy per nucleon pair of 2.76 tev. our direct measurement of the mass-over-charge differences confirm cpt invariance to an unprecedented precision in the sector of light nuclei. this fundamental symmetry of nature, which exchanges particles with anti-particles, implies that all physics laws are the same under the simultaneous reversal of charge(s) (charge conjugation c), reflection of spatial coordinates (parity transformation p) and time inversion (t).
from regulatory obligations to enforceable accountability policies in the cloud. the widespread adoption of the cloud model for service delivery triggered several data protection issues.as a matter of fact, the proper delivery of these services typically involves sharing of personal/businessdata between the different parties involved in the service provisioning. in order to increase cloudconsumer’s trust, there must be guarantees on the fair use of their data. accountability provides thenecessary assurance about the data governance practices to the different stakeholders involved in a cloudservice chain. in this context, we propose a framework for the representation of accountability policies.such policies offer to end-users a clear view of the privacy and accountability clauses asserted by theentities they interact with, as well as means to represent their preferences. our framework offers twoaccountability policy languages: (i) an abstract language called aal devoted for the representation ofpreferences/clauses in an human readable fashion, and (ii) a concrete one for the implementation ofenforceable policies.
abstract accountability language: translation, compliance and application. with the rise of the services-based economy andthe democratization of on-line services, more and more users(individual and/or business) use on-line applications in their dailylives. usually personal data transits between different actorsinvolved in a service’s delivery chain (e.g. application/storageservice providers) and thus might raise some privacy issues.accountability, which is the property of an entity of beingresponsible for its acts, can help mitigate data privacy anddata disclosures issues in such applications. in this paper, wepropose a translational semantics for our accountability languageand we present some expected properties. we introduce anatural criterion to achieve the accountability compliance oftwo clauses and few heuristics to speed up the resolution time.we demonstrate the feasibility of our verification process with arealistic health care use case and the tspass theorem prover.
checking accountability with a prover. today on-line services are the cornerstone of onlineapplications such as e-commerce, e-government and e-healthapplications. however, they raise several challenges about dataprivacy. accountability, which is the property of an entity of beingresponsible for its acts, meets some of these challenges and henceincreases user’s trustworthiness in on-line applications. in thiswork, we propose an approach to assist the design of accountableapplications. in particular, we consider an application’s abstractcomponent design and we introduce a logical approach allowingvarious static verifications. this approach offers effective meansto early check the design and the behavior of an applicationand its offered/required services. we motivate our work with arealistic use case coming from the a4cloud project and validateour proposal with experiments using a theorem prover.
un algorithme de recherche à voisinage large pour le problème de tournées de véhicules à deux échelons, routes multiples et contraintes de synchronisation. null
synchronisation entre tournées dans un problème de tournées de véhicules multi-échelons. null
comparative measurement of prompt fission γ-ray emission from fast-neutron-induced fission of u235 and u238. prompt fission γ-ray (pfg) spectra have been measured in a recent experiment with the novel directional fast-neutron source licorne at the alto facility of the ipn orsay. these first results from the facility involve the comparative measurement of prompt γ emission in fast-neutron-induced fission of u235 and u238. characteristics such as γ multiplicity and total and average radiation energy are determined in terms of ratios between the two systems. additionally, the average photon energies were determined and compared with recent data on thermal-neutron-induced fission of u235. pfg spectra are shown to be similar within the precision of the present measurement, suggesting that the extra incident energy does not significantly impact the energy released by prompt γ rays. the origins of some small differences, depending on either the incident energy or the target mass, are discussed. this study demonstrates the potential of the present approach, combining an innovative neutron source and new-generation detectors, for fundamental and applied research on fission in the near future.
adding storage simulation capacities to the simgrid toolkit: concepts, models, and api. for each kind of distributed computing infrastructures, i.e., clusters, grids, clouds, data centers, or supercomputers, storage is a essential component to cope with the tremendous increase in scientific data production and the ever-growing need for data analysis and preservation. understanding the performance of a storage subsystem or dimensioning it properly is an important concern for which simulation can help by allowing for fast, fully repeatable, and configurable experiments for arbitrary hypothetical scenarios. however, most simulation frameworks tailored for the study of distributed systems offer no or little abstractions or models of storage resources.in this paper, we detail the extension of simgrid, a versatile toolkit for the simulation of large-scale distributed computing systems, with storage simulation capacities. we first define the required abstractions and propose a new api to handle storage components and their contents in simgrid-based simulators. then we characterize the performance of the fundamental storage component that are disks and derive models of these resources. finally we list several concrete use cases of storage simulations in clusters, grids, clouds, and data centers for which the proposed extension would be beneficial.
measurement of an excess in the yield of j/$\psi$ at very low $p_{\rm t}$ in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. we report on the first measurement of an excess in the yield of j/$\psi$ at very low transverse momentum ($p_{\rm t}&lt; 0.3$ gev/$c$) in peripheral hadronic pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev, performed by alice at the cern lhc. remarkably, the measured nuclear modification factor ($r_{\rm aa}$) of j/$\psi$ in the rapidity range $2.5&lt;y&lt;4$ reaches about 7 (2) in the $p_{\rm t}$ range 0-0.3 gev/$c$ in the 70-90% (50-70%) centrality class. the j/$\psi$ production cross section associated with the observed excess is obtained under the hypothesis that coherent photoproduction of j/$\psi$ is the underlying physics mechanism. if confirmed, the observation of j/$\psi$ coherent photoproduction in pb-pb collisions at impact parameters smaller than twice the nuclear radius opens new theoretical and experimental challenges and opportunities. in particular, coherent photoproduction accompanying hadronic collisions may provide insight into the dynamics of photoproduction and nuclear reactions, as well as become a novel probe of the quark-gluon plasma.
asymmetric hydrogen bonding and orientational ordering of water at hydrophobic and hydrophilic surfaces. 
computational molecular modeling of the adsorption and transport in clay nanopores in the context of nuclear waste disposal applications. 
the corrosion behavior of carbon steel in sulfide aqueous media at 30 degrees c. in this paper, we studied the effect of sulfide ions on the corrosion behavior of carbon steel to simulate the geological disposal of high-level radioactive waste. in geological storage conditions, sulfidogenic environment was sustained by sulfate-reducing bacteria. corrosion tests were conducted in systems in a controlled atmosphere of 5% h-2/n-2. batch experiments were conducted at 30a degrees c for 1 month with steel coupons immersed in na2s solutions. the structural characterization of the corrosion products was investigated by scanning electron microscope/energy dispersive x-ray spectroscopy, confocal micro-raman spectrometry, and x-ray diffraction. in the absence of sulfide ion, a magnetite (fe3o4) corrosion product layer was formed on steel surface while in the presence of sulfide ions we observed the formation of a poorly crystallized irons sulfide at low-sulfide concentration (1 mg/l) and a solid adherent pyrrhotite layer at higher sulfide concentration (5-15 mg/l). the strong drop in steel corrosion rate with sulfide concentration was revealed and related to the formation of well-crystallized pyrrhotite.
investigating the extensive air shower properties: tackling the challenges of the next generation cosmic ray observatory with the codalema experiment. our knowledge on ultra-high energy cosmic rays and their underlying sources and acceleration mechanisms is steadily improving thanks to the large observatories nowadays in operation. however the need for a next generation instrument is emerging from their experimental limitations and the scientific questions currently out of reach within a reasonable time line. within this scope, the main features of the radio detection of extensive air showers are investigated and confronted to these challenging requirements. codalema is the last experiment currently running in europe dedicated to the cosmic ray detection using the observation of its induced radio electric field. the latest experimental upgrade and the synthesis of its operation features and the upcoming technical developments are presented. the main results of codalema will be presented with special emphasis put on some of the new aspects of the data analysis offered by the codalema3 autonomous station array. finally, the opportunities provided by the nancay observatory for efficient r&amp;d activities and especially the upcoming technical developments are listed. (c) 2013 elsevier b.v. all rights reserved.
radio detection of extensive air showers at the pierre auger observatory. deployed at the end of 2010 at the pierre auger observatory, the first stage of the auger engineering radio array, aera24, consists of 24 radio stations covering an area of 0.5 km(2). aera measures the radio emission from cosmic-ray induced air showers. the amplitude of this radio emission is used to constrain the characteristics of the primary particle: arrival direction, energy and nature. these studies are possible thanks to an instrumentation development allowing self-triggered and externally triggered measurements in the mhz domain and an improved understanding of radio emission processes. in may 2013, 100 new stations were installed to cover an area of similar or equal to 6 km(2), for a total of 124 stations. this stage 2 will provide higher statistics and will enhance both the estimate of the nature of the primary cosmic ray and the energy resolution above 10(17) ev as an addition to detectors such as the auger fluorescence telescopes and particle detectors. we will present the main results obtained with the stage 1 of aera and the current status of the experiment. we will end with a brief overview of the ghz-experiments installed at the pierre auger observatory. (c) 2013 elsevier b.v. all rights reserved.
use of experimental designs to establish a kinetic law for a gas phase photocatalytic process. the photocatalytic degradation of three common indoor vocs - acetone, toluene and heptane - is investigated in a dynamic photocatalytic oxidation loop using box-behnken designs of experiments. thanks to the experimental results and the establishment of a kinetic rate law based on a simplified mechanism, a predictive model for the voc degradation involving independent factors is developed. the parameters under investigation are initial concentration, light intensity and air velocity through the photocatalytic medium. the obtained model fits properly the experimental curves in the range of concentration, light intensity and air flow studied.
strontium-82 and future germanium-68 production at the arronax facility. the arronax cyclotron is fully operational since the end of 2010. it delivers projectiles (p, d, alpha) at high energy (up to 70 mev for protons) and high intensity(2*375 mu a for protons). the main fields of application of arronax are radionuclide production for nuclear medicine and irradiation of inert or living materials for radiolysis and radio-biology studies. a large part of the beam time will be used to produce radionuclides for targeted radionuclide therapy (copper-67, scandium-47 and astatine-211) as well as for pet imaging (scandium-44, copper-64, strontium-82 for rubidium-82 generators, and germanium-68 for gallium-68 generators). since june 2012, large scale production of sr-82 has started with rubidium chloride (rbcl) targets. several improvements are being explored which consist of changing the target material from rbcl to rb metal and introducing an additional target behind the rubidium assembly. thus, a target alloy of nickel/gallium for germanium-68 production has been developed. it is obtained by electroplating and exhibits a better thermal behavior than the natural gallium target used in most production facilities.
experimental investigation on the combustion, performance and pollutant emissions of biodiesel from animal fat residues on a direct injection diesel engine. fat trap grease is a cheap source for biodiesel production, but until now its transformation into biofuel is not well studied. the effects of the use of biodiesel from degraded raw material on combustion in engines are not discussed by researchers. in this paper, engine tests were performed on biodiesel produced from afr (animal fat residues) collected from fat traps. this fuel presents differences in chemical composition and on physical properties as compared to standard biodiesel produced from vegetable oils. tests were performed on a single cylinder, air cooled, direct injection diesel engine at 1500 rpm, performance and pollutant emissions were measured and the combustion parameters were analyzed. the use of afrbd in the engine resulted in similar performance with slight brake thermal efficiency decrease at low loads and a slight increase at high loads. a decrease of 9% of power output at 1500 rpm was also detected. as per pollutant emissions, a drastic reduction of unburned hydrocarbons between 32% and 45% was obtained with afrbd (animal fat residue biodiesel). particulate matter emissions were also reduced at low and medium load ranges, but no differences were detected at high loads. nitric oxides emissions were also measured and slight increase was detected at low loads and a slight reduction was noticed at full engine load. (c) 2014 elsevier ltd. all rights reserved.
aqueous alteration of vhtr fuels particles under simulated geological conditions. very high temperature reactor (vhtr) fuels consist of the bistructural-isotropic (biso) or tristructural-isotropic (triso)-coated particles embedded in a graphite matrix. management of the spent fuel generated during vhtr operation would most likely be through deep geological disposal. in this framework we investigated the alteration of biso (with pyrolytic carbon) and triso (with sic) particles under geological conditions simulated by temperatures of 50 and 90 c-circle and in the presence of synthetic groundwater. solid state (scanning electron microscopy (sem), micro-raman spectroscopy, electron probe microanalyses (epma) and x-ray photoelectron spectroscopy (xps)) and solution analyses (icp-ms, ionique chromatography (ic)) showed oxidation of both pyrolytic carbon and sic at 90 c-circle. under air this led to the formation of sio2 and a clay-like mg silicate, while under reducing conditions (h-2/n-2 atmosphere) sic and pyrolytic carbon were highly stable after a few months of alteration. at 50 c-circle, in the presence and absence of air, the alteration of the coatings was minor. in conclusion, due to their high stability in reducing conditions, htr fuel disposal in reducing deep geological environments may constitute a viable solution for their long-term management. (c) 2014 elsevier b.v. all rights reserved.
assignment of raman-active vibrational modes of tetragonal mackinawite: raman investigations and ab initio calculations. the mackinawite mineral was prepared as a carbon steel corrosion product in sulfidogenic waters at 90 degrees c after 2 months. the tetragonal crystal structure of the material was confirmed by rietveld refinement of x-ray diffraction (xrd) data, and vibrational modes were analysed by micro-raman spectroscopy. despite a large number of studies on the formation and the stability of tetragonal mackinawite, the interpretation of the raman spectra remains uncertain. in the present study, we report on the first calculation of the raman-active vibrational modes of mackinawite using density functional perturbation theory and direct methods with blyp + dispersion correction. based on the comparison between calculated and experimental results, the four fundamental vibrational modes were assigned as 228 cm(-1) (b-1g), 246 cm(-1) (e-g), 373 cm(-1) (a(1g)) and 402 cm(-1) (e-g).
biodiesel production unit from lab scale to industrial pilot plant: material and energy balance. the modern society is, nowadays, facing two major problems: the energy sources depletion and the degradation of the ecologic system because of wastes rejection. the energetic valorization of wastes contributes on the resolution of both problems. in the present work, a feasibility study of an industrial pilot scale installation for the production of biodiesel from waste grease traps is lead. the installation is meant to transform 1000 tons of fat trap grease per year to biodiesel by transesterification. the daily production of the unit reaches 3200 1 of biodiesel. all necessary equipments were sized following process engineering design and based on lab scale optimization experiments. installation energy balance was also realized and it showed that the energy required for the installation functioning does not exceed 3.5% of the heating value of produced biodiesel.
turbulence-combustion interaction in direct injection diesel engine. the experimental measures of chemical species and turbulence intensity during the closed part of the engine combustion cycle are today unattainable exactly. this paper deals with numerical investigations of an experimental direct injection diesel engine and a commercial turbocharged heavy duty direct injection one. simulations are carried out with the kiva3v2 code using the re-normalized group (k-epsilon) model. a reduced mechanism for n-heptane was adopted for predicting auto-ignition and combustion processes. from the calibrated code based on experimental in-cylinder pressures, the study focuses on the turbulence parameters and combustion species evolution in the attempt to improve understanding of turbulence-chemistry interaction during the engine cycle. the turbulent kinetic energy and its dissipation rate are taken as representative parameters of turbulence. the results indicate that chemistry reactions of fuel oxidation during the auto-ignition delay improve the turbulence levels. the peak position of turbulent kinetic energy coincides systematically with the auto-ignition timing. this position seems to be governed by the viscous effects generated by the high pressure level reached at the auto-ignition timing. the hot regime flame decreases rapidly the turbulence intensity successively by the viscous effects during the fast premixed combustion and heat transfer during other periods. it is showed that instable species such as co are due to deficiency of local mixture preparation during the strong decrease of turbulence energy. also, an attempt to build an innovative relationship between self-ignition and maximum turbulence level is proposed. this work justifies the suggestion to determine otherwise the self-ignition timing.
algebraic technique for the stiffness model reduction in elastostatic calibration of robotic manipulators. the paper deals with elastostatic calibration of a serial industrial robot. in contrast to other works, all compliance sources associated with both links and joints elasticity are taken into account. particular attention is paid to the model parameters identification using end-point measurements only. for such experimental setup, the model is transformed into the form suitable for calibration with the sufficient rank of the corresponding observation matrix. the main contributions are in developing algebraic, physical and heuristic techniques that allow user to obtain complete model with minimal number of parameters. the advantages of the developed approach are confirmed by an experimental study that deals with identification of the elastostatic model parameters for a 6 dof serial industrial robot.
pascs 2014: privacy and accountability for software and cloud services. 
upgrade of corrosiveness nature of fish waste bio-oil using a hybrid catalyst (mgo/na(2)co(3)) optimization process. in this work, catalytic cracking of waste fish oil (wfo) to bio-fuel for diesel engine was studied over hybrid catalysts (sodium carbonate na2co3/magnesium oxide). the experiments were conducted using a fix-bed reactor. the effect of catalyst-to-wfo ratio and the amount of each catalyst were studied over the yields of bio-oil and acid value av, of the bio-oil following central composite design (ccd). the statistical analysis showed that catalyst significantly affected the bio-oil yield and acid value. a higher bio-oil yield over 70 wt% with a lower acid value (2.7 mg(koh)/g(oil)) were identified at catalyst-to-wfo of 1:7 by using the same amount of sodium carbonate and magnesium oxide. the optimum bio-oil was analyzed and properties have been investigated and compared to diesel fuel physical properties.
experimental investigation on the effects of raw materials degradation on performance, combustion and emissions of a single cylinder engine running on biodiesel from waste lipids. in this paper, a single cylinder air cooled for strokes direct injection diesel engine was used to compare biodiesel from fat trap grease (afrbd) with biodiesel from waste cooking oil (wcobd) and with diesel fuel. the main difference between both biodiesel samples resides in the presence of short chain and branched methyl esters on afrbd, and in its lower non saturated fatty acids content. comparison was based on engine performance, combustion parameters and emissions. afrbd resulted on a slight drop of brake power at 1500 rpm but it increased engine efficiency at full load. biodiesel reduced polluting emissions of the engine as compared to diesel fuel. wcobd recorded higher reduction of unburned hydrocarbon, carbon monoxide and particulate matter emissions but it increased the nitric oxides emissions. afrbd has the advantage of reducing all pollutant emissions, including nitric oxides.
experimental study on geometric and elastostatic calibration of industrial robot for milling application. the paper is devoted to geometric and elastostatic calibration of industrial robot for milling application. particular attention is paid to the analysis of the experimental results and enhancement identification routines. in contrast to other works, the identification results have been validated using separate set of measurements that were not used in calibration. the obtained geometric and elastostatic models essentially improve robot positioning accuracy in milling applications.
are icts needed for innovative firms to succeed? a survey of french smes. firms' performance can be explained by many factors, including their innovativeness. investment in icts is also seen as a source of competitiveness. our research tests these two sources of performance and examines possible synergies with ict supporting innovation. based on the few existing academic studies raising the issue of synergies between icts and innovation, three hypotheses are formulated: the positive influences of (1) innovativeness and of (2) ict resources on firm performance and (3) the influence of icts on innovation performance. the model is complemented with two control variables: market dynamism and firms' sectors. an empirical survey is conducted among 1.992 small-and-medium enterprises (smes) in france, complemented with an investigation of their financial performance. with a final sample of 1.088 firms, we test the direct effects of innovativeness and ict resources (software diffusion and level of ict skills) and the combined effect of innovativeness and dedicated icts. dedicated icts variable captures how innovation depends on specific investments in icts or more intensive use of existing icts in the firm. this variable constitutes the major conceptual originality of our research. our econometric results show that innovativeness has a positive effect on performance only if it is accompanied with dedicated icts. on the other side, econometric regression emphasizes an unexpected direct negative effect of innovativeness and of the level of ict skills on smes' financial performance. we discuss these results taking place in the specific organizational context of smes: innovative smes might not always be the most financially efficient firms; the return on investment of icts has also to be questioned. however our results confirm that synergies between innovativeness and icts are a factor of smes' performance.
distributed building temperature control with power constraints. generally, heating, cooling and air conditioning (hvac) systems are designed to handle worst case loads, and this over-design of hvac equipment is one of the main reasons for building energy inefficiency. when the hvac system is not over-designed, there exists a trade-off between the comfort of the building's occupants and the available heating/cooling power at critical load hours. we propose a distributed approach that maximizes the comfort of the building's occupants under several power constraints. we prove by means of graph theoretical tools and passivity analysis, that the proposed controller asymptotically reaches an optimal equilibrium without the need of full information. finally, some simulations and comparisons are presented to illustrate the performance of our method.
computational morphology for a soft micro air vehicle in hovering flight. bio-inspired by moth or humming-birds, several micro air vehicles (mav) have recently been developed. in these systems, the motion of the lifting surface is complex and modeled by flapping motion-controlled soft wings. the synchronization of the actuated periodic flapping motion of the wings and that of passive twisting degrees of freedom (dof) allows producing hovering flight. depending on the feature of the mav such as stiffness, and geometric characteristic, hovering flight can be naturally stable or not. in the first case hovering is obtained via an open loop control (or a local closed loop control) while in the second case a global control strategy is required to achieve the same objectives. thus, it is crucial to take into account the influence of the design of the mav on the control in order to choose an appropriate morphology that can reduce the burden of control. in this paper mathematical tools and methodologies are proposed to achieve this objective and reduce the computational cost of control.
ground-state reversal induced by solvation: electronic structure of ato&lt;sup&gt;+&lt;/sup&gt; in water. null
inclusive quarkonium production at forward rapidity in pp collisions at $\sqrt{s}=8$ tev. we report on the inclusive production cross sections of j/$\psi$, $\psi$(2s), $\upsilon$(1s), $\upsilon$(2s) and $\upsilon$(3s), measured at forward rapidity with the alice detector in pp collisions at a center-of-mass energy $\sqrt{s}=8$ tev. the analysis is based on data collected at the lhc and corresponds to an integrated luminosity of 1.28 pb$^{-1}$. quarkonia are reconstructed in the dimuon-decay channel. the differential production cross sections are measured as a function of the transverse momentum $p_{\rm t}$ and rapidity $y$, over the $p_{\rm t}$ ranges $0&lt;p_{\rm t}&lt;20$ gev/$c$ for j/$\psi$, $0&lt;p_{\rm t}&lt;12$ gev/$c$ for all other resonances, and for $2.5&lt;y&lt;4$. the cross sections, integrated over $p_{\rm t}$ and $y$, and assuming unpolarized quarkonia, are $\sigma_{{\rm j}/\psi} = 8.63\pm0.04\pm0.79$ $\mu$b, $\sigma_{\psi{\rm (2s)}} = 1.18\pm0.08\pm0.21$ $\mu$b, $\sigma_{\upsilon{\rm(1s)}} = 68\pm6\pm7$ nb, $\sigma_{\upsilon{\rm(2s)}} = 25\pm5\pm4$ nb and $\sigma_{\upsilon{\rm(3s)}} = 9\pm4\pm1$ nb, where the first uncertainty is statistical and the second one is systematic. these values agree, within at most $1.2\sigma$, with measurements performed by the lhcb collaboration in the same rapidity range.
charged-particle multiplicities in proton-proton collisions at $\sqrt{s}$ = 0.9 to 8 tev. a detailed study of pseudorapidity densities and multiplicity distributions of primary charged particles produced in proton-proton collisions, at $\sqrt{s} =$ 0.9, 2.36, 2.76, 7 and 8 tev, in the pseudorapidity range $|\eta|&lt;2$, was carried out using the alice detector. measurements were obtained for three event classes: inelastic, non-single diffractive and events with at least one charged particle in the pseudorapidity interval $|\eta|&lt;1$. the use of an improved track-counting algorithm combined with alice's measurements of diffractive processes allows a higher precision compared to our previous publications. a kno scaling study was performed in the pseudorapidity intervals $|\eta|&lt;$ 0.5, 1.0 and 1.5. the data are compared to other experimental results and to models as implemented in monte carlo event generators phojet and recent tunes of pythia6, pythia8 and epos.
measurement of electrons from heavy-flavour hadron decays in p-pb collisions at $\sqrt{s_{\rm nn}} = 5.02$ tev. the production of electrons from heavy-flavour hadron decays was measured as a function of transverse momentum ($p_{\rm t}$) in minimum-bias p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev with alice at the lhc. the measurement covers the $p_{\rm t}$ interval $0.5&lt;p_{\rm t}&lt;12$ gev/$c$ and the rapidity range $-1.06 &lt; y_{\rm cms} &lt; 0.14$ in the centre-of-mass reference frame. the contribution of electrons from background sources was subtracted using an invariant mass approach. the nuclear modification factor $r_{\rm ppb}$ was calculated by comparing the $p_{\rm t}$-differential invariant cross section in p-pb collisions to a pp reference at the same centre-of-mass energy, which was obtained by interpolating measurements at $\sqrt{s}= 2.76$ tev and $\sqrt{s} =7$ tev. the $r_{\rm ppb}$ is consistent with unity within uncertainties of about 25%, which become larger for $p_{\rm t}$ below 1 gev/$c$. the data are described by recent model calculations that include cold nuclear matter effects.
azimuthal anisotropy of charged jet production in $\sqrt{s_{\rm nn}}$ = 2.76 tev pb-pb collisions. we present measurements of the azimuthal dependence of charged jet production in central and semi-central $\sqrt{s_{\mathrm{nn}}}$ = 2.76 tev pb-pb collisions with respect to the second harmonic event plane, quantified as $v_{2}^{\mathrm{ch~jet}}$. jet finding is performed employing the anti-$k_{\mathrm{t}}$ algorithm with a resolution parameter $r$ = 0.2 using charged tracks from the alice tracking system. the contribution of the azimuthal anisotropy of the underlying event is taken into account event-by-event. the remaining (statistical) region-to-region fluctuations are removed on an ensemble basis by unfolding the jet spectra for different event plane orientations independently. significant non-zero $v_{2}^{\mathrm{ch~jet}}$ is observed in semi-central collisions (30-50\% centrality) for 20 $&lt;$ $p_{\mathrm{t}}^{\rm ch~jet}$ $&lt;$ 90 ${\mathrm{gev}\kern-0.05em/\kern-0.02em c}$. the azimuthal dependence of the charged jet production is similar to the dependence observed for jets comprising both charged and neutral fragments, and compatible with measurements of the $v_2$ of single charged particles at high $p_{\mathrm{t}}$. good agreement between the data and predictions from jewel, an event generator simulating parton shower evolution in the presence of a dense qcd medium, is found in semi-central collisions.
direct photon production in pb-pb collisions at $\sqrt{s_\rm{nn}}$ = 2.76 tev. direct photon production at mid-rapidity in pb-pb collisions at $\sqrt{s_{_{\mathrm{nn}}}} = 2.76$ tev was studied in the transverse momentum range $0.9 &lt; p_\mathrm{t} &lt; 14$ gev$/c$. photons were detected with the highly segmented electromagnetic calorimeter phos and via conversions in the alice detector material with the $e^+e^-$ pair reconstructed in the central tracking system. the results of the two methods were combined and direct photon spectra were measured for the 0-20%, 20-40%, and 40-80% centrality classes. for all three classes, agreement was found with perturbative qcd calculations for $p_\mathrm{t} \gtrsim 5$ gev$/c$. direct photon spectra down to $p_\mathrm{t} \approx 1$ gev$/c$ could be extracted for the 20-40% and 0-20% centrality classes. the significance of the direct photon signal for $0.9 &lt; p_\mathrm{t} &lt; 2.1$ gev$/c$ is $2.6\sigma$ for the 0-20% class. the spectrum in this $p_\mathrm{t}$ range and centrality class can be described by an exponential with an inverse slope parameter of $(297 \pm 12^\mathrm{stat}\pm 41^\mathrm{syst})$ mev. state-of-the-art models for photon production in heavy-ion collisions agree with the data within uncertainties.
centrality evolution of the charged-particle pseudorapidity density over a broad pseudorapidity range in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the centrality dependence of the charged-particle pseudorapidity density measured with alice in pb-pb collisions at $\sqrt{s_{\rm nn}}$ over a broad pseudorapidity range is presented. this letter extends the previous results reported by alice to more peripheral collisions. no strong change of the charged-particle pseudorapidity density distributions with centrality is observed, and when normalised to the number of participating nucleons in the collisions, the evolution over pseudorapidity with centrality is likewise small. the broad pseudorapidity range allows precise estimates of the total number of produced charged particles which we find to range from $162\pm22$ (syst.) to $17170\pm770$ (syst.) in 80-90% and 0-5 central collisions, respectively. the total charged-particle multiplicity is seen to approximately scale with the number of participating nucleons in the collision. this suggests that hard contributions to the charged-particle multiplicity are limited. the results are compared to models which describe $\mbox{d}n_{\mbox{ch}}/\mbox{d}\eta$ at mid-rapidity in the most central pb-pb collisions and it is found that these models do not capture all features of the distributions.
measurement of d$_s^+$ production and nuclear modification factor in pb-pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev. the production of prompt d$_s^+$ mesons was measured for the first time in collisions of heavy nuclei with the alice detector at the lhc. the analysis was performed on a data sample of pb-pb collisions at a centre-of-mass energy per nucleon pair, $\sqrt{s_{\rm nn}}$, of 2.76 tev in two different centrality classes, namely 0-10% and 20-50%. d$_s^+$ mesons and their antiparticles were reconstructed at mid-rapidity from their hadronic decay channel d$_s^+\rightarrow\phi\pi^+$, with $\phi\rightarrow$k$^-$k$^+$, in the transverse momentum intervals $4&lt; p_{\rm t}&lt;12$ gev/$c$ and $6&lt; p_{\rm t}&lt;12$ gev/$c$ for the 0-10% and 20-50% centrality classes, respectively. the nuclear modification factor $r_{\rm aa}$ was computed by comparing the $p_{\rm t}$-differential production yields in pb-pb collisions to those in proton-proton (pp) collisions at the same energy. this pp reference was obtained using the cross section measured at $\sqrt{s}= 7$ tev and scaled to $\sqrt{s}= 2.76$ tev. the $r_{\rm aa}$ of d$_s^+$ mesons was compared to that of non-strange d mesons in the 10% most central pb-pb collisions. at high $p_{\rm t}$ ($8&lt; p_{\rm t}&lt;12$ gev/$c$) a suppression of the d$_s^+$-meson yield by a factor of about three, compatible within uncertainties with that of non-strange d mesons, is observed. at lower $p_{\rm t}$ ($4&lt; p_{\rm t}&lt;8$ gev/$c$) the values of the d$_s^+$-meson $r_{\rm aa}$ are larger than those of non-strange d mesons, although compatible within uncertainties. the production ratios d$_s^+$/d$^0$ and d$_s^+$\d$^+$ were also measured in pb-pb collisions and compared to their values in proton-proton collisions.
multiplicity and transverse momentum evolution of charge-dependent correlations in pp, p-pb, and pb-pb collisions at the lhc. we report on two-particle charge-dependent correlations in pp, p-pb, and pb-pb collisions as a function of the pseudorapidity and azimuthal angle difference, $\mathrm{\delta}\eta$ and $\mathrm{\delta}\varphi$ respectively. these correlations are studied using the balance function that probes the charge creation time and the development of collectivity in the produced system. the dependence of the balance function on the event multiplicity as well as on the trigger and associated particle transverse momentum ($p_{\mathrm{t}}$) in pp, p-pb, and pb-pb collisions at $\sqrt{s_{\mathrm{nn}}} = 7$, 5.02, and 2.76 tev, respectively, are presented. in the low transverse momentum region, for $0.2 &lt; p_{\mathrm{t}} &lt; 2.0$ gev/$c$, the balance function becomes narrower in both $\mathrm{\delta}\eta$ and $\mathrm{\delta}\varphi$ directions in all three systems for events with higher multiplicity. the experimental findings favor models that either incorporate some collective behavior (e.g. ampt) or different mechanisms that lead to effects that resemble collective behavior (e.g. pythia8 with color reconnection). for higher values of transverse momenta the balance function becomes even narrower but exhibits no multiplicity dependence, indicating that the observed narrowing with increasing multiplicity at low $p_{\mathrm{t}}$ is a feature of bulk particle production. .
experimental investigation on ng dual fuel engine improvement by hydrogen enrichment. the crude oil graduate depletion, as well as aspects related to environmental pollution and global warming instigated many researches concerning alternative fuels. natural gas (ng) is one of the most attractive available fuels. a promising technique for its use in internal combustion engines is the dual fuel concept. one of the main problems with this technique is that, at low loads, the engine efficiency decreases compared to conventional diesel. the unburned hydrocarbons and carbon monoxide emissions are also higher in dual fuel mode. an effective method to compensate the demerits of limited lean-burn ability and slow burning velocity of ng is to mix it with a fuel that possesses wide flammability limit and fast burning velocity. hydrogen (h-2) is thought to be the best gaseous candidate for natural gas. in the present work, ng enrichment with various h-2 blends is investigated as a technique for improving dual fuel mode, especially at low loads. impact on engine performance and emissions is experimentally examined. total bsfc is considerably reduced. an important benefit in terms of bte, reaching to increase a 12% with the 10%h-2 blend compared to the pure ng case, is also achieved. thc and co emissions are in general reduced as a result of the improvement of gaseous fuel utilization. co2 emissions are also in general reduced. even though a slight increase is in overall observed for no emissions, its almost insignificant. copyright (c) 2014, hydrogen energy publications, llc. published by elsevier ltd. all rights reserved.
quarkonium suppression in heavy-ion collisions from coherent energy loss in cold nuclear matter. the effect of parton energy loss in cold nuclear matter on the suppression ofquarkonia (j/psi, upsilon) in heavy-ion collisions is investigated, byextrapolating a model based on coherent radiative energy loss recently shown todescribe successfully j/psi and upsilon suppression in proton-nucleuscollisions. model predictions in heavy-ion collisions at rhic (au-au, cu-cu,and cu-au) and lhc (pb-pb) show a sizable suppression arising from the soleeffect of energy loss in cold matter. this effect should thus be considered inorder to get a reliable baseline for cold nuclear matter effects in quarkoniumsuppression in heavy-ion collisions, in view of disentangling hot from coldnuclear effects.
chiral transport equation from the quantum dirac hamiltonian and the on-shell effective field theory. we derive the relativistic chiral transport equation for massless fermions and antifermions by performing a semiclassical foldy-wouthuysen diagonalization of the quantum dirac hamiltonian. the berry connection naturally emerges in the diagonalization process to modify the classical equations of motion of a fermion in an electromagnetic field. we also see that the fermion and antifermion dispersion relations are corrected at first order in the planck constant by the berry curvature, as previously derived by son and yamamoto for the particular case of vanishing temperature. our approach does not require knowledge of the state of the system, and thus it can also be applied at high temperature. we provide support for our result by an alternative computation using an effective field theory for fermions and antifermions: the on-shell effective field theory. in this formalism, the off-shell fermionic modes are integrated out to generate an effective lagrangian for the quasi-on-shell fermions/antifermions. the dispersion relation at leading order exactly matches the result from the semiclassical diagonalization. from the transport equation, we explicitly show how the axial and gauge anomalies are not modified at finite temperature and density despite the incorporation of the new dispersion relation into the distribution function.
experimental cross section evaluation for innovative mo-99 production via the (alpha,n) reaction on zr-96 target. the high-specific activity mo-99 accelerator-based production, via the (alpha,n) reaction on zr-96-enriched target, has been investigated in the present work. the excitation function measurement has been performed in the energy range 8-34 mev at the arronax facility, using the well-known stacked foils technique on natural zirconium as target. a general good agreement in the cross section trend has been observed, once compared to former measurements. a different (i.e. higher) peak value and a shift of about 2 mev towards larger energies have however been found. assuming a fully enriched zr-96 target irradiated by an alpha-beam at suitable energy (e = 25 mev), the mo-99 production yield has thus been estimated. at last the alternative production routes, based on the zr-96(alpha,n)mo-99 and mo-100(p,x)mo-99/tc-99m reactions, are compared.
carbon steel corrosion in clay-rich environment. we investigated the carbon steel corrosion in carbon dioxide clay-rich environment to understand its behavior under geological conditions. the results show the formation of magnetite as the main corrosion product in the first step of the corrosion process, followed by the formation of different corrosion products with complex mixtures of iron-oxide, hydroxycarbonate, hydroxychloride and sulfide phases. these results strongly contrast with similar experiments conducted under h-2 atmosphere where the major corrosion products consisted of iron sulfides. it appears then important to consider all the geochemical parameters including gas composition to better study corrosion of steel buried in geological formations. (c) 2014 elsevier ltd. all rights reserved.
adsorption study using optimised 3d organised mesoporous silica coated with fe and al oxides for specific as(iii) and as(v) removal from contaminated synthetic groundwater. this work presents the possibility of optimising 3d organised mesoporous silica (oms) coated with both iron and aluminium oxides for the optimal removal of as(iii) and as(v) from synthetic contaminated water. the materials developed were fully characterised and were tested for removing arsenic in batch experiments. the effect of total al to fe oxides coating on the selective removal of as(iii) and as(v) was studied. it was shown that 8% metal coating was the optimal configuration for the coated oms materials in.removing arsenic. the effect of arsenic initial concentration and ph, kinetics and diffusion mechanisms was studied, modelled and discussed. it was shown that the advantage of an organised material over an un-structured sorbent was very limited in terms of kinetic and diffusion under the experimental conditions. it was shown that physisorption was the main adsorption process involved in as removal by the coated oms. maximum adsorption capacity of 55 mg as(v) g(-1) was noticed at ph 5 for material coated with 8% al oxides while 35 mg as(v) g(-1) was removed at ph 4 for equivalent material coated with fe oxides. (c) 2014 elsevier inc. all rights reserved.
use of coffee mucilage as a new substrate for hydrogen production in anaerobic co-digestion with swine manure. coffee mucilage (cm), a novel substrate produced as waste from agricultural activity in colombia, the largest fourth coffee producer in the world, was used for hydrogen production. the study evaluated three ratios (c1-3) for co-digestion of cm and swine manure (sm), and an increase in organic load to improve hydrogen production (c4). the hydrogen production was improved by a c/n ratio of 53.4 used in c2 and c4. the average hydrogen production rate in c4 was 7.6 nl h-2/l(cm)d, which indicates a high hydrogen potential compare to substrates such as pome and wheat starch. in this condition, the biogas composition was 0.1%, 50.6% and 39.0% of methane, carbon dioxide and hydrogen, respectively. the butyric and acetic fermentation pathways were the main routes identified during hydrogen production which kept a bu/ac ratio at around 1.0. a direct relationship between coffee mucilage, biogas and cumulative hydrogen volume was established. (c) 2014 elsevier ltd. all rights reserved.
geological disposal of nuclear waste: ii. from laboratory data to the safety analysis - addressing societal concerns. after more than 30 years of international research and development, there is a broad technical consensus that geologic disposal of highly-radioactive waste will provide for the safety of humankind and the environment, now, and far into the future. safety analyses have demonstrated that the risk, as measured by exposure to radiation, will be of little consequence. still, there is not yet an operating geologic repository for highly-radioactive waste, and there remains substantial public concern about the long-term safety of geologic disposal. in these two linked papers, we argue for a stronger connection between the scientific data (paper i, grambow et al., 2014) and the safety analysis, particularly in the context of societal expectations (paper ii). in this paper (ii), we assess the meaning of the technical results and derived models (paper i) for the determination of the long-term safety of a repository. we consider issues of model validity and their credibility in the context of a much broader historical, epistemological and societal context. safety analysis is treated in its social and temporal dimensions. this perspective provides new insights into the societal dimension of scenarios and risk analysis. surprisingly, there is certainly no direct link between increased scientific understanding and a public position for or against different strategies of nuclear waste disposal. this is not due to the public being poorly informed, but rather due to cultural cognition of expertise and historical and cultural perception of hazards to regions selected to host a geologic repository. the societal and cultural dimension does not diminish the role of science, as scientific results become even more important in distinguishing between the conflicting views of the risk of geologic disposal of nuclear waste. (c) 2014 elsevier ltd. all rights reserved.
state of fukushima nuclear fuel debris tracked by cs137 in cooling water. it is still difficult to assess the risk originating from the radioactivity inventory remaining in the damaged fukushima nuclear reactors. here we show that cooling water analyses provide a means to assess source terms for potential future releases. until now already about 34% of the inventories of cs-137 of three reactors has been released into water. we found that the release rate of cs-137 has been constant for 2 years at about 1.8% of the inventory per year indicating ongoing dissolution of the fuel debris. compared to laboratory studies on spent nuclear fuel behavior in water, cs-137 release rates are on the higher end, caused by the strong radiation field and oxidant production by water radiolysis and by impacts of accessible grain boundaries. it is concluded that radionuclide analyses in cooling water allow tracking of the conditions of the damaged fuel and the associated risks.
antistrange meson-baryon interaction in hot and dense nuclear matter. we present a study of in-medium cross sections and (off-shell) transition rates for the most relevant binary reactions for strange pseudoscalar meson production close to threshold in heavy-ion collisions at energies available at the facility for antiproton and ion research. our results rely on a chiral unitary approach in coupled channels which incorporates the s and p waves of the kaon-nucleon interaction. the formalism, which is modified in the hot and dense medium to account for pauli blocking effects, mean-field binding on baryons, and pion and kaon self-energies, has been improved to implement unitarization and self-consistency for both the s- and the p-wave interactions at finite temperature and density. this gives access to in-medium amplitudes in several elastic and inelastic coupled channels with strangeness content s = -1. the obtained total cross sections mostly reflect the fate of the lambda (1405) resonance, which melts in the nuclear environment, whereas the off-shell transition probabilities are also sensitive to the in-medium properties of the hyperons excited in the p-wave amplitudes [lambda, sigma, and sigma* (1385)]. the single-particle potentials of these hyperons at finite momentum, density, and temperature are also discussed in connection with the pertinent scattering amplitudes. our results are the basis for future implementations in microscopic transport approaches accounting for off-shell dynamics of strangeness production in nucleus-nucleus collisions.
d-meson propagation in hadronic matter and consequences for heavy-flavor observables in ultrarelativistic heavy-ion collisions. we employ recently published cross sections for d mesons with hadrons and calculate the drag and diffusion coefficients of d mesons in hadronic matter as a function of the momentum of d mesons as well as of the temperature of the medium. calculating in our approach the spatial diffusion coefficient, d-x, at zero chemical potential we see a very smooth transition between our calculations for the hadron gas and the lattice qcd calculations. applying the results for the transport coefficients of d mesons in a fokker-planck equation, which describes the evolution of d mesons during the expansion of a hadron gas created in ultrarelativistic heavy-ion collisions, we find that the value of r-aa is little influenced by hadronic rescattering, whereas in the elliptic flow the effects are stronger. we extend our calculations to the finite chemical potentials and calculate the spatial diffusion coefficients of d mesons propagating through the hadronic medium following isentropic trajectories, appropriate at future fair (facility for antiproton and ion research, darmstadt) and nica (nuclotron-based ion collider facility, dubna) heavy-ion experiments. for the isentropic trajectory with s/rho(net)(b) = 20 we find a perfect matching of results for d mesons in hadronic matter and for charm quarks in partonic matter treated within the dynamical quasiparticle model approach.
inverse determination of convective heat transfer between an impinging jet and a continuously moving flat surface. in this study an inverse method is developed to determine the heat flux distribution on a moving plane wall. the method uses a thin layer of material (the measurement medium) glued on the conveyor belt. the heat flux distribution on the moving wall is then determined by an inverse method based on the temperature measurement by infrared thermography on the upper surface of the measurement medium. a finite element based inverse algorithm of a steady state heat conduction advection in the eulerian frame is performed. the algorithm entails the use of the tikhonov regularization method, along with the l-curve method to select an optimal regularization parameter. both the direct solution of moving boundary problem and the inverse design formulation are presented. the accuracy of the inverse method is examined by simulating the exact and noisy data with four different values of the surface-to-jet velocity ratio, and two different materials (pvc and aluminum) for the measurement medium. the results show a greater sensitivity to the convective heat flux allowing a better estimation of heat flux distribution for the pvc layer. an alternative underdetermined inverse scheme is also studied. this configuration allows a different extend between the retrieval heat flux surface and the measurement temperature surface. (c) 2014 elsevier inc. all rights reserved.
phosphorus adsorption onto an industrial acidified laterite by-product: equilibrium and thermodynamic investigation. the present research investigates the uptake of phosphate ions from aqueous solutions using acidified laterite (als), a by-product from the production of ferric aluminium sulfate using laterite. phosphate adsorption experiments were performed in batch systems to determine the amount of phosphate adsorbed as a function of solution ph, adsorbent dosage and thermodynamic parameters per fixed p concentration. kinetic studies were also carried out to study the effect of adsorbent particle sizes. the maximum removal capacity of als observed at ph5 was 3.68mg pg(-1). it was found that as the adsorbent dosage increases, the equilibrium ph decreases, so an adsorbent dosage of 1.0gl(-1) of als was selected. adsorption capacity (q(m)) calculated from the langmuir isotherm was found to be 2.73mgg(-1). kinetic experimental data were mathematically well described using the pseudo first-order model over the full range of the adsorbent particle size. the adsorption reactions were endothermic, and the process of adsorption was favoured at high temperature; the g and h values implied that the main adsorption mechanism of p onto als is physisorption. the desorption studies indicated the need to consider a naoh 0.1m solution as an optimal solution for practical regeneration applications. (c) 2014 curtin university of technology and john wiley &amp; sons, ltd.
approaches to surface complexation modeling of ni(ii) on callovo-oxfordian clayrock. callovo-oxfordian formation (cox) is as potential host formation for emplacement of long-term nuclear waste repositories in france. the objective of this work is to assess whether a simplified ``bottom-up'' approach may explain the retention of ni(ii) by the cox considering two levels of `upscaling': (i) from clay surfaces to rock clay fraction and (ii) from clay fraction to whole rock samples. to this end, ni(ii) adsorption was investigated by batch equilibrium, xps, and exafs techniques on a representative sample extracted at the location where the storage is supposed to be built (clay content of about 50%) and on the corresponding carbonate-free &lt;2 mu m fractions. the results showed that a simplified ``bottom-up'' approach based on published models available for illite and montmorillonite cannot explain ni(ii) adsorption on the &lt;2 pm fraction when the retention is controlled only by surface complexation on the reactive clay edge sites. an operational model based on the generalized composite modeling approach was used instead. the developed model considers an interaction between two ni(ii) species with one type of clay edge sites. the model developed for the clay fraction gives a satisfactory estimation of ni(ii) adsorption data for the representative clayrock sample. complementary experiments were performed by x-ray photoelectron (xps) and x-ray absorption (exafs) spectroscopies for both clay fraction and raw cox sample at high ni(ii) loadings. spectroscopic data were characterized by similar fitting parameters considering formation of ni phyllosilicate. this indicates that the clay fraction governs the retention of ni(ii), as it was concluded from the batch experiments. complementary adsorption experiments preformed with cox samples having clay contents representative of the variability occurring at the formation scale (i.e. 1.5-47% in weight) show that one cannot neglect the retention properties of the non-clay phases, mainly dominated by calcite, when the clay content becomes the minority. retention values in the range of 60-300 l/kg can finally be given for describing adsorption properties of trace concentrations of ni(ii) for the clay contents representative of the majority of the callovo-oxfordian formation. (c) 2014 elsevier b.v. all rights reserved.
j/psi production in p-pb collisions with alice at the lhc. the alice collaboration has measured the inclusive j/psi production in p-pb collisions at the nucleon-nucleon center-of-mass energy root s-nn = 5.02 tev. the comparison of the results on the nuclear modification factor as a function of rapidity and transverse momentum with theoretical predictions shows a fair agreement with a shadowing based model and energy loss models with or without a shadowing contribution. the j/psi production has also been measured by computing the j/psi nuclear modification factor as a function of the event activity, as well as the yield and mean transverse momentum dependence with the relative charged-particle multiplicity measured at mid-rapidity. (c) 2014 elsevier b.v. all rights reserved.
gluon radiation by heavy quarks at intermediate energies and consequences for the mass hierarchy of energy loss. we extend the gunion bertsch calculation of gluon radiation in single scattering to the case of finite mass quarks. this case applies to the radiative energy-loss of heavy quarks of intermediate energies propagating in a quark gluon plasma. we discuss more specifically the dead cone effect as well as the mass hierarchy of the collisional and radiative energy loss and provide some predictions for observables sensitive to the mass hierarchy of energy loss in ultrarelativistic heavy ion collisions.
heavy-flavor observables at rhic and lhc. we investigate the charm-quark propagation in the qgp media produced in ultrarelativistic heavy-ion collisions at rhic and the lhc. purely collisional and radiative processes lead to a significant suppression of final $d$-meson spectra at high transverse momentum and a finite flow of heavy quarks inside the fluid dynamical evolution of the light partons. the $d$-meson nuclear modification factor and the elliptic flow are studied at two collision energies. we further propose to measure the triangular flow of $d$ mesons, which we find to be nonzero in non-central collisions.
steel slag filters to upgrade phosphorus removal in small wastewater treatment plants: removal mechanisms and performance. electric arc furnace steel slag (eaf-slag) and basic oxygen furnace steel slag (bof-slag) were used as filter substrate in horizontal subsurface flow laboratory-scale filters designed to remove phosphorus (p) from a synthetic solution (similar to 10 mg p/l). the main objective of this study was to evaluate the influence of various parameters, including slag type, slag size, and slag composition, on p removal performance. also, a series of chemical and mineralogical analyses was performed to determine the mechanisms of p removal achieved by steel slag in the filters. over a period of 52 weeks of filter operation, small-size eaf-slag (5-16 mm) and small-size bof-slag (6-12 mm) removed 98% and &gt;99% of the inlet total phosphorus (tp), whereas big-size eaf-slag (20-40 mm) and big-size bof-slag (20-50 mm) removed 88% and 95% of the influent tp, respectively. the main mechanism of p removal was related to cao dissolution from slag followed by ca phosphate precipitation and accumulation of the precipitates into the filters. p removal performance improved with increasing the cao-slag content and with decreasing slag size, most probably because the specific surface available for cao dissolution was increased. also, the experimental results suggested that small-size slag was more efficient than big-size slag for the self-filtration of p precipitates. chemical and mineralogical analyses indicated that, after precipitation, ca phosphates may crystallise into the most stable form of hydroxyapatite. (c) 2014 elsevier b.v. all rights reserved.
effect of thermal regeneration of spent activated carbon on volatile organic compound adsorption performances. thermal swing adsorption (tsa) is widely used as a process in industry for gas purification and air treatment. this study enlightens the effects of thermal regeneration of an activated carbon spent with volatile organic compounds (vocs) on its adsorption capacities. ketone group vocs (acetone and methyl ethyl ketone (mek)) were selected for this study as they are sensitive to oxidation reactions at low temperature, and for this reason, are responsible for many reported fire accidents on industrial units. cyclic adsorption-desorption experiments were performed using a thermo-gravimetric analyzer (tga). first cycle adsorption capacity of acetone and mek at 20 degrees c was 5.06 mol/kg and 6.41 mol/kg, respectively. the regeneration was performed with air at temperature ranging from 80 degrees c to 160 degrees c. multiple cycles were successively repeated to assess the variations in the material adsorption performances. for acetone, it was found that after first regeneration cycle conducted at 80 degrees c, the adsorption capacity was restored at nearly 95%, and remained unchanged after 8 successive cycles. for mek, continuous degradation of the adsorption capacities was observed, which was more drastic when the temperature was low and reached 3.4 mol/kg after 8 cycles. this could be related to the partial decomposition of the chemisorbed mek molecules. effect of air humidity was furthermore examined and a protocol was developed to guarantee good regeneration efficiency of mek spent activated carbon at a moderate temperature. (c) 2014 taiwan institute of chemical engineers. published by elsevier b.v. all rights reserved.
structural arrangements of isomorphic substitutions in smectites: molecular simulation of the swelling properties, inter layer structure, and dynamics of hydrated cs-montmorillonite revisited with new clay models. three new structural models of montmorillonite with differently distributed al/si and mg/al substitutions in the tetrahedral and octahedral clay layers are systematically developed and studied by means of md simulations to quantify the possible effects of such substitutional disorder on the swelling behavior, the interlayer structure, and mobility of aqueous species. a very wide range of water content, from 0 to 700 mg(water)/g(clay) is explored to derive the swelling properties of cs-montmorillonite. the determined layer spacing does not differ much depending on the clay model. however, at low water contents up to 1-layer hydrate (similar to 100 mg(water)/g(clay)) the variation of specific locations of the tetrahedral and octahedral substitutions in the two tot clay layers slightly but noticeably affects the total hydration energy of the system. using atom-atom radial distribution functions and the respective atomic coordination numbers we have identified for the three clay models not only the previously observed binding sites for cs+ on the clay surface but also new ones that are correlated with the position of tetrahedral substitution in the structure. the mobility of cs+ ions and h2o diffusion coefficients, as expected, gradually increase both with increasing water content and with increasing distance from the clay surface, but they still remain 2 to 4 times lower than the corresponding bulk values. only small differences were observed between the three cs-montmorillonite models, but these differences are predicted to increase in the case of higher charge density of the clay layers and/or interlayer cations.
cation exchange on interstratified illite/smectite minerals: experimental study and molecular dynamics simulations. 
indoor air purification by photocatalytic oxidation: fate of contaminants released in the gas phase and key parameter optimization for the improvement of the process efficiency and safety. 
a polynomial approach for optimal control of switched nonlinear systems. optimal control problems for switched nonlinear systems are investigated. we propose an alternative approach for solving the optimal control problem for a nonlinear switched system based on the theory of moments. the essence of this method is the transformation of a nonlinear, nonconvex optimal control problem, that is, the switched system, into an equivalent optimal control problem with linear and convex structure, which allows us to obtain an equivalent convex formulation more appropriate to be solved by high-performance numerical computing. consequently, we propose to convexify the control variables by means of the method of moments obtaining semidefinite programs. copyright (c) 2013 john wiley &amp; sons, ltd.
parton-hadron dynamics in heavy-ion collisions. the dynamics of partons and hadrons in relativistic nucleus-nucleus collisions is analyzed within the novel parton-hadron-string dynamics (phsd) transport approach, which is based on a dynamical quasiparticle model for the partonic phase (dqpm) including a dynamical hadronization scheme. the phsd approach is applied to nucleus-nucleus collisions from low sps to lhc energies. the traces of partonic interactions are found in particular in the elliptic flow of hadrons and in their transverse mass spectra. we investigate also the equilibrium properties of strongly-interacting infinite parton-hadron matter characterized by transport coefficients such as shear and bulk viscosities and the electric conductivity in comparison to lattice qcd results.
core-corona model analysis of the low energy beam scan at rhic. the centrality dependence of spectra of identified particles in collisions between ultrarelativistic heavy ions with a center of mass energy (root s) of 39 and 11.5agev is analyzed in the core-corona model. we show that at these energies the spectra can be well understood assuming that they are composed of two components whose relative fraction depends on the centrality of the interaction: the core component which describes an equilibrated quark gluon plasma and the corona component which is caused by nucleons close to the surface of the interaction zone which scatter only once and which is identical to that observed in proton-proton collisions. the success of this approach at 39 and 11.5 agev shows that the physics does not change between this energy and root s = 200 agev for which this model has been developed (aichelin 2008). this presents circumstantial evidence that a quark gluon plasma is also created at center of mass energies as low as 11.5 agev. (c) 2014 wiley-vch verlag gmbh &amp; co. kgaa, weinheim.
improved measurements of the neutrino mixing angle theta(13) with the double chooz detector (vol 10, 086, 2014). the double chooz experiment presents improved measurements of the neutrino mixing angle θ13 using the data collected in 467.90 live days from a detector positioned at an average distance of 1050 m from two reactor cores at the chooz nuclear power plant. several novel techniques have been developed to achieve significant reductions of the backgrounds and systematic uncertainties with respect to previous publications, whereas the efficiency of the ν¯¯¯e signal has increased. the value of θ13 is measured to be sin2 2θ13 = 0.090+ 0.032− 0.029 from a fit to the observed energy spectrum. deviations from the reactor ν¯¯¯e prediction observed above a prompt signal energy of 4 mev and possible explanations are also reported. a consistent value of θ13 is obtained from a fit to the observed rate as a function of the reactor power independently of the spectrum shape and background estimation, demonstrating the robustness of the θ13 measurement despite the observed distortion.
medium-induced soft gluon radiation in forward dijet production in relativistic proton-nucleus collisions. considering forward dijet production in the q -&gt; qg partonic process, we derive the spectrum of accompanying soft gluon radiation induced by rescatterings in a nuclear target. the spectrum is obtained to logarithmic accuracy for an arbitrary energy sharing between the final quark and gluon, and for final transverse momenta as well as momentum imbalance being large as compared to transverse momentum nuclear broadening. in the case of equal energy sharing and for approximately back-to-back quark and gluon transverse momenta, we reproduce a previous result of liou and mueller. interpreting our result, we conjecture a simple formula for the medium-induced radiation spectrum associated to hard forward 1 -&gt; n processes, which we explicitly check in the case of the g -&gt; gg process.
exergetic study of catalytic steam reforming of bio-ethanol over pd-rh/ceo2 with hydrogen purification in a membrane reactor. an exergy analysis of the ethanol steam reforming process using a pd-rh catalyst supported on ceria in a pd-ag membrane reactor has been performed based on experimental data. both chemical and physical exergy for chemical species together with thermodynamic loss calculations have been considered to evaluate the exergy efficiency of the process. the system has been studied under different operational conditions in terms of temperature (873-923 k), pressure (4-12 bar), and feed flow rate using a water ethanol mixture of 50-50% volume (s/c = 1.6). a significant exergy efficiency dependency on the fuel flow rate and pressure has been encountered when considering the heat loss from the reactor. the best results regarding the exergetic and thermal efficiency have been obtained at high flow rates and 8-12 bar, which has been attributed to the higher hydrogen permeation through the membrane. the chemical reactions involved in the reforming process and the heat losses are the main sources of exergy destruction. copyright (c) 2014, hydrogen energy publications, llc. published by elsevier ltd. all rights reserved.
subcritical hydrothermal liquefaction of microalgae residues as a green route to alternative road binders. the valorization of scenedesmus sp. microalgae byproducts was investigated, as a potential route for the production of road binders from renewable sources. under hydrothermal liquefaction conditions, a water-insoluble viscous material was obtained in a ca. 55% yield, which consists of an oily fatty acid-based fraction mixed with organic and inorganic solid residues (up to 22%). although the chemical composition of the obtained materials completely differs from that of petroleum-based bitumen, similar viscoelastic properties were observed in some cases, depending on the hydrothermal liquefaction experimental conditions. a rheological simple material could thus be obtained, which compared well with a bitumen reference.
multibody system dynamics for bio-inspired locomotion: from geometric structures to computational aspects. this article presents a set of generic tools for multibody system dynamics devoted to the study of bio-inspired locomotion in robotics. first, archetypal examples from the field of bio-inspired robot locomotion are presented to prepare the ground for further discussion. the general problem of locomotion is then stated. in considering this problem, we progressively draw a unified geometric picture of locomotion dynamics. for that purpose, we start from the model of discrete mobile multibody systems (mmss) that we progressively extend to the case of continuous and finally soft systems. beyond these theoretical aspects, we address the practical problem of the efficient computation of these models by proposing a newton-euler-based approach to efficient locomotion dynamics with a few illustrations of creeping, swimming, and flying.
an experimental approach to measure particle deposition in large circular ventilation ducts. the topic of this study is related to airborne particle dynamics in indoor environments. lab-scale experiments have been performed to investigate particle deposition velocity to six different surfaces orientations (with respect to gravity) for fully developed turbulent flow in horizontal large circular ventilation ducts. monodispersed aerosol particles (1-6 mu m) were used in the deposition experiments. a very low particle mass (40 ng) was measured reliably above background level on duct surfaces by a means of a nondestructive stencil technique associated with fluorescence analysis. for 26 mu m particles (diffusion and impaction regime), deposition rates to floors were much greater than rates to the ceiling and greater than rates to the wall. for 1-mu m particles, the effect of surface orientation to particle deposition was not significant. results were compared to the very few similar and published studies. this work was conducted in the frame of the cleanairnet project which aimed at producing new knowledge, models, and techniques to help controlling the safety food stuffs, through a better control of aerosol particle (bioaerosols) transport and deposition in the ventilation networks of the food industry.
optimal pose selection for calibration of planar anthropomorphic manipulators. the paper is devoted to the calibration experiment design for serial anthropomorphic manipulators with arbitrary number of links. it proposes simple rules for the selection of manipulator configurations that allow the user to essentially improve calibration accuracy and reduce identification errors. although the main results have been obtained for the planar manipulators, they can be also useful for calibration of more complicated mechanisms. the efficiency of the proposed approach is illustrated with several examples that deal with typical planar manipulators and an anthropomorphic industrial robot. (c) 2014 elsevier inc. all rights reserved.
vapor hydration of a simulated borosilicate nuclear waste glass in unsaturated conditions at 50 degrees c and 90 degrees c. vapor hydration of a simulated typical french nuclear intermediate-level waste (ilw) glass in unsaturated conditions has been studied in order to simulate its behaviour under repository conditions before complete saturation of the disposal site. the experiments were conducted for one year at 50 degrees c and 90 degrees c and the relative humidity (rh) was maintained at 92% and 95%. the glass hydration was followed by fourier transform infra-red spectroscopy (ftir). the surface of the reacted glass was characterised by scanning electron microscopy (sem) and transmission electron microscopy (tem). the chemical and mineralogical composition of the alteration products were studied by energy dispersive x-ray spectroscopy (edx) and mraman spectroscopy, respectively. the glass hydration increased with temperature and rh and led to the formation of a depolymerized gel layer depleted in alkalis. the glass hydration rate decreased with time and remained almost unchanged for the last three months of exposure. overall, the ilw glass hydration rate was similar to that obtained with the son68 high-level waste glass.
the effect of temperature on carbon steel corrosion under geological conditions. we investigated the role of temperature on the carbon steel corrosion under simulated geological conditions. to simulate the effect of temperature increase due to radioactive decay, we conducted batch experiments using callovo-oxfordian (cox) claystone and synthetic water formation with steel coupons at 30 degrees c and 90 degrees c for 6 months. the corrosion products have been studied by scanning electron microscope/energy dispersive x-ray spectroscopy, x-ray diffraction and micro-raman spectroscopy. at 30 degrees c, experiments showed the formation of magnetite and iron sulphide, indicating the activation of sulphate-reducing bacteria. at 90 degrees c a continuous iron sulphide layer was identified on steel surface due to the reduction by hydrogen of pyrite originating from claystone into pyrrhotite and hydrogen sulphide. thus, sulphide production may occur even in the absence of microbial activity at high temperature and must be taken into consideration regarding the near-field geochemical evolution. (c) 2014 elsevier ltd. all rights reserved.
h2s biofiltration using expanded schist as packing material: influence of packed bed configurations at constant ebrt. backgroundh(2)s biofiltration was carried out using expanded schist as packing material completed with a synthetic material (up20). a comparison of different hydrodynamic configurations was made based on biofilter performances and pressure drop measurements. three biofilters (namely bf52, bf102 and bf160) differing in bed height (52 cm, 102 cm and 160 cm, respectively) and diameter (14 cm, 10 cm and 8 cm, respectively) were designed in order to contain the same volume of expanded schist (8 l). resultsbiofilters were operated for more than 6 months at a constant flow rate (1.5 nm(3) h(-1) corresponding to an empty bed residence time of 19 s). elimination capacities and removal efficiencies were calculated according to loading rates varying from 0 to 57 g m(-3) h(-1) (inlet concentration up to 300 mg m(-3)). biofilter performances were modeled and biokinetic constants were calculated using the ottengraf model and a modified michaelis-menten model. in terms of elimination capacity, biofilter configurations can be ordered from the most to the least efficient: bf160&gt;bf102&gt;bf52 (maximum removal rates of 36.4, 30.3 and 25.1 g m(-3) h(-1), respectively). conclusionfrom the ottengraf model, it was calculated that the specific surface area covered with biofilm, relative to bf52, was 21% and 45% higher for bf102 and bf160, respectively. (c) 2014 society of chemical industry.
constraining the nuclear matter equation of state around twice saturation density. using data on elliptic flow measured by the fopi collaboration we extract constraints for the equation of state (eos) of symmetric nuclear matter with the help of the microscopic transport code iqmd. best agreement between data and calculations is obtained with a `soft' equation of state including a momentum dependent interaction. from the model it can be deduced that the characteristic density related to the observed flow signal is around twice saturation density and that both compression within the fireball and the presence of the surrounding spectator matter is necessary for the development of the signal and its sensitivity to the nuclear equation of state.
measurement of w-boson production in p-pb collisions at root(nn)-n-s=5.02 tev with alice at the lhc. in hadronic collisions, electroweak bosons are produced in initial hard scattering processes and they are not affected by the strong interaction. in proton proton collisions, they have been suggested as standard candles for luminosity monitoring and their measurement can improve the evaluation of detector performances. in nucleus-nucleus and proton nucleus collisions, w-bosons allow one to check at first order the validity of binary collision scaling, while small deviations allow to study the nuclear modifications of parton distribution functions. the w-boson production in p pb collisions at root(nn)-n-s = 5.02 tev is measured via the contribution of w-boson decays to the inclusive pt-differential muon yield reconstructed with the alice muon spectrometer at forward (2.03 &lt; ems &lt; 3.53) and backward (-4.46 &lt; y(cms)(mu) &lt; 2.96) rapidity. this paper reports the production cross section of muons from w-boson decays for p(t)(mu) &gt; 10 gev/c and the yields normalised to the average number of binary nucleon-nucleon collisions as a function of the event activity.
inclusive j/psi production in pp, p-pb and pb-pb collisions at forward rapidity with alice at the lhc. the alice collaboration has measured the inclusive j/psi, production at forward rapidity in pp, p-pb and pb-pb collisions at the lhc. the pp measurements are crucial for a deeper understanding of the physics involving hadroproduction processes and provide the baseline for p-pb and pb-pb measurements. the comparison of the results on the nuclear modification factor in p-pb collisions as a function of rapidity or transverse momentum with theoretical predictions shows a fair agreement with a shadowing based model and energy loss models with or without a shadowing contribution. the nuclear modification factor in pb-pb as a function of collision centrality or transverse momentum shows a weaker suppression than at lower energies and can be described by statistical hadronization or transport models, suggesting a contribution to the j/psi, production due to the (re)combination of charm quarks, especially at low transverse momentum. the extrapolation to pb-pb collisions of the cold nuclear matter effects evaluated in p-pb collisions supports the (re)combination mechanism interpretation.
transverse momentum dependence of d-meson production in pb-pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev. the production of prompt charmed mesons d$^0$, d$^+$ and d$^{*+}$, and their antiparticles, was measured with the alice detector in pb-pb collisions at the centre-of-mass energy per nucleon pair, $\sqrt{s_{\rm nn}}$, of 2.76 tev. the production yields for rapidity $|y|&lt;0.5$ are presented as a function of transverse momentum, $p_{\rm t}$, in the interval 1-36 gev/$c$ for the centrality class 0-10% and in the interval 1-16 gev/$c$ for the centrality class 30-50%. the nuclear modification factor $r_{\rm aa}$ was computed using a proton-proton reference at $\sqrt{s} = 2.76$ tev, based on measurements at $\sqrt{s} = 7$ tev and on theoretical calculations. a maximum suppression by a factor of 5-6 with respect to binary-scaled pp yields is observed for the most central collisions at $p_{\rm t}$ of about 10 gev/$c$. a suppression by a factor of about 2-3 persists at the highest $p_{\rm t}$ covered by the measurements. at low $p_{\rm t}$ (1-3 gev/$c$), the $r_{\rm aa}$ has large uncertainties that span the range 0.35 (factor of about 3 suppression) to 1 (no suppression). in all $p_{\rm t}$ intervals, the $r_{\rm aa}$ is larger in the 30-50% centrality class compared to central collisions. the d-meson $r_{\rm aa}$ is also compared with that of charged pions and, at large $p_{\rm t}$, charged hadrons, and with model calculations.
transverse energy production and charged-particle multiplicity at midrapidity in various systems from $\sqrt{s_{nn}}=7.7$ to 200 gev. measurements of midrapidity charged particle multiplicity distributions, $dn_{\rm ch}/d\eta$, and midrapidity transverse-energy distributions, $de_t/d\eta$, are presented for a variety of collision systems and energies. included are distributions for au$+$au collisions at $\sqrt{s_{_{nn}}}=200$, 130, 62.4, 39, 27, 19.6, 14.5, and 7.7 gev, cu$+$cu collisions at $\sqrt{s_{_{nn}}}=200$ and 62.4 gev, cu$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev, u$+$u collisions at $\sqrt{s_{_{nn}}}=193$ gev, $d$$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev, $^{3}$he$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev, and $p$$+$$p$ collisions at $\sqrt{s_{_{nn}}}=200$ gev. centrality-dependent distributions at midrapidity are presented in terms of the number of nucleon participants, $n_{\rm part}$, and the number of constituent quark participants, $n_{q{\rm p}}$. for all $a$$+$$a$ collisions down to $\sqrt{s_{_{nn}}}=7.7$ gev, it is observed that the midrapidity data are better described by scaling with $n_{q{\rm p}}$ than scaling with $n_{\rm part}$. also presented are estimates of the bjorken energy density, $\varepsilon_{\rm bj}$, and the ratio of $de_t/d\eta$ to $dn_{\rm ch}/d\eta$, the latter of which is seen to be constant as a function of centrality for all systems.
mesurement of am at ultra-trace concentrations in environnemental samples. null
separation, detection and quantification of the actinides in an environmental matrix. null
competitive sorption and selective sequence of cu(ii) and ni(ii) on montmorillonite: batch, modeling, epr and xas studies. heavy metal ions that leach from various industrial and agricultural processes are simultaneously present in the contaminated soil and water systems. the competitive sorption of these toxic metal ions on the natural soil components and sediments significantly influences their migration, bioavailability and ecotoxicity in the geochemical environment. in this study, the competitive sorption and selectivity order of cu(ii) and ni(ii) on montmorillonite are investigated by combining the batch experiments, x-ray diffraction (xrd), electron paramagnetic resonance (epr), surface complexation modeling and x-ray absorption spectroscopy (xas). the batch experimental data show that the coexisting ni(ii) exhibits a negligible influence on the sorption behavior of cu(ii), whereas the coexisting cu(ii) reduces the ni(ii) sorption percentage and changes the shape of the ni(ii) sorption isotherm. the sorption species of cu(ii) and ni(ii) on montmorillonite over the acidic and near-neutral ph range are well simulated by the surface complexation modeling. however, this model cannot identify the occurrence of surface nucleation and the co-precipitation processes at a highly alkaline ph. based on the results of the epr and xas analyses, the microstructures of cu(ii) on montmorillonite are identified as the hydrated free cu(ii) ions at ph 5.0, inner-sphere surface complexes at ph 6.0 and the surface dimers/cu(oh)(2)(s) precipitate at ph 8.0 in the single-solute and the binary-solute systems. for the ni(ii) sorption in the single-solute system, the formed microstructure varies from the hydrated free ni(ii) ions at the ph values of 5.0 and 6.0 to the inner-sphere surface complexes at ph 8.0. for the ni(ii) sorption in the binary-solute system, the coexisting cu(ii) induces the formation of the inner-sphere complexes at ph 6.0. in contrast, ni(ii) is adsorbed on montmorillonite via the formation of ni phyllosilicate co-precipitate/alpha-ni(oh)(2)(s) precipitate at ph 8.0. the selective sequence of cu(ii) &gt; ni(ii) for binding on montmorillonite can be ascribed to the differences in the metal properties and the compatibility between the configurations of the montmorillonite binding sites and those of the cu(ii)o-6/ni(ii)o-6 polyhedra. the derived findings in this study could provide significant information for the evaluation of the competitive sorption behaviors at solid/water interfaces and the fate of the coexisting heavy metal ions in multicomponent environmental systems. (c) 2015 elsevier ltd. all rights reserved.
observables in ultrarelativistic heavy-ion collisions from two different transport approaches for the same initial conditions. for nucleus-nucleus collisions at energies currently available at the bnl relativistic heavy ion collider (rhic), we calculate observables in two different transport approaches, i.e., the n-body molecular dynamical model ``relativistic quantum molecular dynamics for strongly interacting matter with phase transition or crossover'' (rsp) and the two-body parton hadron string dynamics (phsd), starting out from the same distribution in the initial energy density at the quark gluon plasma (qgp) formation time. the rsp dynamics is based on the nambu-jona-lasinio (njl) lagrangian, whereas in phsd the partons are described by the dynamical quasiparticle model (dqpm). despite the very different description of the parton properties and their interactions and of the hadronization in both approaches, the final transverse momentum distributions of pions turn out to be quite similar, which is less visible for the strange mesons owing to the large njl cross sections involved. our findings can be attributed, in part, to a partial thermalization of the quark degrees of freedom in central au + au collisions for both approaches. the rapidity distribution of mesons shows a stronger sensitivity to the nature of the degrees of freedom involved and to their interaction strength in the qgp.
nmr and computational molecular modeling studies of mineral surfaces and interlayer galleries: a review. this paper reviews experimental nuclear magnetic resonance (nmr) and computational molecular dynamics (md) investigations of the structural and dynamical behavior of cations, anions, h2o, and co2 on the surfaces and in the interlayer galleries of layer-structure minerals and their composites with polymers and natural organic matter (nom). the interaction among mineral surfaces, charge-balancing cations or anions, h2o, co2, and nom are dominated by coulombic, h-bond, and van der waals interactions leading to statically and dynamically disordered systems and molecular-scale processes with characteristic room-temperature frequencies varying from at least as small as 10(2) to &gt;10(12) hz. nmr spectroscopy provides local structural information about such systems through the chemical shift and quadrupolar interactions and dynamical information at frequencies from the sub-kilohertz to gigahertz ranges through the t-1 and t-2 relaxation rates and line shape analysis. it is often difficult to associate a specific structure or dynamical process to a given nmr observation, however, and computational molecular modeling is often effective in providing a much more detailed picture in this regard. the examples discussed here illustrate these capabilities of combining experimental nmr and computational modeling in mineralogically and geochemically important systems, including clay minerals and layered double hydroxides.
wetland pollutant dynamics and control. 
accelerator-based production of mo-99: a comparison between the mo-100(p,x) and zr-96(alpha,n) reactions. innovative accelerator-based production routes for mo-99 (and tc-99m) have been studied, comparing the mo-100(p,x)mo-99,tc-99m and zr-96(alpha,n)mo-99 reactions, for which a new set of measurement has been made. theoretical and experimental cross sections have been analysed and used to calculate mo-99 production yields and specific activity (sa), considering fully enriched and commercially available target materials. results show that the low sa resulting from the p-based route forces the use of alternative generator systems, while the alpha-based reaction provides very high sa mo-99 but much lower yield. benefits and drawbacks of direct tc-99m production via the mo-100(p,2p) reaction are also discussed.
identification of the manipulator stiffness model parameters in industrial environment. the paper addresses a problem of robotic manipulator calibration in real industrial environment. the main contributions are in the area of the elastostatic parameter identification. in contrast to other works the considered approach takes into account the elastic properties of both links and joints. particular attention is paid to the practical identifiability of the model parameters, which completely differs from the theoretical one that relies on the rank of the observation matrix only, without taking into account essential differences in the model parameter magnitudes and the measurement noise impact. this problem is relatively new in robotics and essentially differs from that arising in geometrical calibration. to solve the problem, physical algebraic and statistical model reduction methods are proposed. they are based on the stiffness matrix sparseness taking into account the physical properties of the manipulator elements, structure of the observation matrix and also on the heuristic selection of the practically non-identifiable parameters that employ numerical analyses of the parameter estimates. the advantages of the developed approach are illustrated by an application example that deals with the elastostatic calibration of an industrial robot in a real industrial environment. (c) 2015 elsevier ltd. all rights reserved.
underwater navigation based on passive electric sense: new perspectives for underwater docking. in underwater robotics, several homing and docking techniques are currently being investigated. they aim to facilitate the recovery of underwater vehicles, as well as their connection to underwater stations for battery charging and data exchange. developing reliable underwater docking strategies is a critical issue especially in murky water and/or in confined and cluttered environments. commonly used underwater sensors such as sonar and camera can fail under these conditions. we show how a bio-inspired sensor could be used to help guide an underwater robot during a docking phase. the sensor is inspired by the passive electro-location ability of electric fish. exploiting the electric interactions and the morphology of the vehicle, a sensor-based reactive control law is proposed. it allows the guidance of the robot toward the docking station by following an exogenous electric field generated by a set of electrodes fixed to the environment. this is achieved while avoiding insulating perturbative objects. this control strategy is theoretically analysed and validated with experiments carried out on a setup dedicated to the study of electric sense. though promising, these results are but a first step towards the implementation of an approach to docking in more realistic conditions, such as in turbid salt water or in the presence of conductive perturbative objects.
geometric calibration of industrial robots using enhanced partial pose measurements and design of experiments. the paper deals with geometric calibration of industrial robots and focuses on reduction of the measurement noise impact by means of proper selection of the manipulator configurations in calibration experiments. particular attention is paid to the enhancement of measurement and optimization techniques employed in geometric parameter identification. the developed method implements a complete and irreducible geometric model for serial manipulator, which takes into account different sources of errors (link lengths, joint offsets, etc). in contrast to other works, a new industry-oriented performance measure is proposed for optimal measurement configuration selection that improves the existing techniques via using the direct measurement data only. this new approach is aimed at finding the calibration configurations that ensure the best robot positioning accuracy after geometric error compensation. experimental study of heavy industrial robot kuka kr-270 illustrates the benefits of the developed pose strategy technique and the corresponding accuracy improvement. (c) 2015 elsevier ltd. all rights reserved.
new energy value chain through pyrolysis of hospital plastic waste. in this paper, the evolution in thermochemical behaviours of hospital plastic wastes and changes in chemical composition and characteristics of pyrolysis liquid products have been investigated by using different fixed bed reactor scales. the main objective is to identify the critical technical parameters enabling thermochemical process adaptation in function of raw materials chemical structure, with the aim of maximising the yield of condensable fraction and optimising its energetic properties related to internal combustion engines. it is a step-by-step procedure using three reactor capacity levels, which allows various aspects approach of thermochemical process development from the evaluation of global reaction kinetic parameters to the measurement of physicochemical properties of the final pyrolysis products. in order to reduce the gas and solid fractions with corresponding increasing of condensable products, the transposition of thermal and kinetic information provided by thermogravimetric analysis (tga) to larger reactors is used to control of process parameters. in this experimental work the mass of samples increases from 0.05 g in the thermogravimetric analyser to 600 g in the bench scale reactor. gas-chromatography techniques have been used to identify the chemical composition of gases (gc/tcd) and liquids (gc/fid-ms). it was established that changing the reactor scale does not result in significant differences in pyrolysis product distribution, neither in gas composition. on the other hand, the aspect and the quality of condensable fraction display a high variability. also, the energy contained in the final valuable pyrolysis product was compared with the energy demand during the thermochemical transformation in order to evaluate the energy efficiency of the process. (c) 2015 elsevier ltd. all rights reserved.
effect of climate, wastewater composition, loading rates, system age and design on performances of french vertical flow constructed wetlands: a survey based on 169 full scale systems. the main objective of our work was to study the efficiency of the 2 stages french vertical flow constructed wetland system and its compact version by compiling data of 169 full scale systems in operation for up to 12 years. design parameters and treatment performances, mostly based on 24 h composite samples performed by independent local authorities, have been compared to see how climate, wastewater composition, loading rates, system age and design were influencing the treatment performances. a bit more than 97% of the samples analysed at outlet of the plants were fulfilling the most common french discard limits (in mg/l 125 cod, 25 bod). removal efficiencies were not affected by factors including hydraulic and organic loads (until 60 cm/d), defect of maintenance and temperature within the ranges observed. age of treatment plants had an effect only during start-up, but after 0.5-2 years of operation, performance was constant. similarly, the feedback from 5 years of data obtained with ``compact vfcws'' showed that this system met french standards and outperformed the first stage of ``classical vfcws''. this survey of 169 full scale systems represents a confirmation of the good performance and the robustness of the french vfcws. (c) 2014 elsevier b.v. all rights reserved.
on the trail of a new state of matter. following the results collected in the past 30 years within the heavy-ion scientific program, the progress achieved so far at the cern lhc during the first data taking period is reviewed. (c) 2015 academie des sciences. published by elsevier masson sas. all rights reserved.
static stability of manipulator configuration: influence of the external loading. the paper deals with the manipulator static stability analysis under the influence of the external loading. it proposes a new technique that allows evaluating both static stability of the end-effector location and static stability of the kinematic chain configuration. this approach extends the classical notion of the manipulator static stability that is completely defined by the properties of the cartesian stiffness matrix. the advantages of the new approach are illustrated by examples that deal with parallel manipulators and their serial chains. the analysis showed that the manipulator workspace may include elastostatic singularities where the chain configurations become unstable under the influence of external loading. (c) 2014 elsevier masson sas. all rights reserved.
thick wood particle pyrolysis in an oxidative atmosphere. oxidative pyrolysis of pine wood particles was analysed thermo-gravimetrically. the effects of the concentration of oxygen in the surrounding gas and of particle size were investigated. three different oxygen concentrations (0%, 10% and 20% v/v) and three different sized cylindrical pine wood samples (4 mm, 8 mm and 12 mm in diameter and 15 mm long) were tested. two types of macro-tg apparatuses were used; the first was non isothermal and was used at a heating rate of 20 degrees c/min, and the second was isothermal used at two temperatures, 400 degrees c and 600 degrees c. in the low heating rate non isothermal apparatus, results showed that oxygen had a strong influence on pyrolysis behaviour, but particle size did not. in the high heating rate isothermal apparatus, particle size had a significant influence on conversion: transfer phenomena limit oxidative pyrolysis. (c) 2015 elsevier ltd. all rights reserved.
odum-tennenbaum-brown calculus vs emergy and co-emergy analysis. in a recent paper tennenbaum introduced a new method of calculating emergy that requires only ordinary (i.e. linear) algebra. we prove on a simple example with one feedback and one split that ordinary algebra as developed by tennenbaum in his paper is not sufficient to tackle the problem of emergy analysis. in particular, we point out the problem of enumerating pathways which are relevant for emergy analysis, i.e. which avoid the double counting problem of feedbacks. hence, the emergy co-emergy analysis cannot work at least for energy system diagram with splits and feedbacks. le corre and truffet have already proved that the emergy path-finding problem deals with idempotent (thus non-linear) algebra. (c) 2015 elsevier b.v. all rights reserved.
improvement of thermal stability of maghemite nanoparticles coated with oleic acid and oleylamine molecules: investigations under laser irradiation. we investigated the influence of the coating of maghemite nanoparticles (nps) with oleic acid and oleylamine molecules on the thermal stability of maghemite and on the gamma -&gt; alpha-fe2o3 phase transformation. the uncoated maghemite nps were synthesized by coprecipitation and the coated nps by thermal decomposition of organometallic precursors. the morphology and size of the coated nps were characterized by transmission electron microscopy and magnetic and structural properties by fe-57 mossbauer and raman spectroscopies. the phase stability of coated maghemite nps was examined under in situ laser irradiation by raman spectroscopy. the results indicate that coated gamma-fe2o3 nps are thermally more stable than the uncoated nps: the phase transformation of maghemite into hematite was observed at 15 mw for uncoated nps of 4 nm, whereas it occurs at 120 mw for the coated nps of similar size. the analysis of the raman baseline profile reveals clearly that the surface coating of maghemite nps results both in reducing the number of surface defects of nanoparticles and in delaying this phase transition.
cyclotron production of high purity sc-44m,sc-44 with deuterons from (caco3)-ca-44 targets. introduction: due to its longer half-life, sc-44 (t-1/2 = 3.97 h) as a positron emitter can be an interesting alternative to ga-68 (t-1/2 = 67.71 min). it has been already proposed as a pet radionuclide for scouting bone disease and is already available as a ti-44/sc-44 generator. sc-44 has an isomeric state, (sc)-s-44m (t-1/2 = 58.6 h), which can be co-produced with sc-44 and that has been proved to be considered as an in-vivo pet generator sc-44m/sc-44. this work presents the production route of sc-44m/sc-44 generator from ca-44(d,2n), its extraction/purification process and the evaluation of its performances. methods: irradiation was performed in a low activity target station using a deuteron beam of 16 mev, which favors the number of (sc)-s-44m atoms produced simultaneously to sc-44. typical irradiation conditions were 60 min at 02 mu a producing 44 mbq of sc-44 with a (sc)-s-44/(sc)-s-44m activity ratio of 50 at end of irradiation. separations of the radionuclides were performed by means of cation exchange chromatography using a dga (r) resin (triskem). then, the developed process was applied with bigger targets, and could be used for preclinical studies. results: the extraction/purification process leads to a radionucleidic purity higher than 99.99% (sc-43, sc-46, sc-48 &lt; dl). sc-44m/sc-44 labeling towards dota moiety was performed in order to get an evaluation of the specific activities that could be reached with regard to all metallic impurities from the resulting source. reaction parameters of radiolabeling were optimized, reaching yields over 95%, and leading to a specific activity of about 10-20 mbq/nmol for dota. a recycling process for the enriched ca-44 target was developed and optimized. conclusion: the quality of the final batch with regard to radionucleidic purity, specific activity and metal impurities allowed a right away use for further radiopharmaceutical evaluation. this radionucleidic pair of (sc)-s-44m/(sc)-s-44 offers a quite interesting pet radionuclide for being further evaluated as an in-vivo generator. (c) 2015 elsevier inc. all rights reserved.
electronic structures of the xf&lt;sub&gt;3&lt;/sub&gt; (x = cl, br, i, at) fluorides and topology of their potential energy surfaces. null
electronic structures and geometries of the xf&lt;sub&gt;3&lt;/sub&gt; (x = cl, br, i, at) fluorides. the potential energy surfaces of the group 17 xf&lt;sub&gt;3&lt;/sub&gt; (x = cl, br, i, at) fluorides have been investigated for the first time with multiconfigurational wave function theory approaches. in agreement with experiment, bent t-shaped &lt;i&gt;c&lt;/i&gt;&lt;sub&gt;2v&lt;/sub&gt; structures are computed for clf&lt;sub&gt;3&lt;/sub&gt;, brf&lt;sub&gt;3&lt;/sub&gt; and if&lt;sub&gt;3&lt;/sub&gt;, while we predict that an average &lt;i&gt;d&lt;/i&gt;&lt;sub&gt;3h&lt;/sub&gt; structure would be experimentally observed for atf&lt;sub&gt;3&lt;/sub&gt;. electron correlation and scalar relativistic effects strongly reduce the energy difference between the &lt;i&gt;d&lt;/i&gt;&lt;sub&gt;3h&lt;/sub&gt; geometry and the &lt;i&gt;c&lt;/i&gt;&lt;sub&gt;2v&lt;/sub&gt; one, along the xf&lt;sub&gt;3&lt;/sub&gt; series, and in the x = at case, spin-orbit coupling also slightly reduces this energy difference. atf&lt;sub&gt;3&lt;/sub&gt; is a borderline system where the &lt;i&gt;d&lt;/i&gt;&lt;sub&gt;3h&lt;/sub&gt; structure becomes a minimum, i.e., the &lt;i&gt;pseudo&lt;/i&gt; jahn-teller effect is inhibited since electron correlation and scalar-relativistic effects create small energy barriers leading to the global &lt;i&gt;c&lt;/i&gt;&lt;sub&gt;2v&lt;/sub&gt; minima, although both types of effects interfere.
global optimization based on contractor programming. in this paper, we will present a general pattern based on contractor programmingfor designing a global optimization solver. this approach allows to solve problems with awide variety of constraints. the complexity and the performance of the algorithm rely on theconstruction of contractors which characterize the feasible region.
antenna design and distribution for a lofar super station in nançay. the nançay radio astronomy observatory and associated laboratories are developing the concept of a " super station " for extending the lofar station now installed and operational in nançay. the lofar super station (lss) will increase the number of high sensitivity long baselines, provide short baselines and an alternate core, and be a large standalone instrument. it will operate in the low frequency band of lofar (30–80 mhz) and extend this range to lower frequencies. three key developments for the lss are described here: (i) the design of a specific antenna, and the distribution of such antennas (ii) at small-scale (analog-phased mini array) and (iii) at large-scale (the whole lss).
state-of-the-art of low frequency radio astronomy, relevant antenna systems and international cooperation in ukraine. the low frequency radio astronomy (decameter-meter range, frequencies of 10-300 mhz) currently demonstrates rapid progress all over the world. new generations of large antennas-lofar, lwa, mwa and others – have been created in many countries. at the same time ukrainian radio astronomical systems utr-2 and uran still remain the largest and most informative ones at the lowest frequency range available for the ground-based radio astronomy (below 33 mhz), especially after their radical modernization during the most recent years. a great number of top priority results have been obtained on the basis of these radio telescopes. the results prove a high significance of the low frequency radio astronomy for astrophysics. substantial part of these results have been obtained in the course of many year cooperation between ukraine on one side and france, austria, germany and other countries on the other. creation of new low frequency instruments gurt (ukraine) and lss/nenufar (france) for the wide frequency range of 10-80 mhz opens up new possibilities for research and fruitful cooperation.
mox fuel enrichment prediction in pwr using polynomial models. a dynamic fuel cycle simulation code models all the ingoing and outgoing material flow in all facilities ofa nuclear reactor’s fleet as well as their evolutions through the different nuclear processes (irradiation,decay, chemical separation, etc.). one of the main difficulties encountered when performing such calculationcomes from the fuel fabrication of reprocessed fuel such as mox fuel. indeed, the mox fuel isfabricated using a plutonium base completed with depleted uranium. the amount of plutonium in thefuel will directly impact the neutron multiplication factor and its evolution through irradiation, so theduration to keep the fuel in the reactor. the present paper presents the study of different pwr mox fuelfabrication polynomial models. those models will allow the prediction of the amount of plutoniumneeded to reach a wanted burnup from the plutonium isotopics. after defining a method to generate atraining sample, that is to say the set of fuel depletion calculations used to fit the polynomial models, thispapers will discuss their performances on 3 different applications. on the two tested models, one linearand one quadratic, while the linear model fail to properly describe the amount of plutonium needed, thefuel fabricated, using the quadratic one, reaches the wanted burnup with a discrepancy below 2%.
production of medical isotopes from a thorium target irradiated by light charged particles up to 70 mev. the irradiation of a thorium target by light charged particles (protons and deuterons) leads to the production of several isotopes of medical interest. direct nuclear reaction allows the production of protactinium-230 which decays to uranium-230 the mother nucleus of thorium-226, a promising isotope for alpha radionuclide therapy. the fission of thorium-232 produces fragments of interest like molybdenum-99, iodine-131 and cadmium-115g. we focus our study on the production of these isotopes, performing new cross section measurements and calculating production yields. our new sets of data are compared with the literature and the last version of the talys code.
is there an interest to use deuteron beams to produce non-conventional radionuclides?. with the recent interest on the theranostic approach, there has been a renewed interest for alternative radionuclides in nuclear medicine. they can be produced using common production routes, i.e., using protons accelerated by biomedical cyclotrons or neutrons produced in research reactors. however, in some cases, it can be more valuable to use deuterons as projectiles. in the case of cu-64, smaller quantities of the expensive target material, ni-64, are used with deuterons as compared with protons for the same produced activity. for the sc-44m/sc-44g generator, deuterons afford a higher sc-44m production yield than with protons. finally, in the case of re-186g, deuterons lead to a production yield five times higher than protons. these three examples show that it is of interest to consider not only protons or neutrons but also deuterons to produce alternative radionuclides.
cross section measurements of deuteron induced nuclear reactions on natural titanium up to 34 mev. experimental cross sections for deuteron induced nuclear reactions on natural titanium were measured, using the stacked-foil technique and gamma spectrometry, up to 34mev with beams provided by the arronax cyclotron. the experimental cross section values were monitored using the (nat)ti(d,x)(48)v reaction, recommended by the iaea. the excitation functions for (nat)ti(d,x)(44m,46,47,48)sc are presented and compared with the existing ones and with the talys 1.6 code calculations using default models. our experimental values are in good agreement with data found in the literature. talys 1.6 is not able to give a good estimation of the production cross sections investigated in this work. these production cross sections of scandium isotopes fit with the new coordinated research project (crp) launched by the international atomic energy agency (iaea) to expand the database of monitor reactions.
production of scandium-44m and scandium-44g with deuterons on calcium-44: cross section measurements and production yield calculations. among the large number of radionuclides of medical interest, sc-44 is promising for pet imaging. either the ground-state sc-44g or the metastable-state sc-44m can be used for such applications, depending on the molecule used as vector. this study compares the production rates of both sc-44 states, when protons or deuterons are used as projectiles on an enriched calcium-44 target. this work presents the first set of data for the deuteron route. the results are compared with the talys code. the thick-target production yields of sc-44m and sc-44g are calculated and compared with those for the proton route for three different scenarios: the production of sc-44g for conventional pet imaging, its production for the new 3 γ imaging technique developed at the subatech laboratory and the production of a sc-44m/sc-44g in vivo generator for antibody labelling.
collective intelligence within the framework of continuing professional development: acopé, the example of a community of practice. cooperating with a view to furthering professional development, taking decisions and functioning interoperatively within a community implies working with participants committed to fostering strong relationships, reflective practice, and reaching shared understanding.the collaboration between the acopé educational advisors takes place within this context itself part of the wider framework of a higher education environment with intersectoral, interprofessional, interdisciplinary and inter-regional issues.the acopé educational advisors undertake professional development activities thus creating synergies as a basis for the emergence and the enhancement of collective intelligence and competence.after having explicited these two concepts, the educational advisors present an analysis of determining factors specific to their organization which facilitate situations of collective production. they also put forward characteristic situational variables which could potentially hinder the expression and development of cooperative actions. their analysis leads to broader reflections on the professional development issues generated within the association.
correlations between molecular descriptors from various volatile organic compounds and photocatalytic oxidation kinetic constants. the photocatalytic oxidation of seven typical indoor volatile organic compounds (vocs) are experimentally investigated using novel nanocrystalline tio2 dip-coated catalysts. not only the role of hydrophilicity of the reactants but also other physico-chemical properties and molecular descriptors are studied and related to kinetic and equilibrium constants. the main objective of this work consists in establishing simple relationships that will be useful to deepen the understanding of gas phase heterogeneous photocatalytic mechanisms and for the prediction of degradation rates of these vocs using an indoor air treatment process.
exploiting renewable sources: when green sla becomes a possible reality in cloud computing. while the proliferation of cloud services have greatly impacted our society, how green are these services is yet to be answered. although, demand escalation for green services has grown due to societal awareness, the approaches to provide green services and establish green slas remain oblivious for cloud or infrastructure providers. the main challenge for cloud provider is to manage green slas with their customers while satisfying their business objectives, such as maximizing profits by lowering expenditure for green energy. since, green sla needs to be proposed based on the presence of green energy, the intermittent nature of renewable sources makes it difficult to be achieved. in response, this paper presents a scheme for green energy management in the presence of explicit and implicit integration of renewable energy in data center. more specifically we propose three contributions: i) we introduce the concept of virtualization of green energy to address the uncertainty of green energy availability, ii) we extend the cloud service level agreement (csla) language to support green sla by introducing two new threshold parameters and iii) we introduce greensla algorithm which leverages the concept of virtualization of green energy to provide per interval specific green sla. experiments were conducted with real workload profile from planetlab and server power model from specpower to demonstrate that, green sla can be successfully established and satisfied without incurring higher cost.
measurements of elliptic and triangular flow in high-multiplicity $^{3}$he$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev. we present the first measurement of elliptic ($v_2$) and triangular ($v_3$) flow in high-multiplicity $^{3}$he$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev. two-particle correlations, where the particles have a large separation in pseudorapidity, are compared in $^{3}$he$+$au and in $p$$+$$p$ collisions and indicate that collective effects dominate the second and third fourier components for the correlations observed in the $^{3}$he$+$au system. the collective behavior is quantified in terms of elliptic $v_2$ and triangular $v_3$ anisotropy coefficients measured with respect to their corresponding event planes. the $v_2$ values are comparable to those previously measured in $d$$+$au collisions at the same nucleon-nucleon center-of-mass energy. comparison with various theoretical predictions are made, including to models where the hot spots created by the impact of the three $^{3}$he nucleons on the au nucleus expand hydrodynamically to generate the triangular flow. the agreement of these models with data may indicate the formation of low-viscosity quark-gluon plasma even in these small collision systems.
exclusion of leptophilic dark matter models using xenon100 electronic recoil data. null
the giant radio array for neutrino detection. high-energy neutrino astronomy will probe the working of the most violent phenomena in the universe. the giant radio array for neutrino detection (grand) project consists of an array of $\sim10^5$ radio antennas deployed over $\sim$200000km$^2$ in a mountainous site. it aims at detecting high-energy neutrinos via the measurement of air showers induced by the decay in the atmosphere of $\tau$ leptons produced by the interaction of the cosmic neutrinos under the earth surface. our objective with grand is to reach a neutrino sensitivity of $3\times10^{-11}e^{-2}$gev$^{-1}$cm$^{-2}$s$^{-1}$sr$^{-1}$ above $3 \times10^{16}$ev. this sensitivity ensures the detection of cosmogenic neutrinos in the most pessimistic source models, and about 100 events per year are expected for the standard models. grand would also probe the neutrino signals produced at the potential sources of uhecrs. we show how our preliminary design should enable us to reach our sensitivity goals, and present the experimental characteristics. we assess the possibility to adapt grand to other astrophysical radio measurements. we discuss in this token the technological options for the detector and the steps to be taken to achieve the grand project.
search for event rate modulation in xenon100 electronic recoil data. we have searched for periodic variations of the electronic recoil event rate in the (2-6) kev energy range recorded between february 2011 and march 2012 with the xenon100 detector, adding up to 224.6 live days in total. following a detailed study to establish the stability of the detector and its background contributions during this run, we performed an un-binned profile likelihood analysis to identify any periodicity up to 500 days. we find a global significance of less than 1 sigma for all periods suggesting no statistically significant modulation in the data. while the local significance for an annual modulation is 2.8 sigma, the analysis of a multiple-scatter control sample and the phase of the modulation disfavor a dark matter interpretation. the dama/libra annual modulation interpreted as a dark matter signature with axial-vector coupling of wimps to electrons is excluded at 4.8 sigma.
energy estimation of cosmic rays with the engineering radio array of the pierre auger observatory. the auger engineering radio array (aera) is part of the pierre auger observatory and is used to detect the radio emission of cosmic-ray air showers. these observations are compared to the data of the surface detector stations of the observatory, which provide well-calibrated information on the cosmic-ray energies and arrival directions. the response of the radio stations in the 30 to 80 mhz regime has been thoroughly calibrated to enable the reconstruction of the incoming electric field. for the latter, the energy density is determined from the radio pulses at each observer position and is interpolated using a two dimensional function that takes into account signal asymmetries due to interference between the geomagnetic and charge excess emission components. the spatial integral over the signal distribution gives a direct measurement of the energy transferred from the primary cosmic ray into radio emission in the aera frequency range. we measure 15.8 mev of radiation energy for a 1 eev air shower arriving perpendicularly to the geomagnetic field. this radiation energy -- corrected for geometrical effects -- is used as a cosmic-ray energy estimator. performing an absolute energy calibration against the surface-detector information, we observe that this radio-energy estimator scales quadratically with the cosmic-ray energy as expected for coherent emission. we find an energy resolution of the radio reconstruction of 22% for the data set and 17% for a high-quality subset containing only events with at least five radio stations with signal.
coherent $\psi$(2s) photo-production in ultra-peripheral pb-pb collisions at $\sqrt{s}_{\rm nn}$ = 2.76 tev. we have has performed the first measurement of the coherent $\psi$(2s) photo-production cross section in ultra-peripheral pb-pb collisions at the lhc. this charmonium excited state is reconstructed via the $\psi$(2s) $\rightarrow l^{+}l^{-}$ and $\psi$(2s) $\rightarrow$ j/$\psi \pi^{+}\pi^{-}$ decays, where the j/$\psi$ decays into two leptons. the analysis is based on an event sample corresponding to an integrated luminosity of about 22 $\mu\rm{b}^{-1}$. the cross section for coherent $\psi$(2s) production in the rapidity interval $-0.9&lt;y&lt;0.9$ is $\mathrm{d}\sigma_{\psi{\rm(2s)}}^{\rm coh}/\mathrm{d}y =0.83\pm 0.19\big(\mathrm{\rm{stat}+{\rm syst}}\big)$ mb. the $\psi$(2s) to j/$\psi$ coherent cross section ratio is $0.34^{+0.08}_{-0.07}(\rm{stat}+{\rm syst})$. the obtained results are compared to predictions from theoretical models.
a modelling pearl with sortedness constraints. some constraint programming solvers and constraint modelling languages feature the sort(l, p , s ) constraint, which holds if s is a nondecreasing rearrangement of the list l, the permutation being made explicit by the optional list p. however, such sortedness constraints do not seem to be used much in practice. we argue that reasons for this neglect are that it is impossible to require the underlying sort to be stable, so that sort cannot be guaranteed to be a total-function constraint, and that l cannot contain tuples of variables, some of which form the key for the sort. to overcome these limitations, we introduce the stablekeysort constraint, decompose it using existing constraints, and propose a propagator. this new constraint enables a powerful modelling idiom, which we illustrate by elegant and scalable models of two problems that are otherwise hard to encode as constraint programs.
using finite transducers for describing and synthesising structural time-series constraints. we describe a large family of constraints for structural time series by means of function composition. these constraints are on aggregations of features of patterns that occur in a time series, such as the number of its peaks, or the range of its steepest ascent. the patterns and features are usually linked to physical properties of the time series generator, which are important to capture in a constraint model of the system, i.e. a conjunction of constraints that produces similar time series. we formalise the patterns using finite transducers, whose output alphabet corresponds to semantic values that precisely describe the steps for identifying the occurrences of a pattern. based on that description, we automatically synthesise automata with accumulators, as well as constraint checkers. the description scheme not only unifies the structure of the existing 30 time-series constraints in the global constraint catalogue, but also leads to over 600 new constraints, with more than 100,000 lines of synthesised code.
open scope: a pragmatic javascript pattern for modular instrumentation. we report on our experience instrumenting narcissus, a javascript interpreter written in javascript, to allow the dynamic deployment of dynamic program analyses. instrumenting an interpreter is a cross-cutting change that can affect many parts of the interpreter source code. we propose a simple open scope pattern that minimizes the changes to the interpreter, while allowing us to implement program analyses in their own files, and to compose them dynamically. we apply our pattern to narcissus using standard javascript features, and find that the gain in extensibility offsets a small loss in performance.
towards modular instrumentation of interpreters in javascript. with an initial motivation based on the security of web applications written in javascript, we consider the instrumentation of an interpreter for a dynamic analysis as a crosscutting concern.  we define the instrumentation problem – an extension to the expression problem with a focus on modifying interpreters.  we then illustrate how we can instrument an interpreter for a simple language using only the bare language features provided by javascript.
aspectizing javascript security. in this position paper we argue that aspects are well-suited to describe and implement a range of strategies to make secure javascript-based applications. to this end, we review major categories of approaches to make client-side applications secure and discuss uses of aspects that exist for some of them. we also propose aspect-based techniques for the categories that have not yet been studied. we give examples of applications where aspects are useful as a general means to flexibly express and implement security policies for javascript.
charged jet cross sections and properties in proton-proton collisions at $\sqrt{s}=7$ tev. the differential charged jet cross sections, jet fragmentation distributions, and jet shapes are measured in minimum bias proton-proton collisions at centre-of-mass energy $\sqrt{s}=7$ tev using the alice detector at the lhc. jets are reconstructed from charged particle momenta in the mid-rapidity region using the sequential recombination $k_{\rm t}$ and anti-$k_{\rm t}$ as well as the siscone jet finding algorithms with several resolution parameters in the range $r=0.2$ to $0.6$. differential jet production cross sections measured with the three jet finders are in agreement in the transverse momentum ($p_{\rm t}$) interval $20&lt;p_{\rm t}^{\rm jet,ch}&lt;100$ gev/$c$. they are also consistent with prior measurements carried out at the lhc by the atlas collaboration. the jet charged particle multiplicity rises monotonically with increasing jet $p_{\rm t}$, in qualitative agreement with prior observations at lower energies. the transverse profiles of leading jets are investigated using radial momentum density distributions as well as distributions of the average radius containing 80% ($\langle r_{\rm 80} \rangle$) of the reconstructed jet $p_{\rm t}$. the fragmentation of leading jets with $r=0.4$ using scaled $p_{\rm t}$ spectra of the jet constituents is studied. the measurements are compared to model calculations from event generators (pythia, phojet, herwig). the measured radial density distributions and $\langle r_{\rm 80} \rangle$ distributions are well described by the pythia model (tune perugia-2011). the fragmentation distributions are better described by herwig.
neutrino physics with juno. the jiangmen underground neutrino observatory (juno), a 20 kton multi-purpose underground liquid scintillator detector, was proposed with the determination of the neutrino mass hierarchy as a primary physics goal. it is also capable of observing neutrinos from terrestrial and extra-terrestrial sources, including supernova burst neutrinos, diffuse supernova neutrino background, geoneutrinos, atmospheric neutrinos, solar neutrinos, as well as exotic searches such as nucleon decays, dark matter, sterile neutrinos, etc. we present the physics motivations and the anticipated performance of the juno detector for various proposed measurements. by detecting reactor antineutrinos from two power plants at 53-km distance, juno will determine the neutrino mass hierarchy at a 3-4 sigma significance with six years of running. the measurement of antineutrino spectrum will also lead to the precise determination of three out of the six oscillation parameters to an accuracy of better than 1\%. neutrino burst from a typical core-collapse supernova at 10 kpc would lead to ~5000 inverse-beta-decay events and ~2000 all-flavor neutrino-proton elastic scattering events in juno. detection of dsnb would provide valuable information on the cosmic star-formation rate and the average core-collapsed neutrino energy spectrum. geo-neutrinos can be detected in juno with a rate of ~400 events per year, significantly improving the statistics of existing geoneutrino samples. the juno detector is sensitive to several exotic searches, e.g. proton decay via the $p\to k^++\bar\nu$ decay channel. the juno detector will provide a unique facility to address many outstanding crucial questions in particle and astrophysics. it holds the great potential for further advancing our quest to understanding the fundamental properties of neutrinos, one of the building blocks of our universe.
centrality dependence of pion freeze-out radii in pb-pb collisions at $\sqrt{\mathbf{s_{nn}}}$=2.76 tev. we report on the measurement of freeze-out radii for pairs of identical-charge pions measured in pb--pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev as a function of collision centrality and the average transverse momentum of the pair $k_{\rm t}$. three-dimensional sizes of the system (femtoscopic radii), as well as direction-averaged one-dimensional radii are extracted. the radii decrease with $k_{\rm t}$, following a power-law behavior. this is qualitatively consistent with expectations from a collectively expanding system, produced in hydrodynamic calculations. the radii also scale linearly with $\left&lt; \mathrm{d}n_{\rm ch}/\mathrm{d}\eta \right&gt;^{1/3}$. this behaviour is compared to world data on femtoscopic radii in heavy-ion collisions. while the dependence is qualitatively similar to results at smaller $\sqrt{s_{\rm nn}}$, a decrease in the $r_{\rm out}/r_{\rm side}$ ratio is seen, which is in qualitative agreement with specific predictions from hydrodynamic models. the results provide further evidence for the production of a collective, strongly coupled system in heavy-ion collisions at the lhc.
baryon study in the njl model. we have studied the phase transition between hadronic matter and quark gluon plasma using the nambu and jona-lasinio model. this model allows a low energy description especially for hadronization process occurring during the cooling.
event shape engineering for inclusive spectra and elliptic flow in pb-pb collisions at $\sqrt{s_\rm{nn}}=2.76$ tev. we report on results obtained with the event shape engineering technique applied to pb-pb collisions at $\sqrt{s_\rm{nn}}=2.76$ tev. by selecting events in the same centrality interval, but with very different average flow, different initial state conditions can be studied. we find the effect of the event-shape selection on the elliptic flow coefficient $v_2$ to be almost independent of transverse momentum $p_\rm{t}$, as expected if this effect is due to fluctuations in the initial geometry of the system. charged hadron, pion, kaon, and proton transverse momentum distributions are found to be harder in events with higher-than-average elliptic flow, indicating an interplay between radial and elliptic flow.
medium-induced gluon radiation: an update. the theory of radiative parton energy loss in a static qcd medium is updated.we show that for an incoming parton of large energy $e$ undergoing a hard,small angle scattering in the medium rest frame (i.e., $p_\perp /e \ll 1$ with$p_\perp$ the final parton transverse momentum), the medium-induced radiativeenergy loss due to soft rescatterings is proportional to $e$. it arises fromgluon radiation with large formation time $t_f \gg l$, i.e., fully coherentover the size $l$ of the medium. in particular, in a physical (light-cone)gauge, the medium-induced radiation spectrum arises from the interferencebetween initial and final state radiation. this result, rigorously derived toall orders in the opacity expansion, invalidates a common belief that anymedium-induced energy loss through a finite size target should be bounded inthe high-energy limit. we also review the case of a parton suddenly annihilated(created) in the hard process, where the bound on energy loss applies. in thiscase the induced gluon radiation reduces to purely initial (final) stateradiation, and the fully coherent part of the radiation cancels out, leavingonly a contribution from $t_f \lesssim l$. as is well-known, in the high energylimit the resulting parton energy loss is independent of $e$ (neglectinglogarithms) and proportional to $l^2$.
why hadronic resonances and particle unstable states are interesting?. null
s = −1 meson-baryon interaction in hot and dense nuclear matter: chiral symmetry, many-body and unitarization for a road to gsi/fair. null
latest results from epos3 on the production of stable and unstable hadrons. null
support vector machine in prediction of building energy demand using pseudo dynamic approach. building's energy consumption prediction is a major concern in the recent years and many efforts have been achieved in order to improve the energy management of buildings. in particular, the prediction of energy consumption in building is essential for the energy operator to build an optimal operating strategy, which could be integrated to building's energy management system (bems). this paper proposes a prediction model for building energy consumption using support vector machine (svm). data-driven model, for instance, svm is very sensitive to the selection of training data. thus the relevant days data selection method based on dynamic time warping is used to train svm model. in addition, to encompass thermal inertia of building, pseudo dynamic model is applied since it takes into account information of transition of energy consumption effects and occupancy profile. relevant days data selection and whole training data model is applied to the case studies of ecole des mines de nantes, france office building. the results showed that support vector machine based on relevant data selection method is able to predict the energy consumption of building with a high accuracy in compare to whole data training. in addition, relevant data selection method is computationally cheaper (around 8 minute training time) in contrast to whole data training (around 31 hour for weekend and 116 hour for working days) and reveals realistic control implementation for online system as well.
centrality dependence of the nuclear modification factor of charged pions, kaons, and protons in pb-pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev. transverse momentum ($p_{\rm{t}}$) spectra of pions, kaons, and protons up to $p_{\rm{t}} = 20$ gev/$c$ have been measured in pb-pb collisions at $\sqrt{s_{\rm nn}} = 2.76$ tev using the alice detector for six different centrality classes covering 0-80%. the proton-to-pion and the kaon-to-pion ratios both show a distinct peak at $p_{\rm{t}} \approx 3$ gev/$c$ in central pb-pb collisions that decreases towards more peripheral collisions. for $p_{\rm{t}} &gt; 10$ gev/$c$, the nuclear modification factor is found to be the same for all three particle species in each centrality interval within systematic uncertainties of 10-20%. this suggests there is no direct interplay between the energy loss in the medium and the particle species composition in the hard core of the quenched jet. for $p_{\rm{t}} &lt; 10$ gev/$c$, the data provide important constraints for models aimed at describing the transition from soft to hard physics.
one-dimensional pion, kaon, and proton femtoscopy in pb-pb collisions at $\sqrt{s_{\rm {nn}}}$ =2.76 tev. the size of the particle emission region in high-energy collisions can be deduced using the femtoscopic correlations of particle pairs at low relative momentum. such correlations arise due to quantum statistics and coulomb and strong final state interactions. in this paper, results are presented from femtoscopic analyses of $\pi^{\pm}\pi^{\pm}$, ${\rm k}^{\pm}{\rm k}^{\pm}$, ${\rm k}^{0}_s{\rm k}^{0}_s$, ${\rm pp}$, and ${\rm \overline{p}}{\rm \overline{p}}$ correlations from pb-pb collisions at $\sqrt{s_{\mathrm {nn}}}=2.76$ tev by the alice experiment at the lhc. one-dimensional radii of the system are extracted from correlation functions in terms of the invariant momentum difference of the pair. the comparison of the measured radii with the predictions from a hydrokinetic model is discussed. the pion and kaon source radii display a monotonic decrease with increasing average pair transverse mass $m_{\rm t}$ which is consistent with hydrodynamic model predictions for central collisions. the kaon and proton source sizes can be reasonably described by approximate $m_{\rm t}$-scaling.
production of light nuclei and anti-nuclei in pp and pb-pb collisions at lhc energies. the production of (anti-)deuteron and (anti-)$^{3}$he nuclei in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev has been studied using the alice detector at the lhc. the spectra exhibit a significant hardening with increasing centrality. combined blast-wave fits of several particles support the interpretation that this behavior is caused by an increase of radial flow. the integrated particle yields are discussed in the context of coalescence and thermal-statistical model expectations. the particle ratios, $^3$he/d and $^3$he/p, in pb-pb collisions are found to be in agreement with a common chemical freeze-out temperature of $t_{\rm chem} \approx 156$ mev. these ratios do not vary with centrality which is in agreement with the thermal-statistical model. in a coalescence approach, it excludes models in which nucleus production is proportional to the particle multiplicity and favors those in which it is proportional to the particle density instead. in addition, the observation of 31 anti-tritons in pb-pb collisions is reported. for comparison, the deuteron spectrum in pp collisions at $\sqrt{s} = 7$ tev is also presented. while the p/$\pi$ ratio is similar in pp and pb-pb collisions, the d/p ratio in pp collisions is found to be lower by a factor of 2.2 than in pb-pb collisions.
$\phi$-meson production at forward rapidity in p-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev and in pp collisions at $\sqrt{s}$ = 2.76 tev. the first measurement of $\phi$-meson production in p-pb collisions at a nucleon-nucleon centre-of-mass energy $\sqrt{s_{\rm nn}}$ = 5.02 tev has been performed with the alice apparatus at the lhc. the $\phi$-mesons have been identified in the dimuon decay channel in the transverse momentum ($p_{\rm t}$) range $1 &lt; p_{\rm t} &lt; 7$ gev/$c$, both in the p-going ($2.03 &lt; y &lt; 3.53$) and the pb-going ($-4.46 &lt; y &lt; -2.96$) directions, where $y$ stands for the rapidity in the nucleon-nucleon centre-of-mass. differential cross sections as a function of transverse momentum and rapidity are presented. the forward-backward asymmetry for $\phi$-meson production is measured for $2.96&lt;|y|&lt;3.53$, resulting in a factor $\sim 0.5$ with no significant $p_{\rm t}$ dependence within the uncertainties. the $p_{\rm t}$ dependence of the $\phi$ nuclear modification factor $r_{\rm ppb}$ exhibits an enhancement up to a factor 1.6 at $p_{\rm t}$ = 3-4 gev/$c$ in the pb-going direction. the $p_{\rm t}$ dependence of the $\phi$-meson cross section in pp collisions at $\sqrt{s}$ = 2.76 tev, which is used to determine a reference for the p-pb results, is also presented here for $1 &lt; p_{\rm t} &lt; 5$ gev/$c$ and $2.5 &lt;y &lt; 4$.
$^{3}_{\lambda}\mathrm h$ and $^{3}_{\bar{\lambda}} \overline{\mathrm h}$ production in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the production of the hypertriton nuclei $^{3}_{\lambda}\mathrm h$ and $^{3}_{\bar{\lambda}} \overline{\mathrm h}$ has been measured for the first time in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev with the alice experiment at lhc energies. the total yield, d$n$/d$y$ $\times \mathrm{b.r.}_{\left( ^{3}_{\lambda}\mathrm h \rightarrow ^{3}\mathrm{he},\pi^{-} \right)} = \left( 3.86 \pm 0.77 (\mathrm{stat.}) \pm 0.68 (\mathrm{syst.})\right) \times 10^{-5}$ in the 0-10% most central collisions, is consistent with the predictions from a statistical thermal model using the same temperature as for the light hadrons. the coalescence parameter $b_3$ shows a dependence on the transverse momentum, similar to the $b_2$ of deuterons and the $b_3$ of $^{3}\mathrm{he}$ nuclei. the ratio of yields $s_3$ = $^{3}_{\lambda}\mathrm h$/($^{3}\mathrm{he}$ $\times \lambda/\mathrm{p}$) was measured to be $s_3$ = 0.60 $\pm$ 0.13 (stat.) $\pm$ 0.21 (syst.) in 0-10% centrality events; this value is compared to different theoretical models. the measured $s_3$ is fully compatible with thermal model predictions. the measured $^{3}_{\lambda}\mathrm h$ lifetime, $ \tau = 181^{+54}_{-39} (\mathrm{stat.}) \pm 33 (\mathrm{syst.})\ \mathrm{ps}$ is compatible within 1$\sigma$ with the world average value.
centrality dependence of inclusive j/$\psi$ production in p-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev. we present a measurement of inclusive j/$\psi$ production in p-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev as a function of the centrality of the collision, as estimated from the energy deposited in the zero degree calorimeters. the measurement is performed with the alice detector down to zero transverse momentum, $p_{\rm t}$, in the backward ($-4.46 &lt; y_{\rm cms} &lt; -2.96$) and forward ($2.03 &lt; y_{\rm cms} &lt; 3.53$) rapidity intervals in the dimuon decay channel and in the mid-rapidity region ($-1.37 &lt; y_{\rm cms} &lt; 0.43$) in the dielectron decay channel. the backward and forward rapidity intervals correspond to the pb-going and p-going direction, respectively. the $p_{\rm t}$-differential j/$\psi$ production cross section at backward and forward rapidity is measured for several centrality classes, together with the corresponding average $p_{\rm t}$ and $p^2_{\rm t}$ values. the nuclear modification factor, $q_{\rm ppb}$, is presented as a function of centrality for the three rapidity intervals, and, additionally, at backward and forward rapidity, as a function of $p_{\rm t}$ for several centrality classes. at mid- and forward rapidity, the j/$\psi$ yield is suppressed up to 40% compared to that in pp interactions scaled by the number of binary collisions. the degree of suppression increases towards central p-pb collisions at forward rapidity, and with decreasing $p_{\rm t}$ of the j/$\psi$. at backward rapidity, the $q_{\rm ppb}$ is compatible with unity within the total uncertainties, with an increasing trend from peripheral to central p-pb collisions.
differential studies of inclusive j/$\psi$ and $\psi$(2s) production at forward rapidity in pb-pb collisions at $\mathbf{\sqrt{{\textit s}_{_{nn}}}}$ = 2.76 tev. the production of j/$\psi$ and $\psi(2s)$ was measured with the alice detector in pb-pb collisions at the lhc. the measurement was performed at forward rapidity ($2.5 &lt; y &lt; 4 $) down to zero transverse momentum ($p_{\rm t}$) in the dimuon decay channel. inclusive j/$\psi$ yields were extracted in different centrality classes and the centrality dependence of the average $p_{\rm t}$ is presented. the j/$\psi$ suppression, quantified with the nuclear modification factor ($r_{\rm aa}$), was studied as a function of centrality, transverse momentum and rapidity. comparisons with similar measurements at lower collision energy and theoretical models indicate that the j/$\psi$ production is the result of an interplay between color screening and recombination mechanisms in a deconfined partonic medium, or at its hadronization. results on the $\psi(2s)$ suppression are provided via the ratio of $\psi(2s)$ over j/$\psi$ measured in pp and pb-pb collisions.
location of distribution centers in a multi-period collaborative distribution network. this paper presents a research study which aims at determining optimal locations of regional distribution centers in a collaborative distribution network. we consider a multi-layered distribution system between a cluster of suppliers from a given region and several thousands customers spread over the whole country. the optimization problem consists of finding the locations of intermediate logistics facilities called regional distribution centers and assigning customers to these facilities according to one year of historical data. the distribution system combines full truckload (ftl) routes and less-than-truckload (ltl) shipments. the use of several rates for transportation, as well as the high impact of seasonality implies that, for each shipping date, the number of ftl routes and the cost of ltl shipments should be precisely evaluated. this problem is modeled as a mixed integer linear problem and used as a decision aiding tool on a real case study related to the distribution of horticultural products in france.
an adaptive large neighborhood search for a full truckload routing problem in public works. this paper presents a truck routing and scheduling problem faced by a public works company. itconsists of optimizing the collection and delivery of materials between sites, using a heterogeneousfleet of vehicles. these flows of materials arise in levelling works and construction of roads networks.as the quantity of demands usually exceeds the capacity of a truck, several trucks are needed tofulfill them. as a result, demands are split into full truckloads. a set of trucks routes are needed toserve a set of demands sharing a set of resources, available at pickup or delivery sites, which can beloaders or asphalt finishers in our application cases. thus, these routes need to be synchronized ateach resource. we propose an adaptive large neighborhood search (alns) to solve this problem.this approach is evaluated on real instances from a public work company in france.
centrality dependence of high-$p_{\rm t}$ d meson suppression in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the nuclear modification factor, $r_{\rm aa}$, of the prompt charmed mesons ${\rm d^0}$, ${\rm d^+}$ and ${\rm d^{*+}}$, and their antiparticles, was measured with the alice detector in pb-pb collisions at a centre-of-mass energy $\sqrt{s_{\rm nn}} = 2.76$ tev in two transverse momentum intervals, $5&lt;p_{\rm t}&lt;8$ gev/$c$ and $8&lt;p_{\rm t}&lt;16$ gev/$c$, and in six collision centrality classes. the $r_{\rm aa}$ shows a maximum suppression of a factor of 5-6 in the 10% most central collisions. the suppression and its centrality dependence are compatible within uncertainties with those of charged pions. a comparison with the $r_{\rm aa}$ of non-prompt ${\rm j}/\psi$ from b meson decays, measured by the cms collaboration, hints at a larger suppression of d mesons in the most central collisions.
molecular modeling of the effects of 40ar recoil in illite particles on their k–ar isotope dating. null
xemis: a liquid xenon detector for medical imaging. a new medical imaging technique based on the precise 3d location of a radioactive source by the simultaneous detection of 3 gamma rays has been proposed by subatech laboratory. to take advantage of this novel technique a detection device based on a liquid xenon compton telescope and a specific (beta(+), gamma) emitter radionuclide, sc-44, are required. a first prototype of a liquid xenon time projection chamber called xemis1 has been successfully developed showing very promising results for the energy and spatial resolutions for the ionization signal in liquid xenon, thanks to an advanced cryogenics system, which has contributed to a high liquid xenon purity with a very good stability and an ultra-low noise front-end electronics (below 100 electrons) operating at liquid xenon temperature. the very positive results obtained with xemis1 have led to the development of a second prototype for small animal imaging. xemis2, which is now under development. to study the feasibility of the 3 gamma imaging technique and optimize the characteristics of the device, a complete monte carlo simulation has been also carried out. a preliminary study shows very positive results for the sensitivity, energy and spatial resolutions of xemis2. (c) 2014 elsevier b.v. all rights reserved.
crystallographic studies of [nife]-hydrogenase mutants: towards consensus structures for the elusive unready oxidized states. catalytically inactive oxidized o2-sensitive [nife]-hydrogenases are characterized by a mixture of the paramagnetic ni-a and ni-b states. upon o2 exposure, enzymes in a partially reduced state preferentially form the unready ni-a state. because partial o2 reduction should generate a peroxide intermediate, this species was previously assigned to the elongated ni-fe bridging electron density observed for preparations of [nife]-hydrogenases known to contain the ni-a state. however, this proposition has been challenged based on the stability of this state to uv light exposure and the possibility of generating it anaerobically under either chemical or electrochemical oxidizing conditions. consequently, we have considered alternative structures for the ni-a species including oxidation of thiolate ligands to either sulfenate or sulfenic acid. here, we report both new and revised [nife]-hydrogenases structures and conclude, taking into account corresponding characterizations by fourier transform infrared spectroscopy (ftir), that the ni-a species contains oxidized cysteine and bridging hydroxide ligands instead of the peroxide ligand we proposed earlier. our analysis was rendered difficult by the typical formation of mixtures of unready oxidized states that, furthermore, can be reduced by x-ray induced photoelectrons. the present study could be carried out thanks to the use of desulfovibrio fructosovorans [nife]-hydrogenase mutants with special properties. in addition to the ni-a state, crystallographic results are also reported for two diamagnetic unready states, allowing the proposal of a revised oxidized inactive ni-su model and a new structure characterized by a persulfide ion that is assigned to an ni-'sox' species.
crystal structure of hydg from carboxydothermus hydrogenoformans: a trifunctional [fefe]-hydrogenase maturase. the structure of the radical s-adenosyl-l-methionine (sam) [fefe]-hydrogenase maturase hydg involved in cn(-) /co synthesis is characterized by two internal tunnels connecting its tyrosine-binding pocket with the external medium and the c-terminal fe4 s4 cluster-containing region. a comparison with a tryptophan-bound nosl structure suggests that substrate binding causes the closing of the first tunnel and, along with mutagenesis studies, that tyrosine binds to hydg with its amino group well positioned for h-abstraction by sam. in this orientation the dehydroglycine (dhg) fragment caused by tyrosine cα-cβ bond scission can readily migrate through the second tunnel towards the c-terminal domain where both cn(-) and co are synthesized. our hydg structure appears to be in a relaxed state with its c-terminal cluster cysx2 cysx22 cys motif exposed to solvent. a rotation of this domain coupled to fe4 s4 cluster assembly would bury its putatively reactive unique fe ion thereby allowing it to interact with dhg.
[nife]-hydrogenases revisited: nickel-carboxamido bond formation in a variant with accrued o2-tolerance and a tentative re-interpretation of ni-si states. [nife]-hydrogenases are well-studied enzymes capable of oxidizing molecular hydrogen and reducing protons. epr and ftir spectroscopic studies have shown that these enzymes can be isolated in several redox states that include paramagnetic oxidized inactive ni-a and ni-b species and a reduced ni-c form. the latter and the diamagnetic respectively more oxidized ni-si and more reduced ni-r forms are generally thought to be involved in the catalytic cycle of [nife]-hydrogenases. with the exception of ni-si, these different stable states have been well characterized. here, based on the crystal structure of a partially reduced desulfovibrio fructosovorans (df) enzyme and data from the literature we propose that at least one of the ni-si sub-states contains an unexpected combination of hydride and sulfenic acid moieties. we have also determined the structure of the less oxygen-sensitive df [nife]-hydrogenase v74c mutant and found that more than half of the active site nickel occupies a novel position, called ni'. in this new position, the metal ion is coordinated by two cysteine thiolates, a bridging species modeled as sh(-) and a main chain carboxamido n atom. the ni' coordination is similar to the one found in ni superoxide dismutase, an enzyme that operates at significantly more positive potentials than [nife]-hydrogenases. we propose that the oxygen-tolerance of the v74c variant results from a high potential stabilization of a ni'(iii) species induced by the change in the metal ion coordination sphere. we also propose that transient ni'(iii) species can rapidly attract successive electrons from the fe4s4 proximal cluster accelerating the reduction of oxygen to water and hydroxide. the naturally occurring oxygen-tolerant [nife]-hydrogenases have an unusual proximal cluster that has been shown to be exceptionally plastic and capable of undergoing two successive one-electron oxidations. this double oxidation is modulated by the migration of one of the iron atoms in the cluster to the main chain where, as fe(iii), it forms a bond with a carboxamido n ligand. like in the df v74c variant the electrons from the proximal cluster help reducing o2 to h2o and oh(-). in conclusion, in both cases a metal-carboxamido bond may explain, at least partially, the observed oxygen tolerance.
an adaptive large neighborhood search for a vehicle routing problem with cross-dock under dock resource constraints. in this work, we study the impact of dock resource constraints on the cost of vrpcd solutions.
virtual machine introspection: techniques and applications. null
efficient handling of synchronization in three vehicle routing problems. this paper presents a synthesis of contributions to the solving of three vehicle routing problemsinvolving synchronization constraints. these problems are: the pickup and delivery problem withtransfers (pdpt), studied during the phd of renaud masson, co-advised with olivier péton [4], thetwo-echelon multiple-trip vehicle routing problem with satellite synchronization (2e-mtvrpss),studied by philippe grangier during his phd, co-advised with michel gendreau and louis-martin rousseau [2], and the heterogeneous full truckload pickup and delivery problem with timewindows and resource synchronization(hftpdptw-rs), under study by axel grimault in hisphd and co-advised with nathalie bostel [3]. all these problems have been solved with an adaptivelarge neighborhood search (alns).a special focus is given to the temporal feasibility evaluation of an insertion which has beenproposed for the pdpt [5] and extended to the other problems. the concept of forward time slack[6] is extended to provide a constant time feasibility test of temporal constraints. experimentsconfirm the solving time reduction provided by the implementation of this test in a meta-heuristic.
reconstruction of the parameters of cosmic ray induced extensive air showers using radio detection and simulation. null
analysis of the quark-gluon plasma by heavy quarks. null
multi-scale and multi-frequency studies of cosmic ray air shower radio signals at the codalema site. since 2003, the nan\c cay radio observatory hosts the codalema experiment, dedicated to the radio detection of cosmic ray induced extensive air showers. after several instrumental upgrades, codalema is now composed of:\begin{itemize}\item{57 self-triggering radio detection stations working in the 20-250~mhz band, spread over 1 km$^2$;}\item{an array of 13 scintillators acting as a particle detector;}\item{a compact array of 10 cabled antennas, triggered by the particle detector, to test the capabilities of a phased antenna cluster to cleverly select air shower events.}\end{itemize}in addition, codalema supports the extasis project, aiming at detecting the low-frequency signal produced by the sudden deceleration of the air shower particles hitting the ground. beside these dedicated arrays, the nan\c cay site will host the nenufar radio telescope (recognized as a ska pathfinder), made of 1824 dual crossed-polarization antennas similar to the codalema ones. all these arrays present different antenna density and extent, and could be operated in a joint mode to record simultaneously the radio signal coming from an air shower. therefore, the upgraded codalema facilities could offer a complete description of the air shower induced electric field at small, medium and large scale, and over an unique and very wide frequency band (from $\sim2$ to $\sim250$~mhz). the use of multi-band detectors combined with composite trigger algorithms could help boosting the radio detection technique as a candidate for a further very large cosmic ray observatory, or in the frame of a large radio telescope such as ska. we describe the current instrumental set-up and the last results obtained, together with the prospective developments of the radio detection technique.
estimation of driver distraction using the prediction error of a cybernetic driver model. null
identification of a linear parameter varying driver model for the detection of distraction. null
broadband active noise control design through nonsmooth hinf synthesis. null
broadband active noise control design through nonsmooth hinfinity synthesis. this paper deals with active control of a broadband noise in a car cabine. it aims to study the achievable performance of such control in the siso feedback case. the main limitations involved, known as waterbed effect, are critical for such problem due to the presence of non-minimum phase zeros. to evaluate the intrinsic limitations due to these non-minimum phase zeros, a multi-objective control synthesis is proposed, allowing to cope with classical specifications (performance and robustness), without pessimism. the control synthesis is based on a h1 criterion to be minimized under some decoupled constraints. it consists in a non-convex and non-smooth optimization problem, for which a local optimum may be efficiently obtained. a particular control structure is considered in order to reduce the number of decision variables, to set relevant bounds on these parameters and to choose appropriateinitial conditions. then the optimization problem is solved using recent results on non-smooth optimization. the whole design process is detailed, including the identification of the synthesis model. the control strategy is then applied to an instrumented cavity, which shares most of the acoustic characteristics of a car cabin. finally, the analysis of the results gives clear conclusions on siso feedback possibilities, and paves the way for an efficient multivariable design case.
effect of event selection on jetlike correlation measurement in $d$+au collisions at $\sqrt{s_{\rm{nn}}}=200$ gev. dihadron correlations are analyzed in $\sqrt{s_{_{\rm nn}}} = 200$ gev $d$+au collisions classified by forward charged particle multiplicity and zero-degree neutral energy in the au-beam direction. it is found that the jetlike correlated yield increases with the event multiplicity. after taking into account this dependence, the non-jet contribution on the away side is minimal, leaving little room for a back-to-back ridge in these collisions.
compiling results from the count of oscimodes, analytical for light flavored mesons and from fortran programs for like (anti-)baryons. material shown in part at the poster session. null
the schrödinger-langevin equation with and without thermal fluctuations. the schrôdinger-langevin (sl) equation is considered as an effective open quantum system formalism suitable for phenomenological applications. we focus on two open issues relative to its solutions. we first show that the madelung/polar transformation of the wavefunction leads to a nonzero friction for the excited states of the quantum subsystem. we then study analytically and numerically the sl equation ability to bring a quantum subsystem to the thermal equilibrium of statistical mechanics. to do so, concepts about statistical mixed states, quantum noises and their production are discussed and a detailed analysis is carried with two kinds of noise and potential.
dynamical evolution of the chiral magnetic effect: applications to the quark-gluon plasma. we study the dynamical evolution of the so-called chiral magnetic effect in an electromagnetic conductor. to this end, we consider the coupled set of corresponding maxwell and chiral anomaly equations, and we prove that these can be derived from chiral kinetic theory. after integrating the chiral anomaly equation over space in a closed volume, it leads to a quantum conservation law of the total helicity of the system. a change in the magnetic helicity density comes together with a modification of the chiral fermion density. we study in fourier space the coupled set of anomalous equations and we obtain the dynamical evolution of the magnetic fields, magnetic helicity density, and chiral fermion imbalance. depending on the initial conditions we observe how the helicity might be transferred from the fermions to the magnetic fields, or vice versa, and find that the rate of this transfer also depends on the scale of wavelengths of the gauge fields in consideration. we then focus our attention on the quark-gluon plasma phase, and analyze the dynamical evolution of the chiral magnetic effect in a very simple toy model. we conclude that an existing chiral fermion imbalance in peripheral heavy ion collisions would affect the magnetic field dynamics, and consequently, the charge dependent correlations measured in these experiments.
heavy quark scattering and quenching in a qcd medium at finite temperature and chemical potential. the heavy quark collisional scattering on partons of the quark gluon plasma (qgp) is studied in a quantum chromodynamics medium at finite temperature and chemical potential. we evaluate the effects of finite parton masses and widths, finite temperature t, and quark chemical potential μq on the different elastic cross sections for dynamical quasiparticles (on- and off-shell particles in the qgp medium as described by the dynamical quasiparticle model “dqpm”) using the leading order born diagrams. our results show clearly the decrease of the qq and gq total elastic cross sections when the temperature and the quark chemical potential increase. these effects are amplified for finite μq at temperatures lower than the corresponding critical temperature tc(μq). using these cross sections we, furthermore, estimate the energy loss and longitudinal and transverse momentum transfers of a heavy quark propagating in a finite temperature and chemical potential medium. accordingly, we have shown that the transport properties of heavy quarks are sensitive to the temperature and chemical potential variations. our results provide some basic ingredients for the study of charm physics in heavy-ion collisions at beam energy scan at rhic and cbm experiment at fair.
flavor dependence of baryon melting temperature in effective models of qcd. we apply the three-flavor (polyakov-)nambu-jona-lasinio model to generate baryons as quark-diquark bound states using many-body techniques at finite temperature. all the baryonic states belonging to the octet and decuplet flavor representations are generated in the isospin-symmetric case. for each state we extract the melting temperature at which the baryon may decay into a quark-diquark pair. we seek for an evidence of the strangeness dependence of the baryon melting temperature as suggested by the statistical thermal models and supported by lattice-qcd results. a clear and robust signal for this claim is found, pointing to a flavor dependence of the hadronic deconfinement temperature.
tomography of the quark-gluon-plasma by charm quarks. we study charm production in ultra-relativistic heavy-ion collisions by using the parton-hadron-string dynamics (phsd) transport approach. the initial charm quarks are produced by the pythia event generator tuned to fit the transverse momentum spectrum and rapidity distribution of charm quarks from fixed-order next-to-leading logarithm (fonll) calculations. the produced charm quarks scatter in the quark-gluon plasma (qgp) with the off-shell partons whose masses and widths are given by the dynamical quasi-particle model (dqpm) which reproduces the lattice qcd equation-of-state in thermal equilibrium. the relevant cross section are calculated in a consistent way by employing the effective propagators and couplings from the dqpm. close to the critical energy density of the phase transition, the charm quarks are hadronized into $d$ mesons through coalescence and/or fragmentation depending on transverse momentum. the hadronized $d$ mesons then interact with the various hadrons in the hadronic phase with cross sections calculated in an effective lagrangian approach with heavy-quark spin symmetry. finally, the nuclear modification factor $\rm r_{aa}$ and the elliptic flow $v_2$ of $d^0$ mesons from phsd are compared with the experimental data from the star collaboration for au+au collisions at $\sqrt{s_{\rm nn}}$ =200 gev. we find that in the phsd the energy loss of $d$ mesons at high $p_t$ can be dominantly attributed to partonic scattering while the actual shape of $\rm r_{aa}$ versus $p_t$ reflects the heavy quark hadronization scenario, i.e. coalescence versus fragmentation. also the hadronic rescattering is important for the $\rm r_{aa}$ at low $p_t$ and enhances the $d$-meson elliptic flow $v_2$.
total absorption spectroscopy study of $^{92}$rb decay: a major contributor to reactor antineutrino flux. the antineutrino spectra measured in recent experiments at reactors are inconsistent with calculations based on the conversion of integral beta spectra recorded at the ill reactor. $^{92}$rb makes the dominant contribution to the reactor spectrum in the 5-8 mev range but its decay properties are in question. we have studied $^{92}$rb decay with total absorption spectroscopy. previously unobserved beta feeding was seen in the 4.5-5.5 region and the gs to gs feeding was found to be 87.5(25)%. the impact on the reactor antineutrino spectra calculated with the summation method is shown and discussed.
quarkonium suppression from coherent energy loss in fixed-target experiments using lhc beams. quarkonium production in proton-nucleus collisions is a powerful tool to disentangle cold nuclear matter effects. a model based on coherent energy loss is able to explain the available quarkonium suppression data in a broad range of rapidities, from fixed-target to collider energies, suggesting cold energy loss to be the dominant effect in quarkonium suppression in p-a collisions. this could be further tested in a high-energy fixed-target experiment using a proton or nucleus beam. the nuclear modification factors of j/$\psi$ and $\upsilon$ as a function of rapidity are computed in p-a collisions at $\sqrt{s}=114.6$ gev, and in p-pb and pb-pb collisions at $\sqrt{s}=72$ gev. these center-of-mass energies correspond to the collision on fixed-target nuclei of 7 tev protons and 2.76 tev lead nuclei available at the lhc.
scintillation efficiency of liquid argon in low energy neutron-argon scattering. experiments searching for weak interacting massive particles with noble gases such as liquid argon require very low detection thresholds for nuclear recoils. a determination of the scintillation efficiency is crucial to quantify the response of the detector at low energy. we report the results obtained with a small liquid argon cell using a monoenergetic neutron beam produced by a deuterium-deuterium fusion source. the light yield relative to electrons was measured for six argon recoil energies between 11 and 120 kev at zero electric drift field.
enhanced gamma-ray emission from neutron unbound states populated in beta decay. total absorption spectroscopy was used to investigate the beta-decay intensity to states above the neutron separation energy followed by gamma-ray emission in 87,88br and 94rb. accurate results were obtained thanks to a careful control of systematic errors. an unexpectedly large gamma intensity was observed in all three cases extending well beyond the excitation energy region where neutron penetration is hindered by low neutron energy. the gamma branching as a function of excitation energy was compared to hauser-feshbach model calculations. for 87br and 88br the gamma branching reaches 57% and 20% respectively, and could be explained as a nuclear structure effect. some of the states populated in the daughter can only decay through the emission of a large orbital angular momentum neutron with a strongly reduced barrier penetrability. in the case of neutron-rich 94rb the observed 4.5% branching is much larger than the calculations performed with standard nuclear statistical model parameters, even after proper correction for fluctuation effects on individual transition widths. the difference can be reconciled introducing an enhancement of one order-of-magnitude in the photon strength to neutron strength ratio. an increase in the photon strength function of such magnitude for very neutron-rich nuclei, if it proved to be correct, leads to a similar increase in the (n,gamma) cross section that would have an impact on r-process abundance calculations.
measurement of jet quenching with semi-inclusive hadron-jet distributions in central pb-pb collisions at ${\sqrt{\bf{s}_{\mathrm {\bf{nn}}}}}$ = 2.76 tev. we report the measurement of a new observable of jet quenching in central pb-pb collisions at $\sqrt{s_{\rm nn}} = 2.76$ tev, based on the semi-inclusive rate of charged jets recoiling from a high transverse momentum (high-$p_{\rm t}$) charged hadron trigger. jets are measured using collinear-safe jet reconstruction with infrared cutoff for jet constituents of 0.15 gev/$c$, for jet resolution parameters $r = 0.2$, 0.4 and 0.5. underlying event background is corrected at the event-ensemble level, without imposing bias on the jet population. recoil jet spectra are reported in the range $20&lt;p_\mathrm{t,jet}^\mathrm{ch}&lt;100$ gev/$c$. reference distributions for pp collisions at $\sqrt{s} = 2.76$ tev are calculated using monte carlo and nlo pqcd methods, which are validated by comparing with measurements in pp collisions at $\sqrt{s} = 7$ tev. the recoil jet yield in central pb-pb collisions is found to be suppressed relative to that in pp collisions. no significant medium-induced broadening of the intra-jet energy profile is observed within 0.5 radians relative to the recoil jet axis. the angular distribution of the recoil jet yield relative to the trigger axis is found to be similar in central pb-pb and pp collisions, with no significant medium-induced acoplanarity observed. large-angle jet deflection, which may provide a direct probe of the nature of the quasi-particles in hot qcd matter, is explored.
delta scaling: how resources scalability/termination can be taken place economically?. cloud computing promises to completely revolutionize the capacity management of resources. the elasticityand the economy of scale are the intrinsic elements that differentiate it from traditional computing paradigm. a good capacity planning method is a necessary factor but not sufficient to fully exploit cloud elasticity. this paper proposes innovative policies for resource management to achieve the optimal balance between capacity and quality of cloud services while supporting cloud technical and conceptual limitations. the main idea is to control finely the scalability and the termination of virtual machines in regards of several criteria such as the lifecycle of the instances (e.g. initialization time) or their cost. the approach was evaluated with a real infrastructure (amazon ec2) and an application testbed. experimental results illustrate the soundness of the proposed approach and the impact of scalability/termination resource policies. using deltascaling, the cost saving of as much as 30% can be achieved while causing the minimum number of violations, as small as 1%.
sla guarantees for cloud services. quality-of-service and sla guarantees are among the major challenges of cloud-based services. in this paper we first present a new cloud model called slaaas — sla aware service. slaaas considers qos levels and sla as first class citizens of cloud-based services. this model is orthogonal to other saas, paas, and iaas cloud models, and may apply to any of them. more specifically we make three contributions: (i) we provide a novel domain specific language that allows to describe qos-oriented sla associated with cloud services; (ii) we present a general control-theoretic approach for managing cloud service sla; (iii) we apply the proposed language and control approach to guarantee sla in various case studies, ranging from cloud-based mapreduce service, to locking service, and higher-level e-commerce service; these case studies successfully illustrate sla management with different qos aspects of cloud services such as performance, dependability, financial energetic costs.
probing the surface reactivity of mineral phases constituting the callovo-oxfordian argilite by isotope exchange method. null
impact of the radiolysis in the repository disposal. null
investigation of astatine chemistry in solution. null
extraction behavior of polonium-210 from hcl and hno3 solution using tributyl phosphate. null
characterization of at- and ato+ species in simple media by high performance ion exchange chromatography coupled to gamma detector. application to astatine speciation in human serum. null
a new route for polonium-210 production from a bismuth-209 target. null
vmplaces: a generic tool to investigate and compare vm placement algorithms. advanced virtual machines placement policies are evaluated either using limited scale in-vivo experiments or ad hoc simulator techniques. these validation methodologies are unsatisfactory. first they do not model precisely enough real production platforms (size, workload representativeness, etc.). second, they do not enable the fair comparison of different approaches.to resolve these issues, we propose vmplaces, a dedicated simulation framework to perform in-depth investigations and fair comparisons of vm placement algorithms. built on top of simgrid, our framework provides programming support to ease the implementation of placement algorithms and runtime support dedicated to load injection and execution trace analysis. it supports a large set of parameters enabling researchers to design simulations representative of a large space of real-world scenarios. we also report on a comparison using vmplaces of three classes of placement algorithms: centralized, hierarchical and fully-distributed ones.
components of the performance assessment: impact of solubility uncertainties as an example. null
probing the slow processes at the interface solid/water by the isotopic exchange method. null
influence of slow processes close to equilibrium on the fate of rn released in the environment. null
long-term fate and transport of fission products and actinides in geosphere. null
slow processes in close-to-equilibrium conditions for radionuclides in water/solid systems of relevance to nuclear waste management. null
behaviour of carbon in zircaloy and  zirconium in solution. null
thorium oxide solubility behavior vs. the surface crystalline state. null
slow processes in close-to-equilibrium conditions for radionuclides in water/solid systems of relevance to nuclear waste management – skin. null
2nd annual workshop proceedings - 7th ec fp - skin. null
sensorscript: a domain-specific language for sensor networks. null
a neural network approach for burn-up calculation and its application to the dynamic fuel cycle code class. dynamic fuel cycle simulation tools calculate nuclei inventories and mass flows evolution in an entire fuel cycle, from the mine to the final disposal. usually, the fuel depletion in reactor is handled by a fuel loading model and a mean cross section predictor. in the case of a pwr–mox, a fuel loading model provides from a plutonium stock the plutonium fraction in the fresh fuel needed to reach a specific burnup. a mean cross section predictor aims to assess isotopic cross sections required for building bateman equations for any fresh fuel composition with a sufficient accuracy and a reasonable computing time. this paper presents a methodology based on neural networks for building a fuel loading model and a cross section predictor for a pwr reactor loaded with mox fuel. the mean error of the plutonium content prediction from the fuel loading model is 0.37%. furthermore, the mean cross section predictor allows completion of the fuel depletion calculation in less than one minute with excellent accuracy. a maximum deviation of 3% on main nuclei is obtained at the end of cycle between inventories calculated from neural networks and from the reference coupled neutron transport/fuel depletion calculation.
modèles et méthodes d'optimisation combinatoire pour la conception de chaînes logistiques et l'optimisation des transports. null
exploration of the metallic character of astatine. null
a new route of production of polonium-210. null
actinide separations using ionic exchange and extraction chromatography resins. null
synthesis of multiple sensitivity constrained controllers for parametric uncertain lti systems. the purpose of this paper is to propose a synthesis method of multiple parametric sensitivity constrained linear quadratic (slq) controllers for a parametric uncertain lti system. system sensitivity to parameter variation, for each controller, is handled through an additional quadratic trajectory parametric sensitivity term in the criterion to be minimized. the controllers are supposed to cover the whole parametric uncertainty while degrading as less as possible the intrinsic robustness properties of each local linear quadratic controller. in that context, it is difficult to ensure the global optimality. hence an efficient particle swarm optimization (pso) based algorithm is provided to find the best partition of the uncertainty set as well as the set of slq controllers.
mixed matrix sign function/dft inversion method for solving parameterdependent riccati equation. this paper proposes a tractable iterative scheme for computing parameter-dependent matrix sign function. it relies upon two results: (i) a fraction expansion of the matrix sign principal padé rational approximation, (ii) a discrete fourier transform (dft) inversion method for polynomial parameter-dependent matrices. the method is used for solving parameter-dependent riccati equations. some illustrative examples, given throughout the paper, demonstrate the effectiveness of the method. an application for extracting the harmonics of current or voltage waveforms confirms the validity of the approach. a realistic control application dealing with a parameter dependent lqr design problem for an airfoil flutter is also presented.
a parametric sensitivity constrained linear quadratic controller. the purpose of this paper is to give a new insight on a suboptimal linear quadratic control taking explicitly into account the parametric uncertainties. system sensitivity to parameter variations is handled through including a quadratic trajectory parametric sensitivity term in the cost functional to be minimized. the paper main contribution is twofold: - using a descriptor system approach, the paper shows that the underlying singular linear-quadratic optimal control problem leads to a non-standard riccati equation. - a solution to the proposed control problem is given based on a connection to the so-called lur'e matrix equations. some examples are given in order to illustrate the interest of the approach.
particle swarm optimization for the multi-level lot sizing. null
tactical planning for public construction sector. null
integration of maintenance in the tactical production planning process under feasibility constraint. this paper deals with the problem of the joint optimization of the master production schedule and maintenance strategy for a manufacturing system. an efficient production planning and maintenance policy will allow to minimize the impacts of the potential random failures and will let the plan to be feasible. we present a modelisation where we take into account a feasibility constraint; the optimization problem is formulated as a linear program. we propose a heuristic algorithm to solve it and we show the impact of the feasibility constraint on different criteria.
planification conjointe de la production et de la maintenance sous contrainte de faisabilité : cas multi-produits. null
dynamic cluster in particle swarm optimization algorithm. null
search for dark photons from neutral meson decays in p+p and d+au collisions at snn−−−−√=200 gev. the standard model (sm) of particle physics is spectacularly successful, yet the measured value of the muon anomalous magnetic moment (g−2)μ deviates from sm calculations by 3.6σ. several theoretical models attribute this to the existence of a “dark photon,” an additional u(1) gauge boson, which is weakly coupled to ordinary photons. the phenix experiment at the relativistic heavy ion collider has searched for a dark photon, u, in π0,η→γe+e− decays and obtained upper limits of o(2×10−6) on u−γ mixing at 90% c.l. for the mass range 30&lt;mu&lt;90 mev/c2. combined with other experimental limits, the remaining region in the u−γ mixing parameter space that can explain the (g−2)μ deviation from its sm value is nearly completely excluded at the 90% confidence level, with only a small region of 29&lt;mu&lt;32 mev/c2 remaining.
integration approaches of forecasting methods selection with inventory management indicators in the case of spare parts supply chain. null
joint optimization of a master production schedule and a preventive maintenance policy. null
a model and bi-objective solution technique for green sustainable supply chain network design. null
influence of clay content on hto and 36cl transport properties in callovo-oxfordian clayrock : percolation experiments and modelling. null
collaborative distribution: from the network design to an operational load plan. null
a column generation approach for a pooled network design problem with vehicle management constraints and piecewise linear cost structures. null
multi-directional local search for a sustainable supply chain network design model. 1. the problem consideredthe increasing importance of environmental issues has prompted decisionmakersto incorporate environmental factors into supply chain networkdesign (scnd) models. we propose a bi-objective scnd model to minimizetwo conflicting objectives: the total cost and the environmental impactexpressed by co2 emissions.the logistics network consists of four layers: suppliers, plants, distributioncenters (dcs) and customers. the model considers several possibletransportations modes in the network, each transportation mode havinga lower and upper capacity limitation. moreover, we consider differentcandidate technology levels at the plants and dcs. each technology representsa type of service with associated fixed and variable costs and co2emissions. a higher-level technology may reduce carbon emissions, but islikely to require more investment cost.the model considers co2 emissions caused by all industrial and logisticsoperations as well as transportation. the main issues to be addressedin the sustainable scnd model includes determining the number, location,and technology level at plants and dcs, suitable transportation mode, andproduct flows between facilities.2. solution methodwe solve the corresponding bi-objective mixed integer linear programmingmodel with the multi-directional local search (mdls) framework. the efficiency of this recent framework has been proved on the multiobjectiveknapsack, set packing and orienteering problems, but to the bestof our knowledge, this is the first attempt to solve a facility location problemwith it. the mdls is based on the principle of separately using independentsingle-objective local searches to iteratively improve the pareto setapproximation. the motivation for using this framework is the capabilityof using already implemented single objective optimization components.in our case, we use a large neighborhood search algorithm as single objectivemethod. our algorithm can be decomposed in the three followingsteps:phase 1: look for an initial pareto set approximation. the initialphase of the single objective lns is executed separately for each objective.the output is an initial pareto set approximation.phase 2: intensification around the pareto set approximation. thepareto set approximation is improved by exploring the neighborhoodof all the solutions in this set with a multi-directional local search.phase 3: optimization of product flows. after stabilizing the locationand transportation mode decisions for all pareto set approximationsolutions in phase 2, we determine the optimal product flows by applyingthe simplex algorithm to all solutions in the set.3. computational resultswe assess the performance of our approach through a comparison withthe well-known "-constraint method. in particular, we analyze the paretofronts given by both solutions on a set of 60 generated instances and showthat the efficiency of our approach improves when the instance size grows.
sustainable supply chain network design: an optimization-oriented review. supply chain network design (scnd) models and methods have been the subject of severalrecent literature review surveys, but none of them explicitly includes sustainable development as amain characteristic of the problem considered. the aim of this review is to bridge this gap. the paperanalyzes 86 papers in the field of supply chain network design, covering mathematical models thatinclude economic factors as well as environmental and/or social dimensions.the review is organizedalong four research questions asking i) which environmental and social objectives are included, ii) how are they integrated into the models, iii) which methods and tool are used and finally iv) which industrial applications and contexts are covered in these models.the review finds that there are a number oflimitations to the current research in sustainable scnd. the narrow scope of environmental and socialmeasures in current models should go beyond limited greenhouse gas indicators to broader life-cycleapproaches including new social metrics. the more effective inclusion of uncertainty and risk inmodels with improved multi-objective approaches is also needed. there are also significant gaps in thesectors used to test models limiting more general applicability. the paper concludes with promisingnew avenues of research to more effectively include sustainability into scnd models.
study of the surface reactivity of clay mineral phases by isotope exchange. null
determination of dissolution and precipitation rates of clayey materials by 29si/28si isotopic exchange. effect of temperature. null
key factors to understand in-situ behavior of cs in callovo-oxfordian clay rock. understanding the behavior of 137cs and 135cs in soils and geological formations is of considerable interest in the context of nuclear accidents and nuclear waste repositories. although the clay fraction is known to be responsible for sorption, there are still unanswered questions raised by the literature data concerning (i) the reversibility of the sorption process(es), (ii) the validity of the additivity rule (the overall distribution coefficient (kd) for a radionuclide on a mixture of minerals is predicted from the distribution coefficients measured on individual minerals) and (iii) the validity of model transposition from dispersed systems to consolidated/intact systems. because of these uncertainties, the validity of sorption models at equilibrium under in-situ conditions and for very long-term interaction is still pending. these different issues are studied in the present work for the callovo–oxfordian (cox) clay-rich rock formation, which is under investigation in france as a geological barrier for a long-term nuclear waste repository. the work is based on sorption data measured on thirteen samples of different mineralogy taken from five different boreholes at several depths within the cox sedimentary layer. to our knowledge, it is the most extended cs sorption dataset that has been published for a single clay formation in term of (i) sample locations (and thus natural variability), (ii) sorption conditions (powder dispersed in suspension, compacted powders and intact samples) and (iii) equilibration time (from one week to five years). moreover, for the first time ever, radioactive cs sorption results were compared to the natural distribution of non-radioactive cs isotopes between pore water and the solid phase. the experimental system appeared to be in chemical equilibrium as much as can be expected for an ion-exchange reaction. more particularly, no kinetically-controlled process leading to partial cs irreversibility was observed, in contrary to what was found in the literature for soils. this difference in behavior may be related to the difference in the illite studied, i.e. a soil-type illite which would be more altered than a sedimentary formation-type illite. no decrease in site capacity was observed between dispersed and intact/compacted states. a model based on exchange reactions with cations interacting with illite (frayed edge, type-ii and planar sites) and mixed layer illite–smectite (i/s) (planar sites) using parameters published in the literature enabled the kd variation to be described as a function of cs concentration, the mineralogy of the samples, the change in water composition and the temperature (22–80 °c). our study clearly demonstrates that no frayed edge sites should be considered on the illite fraction of i/s, thus emphasizing the difference of sorption properties between an i/s mixed layer mineral and a corresponding mechanical mix of illite and smectite minerals. the robustness of the model was confirmed by data analysis describing the behavior of naturally-occurring cs in the formation thereby demonstrating the effectiveness of the cs sorption processes in a very long-time period prospective. lastly, the model was used to predict the sorption of trace concentrations of cs in the cox formation on the time-scale relevant for nuclear waste disposal performance assessment. as expected, the retention was significant with kd values ranging from 100 to 2000 l/kg whatever the conditions that were probed and a simulation covering a period of over 105 years could show that the cox formation is an efficient barrier to prevent cs transport from the storage facility to the surrounding environment.
effect of callovo-oxfordian clay rock on the dissolution rate of the son68 simulated nuclear waste glass. long-term storage of high-level nuclear waste glass in france is expected to occur in an engineered barrier system (ebs) located in a subsurface callovo-oxfordian (cox) clay rock formation in the paris basin in northeastern france. understanding the behavior of glass dissolution in the complex system is critical to be able to reliably model the performance of the glass in this complex environment. to simulate this multi-barrier repository scenario in the laboratory, several tests have been performed to measure glass dissolution rates of the simulated high-level nuclear waste glass, son68, in the presence of cox claystone at 90 °c. experiments utilized a high-performance liquid chromatography (hplc) pump to pass simulated bure site cox pore water through a reaction cell containing son68 placed between two cox claystone cores for durations up to 200 days. silicon concentrations at the outlet were similar in all experiments, even the blank experiment with only the cox claystone (∼4 mg/l at 25 °c and ∼15 mg/l at 90 °c). the steady-state ph of the effluent, measured at room temperature, was roughly 7.1 for the blank and 7.3–7.6 for the glass-containing experiments demonstrating the ph buffering capacity of the cox claystone. dissolution rates for son68 in the presence of the claystone were elevated compared to those obtained from flow-through experiments conducted with son68 without claystone in silica-saturated solutions at the same temperature and similar ph values. additionally, through surface examination of the monoliths, the side of the monolith in direct contact with the claystone was seen to have a corrosion thickness 2.5× greater than the side in contact with the bulk glass powder. results from one experiment containing 32si-doped son68 also suggest that the movement of si through the claystone is controlled by a chemically coupled transport with a si retention factor, kd, of 900 ml/g.
235u/238u isotope ratio analysis by la-icp-ms-hr for environmental radioactivity monitoring. null
complexity of a one-to-one meeting scheduling problem with two types of actors. this article deals with a new scheduling problem that arises in the organization of one-to-one meetings in parallel.we first introduce applications of one-to-one meeting scheduling problems. then, we give an overview of therelevant complexity results. we then prove several complexity results for different versions of the problem.
a note on np-hardness of preemptive mean flow-time scheduling for parallel machines. in the paper “the complexity of mean flow time scheduling problems with release times”, by baptiste, brucker, chrobak, dürr, kravchenko and sourd, the authors claimed to prove strong np -hardness of the scheduling problem p|pmtn,rj|∑cj , namely multiprocessor preemptive scheduling where the objective is to minimize the mean flow time. we point out a serious error in their proof and give a new proof of strong np -hardness for this problem.
monitoring de l’utilisation mémoire et cpu par processus, basé sur l’introspection de machines virtuelles. null
sensorscript : un langage de requête dédié, orienté métiers, pour les réseaux de capteurs. null
phytoextraction associated with bioaugmentaton of contaminated soils by caesium. null
anthropogenic tritium in the loire river estuary. null
microbial mobilization of cesium from illite: role of organic acids and siderophores. null
investigation of the  ato(oh)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;-&lt;/sup&gt; hydrolysed species: relativistic calculations. null
investigating ato&lt;sup&gt;+&lt;/sup&gt;-(oh&lt;sup&gt;−&lt;/sup&gt;)&lt;sub&gt;n&lt;/sub&gt; complexes at the molecular scale using quantum mechanical methods. null
theoretical investigation of the ato&lt;sup&gt;+&lt;/sup&gt; hydrolyzed species in ligand-exchange reactions including solvation effects. null
behavior of titanium alloys (t40, t64) vs. radiolytic corrosion under 4he2+ irradiation. null
radiolytic corrosion of titanium alloys under 4he2+/γ-ray irradiation. null
radiolytic corrosion occurring at the solid/solution interface investigated at subatech: example of uranium. null
first observation of cp violation in b0-&gt;d(*)cp h0 decays by a combined time-dependent analysis of babar and belle data. we report a measurement of the time-dependent cp asymmetry of b0-&gt;d(*)cp h0 decays, where the light neutral hadron h0 is a pi0, eta or omega meson, and the neutral d meson is reconstructed in the cp eigenstates k+ k-, k0s pi0 or k0s omega. the measurement is performed combining the final data samples collected at the y(4s) resonance by the babar and belle experiments at the asymmetric-energy b factories pep-ii at slac and kekb at kek, respectively. the data samples contain ( 471 +/- 3 ) x 10^6 bb pairs recorded by the babar detector and ( 772 +/- 11 ) x 10^6, bb pairs recorded by the belle detector. we measure the cp asymmetry parameters -eta_f s = +0.66 +/- 0.10 (stat.) +/- 0.06 (syst.) and c = -0.02 +/- 0.07 (stat.) +/- 0.03 (syst.). these results correspond to the first observation of cp violation in b0-&gt;d(*)cp h0 decays. the hypothesis of no mixing-induced cp violation is excluded in these decays at the level of 5.4 standard deviations.
bayesian  networks  to  quantify  transition  rates  in  degradation  modeling:  application  to  a  set  of  steel  bridges  in  the  netherlands. &lt;p&gt;&amp;nbsp;bridge lifetime pose an important challenge in terms of maintenance for decision makers&lt;br /&gt;or asset managers. in this regard markov chains have been used successfully in practice as models for&lt;br /&gt;bridge deterioration. however, one limitation of markov chains can be the assessment of the transition&lt;br /&gt;probabilities. in this paper, we propose an approach based on bayesian networks (bns) to quantify the&lt;br /&gt;transition probabilities of the system state. one of the advantages of doing so is that the bn may be&lt;br /&gt;quantified through physical variables linked to the underlying degradation process in an intuitive way&lt;br /&gt;through&amp;nbsp; expert&amp;nbsp; judgment&amp;nbsp; combined&amp;nbsp; with&amp;nbsp; field&amp;nbsp; measurements.&amp;nbsp; in&amp;nbsp; addition,&amp;nbsp; the&amp;nbsp; possibility&amp;nbsp; of&amp;nbsp; using&lt;br /&gt;bayesian inference&amp;nbsp; allows updating the probabilities when observations become available that could&lt;br /&gt;provide different relevant views of the long-term degradation. an application to a hypothetical stock of&lt;br /&gt;steel bridges in the netherlands is presented and illustrates the method.&lt;/p&gt;.
a condition-based maintenance policy based on a probabilistic meta-model in the case of chloride-induced corrosion. &lt;p&gt;maintenance and management policies are usually focused on minimizing the life-cycle cost only. therefore the optimal solution in this context does not necessarily result in a satisfactory long-term structural performance. in this paper, we will present an approach for modeling the degradation of structures and infrastructures for maintenance purposes. the degradation is modeled using probabilistic data-driven state dependent stochastic processes, hereafter called meta-model. this work implements this degradation model into a maintenance framework and carries out two numerical examples in order to show the applicability of meta-models in a maintenance and management optimization context. this paves the road for future work on meta-model updating and maintenance optimization by considering multi-objective optimization policies.&lt;/p&gt;.
systematic study of azimuthal anisotropy in cu$+$cu and au$+$au collisions at $\sqrt{s_{_{nn}}} = 62.4$ and 200~gev. we have studied the dependence of azimuthal anisotropy $v_2$ for inclusive and identified charged hadrons in au$+$au and cu$+$cu collisions on collision energy, species, and centrality. the values of $v_2$ as a function of transverse momentum $p_t$ and centrality in au$+$au collisions at $\sqrt{s_{_{nn}}}$=200~gev and 62.4~gev are the same within uncertainties. however, in cu$+$cu collisions we observe a decrease in $v_2$ values as the collision energy is reduced from 200 to 62.4~gev. the decrease is larger in the more peripheral collisions. by examining both au$+$au and cu$+$cu collisions we find that $v_2$ depends both on eccentricity and the number of participants, $n_{\rm part}$. we observe that $v_2$ divided by eccentricity ($\varepsilon$) monotonically increases with $n_{\rm part}$ and scales as ${n_{\rm part}^{1/3}}$. the cu$+$cu data at 62.4 gev falls below the other scaled $v_{2}$ data. for identified hadrons, $v_2$ divided by the number of constituent quarks $n_q$ is independent of hadron species as a function of transverse kinetic energy $ke_t=m_t-m$ between $0.1&lt;ke_t/n_q&lt;1$~gev. combining all of the above scaling and normalizations, we observe a near-universal scaling, with the exception of the cu$+$cu data at 62.4 gev, of $v_2/(n_q\cdot\varepsilon\cdot n^{1/3}_{\rm part})$ vs $ke_t/n_q$ for all measured particles.
fusion, fission, alpha emission and superheavy element formation and decay within a generalized liquid drop model. new observed phenomena like cluster emission, cold and asymmetric fission of 252cf,  nuclear molecule formation in 24mg, asymmetric fission of intermediate mass nuclei, quasi-fission of heavy dinuclear systems and alpha emission of superheavy nuclei have renewed interest in investigating the fusion-like fission valley which leads rapidly from a quasi-spherical nucleus to quasi-molecular shapes with deep necks and to two touching quasi-spherical fragments. furthermore, the rotational super and hyperdeformed states as well as the very heavy and superheavy elements are and will be formed in the entrance channel of heavy-ion collisions for which the initial configuration is two close quasi-spherical nuclei.            the balance between the repulsive coulomb forces and attractive surface tension forces alone leads in this quasi-molecular shape valley to a potential barrier with an unrealistic  coulomb peak. a proximity energy term must be added to the usual development of the liquid drop model energy to smoothly describe the transition from one-body shapes to two-body compact shapes. this term takes into account the finite-range effects of the nuclear forces in the crevice between the nascent future fragments or the gap between the incoming nuclei. as a consequence we have developed a particular version of the liquid drop model taking into account both the nuclear proximity energy, the mass and charge asymmetry, the rotational energy, the shell and pairing effects and the temperature. a specific quasi-molecular shape sequence derived from elliptic lemniscatoids has been defined to describe within this generalized liquid drop model the entrance channel of nuclear reactions and also the peculiar decay channel through compact and creviced shapes.           in this deformation valley and within this gldm the calculated l-dependent fission and fusion barriers, alpha and cluster radioactivity half-lives and double-humped barriers and half-lives of actinides are in agreement with the available experimental data [1-4]. in this particular deformation valley, double-humped potential barriers begin to appear even macroscopically for heavy nuclei due to the influence of the proximity forces and quasi-molecular rotational isomeric states are formed at intermediate angular momentum during the fusion process of light or medium mass nuclei. [1] g. royer, nucl. phys. a 848, 279 (2010). [2] g. royer, j. gaudillot, phys. rev. c 84, 044602 (2011).[3] g. royer, m. jaffré, d. moreau, phys. rev. c 86, 044326 (2012).[4] x.j. bao, h.f. zhang, g. royer, j.q. li, nucl. phys. a 906, 1 (2013).  .
inclusive, prompt and non-prompt j/$\psi$ production at mid-rapidity in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the transverse momentum ($p_{\rm t}$) dependence of the nuclear modification factor $r_{\rm aa}$ and the centrality dependence of the average transverse momentum $\langle p_{\rm t}\rangle$ for inclusive j/$\psi$ have been measured with alice for pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev in the e$^+$e$^-$ decay channel at mid-rapidity ($|y|&lt;0.8$). the $\langle p_{\rm t}\rangle$ is significantly smaller than the one observed for pp collisions at the same centre-of-mass energy. consistently, an increase of $r_{\rm aa}$ is observed towards low $p_{\rm t}$. these observations might be indicative of a sizable contribution of charm quark coalescence to the j/$\psi$ production. additionally, the fraction of non-prompt j/$\psi$ from beauty hadron decays, $f_{\rm b}$, has been determined in the region $1.5 &lt; p_{\rm t} &lt; 10$ gev/c in three centrality intervals. no significant centrality dependence of $f_{\rm b}$ is observed. finally, the $r_{\rm aa}$ of non-prompt j/$\psi$ is discussed and compared with model predictions. the nuclear modification in the region $4.5 &lt; p_{\rm t} &lt; 10$ gev/c is found to be stronger than predicted by most models.
measurement of charm and beauty production at central rapidity versus charged-particle multiplicity in proton-proton collisions at $\mathbf{\sqrt{{\textit s}}}=7$ tev. prompt d meson and non-prompt j/$\psi$ yields are studied as a function of the multiplicity of charged particles produced in inelastic proton-proton collisions at a centre-of-mass energy of $\sqrt{s}=7$ tev. the results are reported as a ratio between yields in a given multiplicity interval normalised to the multiplicity-integrated ones (relative yields). they are shown as a function of the multiplicity of charged particles normalised to the average value for inelastic collisions (relative charged-particle multiplicity). d$^0$, d$^+$ and d$^{*+}$ mesons are measured in five $p_{\rm t}$ intervals from 1 to 20 gev/$c$ and for $|y|&lt;0.5$ via their hadronic decays. the d-meson relative yield is found to increase with increasing charged-particle multiplicity. for events with multiplicity six times higher than the average multiplicity of inelastic collisions, a yield enhancement of a factor about 15 relative to the multiplicity-integrated yield in inelastic collisions is observed. the yield enhancement is independent of transverse momentum within the uncertainties of the measurement. the d$^0$-meson relative yield is also measured as a function of the relative multiplicity at forward pseudorapidity. the non-prompt j/$\psi$, i.e. the b hadron, contribution to the inclusive j/$\psi$ production is measured in the di-electron decay channel at central rapidity. it is evaluated for $p_{\rm t}&gt;1.3$ gev/$c$ and $|y|&lt;0.9$, and extrapolated to $p_{\rm t}&gt;0$. the fraction of non-prompt j/$\psi$ in the inclusive j/$\psi$ yields shows no dependence on the charged-particle multiplicity at central rapidity. charm and beauty hadron relative yields exhibit a similar increase with increasing charged-particle multiplicity. the measurements are compared to pythia 8, epos 3 and percolation calculations.
scandium complexes : physico-chemical study and evaluation of stability in vitro and in vivo for nuclear medicine application. among the different isotopes of scandium that can be used in nuclear medicine may be mentioned the ⁴⁷sc and ⁴⁴sc. the first decays by emitting an electron associated with a 159 kev gamma can thus be used either for radiotherapy or temp imaging. the ⁴⁴sc (3.97 h) decays in 94.27% in case by emitting a positron, with a γ photon energy equal to 1.157 mev. this isotope is then an ideal candidate for applications in pet imaging. currently, the cyclotron of high energy and high intensity arronax produce ⁴⁴sc and co-produces the isomeric state the ⁴⁴msc(2.44 d). the ⁴⁴msc has properties (eᵧ = 270 kev, 98.8%), which allows to consider its use as a potential in vivo generator. previous work had demonstrated that the dota ligand is most suitable and stable for sc. this thesis aims; make in evidence the feasibility of the in vivo ⁴⁴m/⁴⁴sc generator. initially a procedure was optimized and validated for the production of ⁴⁴m/⁴⁴sc with a high specific activity and chemical purity. radiolabeling of dota conjugated peptides was then developed and optimized. theoretical and experimental studies have been performed in order to demonstrate the feasibility of ⁴⁴m/⁴⁴sc as a potential in vivo generator. finally, in vitro stability studies on radiolabeled ⁴⁴m / ⁴⁴sc complexes were performed, followed by biodistribution studies and pet imaging.
packing curved objects. this paper deals with the problem of packing two-dimensional objects of quite arbitrary shapes including in particular curved shapes (like ellipses) and assemblies of them. this problem arises in industry for the packaging and transport of bulky objects which are not individually packed into boxes, like car spare parts. there has been considerable work on packing curved objects but, most of the time, with specific shapes; one famous example being the circle packing problem. there is much less algorithm for the general case where different shapes can be mixed together. a successful approach has been proposed recently in [martinez et al., 2013] and the algorithm we propose here is an extension of their work. mar-tinez et al. use a stochastic optimization algorithm with a fitness function that gives a violation cost and equals zero when objects are all packed. their main idea is to define this function as a sum of n!/(2!*(n-2)!) elementary functions that measure the overlapping between each pair of different objects. however, these functions are ad-hoc formulas. designing ad-hoc formulas for every possible combination of object shapes can be a very tedious task, which dramatically limits the applicability of their approach. the aim of this paper is to generalize the approach by replacing the ad-hoc formulas with a numerical algorithm that automatically measures the overlapping between two objects. then, we come up with a fully black-box packing algorithm that accept any kind of objects.[martinez et al., 2013] t. martinez, l. vitorino, f. fages, and a. aggoun. on solving mixed shapes packing problems by continuous optimization with the cma evolution strategy. in proceedings of the first brics countries congress on computational intelligence, 2013.
modeling software application front-ends: introducing the open source ifml graphical editor…. front-ends are important parts of software applications. they can change and evolve from occasionally to very often depending on the type of application. thus, it appears more and more important to be able to properly specify and/or represent them (e.g. to facilitate code generation and automation). based on the new interaction flow modeling language (ifml) standard from the omg, the ifml editor (http://ifml.github.io/) provides a complete open source modeling environment for expressing the content, user interaction and control behavior of software applications front-ends (also offering bindings to the persistence and business logical layers).this short talk is going to introduce this new eclipse-based ifml editor, developed with the sirius technology, and to briefly present its main features.
sensitivity-based pole and input-output errors of linear filters as indicators of the implementation deterioration in fixed-point context. input-output or poles sensitivity is widely used to evaluate the resilience of a filter realization to coefficients quantization in an fwl implementation process. however, these measures do not exactly consider the various implementation schemes and are not accurate in general case. this paper generalizes the classical transfer function sensitivity and pole sensitivity measure, by taking into consideration the exact fixed-point representation of the coefficients. working in the general framework of the specialized implicit descriptor representation, it shows how a statistical quantization error model may be used in order to define stochastic sensitivity measures that are definitely pertinent and normalized. the general framework of mimo filters and controllers is considered. all the results are illustrated through an example.
a language for the composition of privacy-enforcement techniques. today's large-scale computations, e.g., in the cloud, are subjectto a multitude of risks concerning the divulging and ownership ofprivate data. privacy risks are mainly addressed using a largevariety of encryption-based techniques. however, these are costlyto operate, lead to large aggregates of data that are highlyvaluable attack targets and do not allow to flexibly handlesubsets of such aggregates. furthermore, today's computations haveto ensure privacy properties in the context over highly variableand complex software compositions; however, no general support forthe declarative definition and implementation ofprivacy-preserving applications has been put forward.in this article, we present a compositional approach to thedeclarative and correct composition of privacy-preservingapplications in the cloud. our approach provides language supportfor the compositional definition of encryption- andfragmentation-based privacy-preserving algorithms. this languagecomes equipped with a set of laws that allows us to verify privacyproperties. finally, we introduce implementation support in scalathat ensures certain privacy properties by construction usingadvanced features of scala's type system.
an improved limit to the diffuse flux of ultra-high energy neutrinos from the pierre auger observatory. neutrinos in the cosmic ray flux with energies near 1 eev and above are detectable with the surface detector array of the pierre auger observatory. we report here on searches through auger data from 1 january 2004 until 20 june 2013. no neutrino candidates were found, yielding a limit to the diffuse flux of ultra-high energy neutrinos that challenges the waxman-bahcall bound predictions. neutrino identification is attempted using the broad time-structure of the signals expected in the sd stations, and is efficiently done for neutrinos of all flavors interacting in the atmosphere at large zenith angles, as well as for "earth-skimming" neutrino interactions in the case of tau neutrinos. in this paper the searches for downward-going neutrinos in the zenith angle bins 60∘−75∘ and 75∘−90∘ as well as for upward-going neutrinos, are combined to give a single limit. the 90% c.l. single-flavor limit to the diffuse flux of ultra-high energy neutrinos with an e−2 spectrum in the energy range 1.0×1017 ev - 2.5×1019 ev is e2νdnν/deν&lt;6.4×10−9 gev cm−2 s−1 sr−1.
a two-layer lpv based control strategy for input and state constrained problem: application to energy management. this paper proposes a pragmatic solution to solve input and state constrained control problems. taking benefits from a two-layer hierarchical architecture, in particular by working at a different period at each level, the general idea is to combine an explicit lpv controller ensuring the regulation task, and a predictive control at the upper level to comply with the active constraints.  in practice, the two layers are connected as the external loop drives the varying parameter of the inner loop. the proposed scheme is well suited mainly with control problems whose constraints violation risks may be predicted sufficiently in advance. the energy management of a hybrid vehicle is finally considered to illustrate the applicability.
map-based transparent persistence for very large models. the progressive industrial adoption of model-driven engineering (mde) is fostering the development of large tool ecosystems like the eclipse modeling project. these tools are built on top of a set of base technologies that have been primarily designed for small-scale scenarios, where models are manually developed. in particular, efficient runtime manipulation for large-scale models is an under-studied problem and this is hampering the application of mde to several industrial scenarios.in this paper we introduce and evaluate a map-based persistence model for mde tools. we use this model to build a transparent persistence layer for modeling tools, on top of a map-based database engine. the layer can be plugged into the eclipse modeling framework, lowering execution times and memory consumption levels of other existing approaches. empirical tests are performed based on a typical industrial scenario, model-driven reverse engineering, where very large software models originate from the analysis of massive code bases. the layer is freely distributed and can be immediately used for enhancing the scalability of any existing eclipse modeling tool.
optimization of a city logistics transportation system with mixed passengers and goods. in this paper, we propose a mathematical model and an adaptive large neighborhood search to solve a two{tiered transportation problem arising in the distribution of goods in congested city cores. in the first tier, goods are transported in city buses from a consolidation and distribution center to a set of bus 10 stops. the main idea is to use the buses spare capacity to drive the goods in the city core. in the second tier, final customers are distributed by a fleet of near-zero emissions city freighters. this system requires transferring the goods from buses to city freighters at the bus stops. we model the corresponding optimization problem as a variant of the pickup and delivery problem with transfers and solve it with an adaptive large neighborhood search. to evaluate its results, lower bounds are calculated with a column generation approach. the algorithm is assessed on data sets derived from a field study in the medium-sized city of la rochelle in france.
study of the energy and the radio emission point of cosmic rays. the purpose of the codalema experiment, installed at the nançay radio observatory (france), is to study the radio-detection of ultra-high-energy cosmic rays. distributed over an area of 0,25 km2, the original device uses in coincidence an array of particle detectors and an array of short antennas. a new analysis of the energy reconstruction from radio data obtained with this device is presented. we suggest that an energy resolution of less than 20% can be achieved and that, not only the lorentz force, but also another contribution proportional to all charged particles generated during the shower development, could play a significant role in the amplitude of the electric field measured by the antennas (as an effect of coherence or of charge excess). since 2011, a new array of radio-detectors, consisting of 60 stand-alone and self-triggered stations, has been in deployment over an area of 1.5 km2 around the first device. this new development leads to specific challenges which are discussed in terms of recognition of cosmic rays and reconstruction of the curvature of radio wave fronts. for commonly-used minimization algorithms, we emphasize the importance of the convergence process induced by the minimization ofa non-linear least squares function that affects the results in terms of degeneration of the solutions. we derive a simple method to obtain a satisfactory estimate of the location of the apparent emission source, which mitigates the problems previously.
some recent results of the codalema experiment. codalema is one of the experiments devoted to the detection of ultra high energy cosmic rays by the radio method. the main objective is to study the features of the radio signal induced by the development in the atmosphere of extensive air showers (eas) generated by cosmic rays in the energy range of 10 pev-1 eev . after a brief presentation of the detector features, the main results obtained are reported (emission mechanism, lateral distribution of the electric field, energy calibration, etc.). the first studies of the radio wave front curvature are discussed as new preliminary results.
measurement of the cosmic ray spectrum above 4×1018 ev using inclined events detected with the pierre auger observatory. a measurement of the cosmic-ray spectrum for energies exceeding 4×1018 ev is presented, which is based on the analysis of showers with zenith angles greater than 60∘ detected with the pierre auger observatory between 1 january 2004 and 31 december 2013. the measured spectrum confirms a flux suppression at the highest energies. above 5.3×1018 ev, the "ankle", the flux can be described by a power law e−γ with index γ=2.70±0.02(stat)±0.1(sys) followed by a smooth suppression region. for the energy (es) at which the spectral flux has fallen to one-half of its extrapolated value in the absence of suppression, we find es=(5.12±0.25(stat)+1.0−1.2(sys))×1019 ev.
coherent $\rho^0$ photoproduction in ultra-peripheral pb--pb collisions at $\mathbf{\sqrt{\textit{s}_{\rm nn}}} = 2.76$ tev. we report the first measurement at the lhc of coherent photoproduction of $\rho^0$ mesons in ultra-peripheral pb-pb collisions. the invariant mass and transverse momentum distributions for $\rho^0$ production are studied in the $\pi^+ \pi^-$ decay channel at mid-rapidity. the production cross section in the rapidity range $|y|&lt;0.5$ is found to be $\mathrm{d}\sigma/\mathrm{d}y = 425 \pm 10 \, (\mathrm{stat.})$ $^{+42}_{-50} \, (\mathrm{sys.})$ mb. coherent $\rho^0$ production is studied with and without requirement of nuclear breakup, and the fractional yields for various breakup scenarios are presented. the results are compared with those from lower energies and with model predictions based on the glauber model and the color dipole model. the measured cross section is found to be inconsistent with a scaling of the $\gamma$-nucleon cross section using the glauber model.
evidence for the charge-excess contribution in air shower radio emission observed by the codalema experiment. codalema is one of the pioneer experiments dedicated to the radio detection of ultra high energy cosmic rays (uhecr), located at the radio observatory of nançay (france). the codalema experiment uses both a particle detector array and a radio antenna array. data from both detection systems have been used to determine the ground coordinates of the core of extensive air showers (eas). we discuss the observed systematic shift of the core positions determined with these two detection techniques. we show that this shift is due to the charge-excess contribution to the total radio emission of air showers, using the simulation code selfas. the dependences of the radio core shift to the primary cosmic ray characteristics are studied in details. the observation of this systematic shift can be considered as an experimental signature of the charge excess contribution.
rapidity and transverse-momentum dependence of the inclusive j/$\mathbf{\psi}$ nuclear modification factor in p-pb collisions at $\mathbf{\sqrt{\textit{s}_{nn}}}=5.02$ tev. we have studied the transverse-momentum ($p_{\rm t}$) dependence of the inclusive j/$\psi$ production in p-pb collisions at $\sqrt{s_{\rm nn}} = 5.02$ tev, in three center-of-mass rapidity ($y_{\rm cms}$) regions, down to zero $p_{\rm t}$. results in the forward and backward rapidity ranges ($2.03 &lt; y_{\rm cms} &lt; 3.53$ and $-4.46 &lt;y_{\rm cms}&lt; -2.96$) are obtained by studying the j/$\psi$ decay to $\mu^+\mu^-$, while the mid-rapidity region ($-1.37 &lt; y_{\rm cms} &lt; 0.43$) is investigated by measuring the ${\rm e}^+{\rm e}^-$ decay channel. the $p_{\rm t}$ dependence of the j/$\psi$ production cross section and nuclear modification factor are presented for each of the rapidity intervals, as well as the j/$\psi$ mean $p_{\rm t}$ values. forward and mid-rapidity results show a suppression of the j/$\psi$ yield, with respect to pp collisions, which decreases with increasing $p_{\rm t}$. at backward rapidity no significant j/$\psi$ suppression is observed. theoretical models including a combination of cold nuclear matter effects such as shadowing and partonic energy loss, are in fair agreement with the data, except at forward rapidity and low transverse momentum. the implications of the p-pb results for the evaluation of cold nuclear matter effects on j/$\psi$ production in pb-pb collisions are also discussed.
measurement of pion, kaon and proton production in proton-proton collisions at $\sqrt{s}=7$ tev. the measurement of primary $\pi^{\pm}$, k$^{\pm}$, p and $\overline{p}$ production at mid-rapidity ($|y| &lt;$ 0.5) in proton-proton collisions at $\sqrt{s} = 7$ tev performed with alice (a large ion collider experiment) at the large hadron collider (lhc) is reported. particle identification is performed using the specific ionization energy loss and time-of-flight information, the ring-imaging cherenkov technique and the kink-topology identification of weak decays of charged kaons. transverse momentum spectra are measured from 0.1 up to 3 gev/$c$ for pions, from 0.2 up to 6 gev/$c$ for kaons and from 0.3 up to 6 gev/$c$ for protons. the measured spectra and particle ratios are compared with qcd-inspired models, tuned to reproduce also the earlier measurements performed at the lhc. furthermore, the integrated particle yields and ratios as well as the average transverse momenta are compared with results at lower collision energies.
development and optimization of targets dedicated to innovative radioisotope production for medical research (cu-67, ge-68 / ga-68) at the arronax cyclotron. nuclear medicine uses radioactive isotopes for diagnostic or therapeutic purposes. the activity of nuclear medicine today is made with a small number of radio-isotopes, but there is a demand for access to new isotopes like 68ga (diagnosis) and 67cu (therapy). these two isotopes can be produced on arronax and are the subject of this work.to produce 68ge, a target containing gallium (melting point: 30°c) must be used. during irradiation, gallium melts becoming a very corrosive liquid which causes the appearance of cracks that may destroy the target. to circumvent this problem, we developed a ga/ni alloy which remains solid under irradiation. ga3ni2 alloy, with a melting temperature of 369°c, is obtained by electroplating and was characterized by sem, edx, xrd and icp-oes. a first irradiation was performed to validate the production of 68ge and to inventory co-produced radioactive impurities.to produce 67cu, it is important to know the production cross sections 68zn(p,2p)67cu to optimize the irradiation parameters. data available in the literature show a large dispersion. this is due to the difficulty to separate 67cu and 67ga in the experiment. to improve our understanding of this reaction, we performed a new series of measurements using the "stacked foils" technique and an original chemical separation procedure. from the data obtained, we were able to determine the expected production yield for this reaction.
8be, 12 c, 16 o, ... nuclei and alpha clustering within a generalized liquid drop model. a liquid drop model previously used to describe smoothly the transition between two-(or three) body and one-body shapes in entrance and exit channels of nuclear reactions has been used to determine the potential barriers governing the evolution of the light nuclei : 8be, 12c, 16o, 20ne, 24mg and 32s.
outils de spéciation d'isotopes radioactifs innovants à l'échelle des ultra-traces pour la médecine nucléaire. null
effect of alpha radiation on the physical and chemical properties of silicate glasses. borosilicate glasses are intended to be used for the long-term confinement of high-level nuclear wastes. alpha particles from the minor actinides induce modifications of the glass structure which could deteriorate the efficiency of the confinement. external irradiation with 1 mev he ions and 7 mev au ions were performed in the son68 glass in order to simulate effect of alpha particles and recoils nucleus. dual beam irradiations composed by he+au ions were also investigated in order to simulate both effects of those two kind of particles. to understand the fundamental origin in physico-chemical properties, irradiation were also carried out on a 6 oxides borosilicate glass called international simplified glass (isg) and two commercially available glass planilux and spectrosil 2000, both from saint-gobain. the mechanical properties and chemical durability of each glass were studied as a function of the cumulated dose. results show that both alpha particles and heavy ions lead to variation in hardness, reduced young’s modulus and density. characterization techniques such as raman, rmn, and xps spectroscopy were used to analyze structural modifications induced by radiations. chemical durability of pristine and irradiated glasses was determined by monitoring the release of glass alteration elements b, li, si, mo and cs. the alteration layer was characterized by sem imaging and edx spectroscopy.
radiolytic corrosion occurring at the solid/solution interface investigated at subatech: example of uranium and titanium. null
bottom-up statistical analysis of the energy consumption of french single-family dwellings. implementing effective energy policies in the residential sector requires better understanding of the sources for the dispersion of energy consumption amongst households. bottom-up statistical models have been identified as one major modelling technique, particularly accounting for the diversity of inhabitants behaviours. in the various statistical models of the literature, behaviour characteristics are seldom incorporated in the data set but socioeconomic data are most often used as " proxy " of occupant behaviour. the present study is based on a detailed survey, combining energy billing data with technical, geographical, socioeconomic and behavioural variables. the corresponding sample, despite of its limited size (420 individuals), is representative of french households. a statistical model relating energy consumption to the other variables has been applied, enabling simultaneous use of quantitative and qualitative explanatory factors (ancova, analysis of covariance). the main determinants found for energy consumption in the sector of single-family dwellings in france are, by decreasing order of weights, surface area, type of main heating system, age of the household head and climate zone. we detected that the most influential behaviour variable is night temperature setting reduction. explanation and prediction capacities of the model as the accuracy of the model coefficients are studied and some possibilities of improvement are proposed.
ortho-positronium observation in the double chooz experiment. the double chooz experiment measures the neutrino mixing angle θ13 by detectingreactor ¯νe via inverse beta decay. the positron-neutron space and time coincidenceallows for a sizable background rejection, nonetheless liquid scintillator detectors wouldprofit from a positron/electron discrimination, if feasible in large detector, to suppress theremaining background. standard particle identification, based on particle dependent timeprofile of photon emission in liquid scintillator, can not be used given the identical mass ofthe two particles. however, the positron annihilation is sometimes delayed by the orthopositronium(o-ps) metastable state formation, which induces a pulse shape distortion thatcould be used for positron identification. in this paper we report on the first observation ofpositronium formation in a large liquid scintillator detector based on pulse shape analysisof single events. the o-ps formation fraction and its lifetime were measured, finding thevalues of 44 % ± 12 % (sys.) ± 5 % (stat.) and 3.68 ns ± 0.17 ns (sys.) ± 0.15 ns (stat.)respectively, in agreement with the results obtained with a dedicated positron annihilationlifetime spectroscopy setup.
mass transfer between a gas phase and two non miscible liquid phases.use of the equivalent absorption capacity concept for gas​/liquid​/liquidcontactor design. null
optimization of the volume fraction of an absorbent phase (silicone oil)​and biodegradation kinetics of dmds in a tppb. null
synthesizing realistic cloud workload traces for studying dynamic ressource system management. null
the epoc project: energy proportional and opportunistic computing system. with the emergence of the future internet and the dawning of new it models such as cloud computing, the usage of data centers (dc), and consequently their power consumption, increase dramatically. besides the ecological impact, the energy consumption is a predominant criteria for dc providers since it determines the daily cost of their infrastructure. as a consequence, power management becomes one of the main challenges for dc infrastructures and more generally for large-scale distributed systems. in this paper, we present the epoc project which focuses on optimizing the energy consumption of mono-site dcs connected to the regular electrical grid and to renewable energy sources.
reconnaître les régulations autonomes pour organiser le travail : l’exemple de la gestion de l’absentéisme en ehpad. null
the absenteeism of health care professionals. null
behavior of heptavalent technetium in sulfuric acid under a-irradiation: structural determination of technetium sulfate complexes by x-ray absorption spectroscopy and first principles calculations. null
spectrophotometric study of the behaviour of pertechnetate in trifluoromethanesulfonic acid: effect of alpha irradiation on the stability of tc(vii). this paper is devoted to the stability of pertechnetate in trifluoromethanesulfonic acid (htfms) in presence or absence of alpha irradiation. the irradiations were performed using alpha particles (4he2+) generated by arronax cyclotron with an external beam energy of 70 mev. the stability has been determined by uv-visible spectroscopy. in the absence of alpha irradiation, the results have shown that tc(vii) is reduced in htfms between 4 and 11 m. uv-visible spectroscopy measurements in 4-8 m show the presence of one phase of tc(iv) oxopolymeric species. at 9 m htfms, the change of uv-visible spectrum with respect to lower concentrations suggests the formation of a second new species of tc(iv). at highly concentrated htfms (11 m), the tc(vii) species remains stable. these findings exhibit the formation of reduced tc species by partial thermal decomposition and hydrolysis processes due to the highly exothermic hydration reaction of triflic acid with water. under alpha irradiation, the same reduced tc species as those observed without external irradiation were obtained with higher reduction kinetics. the formation of oxopolymeric technetium at important radiolytic yield resulted from reducing radiolytic products of both water and acid.
measurement of dijet &lt;mml:math altimg="si1.gif" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/xmlschema" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance" xmlns="http://www.elsevier.com/xml/ja/dtd" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/math/mathml" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd"&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant="normal"&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt; in p–pb collisions at &lt;mml:math altimg="si2.gif" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/xmlschema" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance" xmlns="http://www.elsevier.com/xml/ja/dtd" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/math/mathml" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd"&gt;&lt;mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant="normal"&gt;nn&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:msqrt&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;5.02&lt;/mml:mn&gt;&lt;mml:mtext&gt; tev&lt;/mml:mtext&gt;&lt;/mml:math&gt;. a measurement of dijet correlations in p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev with the alice detector is presented. jets are reconstructed from charged particles measured in the central tracking detectors and neutral energy deposited in the electromagnetic calorimeter. the transverse momentum of the full jet (clustered from charged and neutral constituents) and charged jet (clustered from charged particles only) is corrected event-by-event for the contribution of the underlying event, while corrections for underlying event fluctuations and finite detector resolution are applied on an inclusive basis. a projection of the dijet transverse momentum, $k_{\rm ty} = p_\rm{t,jet}^\rm{ch+ne} \; \rm{sin}(\delta\varphi_{\rm{dijet}})$ with $\delta\varphi_{\rm{dijet}}$ the azimuthal angle between a full and charged jet and $p_\rm{t,jet}^\rm{ch+ne}$ the transverse momentum of the full jet, is used to study nuclear matter effects in p-pb collisions. this observable is sensitive to the acoplanarity of dijet production and its potential modification in p-pb collisions with respect to pp collisions. measurements of the dijet $k_{\rm ty}$ as a function of the transverse momentum of the full and recoil charged jet, and the event multiplicity are presented. no significant modification of $k_{\rm ty}$ due to nuclear matter effects in p-pb collisions with respect to the event multiplicity or a pythia8 reference is observed.
characterization of radio transient signals at the pierre auger observatory. after more than a century of studies, one of the challenging questions related to ultra-high energy cosmic rays concerns their nature, which remains unclear. improving the knowledge about the composition of cosmic rays will permit to constrain the models concerning their origins and the production mechanisms in the astrophysical sources. simulations show that, the electric field emitted by the shower is sensitive to its development. this electric-field can be measured with a high duty cycle, and thus is apromising technique to identify an observable sensitive to the nature of the primary cosmic ray. the radio signal is also used to measure its arrival direction and its energy. since 2006, the pierre auger observatory hosts several radio detection arrays of cosmic rays, starting from small size prototypes (rauger, maxima) to achieve a large scale array of 124 radio stations: aera, the auger engineering radio array covering 6 km². these different arrays allow the study of the radio emission during the development of the shower in the mhz domain. aera is deployed in the low energy extension of the pierre auger observatory in order to have a larger statistics. it enables interesting hybrid measurements, with the comparison of radio observable with those obtained with the surface detector (sd) and the fluorescence telescopes close to the array. this thesis is dedicated to the characterization of the radio transient signals detected by rauger and aera. as one of the challenges of the radio detection of air-shower is to remove the anthropic background causing accidental triggering, methods for background rejection and sd-aera coincidences selection have been developed. a study of the correlation between the shower development in the atmosphere (longitudinal profile) and the electric-field measured by the radio stations is also presented. this study shows the relationship between the electric-field and the shower development in the atmosphere and confirms that the radio signal is a powerful tool to study the nature of the ultra-high energy cosmic rays.
charm and prompt photon production with the event generator epos. at the lhc, strong interaction is studied by doing collisions of high energy particles. in the case of nucleus-nucleus collision (lead at the lhc), a new state of matter, called quarks gluons plasma (qgp), is created. the study of this qgp is currently a lively research field. hard probes, like heavy quarks and prompt photons, are produced during early times of collisions done at the lhc. this is why they are ideal probes for the study of the qgp. they will go through and interact with the medium produced by the collision. a comparison with a case without qgp (proton-proton- collision) will allow us to see how hard probes properties are modified by themedium. then, medium properties like temperature and density can be extracted. this study requires a good understanding of hard probes production in proton-proton collisions. the aim of my thesis is the implementation of heavy quarks and prompt photons in the event generator epos (computer code for colliders), for p-p collisions. our final aim is the study of the qgp in pb-pb collisions.
aqueous and water vapour alteration of the son68 glass at low temperature (35-90°c). the son68 glass is initially altered in dynamic mode under silica rich cox water (42 mg/l) at ph8, high s/v ratio (14000 m⁻¹) and at 35, 50 and 90°c. the results showed that the glass alteration seems to be governed by both diffusion and surface reaction process. the residual rate at 90°c is around 10-4 g.m⁻².d⁻¹. the activation energy is about 70 kj.mol⁻¹. the dissolution /precipitation and hydrolysis/condensation mechanisms are responsible for the development of the alteration layer. mg silicates and calcites precipitate at 35 and 50°c, the same phases in addition to powellite and apatite precipitate at 90°c. the results predicted by the model reproduce well experimental data. the glass is then hydrated at temperatures ranging from 35 to 125°c and relative humidity values (rh) between 92 an 99.9%. the glass hydration increases with the temperature and rh, the hydration energy is about 34.2 kj.mol⁻¹. the alteration layers thicknesses vary between 0.3μm at 35°c and 5μm at 125°c. the alteration layer is depleted in (b, li, na) and enriched in (si, al, fe, zn and ni). the secondary phases are calcite, powellite, apatite and tobermorite in adition to a hydration gel. the effect of near field materials on the ²⁹si doped son68 glass alteration was studied. the presence of steel increases the ph and decreases the si and mo concentrations without changing the overall rate of glass corrosion. the si is retained on the steel corrosion products, its concentration in solution seems to be controlled by the clay dissolution. the glass corrosion in the presence of steel and clay at 90°c leads to the formation of magnetite, siderite, ironsilicates, pure silica, iron sulphur (pyrite, troilite,pyrrhotite and mackinawite), calcite, apatite, powellite and mg silicates. the modelling results agree well with the experimental data.
a logical study of program equivalence. proving program equivalence for a functional language with references is a notoriously difficult problem. the goal of this thesis is to propose a logical system in which such proofs can be formalized, and in some cases inferred automatically. in the first part, a generic extension method of dependent type theory is proposed, based on a forcing interpretation seen as a presheaf translation of type theory. this extension equips type theory with guarded recursive constructions, which are subsequently used to reason on higher-order references. in the second part, we define a nominal game semantics for a language with higher-order references. it marries the categorical structure of game semantics with a trace representation of denotations of programs, which can be computed operationally and thus have good modularity properties. using this semantics, we can prove the completeness of kripke logical relations defined in a direct way, using guarded recursive types, without using biorthogonality. such a direct definition requires omniscient worlds and a fine control of disclosed locations. finally, we introduce a temporal logic which gives a framework to define these kripke logical relations. the problem of contextual equivalence is then reduced to the satisfiability of an automatically generated formula defined in this logic, i.e. to the existence of a world validating this formula. under some conditions, this satisfiability can be decided using a smt solver. completeness of our methods opens the possibility of getting decidability results of contextual equivalence for some fragments of the language, by giving an algorithm to build such worlds.
modèle logique avancé (plateforme opencloudware). t. aubonnet, e. madelaine , l. henrio , t. ledoux , y. kouki , p. moreaux , f. pourraz , i. ayadi , n. simoni - modèle logique avancé, date de dépot: 2013/10/18, nb pages 1- 28, (tech. rep.: cedric-13-2871).
effective bond orders from two-step spin-orbit coupling approaches: the i&lt;sub&gt;2&lt;/sub&gt;, at&lt;sub&gt;2&lt;/sub&gt; , io&lt;sup&gt;+&lt;/sup&gt;, and ato&lt;sup&gt;+&lt;/sup&gt; case studies. the nature of chemical bonds in heavy main-group diatomics is discussed from the viewpoint of effective bond orders, which are computed from spin-orbit wave functions resulting from contracted spin-orbit configuration interaction calculations. the reliability of the relativistic correlated wave functions obtained in such two-step spin-orbit coupling frameworks is assessed by benchmark studies of the spectroscopic constants with respect to either experimental data, or state-of-the-art fully relativis-tic correlated calculations. the i&lt;sub&gt;2&lt;/sub&gt;, at&lt;sub&gt;2&lt;/sub&gt; , io&lt;sup&gt;+&lt;/sup&gt;, and ato&lt;sup&gt;+&lt;/sup&gt; species are considered, and differences and similarities between the astatine and iodine elements are highlighted. in particular, we demonstrate that spin-orbit coupling weakens the covalent character of the bond in at&lt;sub&gt;2&lt;/sub&gt; even more than electron correlation, making the consideration of spin-orbit coupling compulsory for discussing chemical bonding in heavy (6&lt;i&gt;p&lt;/i&gt;) main group element systems.
measurement of charged jet production cross sections and nuclear modification in p-pb collisions at $\sqrt{s_\rm{nn}} = 5.02$ tev. charged jet production cross sections in p-pb collisions at $\sqrt{s_{\rm nn}} = 5.02$ tev measured with the alice detector at the lhc are presented. using the anti-$k_{\rm t}$ algorithm, jets have been reconstructed in the central rapidity region from charged particles with resolution parameters $r = 0.2$ and $r = 0.4$. the reconstructed jets have been corrected for detector effects and the underlying event background. to calculate the nuclear modification factor, $r_{\rm ppb}$, of charged jets in p-pb collisions, a pp reference was constructed by scaling previously measured charged jet spectra at $\sqrt{s} = 7$ tev. in the transverse momentum range $20 \le p_{\rm t,ch\ jet} \le 120$ gev/$c$, $r_{\rm ppb}$ is found to be consistent with unity, indicating the absence of strong nuclear matter effects on jet production. major modifications to the radial jet structure are probed via the ratio of jet production cross sections reconstructed with the two different resolution parameters. this ratio is found to be similar to the measurement in pp collisions at $\sqrt{s} = 7$ tev and to the expectations from pythia pp simulations and nlo pqcd calculations at $\sqrt{s_{\rm nn}} = 5.02$ tev.
synthesis and structures of plutonyl nitrate complexes: is plutonium heptavalent in puo&lt;sub&gt;3&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; ?. gas-phase plutonium nitrate anion complexes were produced by electrospray ionization (esi) of a plutonium nitrate solution. the esi mass spectrum included species with all four of the common oxidation states of plutonium: pu(iii), pu(iv), pu(v) and pu(vi). plutonium nitrate complexes were isolated in a quadrupole ion trap and subjected to collision induced dissociation (cid). cid of complexes of the general formula puo&lt;sub&gt;x&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;y&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; resulted in the elimination of no&lt;sub&gt;2&lt;/sub&gt; to produce puo&lt;sub&gt;x+1&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;y−1&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, which in most cases corresponds to an increase in the oxidation state of plutonium. plutonyl species, pu&lt;sup&gt;v&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; and pu&lt;sup&gt;vi&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;3&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, were produced from pu&lt;sup&gt;iii&lt;/sup&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;4&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; and pu&lt;sup&gt;iv&lt;/sup&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;5&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, respectively, by the elimination of two no&lt;sub&gt;2&lt;/sub&gt; molecules. cid of pu&lt;sup&gt;vi&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;3&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; resulted in no&lt;sub&gt;2&lt;/sub&gt; elimination to yield puo&lt;sub&gt;3&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, in which the oxidation state of plutonium could be vii, a known oxidation state in condensed phase but not yet in the gas phase. density functional theory confirmed the nature of pu&lt;sup&gt;v&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; and pu&lt;sup&gt;vi&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;3&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; as plutonyl(v/vi) cores coordinated by bidentate equatorial nitrate ligands. the computed structure of puo&lt;sub&gt;3&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt; is essentially a plutonyl(vi) core, pu&lt;sup&gt;vi&lt;/sup&gt;o&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2+&lt;/sup&gt;, coordinated in the equatorial plane by two nitrate ligands and one radical oxygen atom. the computations indicate that in the ground spin-orbit free state of puo&lt;sub&gt;3&lt;/sub&gt;(no&lt;sub&gt;3&lt;/sub&gt;)&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;−&lt;/sup&gt;, the unpaired electron of the oxygen atom is antiferromagnetically coupled to the spin-triplet state of the plutonyl core. the results indicate that pu(vii) is not a readily accessible oxidation state in the gas phase, despite that it is stable in solution and solids, but rather that a pu(vi)-o• bonding configuration is favored, in which an oxygen radical is involved.
etude des mécanismes diffusionnels impliqués dans la séparation cinétique co2-ch4 sur des tamis moléculaires carbonés. null
core library for advanced scenario simulation, c.l.a.s.s.: principle &amp; application. the global warming, the increase of world population and the depletion of fossil resourceshave lead us in a major energy crisis. using electronuclear energy could be one of the meansto solve a part of these issues. the way out of this crisis may be enlightened by the study oftransitional scenarios, guiding the political decisions. the reliability of those studies passesthrough the wide variety of the simulation tools and the comparison between them.from this perspective and in order to perform complex electronuclear scenario simulation,the open source core library for advance scenario simulation (class) is being developed.class main asset is its ability to include any kind of reactor, whether the system is innovativeor standard. a reactor is fully described by its evolution database that must contain a set ofdifferent fuel compositions in order to simulate transitional scenarios. class aims at being auseful tool to study scenarios involving generation iv reactors as well as innovative fuel cycles,like the thorium cycle.the following contribution will present in detail the class software. starting with the workingprinciple of this tool, one will explain the working process of the different modules suchas the evolution module. it will be followed by an exhaustive presentation of the uox-moxbases generation procedure. finally a brief analysis of the error made by the class evolutionmodule will be presented.
effect of heterogeneity in plutonium recycling in steady state pwr. the possible delay of decades for the deployment of fourth generation reactors brings upnew issues. countries like france which are storing plutonium for years in prediction ofstarting fbr could have to adapt their plutonium management strategy.we have compared different strategies for plutonium recycling in pwr reactors. we havechosen to limit the scope of this study to pwrs considering the standard uranium cycle,and we investigated the influence of the heterogeneity in the assembly that would containthe recycled plutonium. the comparison of different strategies is made at steady-state.this paper present a specific method developed to allow complete and detailed studiesof equilibrium scenarios and how it has been implemented and integrated inside the opensourcemure package.we will then discuss of the results obtained through this method apply to pwrs loadedwith homogeneous and heterogeneous assemblies using the following criteria: resourceconsumption, pu inventories in the cycle and waste production.
human ficolin-2 recognition versatility extended: an update on the binding of ficolin-2 to sulfated/phosphated carbohydrates. ficolin-2 has been reported to bind to dna and heparin, but the mechanism involved has not been thoroughly investigated. x-ray studies of the ficolin-2 fibrinogen-like domain in complex with several new ligands now show that sulfate and phosphate groups are prone to bind to the s3 binding site of the protein. composed of arg132, asp133, thr136 and lys221, the s3 site was previously shown to mainly bind n-acetyl groups. furthermore, dna and heparin compete for binding to ficolin-2. mutagenesis studies reveal that arg132, and to a lesser extent asp133, are important for this binding property. the versatility of the s3 site in binding n-acetyl, sulfate and phosphate groups is discussed through comparisons with homologous fibrinogen-like recognition proteins.
anthropogenic tritium in the loire river estuary &amp; hydrogen lability in matrices of interest. null
experimental setup for the determination of exchangeable hydrogen in environmental samples using deuterium and tritium. null
transport properties of iodide in a sandy aquifer: hydrogeological modelling and field tracer tests. the release of radioactive iodine into geological media from nuclear waste disposal is an issue that has tobe considered since iodine is a biophilic element. 129i is, with 99tc, one of the two long-lived radionuclidesthat have the highest mobility in radioactive waste disposal. within this context, iodide retardation is stilla matter of debate. a low value of the retardation factor is generally accepted in soils without organicmatter, but the possibility for sorption cannot be completely ruled out. since isotopic exchange with naturallyoccurring iodine is one of the main potential sorption mechanisms, site-specific retention parametersare needed. in the present paper, we study iodide transport in a sandy aquifer. a hydrogeologicalmodel was built to fit deuterium, bromide and iodide breakthrough data from in situ tracer test experiments.within the precision range of the fitting, iodide is excluded from 2.5% of the effective porosity byanionic exclusion and presents a field retention factor (kd) lower than 0.025 l/kg.
risks and prevention of voc adsorption in installation. null
measurement of jet suppression in central pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. the transverse momentum ($p_{\rm t}$) spectrum and nuclear modificationfactor ($r_{\rm aa}$) of reconstructed jets in 0-10% and 10-30% central pb-pbcollisions at $\sqrt{s_{\rm nn}}=2.76$ tev were measured. jets werereconstructed from charged and neutral particles, utilizing the alice trackingdetectors and electromagnetic calorimeter (emcal), with the anti-$k_{\rm t}$jet algorithm with a resolution parameter of r=0.2. the jet $p_{\rm t}$ spectraare reported in the pseudorapidity interval of $|{\eta}_{\rm jet}|&amp;lt;0.5$ for$40&amp;lt;p_{\rm t,jet}&amp;lt;120$ gev/$c$ in 0-10% and for $30&amp;lt;p_{\rm t,jet}&amp;lt;100$ gev/$c$in 10-30% collisions. reconstructed jets were required to contain a leadingcharged particle with $p_{\rm t}&amp;gt;5$ gev/$c$ to suppress jets constructed fromthe combinatorial background in pb-pb collisions. the effect of the leadingcharged particle requirement has been studied in both pp and pb-pb collisionsand has been shown to have negligible effects on the $r_{\rm aa}$ within theuncertainties of the measurement. the nuclear modification factor is obtainedby dividing the jet spectrum measured in pb-pb by that in pp collisions scaledby the number of independent nucleon-nucleon collisions estimated using aglauber model. $r_{\rm aa}$ is found to be $0.28\pm0.04$ in 0-10% and$0.35\pm0.04$ in 10-30% collisions, independent of $p_{\rm t,jet}$ within theuncertainties of the measurement. the observed suppression is in fair agreementwith expectations from two model calculations with different approaches to jetquenching.
precision measurements of cosmic ray air showers. null
coherent radio emission from the cosmic ray air shower extinction at the ground level. null
latest upgrades and results from the codalema experiment. null
investigation of extensive air shower properties with the codalema experiment: tackling the challenges of the next generation cosmic ray observatory. null
radiodetection of extensive air showers at the pierre {a}uger observatory. null
radio detection of air showers with aera. null
some possible interpretations from data of the codalema experiment. null
simulation of radio emission from cosmic ray air shower with selfas2. null
autonomous detection and analysis of radio emission from air showers at the pierre auger observatory. null
charge excess signature in the codalema data. interpretation with selfas2. null
first results of the standalone antenna array of the codalema radio detection experiment. null
the tianshan radio experiment for neutrino detection. null
the first generation experiments for cosmic ray radiodetection. null
latest results of the codalema experiment: cosmic rays radio detection in a self trigger mode. null
validity area of spheric reconstruction. null
angular resolution estimation using airplane signals with the new rauger setup. null
the first platinium event: fd, sd and radio. null
airplane signals detected by the rauger radio stations: spectral and time domain characteristics, inter-calibration and polarization studies. null
estimation of the azimuth of cosmic rays using data from a single radio station. null
two methods for rejecting background radio traces in rauger data at the level of a single station (t1 or t2). null
aera central trigger. null
coincidences searches with the kit/buw stations. null
calculating the vector equivalent length of the butterfly antenna from nec2 by simulating the antenna in transmitting mode. null
the radio detection of extensive air showers : tackling the challenges of the next generation cosmic ray observatory. null
technological developments for the auger engineering radio array (aera). null
characterisation of the radio signal emission from extensive air showers using the selfas code. null
investigating the extensive air shower properties using the polarization and frequency features of the radio signals measured by the codalema autonomous station array. null
a dedicated antenna array for radio detection of extended air showers. radio pulses associated with extended air showers (eas) produced in terrestrial atmosphere by high energycosmic rays (uhecr) of energy 10^17 ev and above, are now routinely observed by dedicated radioinstruments on ground. this may offer a new and appealing way for elucidating the nature and origin ofinvolved primary particles, an open question still unsolved.unfortunately, the high occupancy of the electromagnetic spectrum by undesired signals from natural andanthropogenic origins has made unambiguous eas radio detection a challenging problem. former attemptsbased on timing coincidences from several independent radio antennas, or using auxiliary triggering byconventional particle detectors, are still not fully satisfying.we present here a solution based on real time, coherent radio detection by using a small array of 10x2 crosspolarized dipoles, distributed over a 150m x 150m surface area and operated in continuous sky surveyingmode.preliminary results obtained with the new system are briefly reviewed and discussed.the final detection scheme will be achieved by using on line, fast computing software based on a dedicatedunsupervised recognition algorithm.the new array is a part of the codalema experiment located in nançay radio astronomy observatory(france).
new developments around the butterfly antenna. null
study of ultra-high energy cosmic rays through their radio signal in the atmosphere. null
radio emission from the air shower sudden death. null
the pierre auger cosmic ray observatory. the pierre auger observatory, located on a vast, high plain in western argentina, is the world's largest cosmic ray observatory. the objectives of the observatory are to probe the origin and characteristics of cosmic rays above $10^{17}$ ev and to study the interactions of these, the most energetic particles observed in nature. the auger design features an array of 1660 water-cherenkov particle detector stations spread over 3000 km$^2$ overlooked by 24 air fluorescence telescopes. in addition, three high elevation fluorescence telescopes overlook a 23.5 km$^2$, 61 detector infill array. the observatory has been in successful operation since completion in 2008 and has recorded data from an exposure exceeding 40,000 km$^2$ sr yr. this paper describes the design and performance of the detectors, related subsystems and infrastructure that make up the auger observatory.
muons in air showers at the pierre auger observatory: mean number in highly inclined events. we present the first hybrid measurement of the average muon number in air showers at ultra-high energies, initiated by cosmic rays with zenith angles between $62^\circ$ and $80^\circ$. the measurement is based on 174 hybrid events recorded simultaneously with the surface detector array and the fluorescence detector of the pierre auger observatory. the muon number for each shower is derived by scaling a simulated reference profile of the lateral muon density distribution at the ground until it fits the data. a $10^{19}$~ev shower with a zenith angle of $67^\circ$, which arrives at the surface detector array at an altitude of 1450 m above sea level, contains on average $(2.68 \pm 0.04 \pm 0.48\,\mathrm{(sys.)}) \times 10^{7}$ muons with energies larger than 0.3 gev. the logarithmic gain $\mathrm{d}\ln{n_\mu} / \mathrm{d}\ln{e}$ of muons with increasing energy between $4\times 10^{18}$ ev and $5 \times 10^{19}$ ev is measured to be $(1.029\, \pm\, 0.024\, \pm 0.030\,\mathrm{(sys.)})$.
two-pion femtoscopy in p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev. we report the results of the femtoscopic analysis of pairs of identical pions measured in p-pb collisions at $\sqrt{s_{\mathrm{nn}}}=5.02$ tev. femtoscopic radii are determined as a function of event multiplicity and pair momentum in three spatial dimensions. as in the pp collision system, the analysis is complicated by the presence of sizable background correlation structures in addition to the femtoscopic signal. the radii increase with event multiplicity and decrease with pair transverse momentum. when taken at comparable multiplicity, the radii measured in p-pb collisions, at high multiplicity and low pair transverse momentum, are 10-20% higher than those observed in pp collisions but below those observed in a-a collisions. the results are compared to hydrodynamic predictions at large event multiplicity as well as discussed in the context of calculations based on gluon saturation.
forward-backward multiplicity correlations in pp collisions at $\sqrt{s}$=0.9, 2.76 and 7 tev. the strength of forward-backward (fb) multiplicity correlations is measured by the alice detector in proton-proton (pp) collisions at $\sqrt{s}=0.9$, 2.76 and 7 tev. the measurement is performed in the central pseudorapidity region ($|\eta| &lt; 0.8$) for the transverse momentum $p_{\rm t}&gt;0.3$ gev/$c$. two separate pseudorapidity windows of width ($\delta \eta$) ranging from 0.2 to 0.8 are chosen symmetrically around $\eta=0$. the multiplicity correlation strength ($b_{\rm cor}$) is studied as a function of the pseudorapidity gap ($\eta_{\rm gap}$) between the two windows as well as the width of these windows. the correlation strength is found to decrease with increasing $\eta_{\rm gap}$ and shows a non-linear increase with $\delta\eta$. a sizable increase of the correlation strength with the collision energy, which cannot be explained exclusively by the increase of the mean multiplicity inside the windows, is observed. the correlation coefficient is also measured for multiplicities in different configurations of two azimuthal sectors selected within the symmetric fb $\eta$-windows. two different contributions, the short-range (sr) and the long-range (lr), are observed. the energy dependence of $b_{\rm cor}$ is found to be weak for the sr component while it is strong for the lr component. moreover, the correlation coefficient is studied for particles belonging to various transverse momentum intervals chosen to have the same mean multiplicity. both sr and lr contributions to $b_{\rm cor}$ are found to increase with $p_{\rm t}$ in this case. results are compared to pythia and phojet event generators and to a string-based phenomenological model. the observed dependencies of $b_{\rm cor}$ add new constraints on phenomenological models.
non-linear control of a narrow tilting vehicle. — narrow tilting vehicles (ntvs) are the convergence of a car and a motorcycle. they are expected to be the new generation of city cars considering their practical dimensions and lower energy consumption. but considering their height to breadth ratio, in order to maintain lateral stability, ntvs should tilt when cornering. unlike the motorcycle's case, where the driver tilts the vehicle himself, the tilting of an ntv should be automatic. two tilting systems are available; direct and steering tilt control, the combined action of these two systems being certainly the key to improve considerably ntvs dynamic performances. focusing on the lateral dynamic of ntvs, multivariable control strategies based on linear robust control theory, were already proposed in the literature, assuming decoupling with the longitudinal dynamic. in this paper a 4 dof model of the main longitudinal and lateral dynamics is considered, and its differential flatness is demonstrated. the three flat outputs have furthermore a particular physical meaning, making possible the design of a simple external control loop complying with the driver demands.
tools developed by subatech for radiolysis studies in solution and/or at the interface. null
cross section measurements of deuteron induced nuclear reactions on natural tungsten up to 34 mev. 186gre is a β-/γ emitter of great interest for nuclear medicine. it has shown successful results on bone metastases palliation and has similar chemical properties as 99mtc, the most commonly used imaging agent. 186gre is routinely produced using rhenium target in nuclear reactor. higher speciﬁc activity could be obtained using accelerators. in this paper, production cross section values are presented for the natw (d, x)186gre reaction up to 34 mev, using the stacked-foils method and gamma spectrometry. from this data set, the thick target production yield of 186gre is determined and compared with the validated values of the iaea and also with the proton route. the production cross sections of the natw(d, x)183,182g,184m,184g,181re and natw(d, x)187w reactions have also been  determined.  a  good  agreement is found with the literature. our data are compared with the version 1.6 (december 2013) of the talys code which shows discrepancies both on the shape and on the amplitude for these deuteron induced reactions.
spectral assignment for neutral-type systems and moment problems. for a large class of linear neutral-type systems the problem of assigning eigenvalues and eigenvectors is investigated, i.e. finding the system that has the given spectrum and, in some sense, allmost all eigenvectors. the solution of this problem enables vector moment problems to be considered using the construction of a neutral-type system. the exact controllability property of the system obtained gives the solution of the vector moment problem.
impact of colloids on uranium transport in groundwater applied to the aube radioactive waste disposal. the presence of colloids, known vectors of radionuclides and chemical contaminantsin groundwater, has been identified in groundwater at the aube radioactivewaste disposal in 2004. this thesis aims to characterize these colloids, and todetermine their potential impact in the transport of uranium, chosen as the elementof interest for this study. the identified 60 nm in diameter clay colloids and thefulvic and humic acids can move in aptian groundwater, as indirectly evidenced bycolumn experiments. a feasibility study of a in situ test has been done througha transport modeling to confirm the colloid mobility at the field scale. using theconditions of the study, the clay colloids do not influence uranium transport. evenwith the greatest concentration assumed on site, they have a very limited impact onthe mobilization of uranium, in the ph range measured on site. on the contrary, theorganic colloids, despite their low concentration, can facilitate uranium transport,the uranyl - organic acid chemical bond being exceptionally strong. therefore theirlow concentration in groundwater makes their impact on uranium mobility equallyinsignificant.
modélisation de molécules et matériaux d'intérêt en radiochimie. null
microbial aerosol filtration: growth and release of bacteria-fungi consortium collected by fibrous filters in different operating conditions. null
centrality dependence of particle production in p-pb collisions at $\sqrt{s_{\rm nn} }$= 5.02 tev. we report measurements of the primary charged particle pseudorapidity density and transverse momentum distributions in p-pb collisions at $\sqrt{s_{\rm nn}}$ = 5.02 tev, and investigate their correlation with experimental observables sensitive to the centrality of the collision. centrality classes are defined using different event activity estimators, i.e. charged particle multiplicities measured in three disjunct pseudorapidity regions as well as the energy measured at beam rapidity (zero-degree). the procedures to determine the centrality, quantified by the number of participants ($n_{\rm part}$), or the number of nucleon-nucleon binary collisions ($n_{\rm coll}$), are described. we show that, in contrast to pb-pb collisions, in p-pb collisions large multiplicity fluctuations together with the small range of participants available, generate a dynamical bias in centrality classes based on particle multiplicity. we propose to use the zero-degree energy, which we expect not to introduce a dynamical bias, as an alternative event-centrality estimator. based on zero-degree energy centrality classes, the $n_{\rm part}$ dependence of particle production is studied. under the assumption that the multiplicity measured in the pb-going rapidity region scales with the number of pb-participants, an approximate independence of the multiplicity per participating nucleon measured at mid-rapitity of the number of participating nucleons is observed. furthermore, at high-$p_{\rm t}$ the p-pb spectra are found to be consistent with the pp spectra scaled by $n_{\rm coll}$ for all centrality classes. our results represent valuable input for the study of the event activity dependence of hard probes in p-pb collision and, hence, help to establish baselines for the interpretation of the pb-pb data.
use of fluorescence spectroscopy and voltammetry for the analysis of metal- organic matter interactions in the new caledonia lagoon. fluorescence, polarographic and potentiometric analysis of sea water from the new caledonia lagoon (located south of noumea) allowed the determination of the specific properties of the dissolved and particulate phases of organic matter (om)-metal complexes according to various regions of the lagoon. in particular, om complexes with ni, zn, pb, cu, cd were chosen in this study due to the sensitivity of these complexes to affect biocenosis of the nearby enclosed coral reef as well as their availability to enter the coast from erosion (terrigenous om) or human activities from nickel extraction or pollution from waste sites (anthopogenic om) that exist throughout new caledonia. combined with geochemical modelling, the om-metal complexes analysis allowed the determination of their conditional stability constants which in turn helped in predicting the fate of the metal pollution in the lagoon. for the first time, fluorescence, polarographic and potentiometric techniques combined with geochemical models that employed discrete pka distribution on om enabled the determination of the origin of the om, as either natural or anthopogenic.
fuml as an assembly language for model transformation. within a given modeling platform, modeling tools, such as model editors and transformation engines, interoperate efficiently. they are generally written in the same general-purpose language, and use a single modeling framework (i.e., an api to access models). however, interoperability between tools from different modeling platforms is much more problematic.in this paper, we propose to leverage fuml in order to address this issue by providing a common execution language. modeling frameworks can then be abstracted into generic actions that perform elementary operations on models. not only can user models benefit from a unified execution semantics, but modeling tools can too.we support this proposal by showing how it can apply to a model transformation engine. to this end, a prototype compiler from atl to fuml has been built, and is described. finally, we conclude that fuml has some useful properties as candidate common execution language for mde, but lacks some features.
btrplace: flexible vm management in data centers. null
the non-overlapping constraint between objects described by non-linear inequalities. packing 2d objects in a limited space is an ubiquitous problem with many academic and industrial variants. in any case, solving this problem requires the ability to determine where a first object can be placed so that it does not intersect a second, previously placed, object. this subproblem is called the non-overlapping constraint. the complexity of this non-overlapping constraint depends on the type of objects considered. it is simple in the case of rectangles. it has also been studied in the case of polygons. this paper proposes a numerical approach for the wide class of objects described bynon-linear inequalities. our goal here is to calculate the non-overlapping constraint, that is, to describe the set of all positions and orientations that can be assigned to the first object so that intersection with the second one is empty. this is done using a dedicated branch &amp; bound approach. we first show that the non-overlapping constraint can be cast into a minkowski sum, even if we take into account orientation. we derive from this an innercontractor, that is, an operator that removes from the current domain a subset of positions and orientations that necessarily violate the non-overlapping constraint. this inner contractor is then embedded in a sweeping loop, a pruning technique that was only used with discrete domains so far. we finally come up with a branch &amp; bound algorithm that outperforms the generic state-of-the-art solver rsolver.
a model-driven approach to generate external dsls from object-oriented apis. developers in modern general-purpose programming languages cre-ate reusable code libraries by encapsulating them in applications programming interfaces (apis). domain-specific languages (dsls) can be developed as an al-ternative method for code abstraction and distribution, sometimes preferable to apis because of their expressivity and tailored development environment. how-ever the cost of implementing a fully functional development environment for a dsl is generally higher. in this paper we propose dslit, a prototype-tool that, given an existing api, reduces the cost of developing a corresponding dsl by analyzing the api, automatically generating a semantically equivalent dsl with its complete development environment, and allowing for user customization. to build this bridge between the api and dsl technical spaces we make use of exist-ing model-driven engineering (mde) techniques, further promoting the vision of mde as a unifying technical space.
on the exact controllability and observability of neutral type systems. neutral type systems considered in infinite-dimensional hilbert space are analyzed for exact controllability characterization. the approach is based on the problem of moments using a riesz basis of eigenvectors. the duality with observability is inves-tigated. a criterion of exact observability is deduced.
model of dynamic interactions. in robotic-based machining, an interaction between the workpiece and technological tool causes essential deflections that significantly decrease the manufacturing accuracy. relevant compliance errors highly depend on the manipulator configuration and essentially differ throughout the workspace. their influence is especially important for heavy serial robots. to overcome this difficulty this report presents a new technique for compensation of the compliance errors caused by technological process. in contrast to previous works, this technique is based on the non-linear stiffness model and the reduced elasto-dynamic model of the robotic based milling process. the advantages and practical significance of the proposed approach are illustrated by milling with of kuka kr270. it is shown that after error compensation technique significantly increase the accuracy of milling.
tools for the identification of robot stiffness parameters using cad software. this report proposes a cad-based approach for identification  of the elasto-static parameters of the robotic manipulators. the main contributions are in the areas of virtual experiment planning and algorithmic data processing, which allows to obtain the stiffness matrix with required accuracy. in contrast to previous works, the developed technique operates with the deflection field produced by virtual experiments in a cad environment. the proposed approach provides high identification accuracy (about 0.1% for the stiffness matrix element) and is able to take into account the real shape of the link, coupling between rotational/translational deflections and joint particularities. to compute the stiffness matrix, the numerical technique has been developed, and some recommendations for optimal settings of the virtual experiments are given. in order to minimize the identification errors, the statistical data processing technique was applied. the advantages of the developed approach have been confirmed by case studies dealing with the links of parallel manipulator of the orthoglide family, for which the identification errors have been reduced to 0.1%.
simulation results using a robot with flexibilities for machining and welding. the objective of this report is to detail the models used in simulation and the results obtained in simulation for both machining and fsw process.this report contains in a first part the details of modeling flexibilities of serial robots primarily through a model of localized flexibilities. the flexibilities are expressed both in cartesian space and in the joint space and taking into account possible couplings.the second part deals with the dynamic model used in the simulator and the simulation environment. a significant work was to also model the company kuka robot controller. machining processes and fsw are modeled by simple models but reflecting the reality of the behavior.
robot comparison based on local and global indices proposed and related to fsw welding and machining. this deliverable deals with the comparison of robots as a function of local and global indices related to machining operations of metallic and composite parts and friction stir welding. some typical industrial operations are first presented. then, some local and global performances indices are to machining operations of metallic and composite parts and friction stir welding are presented. a new method for the stiffness modeling of serial and parallel manipulators is also introduced. as a matter of fact, some performance indices depend on the stiffness of the manipulator under study. finally, the proposed technique is illustrated by means of the comparison of three degrees of freedom translational manipulators.
spinodal instability growth in new stochastic approaches. are spinodal instabilities the leading mechanism in the fragmentation of a fermionic system? numerous experimental indications suggest such a scenario and stimulated much effort in giving a suitable description, without being finalised in a dedicated transport model. on the one hand, the bulk character of spinodal behaviour requires an accurate treatment of the one-body dynamics, in presence of mechanical instabilities. on the other hand, pure mean-field implementations do not apply to situations where instabilities, bifurcations and chaos are present. the evolution of instabilities should be treated in a large-amplitude framework requiring fluctuations of langevin type. we present new stochastic approaches constructed by requiring a thorough description of the mean-field response in presence of instabilities. their particular relevance is an improved description of the spinodal fragmentation mechanism at the threshold, where the instability growth is frustrated by the mean-field resilience.
exposure mode study to xenon-133 in a reactor building. the work described in this thesis focuses on the external and internal dose assessment to xenon-133. during the nuclear reactor operation, fission products and radioactive inert gases, as ¹³³xe, are generated and might be responsible for the exposure of workers incase of clad defect.particle monte carlo transport code is adapted inradioprotection to quantify dosimetric quantities.the study of exposure to xenon-133 is conducted byusing monte-carlo simulations based on geant4, ananthropomorphic phantom, a realistic geometry of thereactor building, and compartmental models.the external exposure inside a reactor building isconducted with a realistic and conservative exposurescenario. the effective dose rate and the eye lensequivalent dose rate are determined by monte-carlosimulations. due to the particular emission spectrum ofxenon-133, the equivalent dose rate to the lens of eyesis discussed in the light of expected new eye doselimits.the internal exposure occurs while xenon-133 isinhaled. the lungs are firstly exposed by inhalation, andtheir equivalent dose rate is obtained by monte-carlosimulations. a biokinetic model is used to evaluate theinternal exposure to xenon-133.this thesis gives us a better understanding to thedosimetric quantities related to external and internalexposure to xenon-133. moreover the impacts of thedosimetric changes are studied on the current andfuture dosimetric limits. the dosimetric quantities arelower than the current and future dosimetric limits.
self-decomposable global constraints. scalability becomes more and more critical to decision support technologies. in order to address this issue in constraint programming, we introduce the family of self-decomposable constraints. these constraints can be satisfied by applying their own filtering algorithms on variable subsets only. we introduce a generic framework which dynamically decompose propagation, by filtering over variable subsets. our experiments over the cumulative constraint illustrate the practical relevance of self-decomposition.
optimization of customer orders routing in a collaborative distribution network. this paper presents a sequential approach for the assessment of a multi-layered distribution network     from a cluster of collaborating suppliers to a large set of customers.      the transportation network includes three segments: suppliers routes from suppliers to a consolidation and distribution center,     full truckload routes toward regional distribution centers, and less-than-truckload distribution toward final customers.     in every shipping date, the optimization problem consists of assigning customers to regional distribution centers     and determining the routes of vehicles through the whole distribution network.     this problem is first modeled as a mixed integer linear problem (milp).     then, we propose to decompose it into three smaller milps that are solved sequentially in order to quickly provide a good approximate solution.     the experiments on real data show that the decomposition method provides near optimal solutions within a few minutes while the original model would require hours of calculation.
searches for anisotropies in the arrival directions of the highest energy cosmic rays detected by the pierre auger observatory. we analyze the distribution of arrival directions of ultra-high energy cosmic rays recorded at the pierre auger observatory in 10 years of operation. the data set, about three times larger than that used in earlier studies, includes arrival directions with zenith angles up to 80∘, thus covering from −90∘ to +45∘ in declination. after updating the fraction of events correlating with the active galactic nuclei (agns) in the v\'eron-cetty and v\'eron catalog, we subject the arrival directions of the data with energies in excess of 40 eev to different tests for anisotropy. we search for localized excess fluxes and for self-clustering of event directions at angular scales up to 30∘ and for different threshold energies between 40~eev and 80~eev. we then look for correlations of cosmic rays with celestial structures both in the galaxy (the galactic center and galactic plane) and in the local universe (the super-galactic plane). we also examine their correlation with different populations of nearby extragalactic objects: galaxies in the 2mrs catalog, agns detected by swift-bat, radio galaxies with jets and the centaurus~a galaxy. none of the tests shows a statistically significant evidence of anisotropy. the strongest departures from isotropy (post-trial probability ∼1.4\%) are obtained for cosmic rays with e&gt;58~eev in rather large windows around swift agns closer than 130~mpc and brighter than 1044~erg/s (18∘ radius) and around the direction of centaurus~a (15∘ radius).
large scale distribution of ultra high energy cosmic rays detected at the pierre auger observatory with zenith angles up to 80$^\circ$. we present the results of an analysis of the large angular scale distribution of the arrival directions of cosmic rays with energy above 4 eev detected at the pierre auger observatory including for the first time events with zenith angle between $60^\circ$ and $80^\circ$. we perform two rayleigh analyses, one in the right ascension and one in the azimuth angle distributions, that are sensitive to modulations in right ascension and declination, respectively. the largest departure from isotropy appears in the $e &gt; 8$ eev energy bin, with an amplitude for the first harmonic in right ascension $r_1^\alpha =(4.4 \pm 1.0){\times}10^{-2}$, that has a chance probability $p(\ge r_1^\alpha)=6.4{\times}10^{-5}$, reinforcing the hint previously reported with vertical events alone.
explanation-based large neighborhood search. one of the most well-known and widely used local search techniques for solving optimization problems in constraint programming is the large neigh-borhood search (lns) algorithm. such a technique is, by nature, very flexible and can be easily integrated within standard backtracking procedures. one of its drawbacks is that the relaxation process is quite often problem dependent. several works have been dedicated to overcome this issue through problem independent parameters. nevertheless, such generic approaches need to be carefully parameter-ized at the instance level. in this paper, we demonstrate that the issue of finding a problem independent neighborhood generation technique for lns can be addressed using explanation-based neighborhoods. an explanation is a subset of constraints and decisions which justifies a solver event such as a domain modification or a conflict. we evaluate our proposal for a set of optimization problems. we show that our approach is at least competitive with or even better than state-of-the-art algorithms and can be easily combined with state-of-the-art neighborhoods. such results pave the way to a new use of explanation-based approaches for improving search.
tree-based graph partitioning constraint. combinatorial problems based on graph partitioning enable us to mathematically represent and model many practical applications. mission planning and the routing problems occurring in logistics perfectly illustrate two such examples. nevertheless, these problems are not based on the same partitioning pattern: generally, patterns like cycles, paths, or trees are distinguished. moreover, the practical applications are often not limited to theoretical problems like the hamiltonian path problem, or k-node disjoint path problems. indeed, they usually combine the graph partitioning problem with several restrictions related to the topology of nodes and arcs. the diversity of implied constraints in real-life applications is a practical limit to the resolution of such problems by approaches considering the partitioning problem independently from each additional restriction.this book focuses on constraint satisfaction problems related to tree partitioning problems enriched by several additional constraints that restrict the possible partitions topology. on the one hand, this title focuses on the structural properties of tree partitioning constraints. on the other hand, it is dedicated to the interactions between the tree partitioning problem and classical restrictions (such as precedence relations or incomparability relations between nodes) involved in practical applications.precisely, tree-based graph partitioning constraint shows how to globally take into account several restrictions within one single tree partitioning constraint. another interesting aspect of this book is related to the implementation of such a constraint. in the context of graph-based global constraints, the book illustrates how a fully dynamic management of data structures makes the runtime of filtering algorithms independent of the graph density.
synchronized sweep algorithms for scalable scheduling constraints. this report introduces a family of synchronized sweep based filtering algorithms for handling scheduling problems involving resource and precedence constraints. the key idea is to filter all constraints of a scheduling problem in a synchronized way in order to scale better. in addition to normal filtering mode, the algorithms can run in greedy mode, in which case they perform a greedy assignment of start and end times. the filtering mode achieves a significant speed-up over the decomposition into independent cumulative and precedence constraints, while the greedy mode can handle up to 1 million tasks with 64 resources constraints and 2 million precedences. these algorithms were implemented in both choco and sicstus.
toward sustainable development in constraint programming. null
propagating regular counting constraints. constraints over finite sequences of variables are ubiquitous in sequencing and timetabling. moreover, the wide variety of such constraints in practical applications led to general modelling techniques and generic propagation algorithms, often based on deterministic finite automata (dfa) and their extensions. we consider counter-dfas (cdfa), which provide concise models for regular counting constraints, that is constraints over the number of times a regular-language pattern occurs in a sequence. we show how to enforce domain consistency in polynomial time for atmost and atleast regular counting constraints based on the frequent case of a cdfa with only accepting states and a single counter that can be incremented by transitions. we also prove that the satisfaction of exact regular counting constraints is np-hard and indicate that an incomplete algorithm for exact regular counting constraints is faster and provides more pruning than the existing propagator from [3]. regular counting constraints are closely related to the costregular constraint but contribute both a natural abstraction and some computational advantages.
linking prefixes and suffixes for constraints encoded using automata with accumulators. consider a constraint on a sequence of variables functionally determining a result variable that is unchanged under reversal of the sequence. most such constraints have a compact encoding via an automaton augmented with accumulators, but it is unknown how to maintain domain consistency efficiently for most of them. using such an automaton for such a constraint, we derive an implied constraint between the result variables for a sequence, a prefix thereof, and the corresponding suffix. we show the usefulness of this implied constraint in constraint solving, both by local search and by propagation-based systematic search.
inclusive photon production at forward rapidities in proton-proton collisions at $\sqrt{s}$ = 0.9, 2.76 and 7 tev. the multiplicity and pseudorapidity distributions of inclusive photons have been measured at forward rapidities ($2.3 &lt; \eta &lt; 3.9$) in proton-proton collisions at three center-of-mass energies, $\sqrt{s}=0.9$, 2.76 and 7 tev using the alice detector. it is observed that the increase in the average photon multiplicity as a function of beam energy is compatible with both a logarithmic and a power-law dependence. the relative increase in average photon multiplicity produced in inelastic pp collisions at 2.76 and 7 tev center-of-mass energies with respect to 0.9 tev are 37.2% $\pm$ 0.3% (stat) $\pm$ 8.8% (sys) and 61.2% $\pm$ 0.3% (stat) $\pm$ 7.6% (sys), respectively. the photon multiplicity distributions for all center-of-mass energies are well described by negative binomial distributions. the multiplicity distributions are also presented in terms of kno variables. the results are compared to model predictions, which are found in general to underestimate the data at large photon multiplicities, in particular at the highest center-of-mass energy. limiting fragmentation behavior of photons has been explored with the data, but is not observed in the measured pseudorapidity range.
gbfs: efficient data-sharing on hybrid platforms. towards adding wan-wide elasticity to dfses. applications dealing with huge amounts of data suffer significant performance impacts when they are deployed on top of an hybrid platform (i.e the extension of a local infras- tructure with external cloud resources). more precisely, through a set of preliminary experiments we show that mechanisms which enable on demand extensions of current distributed file systems (dfses) are required. these mechanisms should be able to leverage external storage resources while taking into account the performance constraints imposed by the physical network topology used to interconnect the different sites. our answer to such a challenge is the group based file system proposal (gbfs), a glue providing the elasticity capability for storage resources by federating on demand any posix file systems. although our first prototype is under heavy development, we discuss in this paper the gbfs model and few preliminary but promising results.
a memetic algorithm for the hub location-routing problem. in many logistic systems for less than truckload (ltl) shipments, transportation of goods is made through collection/delivery tours to/from a hub. the design of such a logistic network corresponds to the hub location routing problem (hlrp). hlrp consists in locating hub facilities concentrating flows in order to take advantage of economies of scale and through which flows are to be routed from origins to destinations, and considers also both collection and distribution routes. we present a generic mip formulation of this problem and a solution method based on a genetic algorithm improved by some local searches. computational experiments are presented.
a taxonomy of domain-specific aspect languages. domain-specific aspect languages (dsals) are domain-specific languages (dsls) designed to express crosscutting concerns. compared to dsls, their aspectual nature greatly amplifies the language design space. we structure this space in order to shed light on and compare the different domain-specific approaches to deal with crosscutting concerns. we report on a corpus of 36 dsals covering the space, discuss a set of design considerations and provide a taxonomy of dsal implementation approaches. this work serves as a frame of reference to dsal and dsl researchers, enabling further advances in the field, and to developers as a guide for dsal implementations.
accountability for data protection. null
geometric and elastostatic calibration of robotic manipulator using partial pose measurements. the paper deals with geometric and elastostatic calibration of robotic manipulator using partial pose measurements, which do not provide the end-effector orientation. the main attention is paid to the efficiency improvement of identification procedure. in contrast to previous works, the developed calibration technique is based on the direct measurements only. to improve the identification accuracy, it is proposed to use several reference points for each manipulator configuration. this allows avoiding the problem of non-homogeneity of the least-square objective, which arises in the classical identification technique with the full-pose information (position and orientation). its efficiency is confirmed by the comparison analysis, which deals with the accuracy evaluation of different identification strategies. the obtained theoretical results have been successfully applied to the geometric and elastostatic calibration of serial industrial robot employed in a machining work-cell for aerospace industry.
q-intersection algorithms for constraint-based robust parameter estimation. given a set of axis-parallel n-dimensional boxes, the q-intersection is defined as the smallest box encompassing all the points that belong to at least q boxes. computing the q-intersection is a combinatorial problem that allows us to han-dle robust parameter estimation with a numerical constraint programming approach. the q-intersection can be viewed as a filtering operator for soft constraints that model measure-ments subject to outliers. this paper highlights the equiva-lence of this operator with the search of q-cliques in a graph whose boxicity is bounded by the number of variables in the constraint network. we present a computational study of the q-intersection. we also propose a fast heuristic and a sophisti-cated exact q-intersection algorithm. first experiments show that our exact algorithm outperforms the existing one while our heuristic performs an efficient filtering on hard problems.
beam monitoring and dosimetry tools for radiobiology experiments at the cyclotron arronax. the arronax (accélérateur pour la recherche en radiochimie et oncologie à nantes atlantique) cyclotron in saint herblain - france is a facility delivering alpha particles at 68 mev (1). one of its purposes is to become a platform for radiobiological studies. the radiobiological studies evolvearound two axes: the low energy range (&lt;10mev) in order to optimize radio-immunotherapy (rit) treatments, and the high energy range (30-68 mev) in order to puzzle out the fundamental mechanisms generated by cells in response to ionizing radiations.the arronax platform for radiobiology is currently preparing to use a time lapse fluorescence confocal microscope suitable for the irradiation of cell wells. this platform should contain tools for beam intensity checks to enable accurate and repeatable irradiation conditions and a device to monitor thedelivered dose.
experience from the post-test analysis of megapie. the (megawatt pilot experiment) megapie target was successfully irradiated in 2006 at the sinq facility of the paul scherrer institut. during the irradiation a series of measurements to monitor the operation of the target, the thermal hydraulics behavior and the neutronic and nuclear aspects, has been performed. in the post-test analysis phase of the project, the data were analyzed and important information relevant to accelerator-driven systems (ads) was gained, in particular: (i) from the operation of the target several recommendations concern the simplification of the system and the improved reliability; (ii) data from the thermal hydraulic measurements have offered the opportunity to validate the codes used in the design phase; (iii) the neutronic analysis confirm the high performance of a liquid metal target and the importance of the delayed neutron measurements in an ads target; (iv) the nuclear measurements of the gas released gave the opportunity to validate the codes used during the design phase and provided indications for the operation. from the results in these different domains recommendations to further development of ads and heavy liquid metal targets are discussed.
gas production in the megapie spallation target. the megawatt pilot experiment (megapie) project was started in 2000 to design, build, and operate a liquid lead-bismuth eutectic (lbe) spallation neutron target at the power level of 1 mw. the target was irradiated for 4 months in 2006 at the paul scherrer institute in switzerland. gas samples wereextracted in various phases of operation and analyzed by g spectroscopy, leading to the determination of the main radioactive isotopes released from the lbe. comparison with calculations performed using several validated codes (mcnpx2.5.0/cinder’90, fluka/orihet, and snt) yields the ratio between simulated in-target isotope production rates and experimental amounts released at any given time. this work underlines the weak points of spallation models for some released isotopes. also, results provide relevant information for safety and radioprotection in an accelerator-driven system and more particularly for the gas management in a spallation target dedicated to neutron production facilities.
a new characterization of relevant intervals for energetic reasoning. energetic reasoning (er) is a powerful filtering algorithm for the cumulative constraint. unfortunately, er is generally too costly to be used in practice. one reason of its bad behavior is that many intervals are considered as relevant, although most of them should be ignored. in the literature, heuristic approaches have been developed in order to reduce the number of intervals to consider, leading to a loss of filtering. in this paper, we provide a sharp characterization that allows to reduce the number of intervals by a factor seven without loss of filtering.
a declarative paradigm for robust cumulative scheduling. this paper investigates cumulative scheduling in uncertain environments, using constraint programming. we present a new declarative characterization of robustness, which preserves solution quality.we highlight the significance of our framework on a crane assignment problem with business constraints.
definition of the accelerator driven system. this report first describes the efit design from a high level. then it analyzes the transmuta-tion capabilities of the core for the minor actinide stream that was selected in work package 1 of this project. next this report describes the impact of the minor actinide loading on the safety parameters of the reactor core, typically the doppler effect and coolant void effect are studied. when the reference system is analyzed, the need to estimate the capacity needed to reach the scenario goals is analyzed. this is done for several units deployment to give an idea on the performance of the second stratum. finally, an exercise has been done to see if the power of the facility could be increased from the reference 400 mwth to 600 mwth, hence increasing the transmutation rate per unit significantly. clearly, this has to be done without safety issues.
loose coupling and substitution principle in objet-oriented frameworks for web services. today, the implementation of services (soap and restful models) and of client applications is increasingly based on object-oriented programming languages. thus, object-oriented frameworks for web services are essentially composed with two levels: an object level built over a service level. in this context, two properties could be particularly required in the specification of these frameworks: (i)first a loose coupling between the two levels, which allows the complex technical details of the service level to be hidden at the object level and the service level to be evolved with a minimal impact on the object level, (ii) second, an interoperability induced by the substitution principle associated to subtyping in the object level, which allows to freely convert a value of a subtype into a supertype. in this thesis, first we present the existing weaknesses of object-oriented frameworks related to these two requirements. then, we propose a new specification for object-oriented web service frameworks in order to resolve these problems. as an application, we provide an implementation of our specification in the cxf framework, for both soap and restful models.
an alns for a two echelon vehicle routing problem arising in city logistics. null
an adaptive large neighborhood search for the two-echelon multiple-trip vehicle routing problem with satellite synchronization. null
an adaptive large neighborhood search for a two echelon vehicle routing problem arising in city logistics. null
advanced plutonium management in pwr, complementarity of thorium and uranium. null
a full truckload routing and scheduling problem with split delivery and resource synchronization. null
estimation of the chooz cores fission rates and associated errors in the framework of the double chooz experiment. the double chooz experiment is designed to search for a non-vanishing mixing angle θ₁₃ characterizing the ability of neutrinos to oscillate. it consists in two identical detectors located respectively at 400 m and 1050 m of the two pressurized water reactors of the chooz nuclear plant in the french ardennes. indeed, nuclear reactor are huge electron antineutrino emitters (about 10²¹ ⊽ₑ/s for a 1gwe reactor). in double chooz, antineutrino sare detected by the inverse beta decay process in the liquid scintillator of the detectors : ⊽ₑ + p −&gt; e⁺ + n. the θ₁₃ parameter can be investigated searching for ⊽ₑ disappearance and ⊽ₑ energy distortion in the far detector with respect to the near detector. the first phase of the experiment during which only the far detector is taking data has started in april 2011. in absence of far detector whose installation will be completed in 2014, a prediction of the non-oscillated antineutrino flux and spectrum shape expected in the far detector is mandatory to measure θ₁₃ . in this manuscript, we present the simulation work performed to predict the fission rates of both chooz cores responsible for the reactor antineutrino flux. in this view, a complete core model has been developed with the mcnp utility for reactor evolution (mure) simulation code. the results of these simulations were used to determine the fission rates and associated systematic errors since the beginning of data taking and led to the first indication for a non-zero θ₁₃ mixing angle in november 2011.
high energy ion beam analysis at arronax. null
measurement of 230pa and 186re production cross sections induced by deuterons at arronax facility. a dedicated program has been launched on production of innovative radionuclides for pet imaging and for beta- and alpha targeted radiotherapy using proton or alpha particles at the arronax cyclotron. since the accelerator is also able to deliver deuteron beams up to 35 mev, we have reconsidered the possibility of using them to produce medical isotopes. two isotopes dedicated to targeted therapy have been considered: 226th, a decay product of 230pa, and 186re. the production cross sections of 230pa and 186re, as well as those of the contaminants created during the irradiation, have been determined by the stacked-foil technique using deuteron beams. experimental values have been quantified using a referenced cross section. the measured cross sections have been used to determine expected production yields and compared with the calculated values obtained using the talys code with default parameters.
ebt2 films response toalpha radiation at 48.3 mev. to advance the development of a radiobiological experimental set-up for alpha particle irradiations at the arronax cyclotron, experiments were performed to get the dose response of gafchomic ebt2 films for alpha particles at 48.3 mev. a system has been developed using a thin monitor copper foil and an x-ray spectrometer to measure the beam intensity and to calculate the delivered dose. on the other hand, the authors have irradiated ebt2 films, with 6-mv x rays, to get the dose response of ebt2 films for photons. the dose response curve for alpha particles shows an effect of polymerisation saturation compared with the dose response curve for photons.
development of a pixe method at high energy with the arronax cyclotron. the high energy pixe (hepixe) method is a multi-elemental non-destructive ion beam analysis technique. it is based on the detection of the x-ray emitted due to the interaction of high energy particle beam with a sample. this technique is fast and allows the analysis of heavy and medium elements in thin (lm), thick (mm) and multilayer samples. at the arronax facility (nantes, france), the hepixe method has been used to determine the composition of natural and synthetic sodalites. photochromic properties of these samples are supposed to come from the trace elements (concentration in the ppm range) present in the samples. taking advantage of the 70 mev proton beam available at our facility, the hepixe methodhas been also used to study multilayer samples. it has been shown that it is possible to determine the composition of each layer, their thicknesses and their depth position by analyzing the recorded x-ray spectra.
232th(d,xn)230,232,233pa cross section measurements. cross sections for the (d,n), (d,2n) and (d,4n) reactions on 232th were measured using the stackedfoil technique with beams provided by the arronax cyclotron. these data are of relevance for the production of radionuclides. the measured cross sections were compared with previous measurements as well as with theoretical calculations using the code talys.
measurement of volatile radionuclides production and release yields followed by a post-irradiation analysis of a pb/bi filled ta target at isolde. a crucial requirement in the development of liquid-metal spallation neutron target is knowledge of the composition and amount of volatile radionuclides that are released from the target during operation. it is also important to know the total amount produced, which could be released if there was an accident. one type is the lead-bismuth eutectic (lbe) target where different radionuclides can be produced following interaction with a high-energy proton beam, notably noble gases (ar, kr, xe isotopes) and other relative volatile isotopes such as hg and at. the results of an irradiation experiment performed at isolde on a lbe target are compared with predictions from the mcnpx code using the latest developments on the li'ege intranuclear cascade model (incl4.6) and the cem03 model. the calculations are able to reproduce the mass distribution of the radioisotopes produced, including the at production, where there is a significant contribution from secondary reactions. subsequently, a post-irradiation examination of the irradiated target was performed. investigations of both the tantalum target structure, in particular the beam window, and the leadbismuth eutectic were performed using several experimental techniques. no sign of severe irradiation damage, previously observed in other isolde targets, was found.
232th(d,4n)230pa cross-section measurements at arronax facility for the production of 230u. introduction: 226th (t1/2 = 31 min) is a promising therapeutic radionuclide since results, published in 2009, showed that it induces leukemia cells death and activates apoptosis pathways with higher efficiencies than 213bi. 226th can be obtained via the 230u α decay. this study focuses on the 230u production using the 232th(d,4n)230pa(β−)230u reaction.methods: experimental cross sections for deuteron-induced reactions on 232th were measured from 30 down to 19 mev using the stacked-foil technique with beams provided by the arronax cyclotron. after irradiation, all foils (targets as well as monitors) were measured using a high-purity germanium detector.results: our new 230pa cross-section values, as well as those of 232pa and 233pa contaminants created during the irradiation, were compared with previous measurements and with results given by the talys code. experimentally, same trends were observed with slight differences in orders of magnitude mainly due to the nuclear data change. improvements are ongoing about the talys code to better reproduce the data fordeuteron-induced reactions on 232th.conclusions: using our cross-section data points from the 232th(d,4n)230pa reaction, we have calculated the thick-target yield of 230u, in bq/μa·h. this value allows now to a full comparison between the different production routes, showing that the proton routes must be preferred.
measurements of 186re production cross section induced by deuterons on natw target at arronax facility. introduction: the arronax cyclotron, acronym for “accelerator for research in radiochemistry and oncology at nantes atlantique” is a new facility installed in nantes, france. a dedicated program has been launched on production of innovative radioisotopes for pet imaging and for β− and α targeted radiotherapy using protons or α particles. since the accelerator is also able to deliver deuteron beams up to 35 mev, we have reconsidered the possibility of using them to produce medical isotopes. indeed, in some cases, the use of deuterons allows higher production yield than protons.methods: 186re is a β− emitter which has chemical properties close to the widely used 99mtc and has been used in clinical trials for palliation of painful bone metastases resulting from prostate and breast cancer. 186re production cross section has been measured between 9 and 23 mev using the arronax deuteron beam and the stacked-foil technique.a novelty in our work is the use of a monitor foil behind each natwtarget foil in order to record efficiently the deuteron incident flux and energies all over the stack relying on the international atomic energy agency (iaea) recommended cross section of the natti(d,x)48v reaction. since a good optimization process is supposed to find the best compromise between production yield and purity of the final product, isotope of interest and contaminants created during irradiation are measured using gamma spectrometry.results: our new sets of data are presented and compared with the existing ones and with results given by the talys code calculations. the thick target yield (tty) has been calculated after the fit of our experimental values and compared with the iaea recommended ones.conclusions: presented values are in good agreement with existing data. the deuteron production route is clearly the best choice with a tty of 7.8 mb/μah at 30 mev compared to 2.4 mbq/μah for proton as projectile at the same energy. the talys code gives satisfactory results for 183,186re isotopes.
study of the single electron charge signals in the xenon100 direct dark matter search experiment. from the observation of the universe, it has been demonstrated that the mass associated to visible matter represents only few percent of its energetic budget, while the remaining part is composed by dark energy, responsible to the cosmological expansion, and by some hidden matter, the dark matter. the likeliest particles family used to describe this dark matter is called wimp (weakly interacting massive particles). that kind of particles could be directly detected by measuring nuclear recoil during an elastic scattering inside a scintillating material. for this, the xenon collaboration has developed a detector consisting in a time projection chamber (tpc) using xenon dual phase (liquid and gas) detector, and placed underground. the different ionization density of nuclear recoils induced by wimps, and electronic recoils induced by b particles or g rays background source, leads to different ratio between both signals, in the liquid and in the gas phase, and is used to discriminate wimps from background. a good knowledge of the ionization signal is strongly required for such a detector. in this context, the xenon100 response to single electron charge signals is investigated. they correspond to very tiny signals emitted in the gas phase by one or few electrons extracted in time coincidence. thanks to this analysis, an innovative method to establish the extraction yield of electrons from the liquid to the gas phase has been drawn, allowing to explore a key information to reject electronic recoils from nuclearones.
computational determination of the dominant triplet population mechanism in photoexcited benzophenone. in benzophenone, intersystem crossing occurs efficiently between s1(nπ*) and the t1 state of dominant nπ* character, leading to excited triplet states after photoexcitation. the transition mechanism between s1(nπ*) and t1 is still a matter of debate, despite several experimental studies. quantum mechanical calculations have been performed in order to asses the relative efficiencies of previously proposed mechanisms, in particular the direct s1 → t1 and indirect s1 → t2(ππ*) → t1 ones. multiconfigurational wave function based methods are used to discuss the nature of the relevant states and also to determine minimum energy paths and conical intersections. it is found that the t1 state has a mixed nπ*/ππ* character and that the t2(ππ*) state acts as an intermediate state between the s1 and t1 states. this result is in line with recent experiments, which suggested a two-step kinetic model to populate the phosphorescent state after photoexcitation [aloïse et al., j. phys. chem. a 2008, 112, 224-231].
energy dependence of k/π, p/π, and k/p fluctuations in au+au collisions from snn−−−√ = 7.7 to 200 gev. a search for the quantum chromodynamics (qcd) critical point was performed by the star experiment at the relativistic heavy ion collider, using dynamical fluctuations of unlike particle pairs. heavy-ion collisions were studied over a large range of collision energies with homogeneous acceptance and excellent particle identification, covering a significant range in the qcd phase diagram where a critical point may be located. dynamical k/π, p/π, and k/p fluctuations as measured by the star experiment in central 0-5% au+au collisions from center-of-mass collision energies snn−−−√ = 7.7 to 200 gev are presented. the observable νdyn was used to quantify the magnitude of the dynamical fluctuations in event-by-event measurements of the k/π, p/π, and k/p pairs. the energy dependences of these fluctuations from central 0-5% au+au collisions all demonstrate a smooth evolution with collision energy.
elliptic and triangular flow of heavy flavor in heavy-ion collisions. we investigate the elliptic and the triangular flow of heavy mesons in ultrarelativistic heavy-ion collisions at rhic and the lhc. the dynamics of heavy quarks is coupled to the locally thermalized and fluid dynamically evolving quark-gluon plasma. the elliptic flow of $d$ mesons and the centrality dependence measured at the lhc is well reproduced for purely collisional and bremsstrahlung interactions. due to the event-by-event fluctuating initial conditions from the epos2 model, the $d$ meson triangular flow is predicted to be nonzero at $\sqrt{s}=200$ gev and $\sqrt{s}=2.76$ tev. we study the centrality dependence and quantify the contributions stemming from flow of the light bulk event and the hadronization process. the flow coefficients as response to the initial eccentricities behave differently for heavy mesons than for light hadrons due to their inertia. higher-order flow coefficients of heavy flavor become important in order to quantify the degree of thermalization.
precision muon reconstruction in double chooz. we describe a muon track reconstruction algorithm for the reactor anti-neutrino experiment double chooz. the double chooz detector consists of two optically isolated volumes of liquid scintillator viewed by pmts, and an outer veto above these made of crossed scintillator strips. muons are reconstructed by their outer veto hit positions along with timing information from the other two detector volumes. all muons are fit under the hypothesis that they are through-going and ultrarelativistic. if the energy depositions suggest that the muon may have stopped, the reconstruction fits also for this hypothesis and chooses between the two via the relative goodness-of-fit. in the ideal case of a through-going muon intersecting the center of the detector, the resolution is ~40 mm in each transverse dimension. high quality muon reconstruction is an important tool for reducing the impact of the cosmogenic isotope background in double chooz.
modeling open-flow steam reforming of methanol over cu/zno/al2o3 catalyst in an axisymmetric reactor. null
environmental performance assessment of retrofitting existing coal fired power plants to co-firing with biomass: carbon footprint and emergy approach. null
the role of water molecules of the first solvation shell in modelling ligand-exchange reactions leading to ato+ hydrolyzed species. null
alpha radiolysis of nitric acid and sodium nitrate with 4he2+ beam of 13.5 mev energy. null
performance degradation of geiger-mode apds at cryogenic temperatures. two-phase cryogenic avalanche detectors (crads) with thgem multipliers, optically read out with geiger-mode apds (gapds), were proposed as potential technique for charge recording in rare-event experiments. in this work we report on the degradation of the gapd performance at cryogenic temperatures revealed in the course of the study of two-phase crad in ar, with combined thgem/gapd-matrix multiplier; the gapds recorded secondary scintillation photons from the thgem holes in the near infrared. the degradation effect, namely the loss of the gapd pulse amplitude, depended on the incident x-ray photon flux. the critical counting rate of photoelectrons produced at the 4.4 mm2 gapd, degrading its performance at 87 k, was estimated as 10000 per second. this effect was shown to result from the considerable increase of the pixel quenching resistor of this cpta-made gapd type. though not affecting low-rate rare-event experiments, the observed effect may impose some limitations on the performance of crads with gapd-based optical readout at higher-rate applications.
comprehensive analysis of fusion data well above the barrier. we report on the comprehensive systematics of nearly 400 fusion-evaporation and/or fusion-fission cross-section data for a very large variety of systems over an energy range ∼3a to 155a mev. scaled by the reaction cross section and expressed as a function of the center-of-mass energy per nucleon, the fusion cross section displays a universal behavior. within experimental errors, this behavior does not depend on system mass, mass asymmetry, or system isospin. the deduced homographic functional dependence for complete and summed complete and incomplete fusion excitation functions is derived from basic strong absorption model formulas for reaction cross sections and allows us to draw the main properties of these functions. the limiting energy for the complete fusion and the main characteristics (onset, maximum, and extinction) of the incomplete fusion excitation functions are determined. the complete fusion reaction process disappears around 6.5 mev/nucleon and the incomplete one disappears at about 13 mev/nucleon in the center-of-mass frame. the regularity in fusion data is particularly obvious for the evaporation-residue subset of the data ensemble. adding the fusion-fission data component does not alter the general data trend but somewhat obscures it owing to the larger uncertainty and/or possible normalization problems.
di-hadron correlations with identified leading hadrons in 200 gev au+au and d+au collisions at star. the star collaboration presents new two-dimensional di-hadron correlations with leading hadrons in 200 gev central au+au and minimum bias d+au collisions to explore hadronization mechanisms in the quark gluon plasma. the enhancement of the jet-like yield for leading pions in au+au data with respect to the d+au reference and the absence of enhancement for leading non-pions (protons and kaons) are discussed within the context of quark recombination. the correlated yield at large angles, specifically in the \emph{ridge region}, is significantly higher for leading non-pions than pions. the consistencies of the constituent quark scaling, azimuthal harmonic model and a mini-jet modification model description of the data are tested, providing further constraints on hadronization.
a large neighborhood search heuristic for supply chain network design. many exact or approximate solution techniques have been used to solve facility location problems and more generally supply chain network design problems. yet, the large neighborhood search technique (lns) has almost never been proposed for solving such problems, although it has proven its efficiency and flexibility in solving other complex combinatorial optimization problems. in this paper we propose an lns framework for solving a four-layer single period multi-product supply chain network design problem involving multimodal transport. location decisions for intermediate facilities (e.g. plants and distribution centers) are made using the lns while transportation modes and product flow decisions are determined by a greedy heuristic. as a post-optimization step, we also use linear programming to determine the optimal product flows once the logistics network is fixed. extensive experiments based on generated instances of different sizes and characteristics show the effectiveness of the method compared with a state-of-the-art solver.
spin physics and tmd studies at a fixed-target experiment at the lhc (after@lhc). we report on the opportunities for spin physics and transverse-momentum dependent distribution (tmd) studies at a future multi-purpose fixed-target experiment using the proton or lead ion lhc beams extracted by a bent crystal. the lhc multi-tev beams allow for the most energetic fixed-target experiments ever performed, opening new domains of particle and nuclear physics and complementing that of collider physics, in particular that of rhic and the eic projects. the luminosity achievable with after@lhc using typical targets would surpass that of rhic by more that 3 orders of magnitude in a similar energy region. in unpolarised proton-proton collisions, after@lhc allows for measurements of tmds such as the boer-mulders quark distributions, the distribution of unpolarised and linearly polarised gluons in unpolarised protons. using the polarisation of hydrogen and nuclear targets, one can measure transverse single-spin asymmetries of quark and gluon sensitive probes, such as, respectively, drell-yan pair and quarkonium production. the fixed-target mode has the advantage to allow for measurements in the target-rapidity region, namely at large x^uparrow in the polarised nucleon. overall, this allows for an ambitious spin program which we outline here.
beam-energy and system-size dependence of the space-time extent of the pion emission source produced in heavy ion collisions. two-pion interferometry measurements are used to extract the gaussian radii $r_{{\rm out}}$, $r_{{\rm side}}$, and $r_{{\rm long}}$, of the pion emission sources produced in cu$+$cu and au$+$au collisions at several beam collision energies $\sqrt{s_{_{nn}}}$ at phenix. the extracted radii, which are compared to recent star and alice data, show characteristic scaling patterns as a function of the initial transverse size $\bar{r}$ of the collision systems and the transverse mass $m_t$ of the emitted pion pairs, consistent with hydrodynamiclike expansion. specific combinations of the three-dimensional radii that are sensitive to the medium expansion velocity and lifetime, and the pion emission time duration show nonmonotonic $\sqrt{s_{_{nn}}}$ dependencies. the nonmonotonic behaviors exhibited by these quantities point to a softening of the equation of state that may coincide with the critical end point in the phase diagram for nuclear matter.
production of inclusive $\upsilon$(1s) and $\upsilon$(2s) in p-pb collisions at $\mathbf{\sqrt{s_{{\rm nn}}} = 5.02}$ tev. we report on the production of inclusive $\upsilon$(1s) and $\upsilon$(2s) in p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev at the lhc. the measurement is performed with the alice detector at backward ($-4.46&lt; y_{{\rm cms}}&lt;-2.96$) and forward ($2.03&lt; y_{{\rm cms}}&lt;3.53$) rapidity down to zero transverse momentum. the production cross sections of the $\upsilon$(1s) and $\upsilon$(2s) are presented, as well as the nuclear modification factor and the ratio of the forward to backward yields of $\upsilon$(1s). a suppression of the inclusive $\upsilon$(1s) yield in p-pb collisions with respect to the yield from pp collisions scaled by the number of binary nucleon-nucleon collisions is observed at forward rapidity but not at backward rapidity. the results are compared to theoretical model calculations including nuclear shadowing or partonic energy loss effects.
review on the tc chemistry at subatech in inorganic media (chloride, carbonate) with radiation effect. null
study induced oxidation/reduction of tc in carbonate media by α and γ radiolysis. null
search for patterns by combining cosmic-ray energy and arrival directions at the pierre auger observatory. energy-dependent patterns in the arrival directions of cosmic rays are searched for using data of the pierre auger observatory. we investigate local regions around the highest-energy cosmic rays with $e \geq 6 \cdot 10^{19}$ ev by analyzing cosmic rays with energies above $e = 5 \cdot 10^{18}$ ev arriving within an angular separation of approximately $15{\deg}$. we characterize the energy distributions inside these regions by two independent methods, one searching for angular dependence of energy-energy correlations and one searching for collimation of energy along the local system of principal axes of the energy distribution. no significant patterns are found with this analysis. the comparison of these measurements with astrophysical scenarios can therefore be used to obtain constraints on related model parameters such as strength of cosmic-ray deflection and density of point sources.
thermodynamic and kinetic study of scandium(iii) complexes of dtpa and dota: a step toward scandium radiopharmaceuticals. diethylenetriamine-n,n,n′,n′′,n′′-pentaacetic acid (dtpa) and 1,4,7,10-tetraazacyclododecane-1,4,7,10-tetraacetic acid (dota) scandium(iii) complexes were investigated in the solution and solid state. three 45sc nmr spectroscopic references suitable for aqueous solutions were suggested: 0.1 m sc(clo4)3 in 1 m aq. hclo4 (δsc=0.0 ppm), 0.1 m sccl3 in 1 m aq. hcl (δsc=1.75 ppm) and 0.01 m [sc(ox)4]5− (ox2−=oxalato) in 1 m aq. k2c2o4 (δsc=8.31 ppm). in solution, [sc(dtpa)]2− complex (δsc=83 ppm, ▵ν=770 hz) has a rather symmetric ligand field unlike highly unsymmetrical donor atom arrangement in [sc(dota)]− anion (δsc=100 ppm, ▵ν=4300 hz). the solid-state structure of k8[sc2(ox)7]⋅13 h2o contains two [sc(ox)3]3− units bridged by twice "side-on" coordinated oxalate anion with sc3+ ion in a dodecahedral o8 arrangement. structures of [sc(dtpa)]2− and [sc(dota)]− in [(hguanidine)]2[sc(dtpa)]⋅3 h2o and k[sc(dota)][h6dota]cl2⋅4 h2o, respectively, are analogous to those of trivalent lanthanide complexes with the same ligands. the [sc(dota)]− unit exhibits twisted square-antiprismatic arrangement without an axial ligand (tsa′ isomer) and [sc(dota)]− and (h6dota)2+ units are bridged by a k+ cation. a surprisingly high value of the last dota dissociation constant (pka=12.9) was determined by potentiometry and confirmed by using nmr spectroscopy. stability constants of scandium(iii) complexes (log kscl 27.43 and 30.79 for dtpa and dota, respectively) were determined from potentiometric and 45sc nmr spectroscopic data. both complexes are fully formed even below ph 2. complexation of dota with the sc3+ ion is much faster than with trivalent lanthanides. proton-assisted decomplexation of the [sc(dota)]− complex (τ1/2=45 h; 1 m aq. hcl, 25 °c) is much slower than that for [ln(dota)]− complexes. therefore, dota and its derivatives seem to be very suitable ligands for scandium radioisotopes.
multidimensional dirac strings and the witten index of symcs theories with groups of higher rank. we discuss generalized dirac strings associated with a given lie group. they live in r-dimensional complex space (r being the rank of the group). such strings show up in the effective born-oppenheimer hamiltonian for 3d supersymmetric yang-mills-chern-simons theories, brought up by the gluon loops. we calculate accurately the number of the vacuum states in the effective hamiltonian associated with these strings. we also show that these states are irrelevant for the final symcs vacuum counting. the witten index of symcs theories depends thus only on the strings generated by fermion loops and carrying fractional generalized fluxes.
surface complexation modeling of eu(iii) and phosphate on na-bentonite: binary and ternary adsorption systems. this study aims to investigate and model the adsorption of eu(iii) on bentonite in the presence of phosphate. binary (phosphate/bentonite) and ternary (eu(iii)/phosphate/bentonite) systems were studied as a function of contact time, ph, solid-to-liquid ratio and eu(iii)/phosphate concentration by using a batch experimental method. the adsorption of phosphate on bentonite slightly increased in the ph range of 2.5-6.5, and decreased in the ph range of 6.5-9.4. this adsorption can be quantitatively interpreted by a model considering the formation of three monodentate surface complexes. in the ternary system, a synergistic adsorption was observed in the presence of both phosphate and eu(iii). in addition to the two sub-models describing eu(iii) and phosphate adsorption, the formation of ternary surface complexes had to be considered in order to explain the synergistic effect experimentally observed. the experimental data could be quantitatively explained when eu(iii) (6-point triple bond; length half of m-dashsoeuh2po4+ and 6-point triple bond; length half of m-dashsoeuhpo4) or phosphate (6-point triple bond; length half of m-dashspo4eu+) are the bridged atoms. complementary experiments carried out by x-ray photoelectron spectroscopy suggested that the second case is the most probable. the proposed model can be used in order to predict eu(iii) adsorption on buffer/backfilling material in the presence of phosphate.
c3po: a spontaneous and ephemeral social networking framework for a collaborative creation and publishing of multimedia contents. online social networks have been adopted by a large part of the population, and have become in few years essential communication means and a source of information for journalists. nevertheless, these networks have some drawbacks that make people reluctant to use them, such as the impossibility to claim for ownership of data and to avoid commercial analysis of them, or the absence of collaborative tools to produce multimedia contents with a real editorial value. in this paper, we present a new kind of social networks, namely spontaneous and ephemeral social networks (sesns). sesns allow people to collaborate spontaneously in the production of multimedia documents so as to cover cultural and sport events.
optimization of the volume fraction of an absorbent phase (silicone oil)and biodegradation kinetics of dmds in a tppb. to improve the removal of hydrophobic volatile org. compds. (voc), an integrated process coupling an absorption step and a biol. treatment in a two-phase partitioning bioreactor was considered. a preliminary bibliog. review allowed to select a silicon oil (polydimethylsiloxane),which appeared efficient for both steps, voc absorption and biodegrdn., since this oil was biocompatible, non-biodegradable and had a negligible soly. in water. the purpose of this work was the implementation of this org. phase in the bioreactor. bod5 measurements showed that a ratio ranging between 20 and 30% of silicone oil in water was found to be optimal for the treatment of dmds, in relation with interfacial area considerations. for a small proportion of oil, the emulsion is homogeneous but the interfacial area is small; while for high ratios, the oil is not totally emulsified and hence the transfer appears limiting leading to decreasing substrate availability. [on scifinder(r)].
mass transfer between a gas phase and two non miscible liquid phases.use of the equivalent absorption capacity concept for gas/liquid/liquidcontactor design. a methodol. for the design of absorbers contacting a gas phase loaded with hydrophobic voc and two immiscible liq. phases (air/water/silicone oil) is presented. the design is based on a concept of mass transfer between phases called the "equiv. absorption capacity". it is demonstrated that voc absorption should be restricted to pollutants characterized by an air/silicone oil partition coeff. around 3 to 4 pa.m3.mol-1 or less. in this case, the gas-liq. mass transfer must be carried out between air and pure silicone oil, allowing a minimization of the diam. of the gas/liq. contactor. [on scifinder(r)].
on developing open source mde tools: our eclipse stories and lessons learned. tool development has always been a fundamental activity of software engineering. nowadays, open source is changing the way this is done in many organizations. traditional ways of doing things are progressively enhanced or even sometimes replaced by new organizational schemes, benefiting as much as possible from the properties of open source (os). this is especially true in innovative areas such as model driven engineering (mde) in which new tools are constantly created, developed and disseminated, many of them coming from research teams. this poses some hard questions: what is the actual impact of os in terms of tool development? how to best take advantage of os communities? and what are the opportunities for research teams in this context? capitalizing on experiences in developing mde os tools on top of the eclipse platform and its license model, we try to give some insights on these questions in this paper.
beyond the clouds, how should next generation utility computing infrastructures be designed?. to accommodate the ever-increasing demand for utility computing (uc) resources while taking into account both energy and economical issues, the current trend consists in building larger and larger data centers in a few strategic locations. although such an approach enables to cope with the actual demand while continuing to operate uc resources through centralized software system, it is far from delivering sustainable and efficient uc infrastructures. we claim that a disruptive change in uc infrastructures is required: uc resources should be man-aged differently, considering locality as a primary concern. to this aim, we pro-pose to leverage any facilities available through the internet in order to deliver widely distributed uc platforms that can better match the geographical dispersal of users as well as the unending resource demand. critical to the emergence of such locality-based uc (luc) platforms is the availability of appropriate operat-ing mechanisms. we advocate the implementation of a unified system driving the use of resources at an unprecedented scale by turning a complex and diverse infra-structure into a collection of abstracted computing facilities that is both easy to operate and reliable. by deploying and using such a luc operating system on backbones, our ultimate vision is to make possible to host/operate a large part of the internet by its internal structure itself: a scalable and nearly infinite set of resources delivered by any computing facilities forming the internet, starting from the larger hubs operated by isps, governments and academic institutions to any idle resources that may be provided by end-users.
effective aspects : a typed monadic model to control and reason about aspect interference. aspect-oriented programming (aop) aims to enhance modularity and reusability in software systems by offering an abstraction mechanism to deal with crosscutting concerns. but, in most general-purpose aspect languages aspects have almost unrestricted power, eventually conflicting with these goals. this work presents effective aspects: a novel approach to embed the pointcut/advice model of aop in a statically-typed functional programming language like haskell; along two main contributions. first, we define a monadic embedding of the full pointcut/advicemodel of aop. type soundness is guaranteed by exploiting the underlying type system, in particular phantom types and a new anti-unification type class. in this model aspects are first-class, can be deployed dynamically, and the pointcut language is extensible, therefore combining the flexibility of dynamically-typed aspect languages with the guarantees of a static type system. monads enable us to directly reason about computational effects both in aspects and base programs using traditional monadic techniques. using this we extend the notion of open modules with effects, and also with protected pointcut interfaces to external advising. these restrictions are enforced statically using the type system. also, we adapt the techniques of effectiveadvice to reason about and enforce control flow properties as well as to control effect interference. we show that the parametricity-based approach to effect interference falls short in the presence of multiple aspects and propose a different approach using monad views, a novel technique for handling the monad stack, developed by schrijvers and oliveira. then, we exploit the properties of our model to enable the modular construction of new semantics for aspect scoping and weaving. our second contribution builds upon a powerful model to reason about mixin-based composition of effectful components and their interference, based on equational reasoning, parametricity, and algebraic laws about monadic effects. our contribution is to show how to reason about interference in the presence of unrestricted quantification through pointcuts. we show that global reasoning can be compositional, which is key for the scalability of the approach in the face of large and evolving systems. we prove a general equivalence theorem that is based on a few conditions that can be established, reused, and adapted separately as the system evolves. the theorem is defined for an abstract monadic aop model; we illustrate its use with a simple version of the model just described. this work brings type-based reasoning about effects for the first time in the pointcut/advice model, in a framework that is expressive, extensible and well-suited for development of robust aspect-oriented systems as well as a research tool for new aspect semantics.
influence of gamma irradiation on uranium determination by arsenazo iii in the presence of fe(ii)/fe(iii). arsenazo iii is a widely used reagent for the concentration measurement of uranium and other actinides in aqueous samples. this study indicates that, for routine aqueous samples, due to the strong complexing ability with arsenazo iii, fe(iii) can significantly decrease the uv-vis absorbance of the u(vi)-arsenazo iii complex, whereas the influence of fe(ii) on the absorbance is negligible. however, when fe(ii) is present in a gamma-irradiated u(vi) aqueous sample, it can give rise to the fenton reaction, which produces oxidizing radicals that decompose the subsequently added arsenazo iii, leading to a sharp decrease in the absorbance of the u(vi)-arsenazo iii complex. the decrease in absorbance depends on the iron content and irradiation dose. furthermore, the oxidizing radicals from the fenton reaction induced by gamma irradiation can be continually produced. even if the irradiated solution has been aged for more than one month in the absence of light at room temperature and without the exclusion of oxygen, the reactivity of the radicals did not decrease toward the subsequently added arsenazo iii. this finding demonstrates that the presence of fe(ii) in gamma-irradiated u(vi) aqueous samples can lead to incorrect u(vi) measurement using the arsenazo iii method, and a new method needs to be developed for the quantitative determination of u(vi) in the presence of gamma radiation and ferrous iron.
accuracy improvement of robot-based milling using an enhanced manipulator model. the paper is devoted to the accuracy improvement of robot-based milling by using an enhanced manipulator model that takes into account both geometric and elastostatic factors. particular attention is paid to the model parameters identification accuracy. in contrast to other works, the proposed approach takes into account impact of the gravity compensator and link weights on the manipulator elastostatic properties. in order to improve the identification accuracy, the industry oriented performance measure is used to define optimal measurement configurations and an enhanced partial pose measurement method is applied for the identification of the model parameters. the advantages of the developed approach are confirmed by experimental results that deal with the elastostatic calibration of a heavy industrial robot used for milling. the achieved accuracy improvement factor is about 2.4.
compliance error compensation in robotic-based milling. the paper deals with the problem of compliance errors compensation in robotic-based milling. contrary to previous works that assume that the forces/torques generated by the manufacturing process are constant, the interaction between the milling tool and the workpiece is modeled in details. it takes into account the tool geometry, the number of teeth, the feed rate, the spindle rotation speed and the properties of the material to be processed. due to high level of the disturbing forces/torques, the developed compensation technique is based on the non-linear stiffness model that allows us to modify the target trajectory taking into account nonlinearities and to avoid the chattering effect. illustrative example is presented that deals with robotic-based milling of aluminum alloy.
ethylene glycol intercalation in smectites. molecular dynamics simulation studies. intercalation of ethylene glycol in smectites (glycolation) is widely used to discriminate smectites and vermiculites from other clays and among themselves. during this process, ethylene glycol molecules enter into the interlayer spaces of the swelling clays, leading to the formation of two-layer structure (~17 å) in the case of smectites, or one-layer structure (~14 å) in the case of vermiculites. in spite of the relatively broad literature on the understanding/characterization of ethylene glycol/water-clays complexes, the simplified structure of this complex presented by reynolds (1965) is still used in the contemporary x-ray diffraction computer programs, which simulate structures of smectite and illite-smectite. the monolayer structure is only approximated using the assumption of the interlayer cation and ethylene glycol molecules lying in the middle of interlayer spaces. this study was therefore undertaken to investigate the structure of ethylene glycol/water-clays complex in more detail using molecular dynamics simulation. the structural models of smectites were built on the basis of pyrophyllite crystal structure (lee and guggenheim, 1981), with substitution of particular atoms. in most of simulations, the structural model assumed the following composition, considered as the most common in the mixed layer illite-smectites (środoń et al. 2009): exch0.4(si3.96al0.04)(al1.46fe0.17mg0.37)o10(oh)2 atoms of the smectites were described with clayff force field (cygan et al., 2004), while atoms of water and ethylene glycol with flexible spc (berendsen et al., 1981) and opls (jorgensen et al., 1996) force fields, respectively. ewald summation was used to calculate long range coulombic interactions and the cutoff was set at 8.5 å. results of the simulations show that in the two-layer glycolate the content of water is relatively small: up to 0.8 h2o per half of the smectite unit cell (thereafter phuc). clear thermodynamic preference of mono- or two-layer structure of the complex is observed for typical smectite. based on the calculated radial distribution functions, it was confirmed that water and ethylene glycol molecules compete for the coordination sites of the calcium ions in the clay interlayers. it was also found that the differences in the smectite layer charge, charge location, and the type of the interlayer cation affect the ethylene glycol and water packing in the interlayer space and as result have strong influence on the basal spacing and on the structure of complex. varying amounts and ratio of both ethylene glycol and water are, however, the most important factor influencing the extent of the smectite expansion. comparison of two-layer structure obtained from molecular dynamics simulations with previous models leads to the conclusion that the arrangement of ethylene glycol molecules in the interlayers, used in simulations of x-ray diffractograms of clays, should be modified. in contrast to the reynolds (1965) model, the main difference is that, for different location of the clay charge, interlayer ions tend to change their positions. in the case of montmorillonite, calcium ions are located in the middle of the interlayer space, while for beidellite they are located much closer to the clay surface. water in these structures does not form distinct layers but is distributed rather broadly with a tendency to be concentrated close to the smectite surface. one-layer structure of ethylene glycol/water-smectite complex, characteristic of vermiculite was also proposed. references berendsen, h.j.c., postma, j.p.m., van gunsteren, w.f., hermans, j. (1981) interaction models for water in relation to protein hydration. in intermolecular forces; pullman, b., ed.; d. reidel: amsterdam, pp 331. cygan, r. t., liang, j. j., and kalinichev, a. g. (2004) molecular models of hydroxide, oxyhydroxide, and clay phases and the development of a general force field. journal of physical chemistry b, 108, 1255-1266. jorgensen,w.l., maxwell, d.s., tirado-rives, j. (1996) development and testing of the opls all-atom force field on conformational energetics and properties of organic liquids. j. am. chem. soc., 118, 11225-11236. lee j.h., guggenheim s. (1981) single crystal x-ray refinement of pyrophyllite-1tc. american mineralogist, 66, 350-357 reynolds r. c. (1965) an x-ray study of an ethylene glycol-montmorillonite complex. american mineralogist, 50, 990-1001 środoń j., zeelmaekers e., derkowski a. (2009) the charge of component layers of illite-smectite in bentonites and the nature of end-member illite. clays and clay minerals, 57, 650-672.
automatic reconstruction and analysis of security policies from deployed security components. security is a critical concern for any information system. security properties such as confidentiality, integrity and availability need to be enforced in order to make systems safe. in complex environments, where information systems are composed by a number of heterogeneous subsystems, each subsystem plays a key role in the global system security. for the specific case of access-control, access-control policies may be found in several components (databases, networksand applications) all, supposedly, working together. nevertheless since most times these policies have been manually implemented and/or evolved separately they easily become inconsistent. in this context, discovering and understanding which security policies are actually being enforced by the information system comes out as a critical necessity. the main challenge to solve is bridging the gap between the vendor-dependent security features and a higher-level representation that express these policies in a way that abstracts from the specificities of concrete system components, and thus, it´s easier to understand and reason with. this high-level representation would also allow us to implement all evolution/refactoring/manipulation operations on the security policies in a reusable way. in this work we propose such a reverse engineering and integration mechanism for access-control policies. we rely on model-driven technologies to achieve this goal.
surface complexation modeling of eu(iii) adsorption on silica in the presence of fulvic acid. humic substances (hs) substantially affect heavy metal (m) adsorption on mineral surfaces. however, quantitative descriptions of ternary systems involving m, hs and mineral surfaces remain unclear. this study examines adsorption in a model ternary system including eu(iii), fulvic acid (fa) and silica, and describes the adsorption of eu(iii) and fa by combining a double-layer model (dlm) and the stockholm humic model (shm). shm explains the binding of h+ and eu3+ to fa and the dlm for fa and eu(iii) adsorption on silica. experimental results showed that the presence of fa promotes eu(iii) adsorption at acidic ph values, but decreases it at basic ph values, which indicates the formation of ternary surface complexes. modeling calculations have shown that two ternary surface complexes are required to describe the experimental results in which eu3+ acts as a bridge between the surface site and fa. the present study suggests that the discrete-site approach to hs is a promising method for interpreting the adsorption data for m, hs and mineral ternary systems.
depth of maximum of air-shower profiles at the pierre auger observatory. ii. composition implications. using the data taken at the pierre auger observatory between december 2004 and december 2012, we have examined the implications of the distributions of depths of atmospheric shower maximum (xmax), using a hybrid technique, for composition and hadronic interaction models. we do this by fitting the distributions with predictions from a variety of hadronic interaction models for variations in the composition of the primary cosmic rays and examining the quality of the fit. regardless of what interaction model is assumed, we find that our data are not well described by a mix of protons and iron nuclei over most of the energy range. acceptable fits can be obtained when intermediate masses are included, and when this is done consistent results for the proton and iron-nuclei contributions can be found using the available models. we observe a strong energy dependence of the resulting proton fractions, and find no support from any of the models for a significant contribution from iron nuclei. however, we also observe a significant disagreement between the models with respect to the relative contributions of the intermediate components.
depth of maximum of air-shower profiles at the pierre auger observatory. i. measurements at energies above 10^17.8 ev. we report a study of the distributions of the depth of maximum, xmax, of extensive air-shower profiles with energies above 10^17.8 ev as observed with the fluorescence telescopes of the pierre auger observatory. the analysis method for selecting a data sample with minimal sampling bias is described in detail as well as the experimental cross-checks and systematic uncertainties. furthermore, we discuss the detector acceptance and the resolution of the xmax measurement and provide parameterizations thereof as a function of energy. the energy dependence of the mean and standard deviation of the xmax-distributions are compared to air-shower simulations for different nuclear primaries and interpreted in terms of the mean and variance of the logarithmic mass distribution at the top of the atmosphere.
stiffness modeling for perfect and non-perfect parallel manipulators under internal and external loadings. the paper presents an advanced stiffness modeling technique for perfect and non-perfect parallel manipulators under internal and external loadings. particular attention is paid to the manipulators composed of non-perfect serial chains, whose geometrical parameters differ from the nominal ones and do not allow to assemble manipulator without internal stresses that considerably affect the stiffness properties and also change the end-effector location. in contrast to other works, several types of loadings are considered simultaneously: an external force applied to the end-effector, internal loadings generated by the assembling of non-perfect serial chains and external loadings applied to the intermediate points (auxiliary loading due to the gravity forces and relevant compensator mechanisms, etc.). for this type of manipulators, a non-linear stiffness modeling technique is proposed that allows to take into account inaccuracy in the chains and to aggregate their stiffness models for the case of both small and large deflections. advantages of the developed technique and its ability to compute and compensate the compliance errors caused by the considered factors are illustrated by an example that deals with parallel manipulators of the orthoglide family.
potential barriers governing the 12c formation and decay through quasimolecular shapes. the l-dependent potential barriers that govern the 8be and 12c formation and decay through quasimolecular shapes have been determined using a generalized liquid-drop model and adjusted to reproduce the experimental q value. for the ternary channel of 12c, the energies of prolate linear chain configurations and oblate triangular configurations of three α particles have been compared. the triangular shape with three α nuclei in contact allows the experimental rms radius and the negative quadrupole moment of the 12c ground state to be reproduced. the difference between the energies of the minima in the prolate and oblate ternary shape paths is very close to the energy of the excited hoyle state of the 12c nucleus.
adsorption and transport of polymaleic acid on callovo-oxfordian clay stone: batch and transport experiments. dissolved organic matter (dom) can affect the mobility of radionuclides in pore water of clay-rich geological formations, such as those intended to be used for nuclear waste disposal. the present work studies the adsorption and transport properties of a polycarboxylic acid, polymaleic acid (pma, mw = 1.9 kda), on callovo-oxfordian argillite samples (cox). even though this molecule is rather different from the natural organic matter found in clay rock, the study of its retention properties on both dispersed and intact samples allows assessing to which extent organic acids may undergo sorption under natural conditions (ph 7) and what could be the impact on their mobility. pma sorption and desorption were investigated in dispersed systems. the degree of sorption was measured after 1, 8 and 21 days and for a range of pma initial concentrations from 4.5 × 10− 7 to 1.4 × 10− 3 mol.l− 1. the reversibility of the sorption process was estimated by desorption experiments performed after the sorption experiments. at the sorption steady state, the sorption was described by a two-site langmuir model. a total sorption capacity of cox for pma was found to be 1.01×10− 2 mol.kg− 1 distributed on two sorption sites, one weak and one strong. the desorption of pma was incomplete, independently of the duration of the sorption phase. the amount of desorbable pma even appeared to decrease for sorption phases from 1 to 21 days. to describe the apparent desorption hysteresis, two conceptual models were applied. the two-box diffusion model accounted for intraparticle diffusion and more generally for nonequilibrium processes. the two-box first-order non-reversible model accounted for a first-order non-reversible sorption and more generally for kinetically-controlled irreversible sorption processes. the use of the two models revealed that desorption hysteresis was not the result of nonequilibrium processes but was due to irreversible sorption. irreversible sorption on the strong site was completed after 1 day and represented 96% of the total sorption on this site. on the weak site the irreversible uptake was slower and completed only after 16 days but it also dominated the sorption. 85% of the pma sorbed on the weak site was not desorbable after 21 days of sorption. the migration of pma was studied by applying a hydraulic gradient to a clay core inserted in a stainless steel cell. breakthrough of polymaleic acid, simulated with a 1d transport model including the two-box first-order non-reversible model, revealed that the mobility of pma was limited by the same set of reversible/irreversible interactions as observed in the dispersed system. however, to describe efficiently the transport, the total sorption capacity had to be reduced to 33% of the capacity estimated in batch experiments. the irreversible sorption on the weak site was also slower in the intact sample than in the crushed sample. geometrical constraints would therefore affect both the accessibility to the sorption sites and the kinetics of the irreversible sorption process.
searches for large-scale anisotropy in the arrival directions of cosmic rays detected above energy of $10^{19}$ ev at the pierre auger observatory and the telescope array. spherical harmonic moments are well-suited for capturing anisotropy at any scale in the flux of cosmic rays. an unambiguous measurement of the full set of spherical harmonic coefficients requires full-sky coverage. this can be achieved by combining data from observatories located in both the northern and southern hemispheres. to this end, a joint analysis using data recorded at the telescope array and the pierre auger observatory above $10^{19}$ ev is presented in this work. the resulting multipolar expansion of the flux of cosmic rays allows us to perform a series of anisotropy searches, and in particular to report on the angular power spectrum of cosmic rays above $10^{19}$ ev. no significant deviation from isotropic expectations is found throughout the analyses performed. upper limits on the amplitudes of the dipole and quadrupole moments are derived as a function of the direction in the sky, varying between 7% and 13% for the dipole and between 7% and 10% for a symmetric quadrupole.
a mixed passengers-goods transportation network for territories with a low population density. null
a large neighborhood search based heuristic for supply chain network design. facility location problems and more generally supply chain design models have been the subject of a large number of models and exact or approximate solution techniques. yet, the large neighborhood search technique (lns) has almost never been proposed for solving such problems, although it has proven its efficiency and flexibility to solve complex combinatorial optimization problems. in this paper we propose a new solution framework for solving a four-layer single period multi-product supply chain network design problem involving multimodal transport. location decisions for intermediate facilities (e.g. plants and distribution centers) are made using lns while transportation modes and product flow decisions are determined by a greedy heuristic. finally, an a posteriori flow optimization dual simplex procedure is used to finalize the solution. extensive experiments based on randomly generated instances of different sizes and characteristics show the effectiveness of the method compared with a state-of-the-art solver. further research aims at extending the model and solution technique to encompass other advanced supply chain design features.
a two-phase heuristic for full truckload routing and scheduling with split delivery and resource synchronization in public works. this paper presents a two-phase method to solve a routing and scheduling problem that arises in public works. in this problem, the quantity of demands generally exceeds the capacity of a truck. as a result, demands have to be split into full truckloads, taking into account the various capacities in the heterogeneous fleet of vehicles that perform the transportation. full truckload routes have to be designed and scheduled according to construction or loading constraints on pickup and delivery sites. in the first phase, we propose a linear program to split demands into full truckload requests. the second phase is a heuristic that solves a heterogeneous full truckload pickup and delivery problem with time windows and resource synchronization. the method is evaluated on instances from a real case study.
development of the pixe analysis technique at high energy with the arronax cyclotron. particle induced x-ray emission (pixe) is a fast, nondestructive, multi-elemental analysis technique. it is based on the detection of characteristic x-rays due to the interaction of accelerated charged particles with matter. this method is successfully used in various application fields using low energy protons (energies around few mev), reaching a limit of detection of the order the μg/g (ppm). at this low energy, the depth of analysis is limited. at the arronax cyclotron, protons and alpha particles are delivered with energy up to 70 mev, allowing the development of the high energy pixe technique. thanks to these beams, we mainly produce kx-rays, more energetic than the lx-rays used with standard pixe for the heavy elements analysis. thus, in depth analysis in thick materials is achievable. for light element analysis, the pige technique, based on the detection of gamma rays emitted by excited nuclei, may be used in combination with pixe. first of all, we will introduce the characteristics and principles of high energy pixe analysis that we have developed at arronax. then we will detail the performance achieved, particularly in terms of detection limit in various experimental conditions. finally, we present the results obtained for the analysis of multilayer samples and quantification of trace elements in thick samples.
filtering atmostnvalue with difference constraints : application to the shift minimisation personnel task scheduling problem. the problem of minimising the number of distinct values among a set of variables subject to difference constraints occurs in many real-life contexts. this is the case of the shift minimisation personnel task scheduling problem, introduced by krishnamoorthy et. al., which is used as a case study all along this paper. constraint-programming enables to formulate this problem easily, through several alldifferent constraints and a single atmostnvalue constraint. however, the independence of these constraints results in a poor lower bounding, hence a difficulty to prove optimality. this paper introduces a formalism to describe a family of propagators for atmostnvalue . in particular, we provide simple but significant improvement of the state-of-the-art atmostnvalue propagator of bessière et. al., to filter the conjunction of an atmostnvalue constraint and disequalities. in addition, we provide an original search strategy which relies on constraint reification. extensive experiments show that our contribution significantly improves a straightforward model, so that it competes with the best known approaches from operational research.
isolation of flow and nonflow correlations by two- and four-particle cumulant measurements of azimuthal harmonics in &lt;mml:math altimg="si1.gif" overflow="scroll" xmlns:xocs="http://www.elsevier.com/xml/xocs/dtd" xmlns:xs="http://www.w3.org/2001/xmlschema" xmlns:xsi="http://www.w3.org/2001/xmlschema-instance" xmlns="http://www.elsevier.com/xml/ja/dtd" xmlns:ja="http://www.elsevier.com/xml/ja/dtd" xmlns:mml="http://www.w3.org/1998/math/mathml" xmlns:tb="http://www.elsevier.com/xml/common/table/dtd" xmlns:sb="http://www.elsevier.com/xml/common/struct-bib/dtd" xmlns:ce="http://www.elsevier.com/xml/common/dtd" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:cals="http://www.elsevier.com/xml/common/cals/dtd" xmlns:sa="http://www.elsevier.com/xml/common/struct-aff/dtd"&gt;&lt;mml:msqrt&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant="normal"&gt;nn&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:msqrt&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;200&lt;/mml:mn&gt;&lt;mml:mtext&gt; &lt;/mml:mtext&gt;&lt;mml:mtext&gt;gev&lt;/mml:mtext&gt;&lt;/mml:math&gt; au+au collisions. a data-driven method was applied to measurements of au+au collisions at $\sqrt{s_{_{\rm nn}}} =$ 200 gev made with the star detector at rhic to isolate pseudorapidity distance $\delta\eta$-dependent and $\delta\eta$-independent correlations by using two- and four-particle azimuthal cumulant measurements. we identified a component of the correlation that is $\delta\eta$-independent, which is likely dominated by anisotropic flow and flow fluctuations. it was also found to be independent of $\eta$ within the measured range of pseudorapidity $|\eta|&lt;1$. the relative flow fluctuation was found to be $34\% \pm 2\% (stat.) \pm 3\% (sys.)$ for particles of transverse momentum $p_{t}$ less than $2$ gev/$c$. the $\delta\eta$-dependent part may be attributed to nonflow correlations, and is found to be $5\% \pm 2\% (sys.)$ relative to the flow of the measured second harmonic cumulant at $|\delta\eta| &gt; 0.7$.
a two-phase method for the shift design and personnel task scheduling problem with equity objective. in this paper, we study the shift design and personnel task scheduling problem with equity objective (sdptsp-e), initially introduced in [11]. this problem consists in designing the shifts of workers and assigning a set of tasks to quali ed workers, so as to maximise the equity between workers. we propose a natural two-phase approach consisting in rst designing shifts and then assigning tasks to workers, and we iterate between these two phases to improve solutions. we compare our experimental results with existing literature and show that our approach outperforms previous known results.
upper bounding in inner regions for global optimization under inequality constraints. in deterministic constrained global optimization, upper bounding the objective function generally resorts to local minimization at the nodes of the branch and bound. the local minimization process is sometimes costly when constraints must be respected. we propose in this paper an alternative approach when the constraints are inequalities or relaxed equalities so that the feasible space has a non-null volume. first, we extract an inner region, i.e., an (entirely feasible) convex polyhedron or box in which all points satisfy the constraints. second, we select a point inside the extracted inner region and update the upper bound with its cost. we use two inner region extraction algorithms implemented in our interval b&amp;b called ibexopt [7]. this upper bounding shows good performance in medium-sized systems proposed in the coconut suite.
maintaining a system subject to uncertain technological evolution. maintenance decisions can be directly affected by the introduction of a new asset on the market, especially when the new asset technology could increase the expected profit. however new technology has a high degree of uncertainty that must be considered such as, e.g., its appearance time on the market, the expected revenue and the purchase cost. in this way, maintenance optimization can be seen as an investment problem where the repair decision is an option for postponing a replacement decision in order to wait for a potential new asset. technology investment decisions are usually based primarily on strategic parameters such as current probability and expected future benefits while maintenance decisions are based on “functional” parameters such as deterioration levels of the current system and associated maintenance costs. in this paper, we formulate a new combined mathematical optimization framework for taking into account both maintenance and replacement decisions when the new asset is subject to technological improvement. the decision problem is modelled as a non-stationary markov decision process. structural properties of the optimal policy and forecast horizon length are then derived in order to guarantee decision optimality and robustness over the infinite horizon. finally, the performance of our model is highlighted through numerical examples.
influence of air handling units (ahu) management of ventilation systems of buildings on microbial aerosols behavior. filtration performances of air handling units (ahu) filters regarding particles and microbial aerosols have been studied, as well as the influence of the ahu operational conditions on behavior of microorganisms collected on the filters. a lab-scale ahu with two successive filtration stages was developed and validated for the study of prototype filters with industrial geometries. three types of filters of different efficiency have been considered : g4, f7 and f9 according to en 779 standard. two configurations of filters were considered: 1) g4 pleated/f7 bag and 2) f7/f9 bag. filters were sequentially clogged by alumina particles which assured a mineral fraction, and then by micronized rice particles which provides the fungi penicillium chrysogenum and assures an organic fraction which acts as a substrate for microorganisms. finally, a microbial aerosol composed by endospores of bacillus subtilis and spores of aspergillus niger was nebulized for filters contamination. after clogging, stops and restarts of ventilation were simulated for different durations (10 days or 6 weeks). during restarts of ventilation, particles and microbial aerosols samplings were performed downstream of the filters. main results are: (i) level of clogging is significantly less important for the 2nd filtration stage than for the first one, (ii) survival of b. subtilis, growth of p. chrysogenum and decline of a.niger on the filters whatever the period of time studied, and (iii) during restarts of ventilation, microbial aerosols releasing was not detected for sampled fraction. moreover, two full-scale ahu were studied during 6 months. one of the ahu studied is equipped with two filters in series: a g4 pleated filter in 1st stage and a f7 bag filter in 2nd stage. this ahu treats the outdoor air to blow it towards the indoor environments. the other one extracts the indoor air to reject it back outdoors. the filters pressure drop, relative humidity and temperature of the air were measured continuously. filters efficiency regarding particles and microbial aerosols were measured once a month. an original methodology for the monthly estimation of the concentration of microorganisms on the filters was implemented. main results are: (i) no significant evolution of the filter pressure drop in 2nd stage, (ii) efficiency of g4 filters are comparable to the prototype filtersone, (iii) efficiency of f7 filters are lower than prototype filters one, which can be explained by differences of filtration velocity between the two scales, (iv) after 6 months of operation, concentration of microorganisms on g4 filter of the ahu of extraction is 10 times higher than the g4 filter one of ahu who treats outdoor air.
controlling propagation and search within a constraint solver. constraint programming is often described, idealistically, as a declarative paradigm in which the user describes the problem and the solver solves it. obviously, the reality of constraint solvers is more complex, and the needs in customization of modeling and solving techniques change with the level of expertise of users. this thesis focuses on enriching the arsenal of available techniques in constraint solvers. on the one hand, we study the contribution of an explanation system to the exploration of the search space in the specific context of a local search. two generic neighborhood heuristics which exploit explanations singularly are described. the first one is based on the difficulty of repairing a partially destroyed solution, the second one is based on the non-optimal nature of the current solution. these heuristics discover the internal structure of the problems to build good neighbors for large neighborhood search. they are complementary to other generic neighborhood heuristics, with which they can be combined effectively. in addition, we propose to make the explanation system lazy in order to minimize its footprint. on the other hand, we undertake an inventory of know-how relative to propagation engines of constraint solvers. these data are used operationally through a domain specific language that allows users to customize the propagation schema, providing implementation structures and defining check points within the solver. this language offershigh-level concepts that allow the user to ignore the implementation details, while maintaining a good level of flexibility and some guarantees. it allows the expression of propagation schemas specific to the internal structure of each problem solved. implementation and experiments were carried out in the choco constraint solver, developed in this thesis. this has resulted in a new version of the overall effectiveness and natively explained tool.
closing the door for dark photons as the explanation for the muon g-2 anomaly. the standard model (sm) of particle physics is spectacularly successful, yet the measured value of the muon anomalous magnetic moment (g-2)_\mu deviates from sm calculations by 3.6 sigma. several theoretical models attribute this to the existence of a "dark photon", an additional u(1) gauge boson, which is weakly coupled to ordinary photons. the phenix experiment at the relativistic heavy ion collider has searched for a dark photon, u, in \pi^0,\eta \rightarrow \gamma e^+e^- decays and obtained upper limits on u-\gamma mixing at 90% cl for the mass range 30 &lt; m_u &lt; 90 mev/c^2. combined with other experimental limits, the remaining region in the u-\gamma mixing parameter space that can explain the (g-2)_\mu deviation from its sm value is nearly completely excluded at the 90% confidence level, with only a small region of 30 &lt; m_u &lt; 36 mev/c^2 remaining.
reactor and antineutrino spectrum calculations for the double chooz first phase results. the double chooz reactor oscillation experiment is designed to search for a non-vanishing value of the mixing angle θ13θ13. for the first phase of the experiment with only the far detector running, the reactor electron antineutrino flux is normalized via reactor simulation. for this first phase and from its last results, double chooz observed an evidence for a reactor electron antineutrino disappearance. in 227.93 days of far detector live time, we obtained view the mathml sourcesin22θ13=0.109±0.030(stat)±0.025(syst). this result excludes the no-oscillation hypothesis at 99.8% cl.
the nucifer experiment. in nuclear reactors, a large number of antineutrinos are generated in the decay chains of the fission products; thus a survey of the antineutrino flux could provide valuable information related to the uranium and plutonium content of the core. this application generated interest by the iaea in using antineutrino detectors as a potential safeguard tool. here we present the nucifer experiment, developed in france, by cea and cnrs/in2p3. the design of this new antineutrino detector has focused on safety, size reduction, reliability and high detection efficiency with a good background rejection. the nucifer detector is currently taking data at the osiris research reactor, inside cea-saclay. presently, the ongoing analyses are considering the main sources of background for the antineutrino detection; the first antineutrino result is expected in 2013. a possible contribution to the understanding of the so called "reactor antineutrino anomaly" is also discussed. finally, we present a brief description of the proposed experiments at very short baselines (vsbl) from reactors in france.
determination of the sensitivity of the antineutrino probe for reactor core monitoring. this paper presents a feasibility study of the use of the detection of reactor-antineutrinos view the mathml source(ν¯e) for non proliferation purpose. to proceed, we have started to study different reactor designs with our simulation tools. we use a package called mcnp utility for reactor evolution (mure), initially developed by cnrs/in2p3 labs to study generation iv reactors. the mure package has been coupled to fission product beta decay nuclear databases for studying reactor antineutrino emission. this method is the only one able to predict the antineutrino emission from future reactor cores, which don't use the thermal fission of 235u, 239pu and 241pu. it is also the only way to include off-equilibrium effects, due to neutron captures and time evolution of the fission product concentrations during a reactor cycle. we will present here the first predictions of antineutrino energy spectra from innovative reactor designs (generation iv reactors). we will then discuss a summary of our results of non-proliferation scenarios involving the latter reactor designs, taking into account reactor physics constraints.
the detection of reactor antineutrinos for reactor core monitoring: an overview. there have been new developments in the field of applied neutrino physics during the last decade. the international atomic energy agency (iaea) has expressed interest in the potentialities of antineutrino detection as a new tool for reactor monitoring and has created an ad hoc working group in late 2010 to follow the associated research and development. several research projects are ongoing around the world to build antineutrino detectors dedicated to reactor monitoring, to search for and develop innovative detection techniques, or to simulate and study the characteristics of the antineutrino emission of actual and innovative nuclear reactor designs. we give, in these proceedings, an overview of the relevant properties of antineutrinos, the possibilities of and limitations on their detection, and the status of the development of a variety of compact antineutrino detectors for reactor monitoring.
contribution of recently measured nuclear data to reactor antineutrino energy spectra predictions. the aim of this work is to study the impact of the inclusion of the recently measured β decay properties of the 102,104,105,106,107tc, 105mo, and 101nb nuclei in the calculation of the antineutrino (anti-ν) energy spectra arising after the fissions of the four main fissile isotopes 235,238u, and 239,241pu in pwrs. these β feeding probabilities, measured using the total absorption technique (tas) at the jyfl facility of jyväskylä, have been found to play a major role in the γ component of the decay heat for 239pu in the 4-3000 s range. following the fission product summation method, the calculation was performed using the mcnp utility reactor evolution code (mure) coupled to the experimental spectra built from β decay properties of the fission products taken from evaluated databases. these latest tas data are found to have a significant effect on the pu isotope energy spectra and on the spectrum of 238u showing the importance of their measurement for a better assessment of the reactor anti-ν energy spectrum, as well as importance for fundamental neutrino physics experiments and neutrino applied physics.
total absorption study of beta decays relevant for nuclear applications and nuclear structure. an overview is given of our activities related to the study of the beta decay of neutron rich nuclei relevant for nuclear applications. recent results of the study of the beta decay of 87,88br using a new segmented total absorption spectrometer are presented. the measurements were performed at the igisol facility using trap-assisted total absorption spectroscopy.
an improved nuclear mass formula with a unified prescription for the shell and pairing corrections. an improvedmacroscopic-microscopic nuclear mass formula is presented in which shell and pairing effects are simultaneously evaluated by a procedure similar to strutinsky method. the coefficients of the macroscopic-microscopic mass formula have been adjusted on 2267 experimental atomic masses extracted from the atomic mass evaluation of 2012 (ame2012). same as inthe weizsäcker-skyrme (ws) model, the influence of the nuclear deformation on the macroscopic energy as well as the mirror nuclei constraint istaken into account, and for the sake of the consistency of the model parameters between the macroscopic and the microscopic parts we approximate the isospin-dependent component of the macroscopic energy to the depth of the woods-saxon potential. as a result, the root-mean square (rms) deviation with respect to 2267 measured nuclear masses is 0.493mev. then,based on the fitted formula we predict the remaining 988 nuclei from the ame2012 for which the masses are still unknown or not well-known, and calculate the α-decay energies of seven chains in the superheavy nuclei region with z=117 and 118.
charged-to-neutral correlation at forward rapidity in au+au collisions at $\sqrt{s_{nn}}$=200 gev. event-by-event fluctuations of the ratio of inclusive charged to photon multiplicities at forward rapidity in au+au collision at $\sqrt{s_{nn}}$=200 gev have been studied. dominant contribution to such fluctuations is expected to come from correlated production of charged and neutral pions. we search for evidences of dynamical fluctuations of different physical origins. observables constructed out of moments of multiplicities are used as measures of fluctuations. mixed events and model calculations are used as baselines. results are compared to the dynamical net-charge fluctuations measured in the same acceptance. a non-zero statistically significant signal of dynamical fluctuations is observed in excess to the model prediction when charged particles and photons are measured in the same acceptance. we find that, unlike dynamical net-charge fluctuation, charge-neutral fluctuation is not dominated by correlation due to particle decay. results are compared to the expectations based on the generic production mechanism of pions due to isospin symmetry, for which no significant (&lt;1%) deviation is observed.
measurement of the effective weak mixing angle in $p\bar{p}\rightarrow z/\gamma^{*}\rightarrow e^{+}e^{-}$ events. we present a measurement of the fundamental parameter of the standard model, the weak mixing angle, in $p\bar{p}\rightarrow z/\gamma^{*}\rightarrow e^{+}e^{-}$ events at a center of mass energy of 1.96 tev, using data corresponding to 9.7 fb$^{-1}$ of integrated luminosity collected by the d0 detector at the fermilab tevatron. the effective weak mixing angle is extracted from the forward-backward charge asymmetry as a function of the invariant mass around the z boson pole. the measured value of $\sin^2\theta_{\text{eff}}^{\text{$\ell$}}=0.23146 \pm 0.00047$ is the most precise measurement from light quark interactions to date, with a precision close to the best lep and sld results.
reconstruction of inclined air showers detected with the pierre auger observatory. we describe the method devised to reconstruct inclined cosmic-ray air showers with zenith angles greater than $60^\circ$ detected with the surface array of the pierre auger observatory. the measured signals at the ground level are fitted to muon density distributions predicted with atmospheric cascade models to obtain the relative shower size as an overall normalization parameter. the method is evaluated using simulated showers to test its performance. the energy of the cosmic rays is calibrated using a sub-sample of events reconstructed with both the fluorescence and surface array techniques. the reconstruction method described here provides the basis of complementary analyses including an independent measurement of the energy spectrum of ultra-high energy cosmic rays using very inclined events collected by the pierre auger observatory.
charge transfer complexes and radical cation salts of chiral methylated organosulfur donors. null
determination and speciation of anthropogenic tritium in the loire river estuary (france). the aim of radioecology is to understand the transfer of radionuclides through the ecosystem. it relies strongly on field studies which can provide useful information on the presence of radionuclides in the environment, and their origins (natural and anthropogenic). in this study, the radioactive isotope of hydrogen, i.e. tritium (3h or t), is considered. tritium is a beta emitter with a radioactive half life of 12.3 years. it is present in the environment in three principal forms: tritiated water (hto or tissue free water), organically bound tritium (obt) and tritiated gas (ht). tritiated water is the most abundant chemical form of tritium in the aquatic and terrestrial environment. obt can be subdivided in two fractions: the exchangeable obt refers to tritium atoms that are easily exchanged (e.g. bound to nitrogen, oxygen or sulfur atoms), while the non-exchangeable obt refers to the remaining obt covalently bound to carbon atoms. the non exchangeable hydrogen pool is considered as the only hydrogen fraction that faithfully records the history of environmental tritium seen by living organisms. in this study, mud and water samples from the loire estuary, the outlet of a watershed where several nuclear power plants are located, were analyzed. mud samples were subjected to freeze-drying and combustion as pre treatment in order to recover free hto and total obt. hto and total obt activities ranged between 4 and 26 bq.l-1 and between 10 and 25 bq.l-1 of combustion water, respectively. to estimate the non exchangeable obt activity in these samples, the exchangeable pool of hydrogen within the matrix has to be known. a dedicated experimental set up was thus developped in order to determine the fraction of exchangeable hydrogen (). it consists in a temperature and humidity controlled glove box where different environmental matrixes are exposed to specific atmospheres with fixed h/d (deuterium) or h/t pressure ratios. the calibration phase of the method was perfomed using cellulose matrix.
study of inclusive j/ψ production in pb-pb collisions at √snn=2,76 tev with the alice muon spectrometer at the lhc. the quantum chromodynamics theory predicts the existence of a deconfined state of matter called quark gluon plasma (qgp). experimentally, the formation of a qgp is expected under the extreme conditions of temperature and density reached in ultra-relativisticheavy-ion collisions. many observables were proposed to observe and characterize indirectly such a state of matter. in particular, the phenomena of suppression and (re)combination of the j/ψ meson in the qgp are extensively studied. this thesis presents the analysis of the inclusive production of j/ψ in pb-pb collisions, at a center of mass energy √snn = 2.76 tev, detected with the alice muon spectrometer at the lhc. from the high statistics of events collected during 2011 datataking, the j/ψ nuclear modification factor was measured as a function of transverse momentum, rapidity and collision centrality. the j/ψ mean transverse momentum was also measured as a function of centrality. the predictions of theoretical models, all including a (re)combination contribution, are in good agreement with data. finally, an excess of j/ψ yield at very low transverse momentum (&lt;300 mev/c) with respect to the expected hadronic production was observed fort he first time.
power management in virtualized data centers : form a load scenario to the optimization of the tasks placement. this thesis considers the virtualized it services hosting and makes two contributions. it first proposes a modular system of management aids, to move the virtual machines of the center in order to keep it in a good condition. this system allows in particular to integrate the concept of server power consumption and rules specific to that concept. what's more, its modularity allows to adjust its components to handle larger problems. this thesis proposes also a tool to compare different virtualized centers managers. this tool injects a reproductible load increase scenario in a virtualized infrastructure. the injection of such a scenario is used to evaluate the performance of the system center manager, using performances probes. the language used for this injection is extensible and allows the creation of parameterized scenarios. the contributions of this thesis were presented in two international conferences and a french conference.
effect capabilities for haskell. computational effects complicate the tasks of reasoning about and maintaining software, due to the many kinds of interferences that can occur. while different proposals have been formulated to alleviate the fragility and burden of dealing with specific effects, such as state or exceptions, there is no prevalent robust mechanism that addresses the general interference issue. build- ing upon the idea of capability-based security, we propose effect capabilities as an effective and flexible manner to control monadic effects and their interfer- ences. capabilities can be selectively shared between modules to establish secure effect-centric coordination. we further refine capabilities with type-based per- mission lattices to allow fine-grained decomposition of authority. we provide an implementation of effect capabilities in haskell, using type classes to establish a way to statically share capabilities between modules, as well as to check proper access permissions to effects at compile time. we exemplify how to tame effect interferences using effect capabilities, by treating state and exceptions.
muons in air showers at the pierre auger observatory: measurement of atmospheric production depth. the surface detector array of the pierre auger observatory provides information about the longitudinal development of the muonic component of extensive air showers. using the timing information from the flash analog-to-digital converter traces of surface detectors far from the shower core, it is possible to reconstruct a muon production depth distribution. we characterize the goodness of this reconstruction for zenith angles around 60 deg. and different energies of the primary particle. from these distributions we define x(mu)max as the depth along the shower axis where the production of muons reaches maximum. we explore the potentiality of x(mu)max as a useful observable to infer the mass composition of ultrahigh-energy cosmic rays. likewise, we assess its ability to constrain hadronic interaction models.
towards an open set of real-world benchmarks for model queries and transformations. with the growing size and complexity of systems under design, industry needs a generation of model-driven engineering (mde) tools, especially model query and transformation, with the proven capability to handle large-scale scenarios. while researchers are proposing several technical solutions in this sense, the community lacks a set of shared scalability benchmarks, that would simplify quantitative assessment of advancements and enable cross-evaluation of different proposals. benchmarks in previous work have been synthesized to stress specific features of model management, lacking both generality and industrial validity. in this paper, we initiate an effort to define a set of shared benchmarks, gathering queries and transformations from real-world mde case studies. we make these case available to community evaluation via a public mde benchmark repository.
improving memory efficiency for processing large-scale models. scalability is a main obstacle for applying model-driven engineering to reverse engineering, or to any other activity manipulating large models. existing solutions to persist and query large models are currently ine cient and strongly linked to memory availability. in this paper, we propose a memory unload strategy for neo4emf, a persistence layer built on top of the eclipse modeling framework and based on a neo4j database backend. our solution allows us to partially unload a model during the execution of a query by using a periodical dirty saving mechanism and transparent reloading. our experiments show that this approach enables to query large models in a restricted amount of memory with an acceptable performance.
product line-based customization of e-government documents. content personalization has been one of the major trends in recent document engineering research. the "one docum ent for n users" paradigm is being replaced by the "one user, one document" model, where the content to be delivered to a particular user is generated by some means. this is a very promising approach for e-government, where personalized government services, including document generation, are more and more required by users. in this paper, we introduce a method to the generation of personalized documents called document product lines (dpl). dpl allows generating content in domains with high variability and with high levels of reuse. we describe the basic principles underlying dpl and show its application to the e-government field using the personalized tax statement as case study.
event-by-event mean $p_{\rm t}$ fluctuations in pp and pb-pb collisions at the lhc. event-by-event fluctuations of the mean transverse momentum of charged particles produced in pp collisions at $\sqrt{s}$ = 0.9, 2.76 and 7 tev, and pb-pb collisions at $\sqrt{s_{nn}}$ = 2.76 tev are studied as a function of the charged-particle multiplicity using the alice detector at the lhc. non-statistical fluctuations are observed in all systems. the results in pp collisions show little dependence on collision energy. the monte carlo event generators pythia and phojet are in qualitative agreement with the data. peripheral pb-pb data exhibit a similar multiplicity dependence as that observed in pp. in central pb-pb, the results deviate from this trend, featuring a significant reduction of the fluctuation strength. the results in pb-pb are in qualitative agreement with previous measurements in au-au at lower collision energies and with expectations from models that incorporate collective phenomena.
extending the interaction flow modeling language (ifml) for model driven development of mobile applications front end. front-end design of mobile applications is a complex and multidisciplinary task, where many perspectives intersect and the user experience must be perfectly tailored to the application objectives. however, development of mobile user interactions is still largely a manual task, which yields to high risks of errors, inconsistencies and ine ciencies. in this paper we propose a model-driven approach to mobile application development based on the ifml standard. we propose an extension of the interaction flow modeling language tailored to mobile applications and we describe our implementation experience that comprises the development of automatic code generators for cross-platform mobile applications based on html5, css and javascript optimized for the apache cordova framework. we show the approach at work on a popular mobile application, we report on the application of the approach on an industrial application development project and we provide a productivity comparison with traditional approaches.
neo4emf : when big models are no longer an issue. handling effectively big emf models has often been one of the main barriers that can retain from adopting modeling technologies in very large-scale complex systems. in this talk we present neo4emf, a neo4j-based persistence framework allowing on-demand loading, storage, unloading and actual use of very large emf models. neo4emf provides a no-sql database persistence framework based on neo4j, which is a transactional property-graph database that has proved having a remarkable running speed for connected data operations compared to relational databases. in terms of performance, neo4emf eases data access and storage not only in a manner to reduce time and memory usage but also to allow big emf models to fit into a reduced amount of memory. this is made possible through a lightweight and on-demand loading mechanism. moreover, neo4emf comes with a dirty saving mechanism allowing to store huge chunks of data even with limited memory resources.
impact of 4he2+ radiolysis of water on uo2 corrosion. null
combination of cdf and do results on the mass of the top quark using up to 9.7 fb$^{-1}$ at the tevatron. we summarize the current top-quark mass measurements from the cdf and do experiments at fermilab. we combine published run i (1992--1996) results with the most precise published and preliminary run ii (2001--2011) measurements based on data corresponding to up to 9.7 fb$^{-1}$ of $p\bar{p}$ collisions. taking correlations of uncertainties into account, and combining the statistical and systematic uncertainties, the resulting preliminary tevatron average mass of the top quark is $m_{top} = 174.34 \pm 0.64 ~gev/c^2$, corresponding to a relative precision of 0.37%.
spin-orbit coupling and physico-chemical properties of molecules and materials. null
a tabu search procedure for multi-skill project scheduling problem.  an appropriate tabu search implementation is designed to solve the resource constrained project scheduling problem. this approach uses well defined move strategies and a structured neighbourhood, defines appropriate tabu status and tenure and takes account of objective function approximation to speed up the search process. a sound understanding of the problem has helped in many ways in designing and enhancing the tabu search methodology. the method uses diversification, intensification and handles infeasibility via strategic oscillation.
a three step decomposition approach for a transportation network design problem with non-linear costs. null
software modernization and cloudification using the artist migration methodology and framework. cloud computing has leveraged new software development and provisioning approaches by changing the way computing, storage and networking resources are purchased and consumed. the variety of cloud offerings on both technical and business level has considerably advanced the development process and established new business models and value chains for applications and services. however, the modernization and cloudification of legacy software so as to be offered as a service still encounters many challenges. in this work, we present a complete methodology and a methodology instantiation framework for the effective migration of legacy software to modern cloud environments.
integrated models for evaluation of local actions for the reduction of greenhouse gases emissions : heatgrid, an energy simulation model for a strategic management of district heating networks. because of the energy flexibility that they offer and their potential to reduce ghg emissions, disctrict heating (dh) networks are a tool of local energy policies in constant progression. their develpment and/or renovation is not only a classic technico-economical question, insofar as interconnected stakeholders of local energy policies, taking into account specific objectives, are concerned by dh networks. in this context, tools which enable these different stakeholders to evaluate actions related to dh networks are essential. they must be helpful for the assessment of renovation actions, the monitoring and the evaluation of performances....among the tools that allow theses evaluations, the modelling approaches are often too specific to a situation, a type of network, a stakeholder... the work of the thesis consists in developing a dh modeling tool that has this desired flexibility. the proposed tool "heatgrid" can model various network architectures. at each time step, the network running is simulated via linear programming formalism. this tool can be applied either at the design stage of a dh or at the operating stage. the model based approach enables the evaluation and comparison of economic, energy and technical aspects of the dh system in different scenarios. several examples are simulated and analyzed in order to illustrate the potential of the model.
quarkonia measurement in the alice upgrades. null
parton energy loss: an update. null
hard probes and the event generator epos. null
dilepton production in pp and aa at sis energies: a challenge for transport and experiment. null
heavy quark quenching and elliptic flow from rhic to lhc: can the experimental results be understood by pqcd?. null
heavy-quark azimuthal correlations in heavy-ion collisions. null
nuclear modification factor and elliptic flow of muons from heavy-flavour hadron decays in pb-pb collisions at sqrt(s_nn)=2.76 tev with alice. null
upgrade of the alice experiment: letter of intent. null
technical design report for the upgrade of the alice inner tracking system. null
the neutron background of the xenon100 dark matter experiment. the xenon100 experiment, installed underground at the laboratori nazionali del gran sasso (lngs), aims to directly detect dark matter in the form of weakly interacting massive particles (wimps) via their elastic scattering off xenon nuclei. this paper presents a study on the nuclear recoil background of the experiment, taking into account neutron backgrounds from ($\alpha$,n) and spontaneous fission reactions due to natural radioactivity in the detector and shield materials, as well as muon-induced neutrons. based on monte carlo simulations and using measured radioactive contaminations of all detector components, we predict the nuclear recoil backgrounds for the wimp search results published by the xenon100 experiment in 2011 and 2012, 0.11$^{+0.08}_{-0.04}$ events and 0.17$^{+0.12}_{-0.07}$ events, respectively, and conclude that they do not limit the sensitivity of the experiment.
first axion results from the xenon100 experiment. we present the first results of searches for axions and axion-like-particles with the xenon100 experiment. the axion-electron coupling constant, $g_{ae}$, has been tested by exploiting the axio-electric effect in liquid xenon. a profile likelihood analysis of 224.6 live days $\times$ 34 kg exposure has shown no evidence for a signal. by rejecting $g_{ae}$, larger than $7.7 \times 10^{-12}$ (90% cl) in the solar axion search, we set the best limit to date on this coupling. in the frame of the dfsz and ksvz models, we exclude qcd axions heavier than 0.3 ev/c$^2$ and 80 ev/c$^2$, respectively. for axion-like-particles, under the assumption that they constitute the whole abundance of dark matter in our galaxy, we constrain $g_{ae}$, to be lower than $1 \times 10^{-12}$ (90% cl) for masses between 5 and 10 kev/c$^2$.
conceptual design and simulation of a water cherenkov muon veto for the xenon1t experiment. xenon is a direct detection dark matter project, consisting of a time projection chamber (tpc) that uses xenon in double phase as a sensitive detection medium. xenon100, located at the laboratori nazionali del gran sasso (lngs) in italy, is one of the most sensitive experiments of its field. during the operation of xenon100, the design and construction of the next generation detector (of ton-scale mass) of the xenon project, xenon1t, is taking place. xenon1t is being installed at lngs as well. it has the goal to reduce the background by two orders of magnitude compared to xenon100, aiming at a sensitivity of $2 \cdot 10^{-47} \mathrm{cm}^{\mathrm{2}}$ for a wimp mass of 50 gev/c$^{2}$. with this goal, an active system that is able to tag muons and their induced backgrounds is crucial. this active system will consist of a water cherenkov detector realized with a water volume $\sim$10 m high and $\sim$10 m in diameter, equipped with photomultipliers of 8 inches diameter and a reflective foil. in this paper we present the design and optimization study for this muon veto water cherenkov detector, which has been carried out with a series of monte carlo simulations, based on the geant4 toolkit. this study showed the possibility to reach very high detection efficiencies in tagging the passage of both the muon and the shower of secondary particles coming from the interaction of the muon in the rock: &gt;99.5% for the former type of events (which represent $\sim$ 1/3 of all the cases) and &gt;70% for the latter type of events (which represent $\sim$ 2/3 of all the cases). in view of the upgrade of xenon1t, that will aim to an improvement in sensitivity of one order of magnitude with a rather easy doubling of the xenon mass, the results of this study have been verified in the upgraded geometry, obtaining the same conclusions.
transport coefficients of heavy quarks around $t_c$ at finite quark chemical potential. the interactions of heavy quarks with the partonic environment at finite temperature $t$ and finite quark chemical potential $\mu_q$ are investigated in terms of transport coefficients within the dynamical quasi-particle model (dqpm) designed to reproduce the lattice-qcd results (including the partonic equation of state) in thermodynamic equilibrium. these results are confronted with those of nuclear many-body calculations close to the critical temperature $t_c$. the hadronic and partonic spatial diffusion coefficients join smoothly and show a pronounced minimum around $t_c$, at $\mu_q=0$ as well as at finite $\mu_q$. close and above $t_c$ its absolute value matches the lqcd calculations for $\mu_q=0$. the smooth transition of the heavy quark transport coefficients from the hadronic to the partonic medium corresponds to a cross over in line with lattice calculations, and differs substantially from perturbative qcd (pqcd) calculations which show a large discontinuity at $t_c$. this indicates that in the vicinity of $t_c$ dynamically dressed massive partons and not massless pqcd partons are the effective degrees-of-freedom in the quark-gluon plasma.
exclusive $\mathrm{j/}\psi$ photoproduction off protons in ultra-peripheral p-pb collisions at $\sqrt{s_{\rm nn}}=5.02$ tev. we present the first measurement at the lhc of exclusive j/$\psi$ photoproduction off protons, in ultra-peripheral proton-lead collisions at $\sqrt{s_{\rm nn}}=5.02$ tev. events are selected with a dimuon pair produced either in the rapidity interval, in the laboratory frame, $2.5.
improved measurements of the neutrino mixing angle $\theta_{13}$ with the double chooz detector. the double chooz experiment presents improved measurements of the neutrino mixing angle $\theta_{13}$ using the data collected in 467.90 live days from a detector positioned at an average distance of 1050 m from two reactor cores at the chooz nuclear power plant. several novel techniques have been developed to achieve significant reductions of the backgrounds and systematic uncertainties with respect to previous publications, whereas the efficiency of the $\bar\nu_{e}$ signal has increased. the value of $\theta_{13}$ is measured to be $\sin^{2}2\theta_{13} = 0.090 ^{+0.032}_{-0.029}$ from a fit to the observed energy spectrum. deviations from the reactor $\bar\nu_{e}$ prediction observed above a prompt signal energy of 4 mev and possible explanations are also reported. a consistent value of $\theta_{13}$ is obtained from a fit to the observed rate as a function of the reactor power independently of the spectrum shape and background estimation, demonstrating the robustness of the $\theta_{13}$ measurement despite the observed distortion.
a new consistent vehicle routing problem for the transportation of people with disabilities. n this article, we address a problem of the transportation of people with disabilities where customers are served on an almost daily basis and expect some consistency in the service. we introduce an original model for the time-consistency of the service, based on so-called time-classes. we then define a new multiday vehicle routing problem (vrp) that we call the time-consistent vrp. we address the solution of this new problem with a large neighborhood search heuristic. each iteration of the heuristic requires solving a complex vrp with multiple time windows and no waiting time which we tackle with a heuristic branch-and-price method. computational tests are conducted on benchmark sets and modified real-life instances. results demonstrate the efficiency of the method and highlight the impact of time-consistency on travel costs. © 2014 wiley periodicals, inc. networks, vol. 63(3), 211-224 2014.
absorption of hydrophobic voc in an organic phase. toluene or dimethyldisulfide (dmds)​. a preliminary bibliog. review allowed to select a silicone oil (polydimethylsiloxane, pdms) that seems efficient for voc absorption, as well as for biol. regeneration. the aim of this work was to evaluate the performances of the 1st step of the process, namely the absorption step. the partition coeffs. of the targeted voc in pdms were quantified. the expts. permitted to demonstrate that the voc have more affinity for pdms than for water (partition coeffs. 267 and 36 times higher for toluene and dmds, resp.)​. nevertheless, the measured diffusion coeffs. of the voc in pdms are lower than those measured in water (∼30 times lower)​, which can lead to a lower efficiency of the process. expts. were then carried out in a packed column pilot plant. as expected, efficiency increased with the liq. flow rate. in optimized operating conditions, toluene and dmds are totally absorbed in pdms for inlet gaseous concns. of ∼68 and 84 ppm, resp.
nh3 biofiltration of piggery air. an aboveground pilot-scale biofilter filled with wood chips was tested to treat ammonia emissions from a piggery located in brittany (france). two long-term tests ("summer" and "autumn" experiments) were carried out to improve biofilter applications for agriculture. the influence of climatic conditions on biofilter performance was taken into account. during summer 2012, the biofilter was operated for 74 days at different empty bed residence times (ebrts) from 6 to 15 s. inlet nh3 concentrations were relatively constant (around 15 mg m 3). significant nh3 reductions were achieved at ebrt ¼ 12 s (removal efficiencies, re, ranged between 90 and 100% for loading rates, lr, of around 4 g m 3 h 1). at a lower ebrt (6 s), re dropped to roughly 30e50%. this was due to the dramatic increase in the loading rate (lr up to 12 g m 3 h 1) but the results showed that the change in atmospheric conditions (temperature and relative humidity) also had a significant influence on biofilter performance. it was evidenced that the use of a humidifier upstream of the biofilter must be taken into account for large-scale biofilter design, but only for specific conditions (the spraying of the biofilter having to be carried out exceptionally). during autumn 2012, the biofilter was operated for 116 days at ebrt ¼ 12 s. re were around 80% for lr of around 3 g m 3 h 1. in such autumnal atmospheric conditions, a demister system should be installed upstream of the biofilter in order to avoid water accumulation in the bed material. although biofiltration was suitable for nh3 treatment of piggery air, the need to control accurately the medium moisture content implies that biofilters would not be easily managed by a pig farmer.
kinetic separation of co2 and ch4 on carbon molecular sieves: study of internal diffusion and surface resistance of pure gases and binary gas mixtures. null
soft synthesis of mil-101(cr) for hydrogen storage by adsorption. null
simple method based on the potential theory for buoyancy effect correction of pure gases adsorption and gas mixtures adsorption. null
o2: a novel combined online and offline computing system for the alice experiment after 2018. null
multiplicity dependence of jet-like two-particle correlations in p-pb collisions at $\sqrt{s_{nn}}$ = 5.02 tev. two-particle angular correlations between unidentified charged trigger and associated particles are measured by the alice detector in p-pb collisions at a nucleon-nucleon centre-of-mass energy of 5.02 tev. the transverse-momentum range 0.7 $ &lt; p_{\rm{t}, assoc} &lt; p_{\rm{t}, trig} &lt;$ 5.0 gev/$c$ is examined, to include correlations induced by jets originating from low momen\-tum-transfer scatterings (minijets). the correlations expressed as associated yield per trigger particle are obtained in the pseudorapidity range $|\eta|&lt;0.9$. the near-side long-range pseudorapidity correlations observed in high-multiplicity p-pb collisions are subtracted from both near-side short-range and away-side correlations in order to remove the non-jet-like components. the yields in the jet-like peaks are found to be invariant with event multiplicity with the exception of events with low multiplicity. this invariance is consistent with the particles being produced via the incoherent fragmentation of multiple parton--parton scatterings, while the yield related to the previously observed ridge structures is not jet-related. the number of uncorrelated sources of particle production is found to increase linearly with multiplicity, suggesting no saturation of the number of multi-parton interactions even in the highest multiplicity p-pb collisions. further, the number scales in the intermediate multiplicity region with the number of binary nucleon-nucleon collisions estimated with a glauber monte-carlo simulation.
a targeted search for point sources of eev neutrons. a flux of neutrons from an astrophysical source in the galaxy can be detected in the pierre auger observatory as an excess of cosmic-ray air showers arriving from the direction of the source. to avoid the statistical penalty for making many trials, classes of objects are tested in combinations as nine "target sets", in addition to the search for a neutron flux from the galactic center or from the galactic plane. within a target set, each candidate source is weighted in proportion to its electromagnetic flux, its exposure to the auger observatory, and its flux attenuation factor due to neutron decay. these searches do not find evidence for a neutron flux from any class of candidate sources. tabulated results give the combined p-value for each class, with and without the weights, and also the flux upper limit for the most significant candidate source within each class. these limits on fluxes of neutrons significantly constrain models of eev proton emission from non-transient discrete sources in the galaxy.
a route for polonium 210 production from alpha-particle irradiated bismuth-209 target. a method is proposed for production of polonium-210 via the 209bi(α, 3n)210at nuclear reaction. bombardment of a bismuth-209 target was performed with a 37 mev alpha-particle beam that leads to the production of astatine-210 (t1/2 = 8.1 h), which decays to polonium-210. it is purified from the bismuth target matrix by employing liquid-liquid extraction using tributyl phosphate (tbp) in para-xylene from 7 m hydrochloric acid. back extraction of polonium-210 was performed by 9 m nitric acid. this method allows to purify a tracer amount of po-210 (2.6 * 10-13 mol) from macroscopic amount of bi (2.8 * 10-2 mol).
multi-objective design optimization of the leg mechanism for a piping inspection robot. this paper addresses the dimensional synthesis of an adaptive mechanism of contact points ie a leg mechanism of a piping inspection robot operating in an irradiated area as a nuclear power plant. this studied mechanism is the leading part of the robot sub-system responsible of the locomotion. firstly, three architectures are chosen from the literature and their properties are described. then, a method using a multi-objective optimization is proposed to determine the best architecture and the optimal geometric parameters of a leg taking into account environmental and design constraints. in this context, the objective functions are the minimization of the mechanism size and the maximization of the transmission force factor. representations of the pareto front versus the objective functions and the design parameters are given. finally, the cad model of several solutions located on the pareto front are presented and discussed.
core - corona model analysis of the low energy beam scan at rhic (relativistic heavy ion collider) in brookhaven (usa). the centrality dependence of spectra of identified particles in collisions between ultrarelativistic heavy ions with a center of mass energy ($\sqrt{s}$) of 39 and 11.5 $agev$ is analyzed in the core - corona model. we show that at these energies the spectra can be well understood assuming that they are composed of two components whose relative fraction depends on the centrality of the interaction: the core component which describes an equilibrated quark gluon plasma and the corona component which is caused by nucleons close to the surface of the interaction zone which scatter only once and which is identical to that observed in proton-proton collisions. the success of this approach at 39 and 11.5 $agev$ shows that the physics does not change between this energy and $\sqrt{s}=200~ agev$ for which this model has been developed (aichelin 2008). this presents circumstantial evidence that a quark gluon plasma is also created at center of mass energies as low as 11.5 $agev$.
d mesons in non-central heavy-ion collisions: fluctuating vs. averaged initial conditions. the suppression of d mesons in non-central heavy-ion collisions is investigated. the anisotropy in collisions at finite impact parameter leads to an ordering of all-angle, in- and out-of-plane nuclear modification factors due to the different in-medium path lengths. within our mc@shq+epos model of heavy-quark propagation in the qgp we demonstrate that fluctuating initial conditions lead to an effective reduction of the energy loss of heavy quarks, which is seen in a larger nuclear modification factor at intermediate and high transverse momenta. the elliptic flow at small transverse momenta is reduced.
dynamical collisional energy loss and transport properties of on- and off-shell heavy quarks in vacuum and in the quark gluon plasma. in this study we evaluate the dynamical collisional energy loss of heavy quarks, their interaction rate as well as the different transport coefficients (drag and diffusion coefficients, $\hat{q}$, etc). we calculate these different quantities for i) perturbative partons (on-shell particles in the vacuum with fixed and running coupling) and ii) for dynamical quasi-particles (off-shell particles in the qgp medium at finite temperature $t$ with a running coupling in temperature as described by the dynamical quasi-particles model). we use the perturbative elastic $(q(g) q \rightarrow q (g) q)$ cross section for the first case, and the infrared enhanced hard thermal loop cross sections for the second. the results obtained in this work demonstrate the effects of a finite parton mass and width on the heavy quark transport properties and provide the basic ingredients for an explicit study of the microscopic dynamics of heavy flavors in the qgp - as formed in relativistic heavy-ion collisions - within transport approaches developed previously by the authors.
anti-strange meson-baryon interaction in hot and dense nuclear matter. we present a study of in-medium cross sections and (off-shell) transition rates for the most relevant binary reactions for strange pseudoscalar meson production close to threshold in heavy-ion collisions at fair energies. our results rely on a chiral unitary approach in coupled channels which incorporates the $s$- and $p$-waves of the kaon-nucleon interaction. the formalism, which is modified in the hot and dense medium to account for pauli blocking effects, mean-field binding on baryons, and pion and kaon self-energies, has been improved to implement full unitarization and self-consistency for both the $s$- and $p$-wave interactions at finite temperature and density. this gives access to in-medium amplitudes in several elastic and inelastic coupled channels with strangeness content $s=-1$. the obtained total cross sections mostly reflect the fate of the $\lambda(1405)$ resonance, which melts in the nuclear environment, whereas the off-shell transition probabilities are also sensitive to the in-medium properties of the hyperons excited in the $p$-wave amplitudes [$\lambda$, $\sigma$ and $\sigma^*(1385)$]. the single-particle potentials of these hyperons at finite momentum, density and temperature are also discussed in connection with the pertinent scattering amplitudes. our results are the basis for future implementations in microscopic transport approaches accounting for off-shell dynamics of strangeness production in nucleus-nucleus collisions.
a search for point sources of eev photons. measurements of air showers made using the hybrid technique developed with the fluorescence and surface detectors of the pierre auger observatory allow a sensitive search for point sources of eev photons anywhere in the exposed sky. a multivariate analysis reduces the background of hadronic cosmic rays. the search is sensitive to a declination band from -85{\deg} to +20{\deg}, in an energy range from 10^17.3 ev to 10^18.5 ev. no photon point source has been detected. an upper limit on the photon flux has been derived for every direction. the mean value of the energy flux limit that results from this, assuming a photon spectral index of -2, is 0.06 ev cm^-2 s^-1, and no celestial direction exceeds 0.25 ev cm^-2 s^-1. these upper limits constrain scenarios in which eev cosmic ray protons are emitted by non-transient sources in the galaxy.
o2: a novel combined online and o ine computing system for the alice experiment after 2018. null
production of $\sigma(1385)^{\pm}$ and $\xi(1530)^{0}$ in proton-proton collisions at $\sqrt{s}=$ 7 tev. the production of the strange and double-strange baryon resonances ($\sigma(1385)^{\pm}$, $\xi(1530)^{0}$) has been measured at mid-rapidity ($\left | y \right |&lt;0.5$) in proton-proton collisions at $\sqrt{s}$ = 7 tev with the alice detector at the lhc. transverse momentum spectra for inelastic collisions are compared to qcd-inspired models, which in general underpredict the data. a search for the $\phi(1860)$ pentaquark, decaying in the $\xi\pi$ channel, has been carried out but no evidence is seen.
geometrical patterns for measurement pose selection in calibration of serial manipulators. null
multi-particle azimuthal correlations in p-pb and pb-pb collisions at the cern large hadron collider. measurements of multi-particle azimuthal correlations (cumulants) for charged particles in p-pb at $\sqrt{s_{\rm nn}} = 5.02$ tev and pb-pb at $\sqrt{s_{\rm nn}} = 2.76$ tev collisions are presented. they help address a question if there is evidence for global, flow-like, azimuthal correlations in the p-pb system. comparisons are made to measurements from the larger pb-pb system, where such evidence is established. in particular, the second harmonic two-particle cumulants are found to decrease with multiplicity, characteristic of a dominance of few-particle correlations in p-pb collisions. however, when a $|\delta \eta|$ gap is placed to suppress such correlations, the two-particle cumulants begin to rise at high-multiplicity, indicating the presence of global azimuthal correlations. the pb-pb values are higher than the p-pb values at similar multiplicities. in both systems, the second harmonic four-particle cumulants exhibit a transition from positive to negative values when the multiplicity increases. the negative values allow for a measurement of $v_{2}\{4\}$ to be made, which is found to be higher in pb-pb collisions at similar multiplicities. the second harmonic six-particle cumulants are also found to be higher in pb-pb collisions. in pb-pb collisions, we generally find $v_{2}\{4\} \simeq v_{2}\{6\}\neq 0$ which is indicative of a bessel-gaussian function for the $v_{2}$ distribution. for very high-multiplicity pb-pb collisions, we observe that the four- and six-particle cumulants become consistent with 0. finally, third harmonic two-particle cumulants in p-pb and pb-pb are measured. these are found to be similar for overlapping multiplicities, when a $|\delta\eta| &gt; 1.4$ gap is placed.
turning emergency plans into executable artifacts. on the way to the improvement of emergency plans, we show how a structured specification of the response procedures allows transforming static plans into dynamic, executable entities that can drive the way different actors participate in crisis responses. additionally, the execution of plans requires the definition of information access mechanisms allowing execution engines to provide an actor with all the information resources he or she needs to accomplish a response task. we describe work in progress to improve the saga's plan definition module and plan execution engine to support information-rich plan execution.
molecular modeling of the hydration, the structure, and the mobility of ions and water in the interlayer space and at the surface of a smectitic clay. the study of adsorption and ion mobility in clay minerals is important for a better understanding of many geochemical and environmental processes, as well as to predict the behavior of radionuclides in geological storage conditions. because of their very small size (&lt;2μm), it is not always easy to study clays by using the existing experimental methods and techniques. one alternative to this issue is to use computational molecular modeling to carry out clay studies. in addition to their tiny size, clays minerals also have complex structures, which can appear due to various possibilities in the distribution and arrangement of isomorphic substitutions in their layers. it has been clearly demonstrated that there is a strong correlation between the distribution of substitutions in the clay layers and their properties. however, this remains to be shown regarding the arrangement of the substitutions in the layers of the clay. in this work, computational molecular modeling techniques are used to determine and compare the hydration properties, as well as the structure and mobility of ions (li⁺, na⁺, k⁺, rb⁺, cs⁺, mg²⁺, ca²⁺, sr²⁺, ba²⁺, ni²⁺,uo₂²⁺) and water in the interlayer space of the three models of montmorillonite, that differ from each other by the arrangement of isomorphic substitutions in the clay layers.the adsorption and diffusion of the previously listed cations and water are also studied on the surface of montmorillonite clay and the results are compared to those obtained in the interlayer space both at 298 k and at 363 k. the data generated in this work agree well with experimental observations, and show a more or less significant correlation between the clay model used and the type of property calculated.
flow in proton-nucleus collisions. null
origin of atmospheric aerosols at the pierre auger observatory using studies of air mass trajectories in south america. the pierre auger observatory is making significant contributions towards understanding the nature and origin of ultra-high energy cosmic rays. one of its main challenges is the monitoring of the atmosphere, both in terms of its state variables and its optical properties. the aim of this work is to analyze aerosol optical depth $\tau_{\rm a}(z)$ values measured from 2004 to 2012 at the observatory, which is located in a remote and relatively unstudied area of the pampa amarilla, argentina. the aerosol optical depth is in average quite low - annual mean $\tau_{\rm a}(3.5~{\rm km})\sim 0.04$ - and shows a seasonal trend with a winter minimum - $\tau_{\rm a}(3.5~{\rm km})\sim 0.03$ -, and a summer maximum - $\tau_{\rm a}(3.5~{\rm km})\sim 0.06$ -, and an unexpected increase from august to september - $\tau_{\rm a}(3.5~{\rm km})\sim 0.055$). we computed backward trajectories for the years 2005 to 2012 to interpret the air mass origin. winter nights with low aerosol concentrations show air masses originating from the pacific ocean. average concentrations are affected by continental sources (wind-blown dust and urban pollution), while the peak observed in september and october could be linked to biomass burning in the northern part of argentina or air pollution coming from surrounding urban areas.
software architecture for product lines. null
quarkonium production measurement with the alice detector at the lhc. null
inclusive production of neutral mesons in alice. null
elliptic flow of non-photonic electrons in au+au collisions at $\sqrt{s_{\rm nn}} = $ 200, 62.4 and 39 gev. we present the measurements of elliptic flow ($v_2$) of non-photonic electrons (npe) by the star experiment using 2- and 4-particle correlations, $v_2${2} and $v_2${4}, and the event plane method in au+au collisions at $\sqrt{s_{nn}} = 200$ gev, and $v_2${2} at 62.4 and 39 gev. $v_2${2} and $v_2${4} are non-zero at low and intermediate transverse momentum ($p_t$) at 200 gev, and $v_2${2} is consistent with zero at low $p_t$ at other energies. for au+au collisions at $p_t&lt;1$ gev/c, there is a statistically significant difference between $v_2${2} at 200 gev and $v_2${2} at the two lower beam energies.
electric sensor-based control of underwater robot groups. some fish species use electric sense to navigate efficiently in the turbid waters of confined spaces. this paper presents a first attempt to use this sense to control a group of nonholonomic rigid underwater vehicles navigating in a cooperative way. a leader whose motion is unknown to the others serves as an active agent for its passive neighbor, which perceives the leader's electric field via current measurements and moves in order to follow a trajectory relative to it. then, this passive agent, becomes in its turn the leader for the next agent and so on. sufficient conditions of convergence of the control law are derived for electric current servoing. this is achieved without the explicit knowledge of the location of the agents. some limits on the possible motion of the leader along with the importance of the choice of controlled outputs are demonstrated. switching between different group configurations by following a virtual agent is also described. simulation and experimental results illustrate the theoretical study.
synthesis of an electric sensor based control for underwater multi-agents navigation in a file. thanks to an electro-sensible skin, some species of fish can feel the surrounding electric field generated by them-self or other fish. known under the name of "electric-sense", this ability allows these fish to navigate in confined surroundings. based on a bio-inspired electric sensor, this article presents how this electric sense can be used for the navigation in formation of several underwater vehicles. the formation considered is a file, each vehicle is assumed to follow its predecessor at a given distance. in confined environment, the file formation is interesting since fish can follow the same safe path. being based on the servoing of the electric measurements, these laws do not require the knowledge of the location of the agents. the underwater vehicle studied have non holonomic properties, their forward velocity has no lateral component. depending on the choice of the controlled outputs (combination of electric measures) we will see that path followed by the follower agents can be different and a methodology to choose the output will be defined in order that all the agents follow the leader path in presence of curved motion of the leader. the influence of the number of electrodes is discussed. simulation results illustrate the proposed approach.
electric sensor based control for underwater multi-agents navigation in formation. thanks to an electro-sensible skin, some species of fish can feel the perturbations of a self generated electric field caused by their surroundings variations. known under the name of "electric-sense", this ability allows these fish to communicate and navigate in confined surroundings wetted by turbid waters where vision and sonar cannot work. based on a bio-inspired electric sensor recently proposed in [1], this article presents a first attempt to use electric sense for the navigation in formation of a set of rigid underwater vehicles. the navigation strategy combines some behaviours observed in electric fish as well as a follower-leader strategy well known from multi-robot navigation. being based one the servoing of the electric measurements, these laws do not require the knowledge of the location of the agents. sufficient convergence conditions of the resulting control laws are given. moreover, some limits on the possible motion of the leader are exhibited and the importance of the choice of controlled outputs is discussed too. finally, simulation results illustrate the feasibility of the approach.
results of fission products $\beta$ decay properties measurement performed with a total absorption spectrometer. null
precision measurement of the longitudinal double-spin asymmetry for inclusive jet production in polarized proton collisions at $\sqrt{s}=200$ gev. we report a new high-precision measurement of the mid-rapidity inclusive jet longitudinal double-spin asymmetry, $a_{ll}$, in polarized $pp$ collisions at center-of-mass energy $\sqrt{s}=200$ gev. the star data place stringent constraints on polarized parton distribution functions extracted at next-to-leading order from global analyses of inclusive deep inelastic scattering (dis), semi-inclusive dis, and rhic $pp$ data. the measured asymmetries provide evidence for positive gluon polarization in the bjorken-$x$ region $x&gt;0.05$.
observability and controllability for linear neutral type systems. for a large class of linear neutral type systems which include distributed delays we give the duality relation between exact controllability and exact observability. this duality is based on the representation of the abstract adjoint system as a special neutral type system. as a consequence of this duality relation, a characterization of exact observability is obtained. the time of observability is precised.
elliptic flow of identified hadrons in pb-pb collisions at $\sqrt{s_{\rm{nn}}}$ = 2.76 tev. the elliptic flow coefficient ($v_{2}$) of identified particles in pb--pb collisions at $\sqrt{s_\mathrm{{nn}}} = 2.76$ tev was measured with the alice detector at the lhc. the results were obtained with the scalar product method, a two-particle correlation technique, using a pseudo-rapidity gap of $|\delta\eta| &gt; 0.9$ between the identified hadron under study and the reference particles. the $v_2$ is reported for $\pi^{\pm}$, $\mathrm{k}^{\pm}$, $\mathrm{k}^0_\mathrm{s}$, p+$\overline{\mathrm{p}}$, $\mathrm{\phi}$, $\lambda$+$\overline{\mathrm{\lambda}}$, $\xi^-$+$\overline{\xi}^+$ and $\omega^-$+$\overline{\omega}^+$ in several collision centralities. in the low transverse momentum ($p_{\mathrm{t}}$) region, $p_{\mathrm{t}} &lt; 2 $gev/$c$, $v_2(p_\mathrm{t})$ exhibits a particle mass dependence consistent with elliptic flow accompanied by the transverse radial expansion of the system with a common velocity field. the experimental data for $\pi^{\pm}$ and $\mathrm{k}$ are described fairly well by hydrodynamical calculations coupled to a hadronic cascade model (vishnu) for central collisions. however, the same calculations fail to reproduce the $v_2(p_\mathrm{t})$ for p+$\overline{\mathrm{p}}$, $\mathrm{\phi}$, $\lambda$+$\overline{\mathrm{\lambda}}$ and $\xi^-$+$\overline{\xi}^+$. for transverse momentum values larger than about 3 gev/$c$, particles tend to group according to their type, i.e. mesons and baryons. however, the experimental data at the lhc exhibit deviations from the number of constituent quark (ncq) scaling at the level of $\pm$20$\%$ for $p_{\mathrm{t}} &gt; 3 $gev/$c$.
suppression of $\upsilon$(1s) at forward rapidity in pb-pb collisions at $\sqrt{s_{\rm nn}}$ = 2.76 tev. we report on the measurement of the inclusive $\upsilon$(1s) production in pb-pb collisions at $\sqrt{s_{\rm nn}}=2.76$ tev carried out at forward rapidity ($2.5.
la place des modèles de trafic dans les récentes modélisations des impacts environnementaux des transports : importance de l'explicitation des méthodes et hypothèses. the role of travel demand models in recent modelings of environmental impacts of transport : importance of the explicitation of methods and hypotheses. abstract : the authors present a recent evolution in the use of the classic travel demand models (tdm) to environmental impact assessment of transport, far from its initial target. by comparing previous cases found in the literature (chester, seoul, florence, brisbane and saint-etienne) with their own present works (eval-pdu in nantes), the authors notice reluctantly that their predecessors tend to be evasive on their use of tdm. so, traffic data are little discussed in these works while they constitute one of the main stakes in this kind of study. indeed the hypotheses assumed for traffic modeling are impacting the next steps of the modeling chain (pollutants emission/dispersion). the importance of this first modeling stage excludes the possibility to leave it off the scene.
oscillatory modes of quarks in baryons for 3 quark flavors u,d,s. null
thermal characteristics confronting trace anomaly and intrinsic canonical structure of qcd. null
beam-energy dependence of charge separation along the magnetic field in au+au collisions at rhic. local parity-odd domains are theorized to form inside a quark-gluon-plasma (qgp) which has been produced in high-energy heavy-ion collisions. the local parity-odd domains manifest themselves as charge separation along the magnetic field axis via the chiral magnetic effect (cme). the experimental observation of charge separation has previously been reported for heavy-ion collisions at the top rhic energies. in this paper, we present the results of the beam-energy dependence of the charge correlations in au+au collisions at midrapidity for center-of-mass energies of 7.7, 11.5, 19.6, 27, 39 and 62.4 gev from the star experiment. after background subtraction, the signal gradually reduces with decreased beam energy, and tends to vanish by 7.7 gev. the implications of these results for the cme will be discussed.
measurement of longitudinal spin asymmetries for weak boson production in polarized proton-proton collisions at rhic. we report measurements of single and double spin asymmetries for $w^{\pm}$ and $z/\gamma^*$ boson production in longitudinally polarized $p+p$ collisions at $\sqrt{s} = 510$ gev by the star experiment at rhic. the asymmetries for $w^{\pm}$ were measured as a function of the decay lepton pseudorapidity, which provides a theoretically clean probe of the proton's polarized quark distributions at the scale of the $w$ mass. the results are compared to theoretical predictions, constrained by recent polarized dis measurements, and show a preference for a sizable, positive up antiquark polarization in the range $0.05.
measurement of $k_s^0$ and $k^{*0}$ in $p$$+$$p$, $d$$+$au, and cu$+$cu collisions at $\sqrt{s_{_{nn}}}=200$ gev. the phenix experiment at the relativistic heavy ion collider has performed a systematic study of $k_s^0$ and $k^{*0}$ meson production at midrapidity in $p$$+$$p$, $d$$+$au, and cu$+$cu collisions at $\sqrt{s_{_{nn}}}=200$ gev. the $k_s^0$ and $k^{*0}$ mesons are reconstructed via their $k_s^0 \rightarrow \pi^0(\rightarrow \gamma\gamma)\pi^0(\rightarrow\gamma\gamma)$ and $k^{*0} \rightarrow k^{\pm}\pi^{\mp}$ decay modes, respectively. the measured transverse-momentum spectra are used to determine the nuclear modification factor of $k_s^0$ and $k^{*0}$ mesons in $d$$+$au and cu$+$cu collisions at different centralities. in the $d$$+$au collisions, the nuclear modification factor of $k_s^0$ and $k^{*0}$ mesons is almost constant as a function of transverse momentum and is consistent with unity showing that cold-nuclear-matter effects do not play a significant role in the measured kinematic range. in cu$+$cu collisions, within the uncertainties no nuclear modification is registered in peripheral collisions. in central collisions, both mesons show suppression relative to the expectations from the $p$$+$$p$ yield scaled by the number of binary nucleon-nucleon collisions in the cu$+$cu system. in the $p_t$ range 2--5 gev/$c$, the strange mesons ($k_s^0$, $k^{*0}$) similarly to the $\phi$ meson with hidden strangeness, show an intermediate suppression between the more suppressed light quark mesons ($\pi^0$) and the nonsuppressed baryons ($p$, $\bar{p}$). at higher transverse momentum, $p_t&gt;5$ gev/$c$, production of all particles is similarly suppressed by a factor of $\approx$ 2.
beauty production in pp collisions at $\sqrt{s}$ = 2.76 tev measured via semi-electronic decays. the alice collaboration at the lhc reports measurement of the inclusive production cross section of electrons from semi-leptonic decays of beauty hadrons with rapidity $|y|&lt;0.8$ and transverse momentum 1 &lt; p(t)&lt; 10 gev/c, in pp collisions at root s = 2.76 tev. electrons not originating from semi-electronic decay of beauty hadrons are suppressed using the impact parameter of the corresponding tracks. the production cross section of beauty decay electrons is compared to the result obtained with an alternative method which uses the distribution of the azimuthal angle between heavy-flavour decay electrons and charged hadrons. perturbative qcd predictions agree with the measured cross section within the experimental and theoretical uncertainties. the integrated visible cross section, sigma(b -&gt; e) = 3.47 +/- 0.40(stat)(+1.12)(-1.33)(sys) +/- 0.07(norm) mu b, was extrapolated to full phase space using fixed order plus next-to-leading log (fonll) calculations to obtain the total b (b) over bar production cross section, sigma(b (b) over bar) = 130 +/- 15.1(stat)(+42.1)(-49.8)(sys)(+3.4)(-3.1)(extr) +/- 2.5(norm) +/- 4.4(br) mu b. (c).
measurement of electrons from semileptonic heavy-flavor hadron decays in pp collisions at $\sqrt{s} = 2.76$ tev. the $p_{\rm t}$-differential production cross section of electrons from semileptonic decays of heavy-flavor hadrons has been measured at mid-rapidity in proton-proton collisions at $\sqrt{s} = 2.76$ tev in the transverse momentum range 0.5 &lt; $p_{\rm t}$ &lt; 12 gev/$c$ with the alice detector at the lhc. the analysis was performed using minimum bias events and events triggered by the electromagnetic calorimeter. predictions from perturbative qcd calculations agree with the data within the theoretical and experimental uncertainties.
open heavy-flavour results from alice. null
suppression of $\psi$(2s) production in p-pb collisions at $\sqrt{s_{nn}}$ = 5.02 tev. the alice collaboration has studied the inclusive production of the charmonium state ψ(2s) in proton-lead (p-pb) collisions at the nucleon-nucleon centre of mass energy snn−−−−√ = 5.02 tev at the cern lhc. the measurement was performed at forward (2.03&lt;ycms&lt;3.53) and backward (−4.46&lt;ycms&lt;−2.96) centre of mass rapidities, studying the decays into muon pairs. in this paper, we present the inclusive production cross sections σψ(2s), both integrated and as a function of the transverse momentum pt, for the two ycms domains. the results are compared to those obtained for the 1s vector state (j/ψ), by showing the ratios between the production cross sections, as well as the double ratios [σψ(2s)/σj/ψ]ppb/[σψ(2s)/σj/ψ]pp between p-pb and proton-proton collisions. finally, the nuclear modification factor for inclusive ψ(2s) is evaluated and compared to the measurement of the same quantity for j/ψ and to theoretical models including parton shadowing and coherent energy loss mechanisms. the results show a significantly larger suppression of the ψ(2s) compared to that measured for j/ψ and to models. these observations represent a clear indication for sizeable final state effects on ψ(2s) production.
neutral pion production at midrapidity in pp and pb-pb collisions at $\sqrt{s_{nn}}$ = 2.76 tev. invariant yields of neutral pions at midrapidity in the transverse momentum range $0.6 &lt; p_{t} &lt; 12 gev/c$ measured in pb-pb collisions at $\sqrt{s_{nn}} = 2.76 tev$ are presented for six centrality classes. the pp reference spectrum was measured in the range $0.4 &lt; p_{t} &lt; 10 gev/c$ at the same center-of-mass energy. the nuclear modification factor, $r_{aa}$, shows a suppression of neutral pions in central pb-pb collisions by a factor of up to about $8-10$ for $5 \lesssim p_{t} \lesssim 7 gev/c$. the presented measurements are compared with results at lower center-of-mass energies and with theoretical calculations.
locality-aware cooperation for vm scheduling in distributed clouds. the promotion of distributed cloud computing infrastructures as the next platform to deliver the utility computing paradigm, leads to new virtual machines (vms) scheduling algorithms leveraging peer to peer approaches. although these proposals considerably improve the scalability, leading to the management of hundreds of thousands of vm over thousands of physical machines (pms), they do not consider the network overhead introduced by multi-site infrastructures. this overhead can have a dramatic impact on performance if there is no mechanism for favoring intra-site vs. inter-site manipulations. this paper introduces a new building block designed over a vivaldi over- lay which maximizes efficient collaborations between pms. we combined this mechanism with dvms, a large scale virtual machine scheduler and showed its benefit by discussing several experiments performed on four distinct sites of the grid'5000 testbed. thanks to our proposal and with- out changing the scheduling decision algorithm, the number of inter-site operations has been reduced by 72%. this result provides a glimpse of the promising future of locality properties to improve performance of massive distributed cloud platforms.
transverse momentum dependence of inclusive primary charged-particle production in p-pb collisions at sqrt(snn) = 5.02 tev. the transverse momentum (pt) distribution of primary charged particles is measured at midrapidity in minimum-bias p-pb collisions at sqrt(snn) = 5.02 tev with the alice detector at the lhc in the range 0.15 &lt; pt &lt; 50 gev/c. the spectra are compared to the expectation based on binary collision scaling of particle production in pp collisions, leading to a nuclear modification factor consistent with unity for pt larger than 2 gev/c. the measurement is compared to theoretical calculations and to data in pb-pb collisions at sqrt(snn) = 2.76 tev.
azimuthal anisotropy of d meson production in pb-pb collisions at sqrt(snn) = 2.76 tev. the production of the prompt charmed mesons d0, d+ and d*+ relative to the reaction plane was measured in pb-pb collisions at a centre-of-mass energy per nucleon--nucleon collision of sqrt(snn) = 2.76 tev with the alice detector at the lhc. d mesons were reconstructed via their hadronic decays at central rapidity in the transverse momentum (pt) interval 2-16 gev/c. the azimuthal anisotropy is quantified in terms of the second coefficient v_2 in a fourier expansion of the d meson azimuthal distribution, and in terms of the nuclear modification factor r_aa, measured in the direction of the reaction plane and orthogonal to it. the v_2 coefficient was measured with three different methods and in three centrality classes in the interval 0-50%. a positive v_2 is observed in mid-central collisions (30-50% centrality class), with an mean value of 0.204_{-0.036}^{+0.099},(tot.unc.) in the interval 2 &lt; pt &lt; 6 gev/c, which decreases towards more central collisions (10-30% and 0-10% classes). the positive v_2 is also reflected in the nuclear modification factor, which shows a stronger suppression in the direction orthogonal to the reaction plane for mid-central collisions. the measurements are compared to theoretical calculations of charm quark transport and energy loss in high-density strongly-interacting matter at high temperature. the models that include substantial elastic interactions with an expanding medium provide a good description of the observed anisotropy. however, they are challenged to simultaneously describe the strong suppression of high-pt yield of d mesons in central collisions and their azimuthal anisotropy in non-central collisions.
measurement of visible cross sections in proton-lead collisions at sqrt(snn) = 5.02 tev in van der meer scans with the alice detector. in 2013, the large hadron collider provided proton-lead and lead-proton collisions at the center-of-mass energy per nucleon pair sqrt(snn) = 5.02 tev. van der meer scans were performed for both configurations of colliding beams, and the cross section was measured for two reference processes, based on particle detection by the t0 and v0 detectors, with pseudo-rapidity coverage 4.6 &lt; eta&lt; 4.9, -3.3 &lt; eta &lt; -3.0 and 2.8 &lt; eta &lt; 5.1, -3.7 &lt; eta &lt; -1.7, respectively. given the asymmetric detector acceptance, the cross section was measured separately for the two configurations. the measured visible cross sections are used to calculate the integrated luminosity of the proton-lead and lead-proton data samples, and to indirectly measure the cross section for a third, configuration-independent, reference process, based on neutron detection by the zero degree calorimeters.
accountability for abstract component design. the importance of the services-based market, 62.9% of the world gross domestic product (gdp), triggered an increase in the use of software offered on-line as services (saas). the use of such software usually implies the flow of personal data on-line between several parties. this can make users reluctant to their use. in this work, we consider this issue at the design-time of the software and we propose some foundations for an accountable software design. accountability for a software is a property describing, among other aspects, its liability to end-users for the usage of the data it has been entrusted. we propose to enrich software's component design by accountability obligations using an abstract accountability language (aal). we also define conditions for the well-formedness of an accountable component design and show how they can be checked using a model-checking tool.
modeling and simulating a narrow tilting car using robotics formalism. modeling and simulation are fundamental tools to develop new urban vehicles. the aim of this work is to model and simulate a narrow urban tilting car, which should significantly decrease traffic congestion, pollution, and parking problems. the structure of the vehicle contains closed kinematic chains. the modeling approach is based on the modified denavit and hartenberg description, which is commonly used in robotics, by considering the vehicle as a mobile robot composed of a multibody poly-articulated system in which the terminal links are the wheels. this description allows automatic calculating of the symbolic expressions of the geometric, kinematic, and dynamic models. a simulator is developed with matlab/simulink, and the simulation of different scenarios is performed and analyzed.
toward a rational matrix approximation of the parameter-dependent riccati equation solution. this paper considers the problem of solving parameter-dependent riccati equations. in this paper, a tractable iterative scheme involving mainly additions and multiplications is developed for finding solutions to arbitrary accuracy. it is first presented in the parameter-independent case and then extended to the parametric case. it hinges upon two results: (i) a palindromic quadratic polynomial matrix characterization of the matrix sign and square root functions. (ii) a particular representation of parameter dependent matrices with negative and positive power series with respect to parameters. several numerical examples are given throughout the paper to prove the validity of the proposed results.
a matrix sign function based solution of parameter dependent sylvester equations. this paper focuses on some parameter dependent sylvester equations arising in systems and control theory. the matrices involved are assumed to be i) parameter dependent, ii) not necessarily of the same size, and iii) with possible common eigenvalues depending on the parameter value. a matrix sign function based solution is proposed, considering two main cases: the square coefficient matrices are diagonalizable or block-diagonalizable.
parametric lyapunov equation approach for robust h2 analysis and structured h2 control problems. this paper presents a new framework to deal with robust h2 analysis and structured h2 control problems for linear time-invariant multi-parameter dependent systems. these two problems still hold a special place for practical reasons and will be formulated, in the current paper, as a problem of finding an exact solution of some parametric lyapunov equation. two results are then proposed: i) a direct inversion method of a particular type parameter-dependent matrix. ii) a discrete fourier transform (dft) based inversion method for polynomial parameter dependent matrices. some didactic examples are given, throughout the paper, to illustrate the validity of the proposed results.
a matrix sign function framework for robust stability analysis and parameter-dependent lyapunov and riccati equalities. this paper presents a new framework to deal with robust stability analysis for time-invariant parameter dependent systems. it hinges upon two results: -the matrix sign function integral definition. -a particular representation of parameter-dependent matrices with negative and positive power series with respect to parameters. exact solutions to parameter dependent lyapunov and riccati equalities are derived. several didactic examples are given throughout the paper to prove the validity of the proposed results.
freeze-out radii extracted from three-pion cumulants in pp, p-pb and pb-pb collisions at the lhc. in high-energy collisions, the spatio-temporal size of the particle production region can be measured using the bose-einstein correlations of identical bosons at low relative momentum. the source radii are typically extracted using two-pion correlations, and characterize the system at the last stage of interaction, called kinetic freeze-out. in low-multiplicity collisions, unlike in high-multiplicity collisions, two-pion correlations are substantially altered by background correlations, e.g. mini-jets. such correlations can be suppressed using three-pion cumulant correlations. we present the first measurements of the size of the system at freeze-out extracted from three-pion cumulant correlations in pp, p-pb and pb-pb collisions at the lhc with alice. at similar multiplicity, the invariant radii extracted in p-pb collisions are found to be 5-15% larger than those in pp, while those in pb-pb are 35-55% larger than those in p-pb. our measurements disfavor models which incorporate substantially stronger collective expansion in p-pb as compared to pp collisions at similar multiplicity.
a model-based approach for extracting business rules out of legacy information systems. today's business world is very dynamic and organizations have to quickly adjust their internal policies to follow the market changes. such adjustments must be propagated to the business logic embedded in the organization's information systems, that are often legacy applications not designed to represent and operationalize the business logic independently from the technical aspects of the programming language employed. consequently, the business logic buried in the system must be discovered and understood before being modified. unfortunately, such activities slow down the modification of the system to new requirements settled in the organization policies and threaten the consistency and coherency of the organization business. in order to simplify these activities, we provide amodel-based approach to extract and represent the business logic, expressed as a set of business rules, from the behavioral and structural parts of information systems. we implement such approach for java, cobol and relational database management systems. the proposed approach is based on model driven engineering,that provides a generic and modular solution adaptable to different languages by offering an abstract and homogeneous representation of the system.
optimal pose selection for the identification of geometric and elastostatic parameters of machining robots. the thesis deals with the optimal pose selection for geometric and elastostatic calibration for industrial robots employed in machining of large parts. particular attention is paid to the improvement of robot positioning accuracy after compensation of the geometric and elastostatic errors. to meet the industrial requirements of machining operations, a new approach for calibration experiments design for serial and quasi-serial industrial robots is proposed. this approach is based on a new industry-oriented performance measure that evaluates the quality of calibration experiment plan via the manipulator positioning accuracy after error compensation, and takes into account the particularities of prescribed manufacturing task by introducing manipulator test-poses. contrary to previous works, the developed approach employs an enhanced partial pose measurement method, which uses only direct position measurements from an external device and allows us to avoid the non-homogeneity of relevant identification equations. in order to consider the impact of gravity compensator that creates closed-loop chains, the conventional stiffness model is extended by including in it some configuration dependent elastostatic parameters, which are assumed to be constant for strictly serial robots. corresponding methodology for calibration of the gravity compensator models is also proposed. the advantages of the developed calibration techniques are validated via experimental study, which deals with geometric and elastostatic calibration of a kuka kr-270 industrial robot.
observation of $d^0$ meson nuclear modifications in au+au collisions at $\sqrt{s_{_{\mathrm{nn}}}}$ = 200 gev. we report the first measurement of charmed-hadron ($d^0$) production via the hadronic decay channel ($d^0\rightarrow k^- + \pi^+$) in au+au collisions at $\sqrt{s_{_{\mathrm{nn}}}}$ = 200\,gev with the star experiment. the charm production cross-section per nucleon-nucleon collision at mid-rapidity scales with the number of binary collisions, $n_{bin}$, from $p$+$p$ to central au+au collisions. the $d^0$ meson yields in central au+au collisions are strongly suppressed compared to those in $p$+$p$ scaled by $n_{bin}$, for transverse momenta $p_{t}&gt;3$ gev/$c$, demonstrating significant energy loss of charm quarks in the hot and dense medium. an enhancement at intermediate $p_{t}$ is also observed. model calculations including strong charm-medium interactions and coalescence hadronization describe our measurements.
interplay between local anisotropies in binuclear complexes. a systematic study has been undertaken to determine how local distortions affect the overall (molecular) magnetic anisotropies in binuclear complexes. for this purpose we have applied a series of distortions to two binuclear ni(ii) model complexes and extracted the magnetic anisotropy parameters of multispin and giant-spin model hamiltonians. furthermore, local and molecular magnetic axes frames have been determined. it is shown that certain combinations of local distortions can lead to constructive interference of the local anisotropies and that the largest contribution to the anisotropic exchange does not arise from the second-rank tensor normally included in the multispin hamiltonian, but rather from a fourth-rank tensor. from the comparison of the extracted parameters, simple rules are obtained to maximize the molecular anisotropy by controlling the local magnetic anisotropy, which opens the way to tune the anisotropy in binuclear or polynuclear complexes.
energy management in multi-consumers multi-sources system: a practical framework. the paper develops a methodology to design a practical multi-sources and multi- consumers energy management systems (ems), applicable in particular to transport systems. a global ems optimization problem is formulated underlining generics criterion and constraints. remarking that a modularity property is essential to add easily (i.e. without redesigning the whole problem) new energy consumers (or sources), a new ems structure is presented, splitting the problem into two independent sub-problems. in compensation of sub-optimality, the computing burden is lighten and robustness to energy shortage enhanced. the framework is introduced through the ems design of an hybrid vehicle transporting conditioned merchandises.
co&lt;sub&gt;2&lt;/sub&gt; adsorption in fe&lt;sub&gt;2&lt;/sub&gt;(dobdc): a classical force field parameterized from quantum mechanical calculations. carbon dioxide adsorption isotherms have been computed for the metal−organic framework (mof) fe2(dobdc), where dobdc4− = 2,5- dioxido-1,4-benzenedicarboxylate. a force field derived from quantum mechanical calculations has been used to model adsorption isotherms within a mof. restricted open-shell møller−plesset second-order perturbation theory (romp2) calculations have been performed to obtain interaction energy curves between a co2 molecule and a cluster model of fe2(dobdc). the force field parameters have been optimized to best reproduced these curves and used in monte carlo simulations to obtain co2 adsorption isotherms. the experimental loading of co2 adsorbed within fe2(dobdc) was reproduced quite accurately. this parametrization scheme could easily be utilized to predict isotherms of various guests inside this and other similar mofs not yet synthesized.
greywater treatment by a fluidized bed reactor and impacts related to their use for irrigation of urban green spaces. a level of water quality intended for human consumption does not seem necessary for domestic uses such as irrigation of green spaces. alternative water supplies like the use of greywater (gw) can thus be considered. however, gw contains pathogenic microorganisms and organic compounds which can cause environmental and health risks. as the risks related to recycling are unknown, gw treatment is necessary before reusing. to describe the risks related to gw reuses, the scientific approach performed in this study was to characterize domestic gw in order to select an appropriate treatment. the biological process chosen is an aerobic fluidized bed reactor. as this process has never been developed for gw, an optimization step based on the study of its hydrodynamic behavior and the kinetics of biodegradation of gw was performed. the treatment performances were then determined. the treated gw produced in this study reached the threshold values expected by the french regulation for irrigation of green spaces with treated wastewater. indeed, the cod and the tss obtained in treated gw were respectively 26 mg o2.l-1 and 5.6 mg.l-1. the fluidized bed reactor has been used to treat 144 l.d-1 of gw for 16 months. three lawn plots were irrigated respectively with raw gw, treated gw and tap water asa reference. contrary to the lawn plot irrigated with raw gw, the risk analysis performed in this study has shown no significant difference between the law plot irrigated with treated gw and the one irrigated with tap water. this study shows that treated gw produced from the fluidized bed reactor developed in this experiment can be used for irrigation of green spaces.
imaging current induced magnetic domain wall motion in la0.7sr0.3mno3 nanowires by xmcd-peem. null
beyond c&lt;i&gt;max&lt;/i&gt;: an optimization-oriented framework for constraint-based scheduling. this paper presents a framework taking advantage of both the flexibility of constraint programming and the efficiency of operations research algorithms for solving scheduling problems under various objectives and constraints. built upon a constraint programming engine, the framework allows the use of scheduling global constraints, and it offers, in addition, a modular and simplified way to perform optimality reasoning based on well-known scheduling relaxations. we present a first instantiation on the single machine problem with release dates and lateness minimization. beyond the simplicity of use, the ptimizationoriented framework appears to be, from the experiments, effective for dealing with such a pure problem even without any ad-hoc heuristics.
radio detection of ultra high energy cosmic rays : commissioning and data analysis of an array of autonomous stations. ultra high energy cosmic rays are undoubtedly the product of the most energetic process in the universe. their energy can reach macroscopic values (up to around 10 joules!) and it is converted to kinetic energy of the particles which compose the air shower in the atmosphere. questions about their source and propagation in the interstellar medium are still open, making the field of the astroparticle physics very attractive. the codalema experiment is an instrument devoted to indirect cosmic ray detection through the radio emission induced by charged particles of air showers. this thesis presents results of data analysis since 2010 concerning the latest setup of the experiment, which is composed of 34 autonomous stations. the noise analysis has revealed that autonomous stations are sensitive to several human-made interferences. since, methods for background rejection has been developed with the aim of embedding them into the next generation electronic boards. likewise, the radio emission process taking place in the shower during its development are studied through their polarization pattern. the presence of the two preponderant emission mechanisms (transverse current and charge excess) are highlighted in this data set. a new parametrization of the lateral profile is also suggested.
composing json-based web apis. the development of web apis has become a discipline that companies have to master to succeed in the web. the so-called api economy is pushing companies to provide access to their data by means of web apis, thus requiring web developers to study and integrate such apis into their applications. the exchange of data with these apis is usually performed by using json, a schemaless data format easy for computers to parse and use. while json data is easy to read, its structure is implicit, thus entailing serious problems when integrating apis coming from di erent vendors. web developers have therefore to understand the domain behind each api and study how they can be composed. we tackle this issue by presenting an approach able to both discover the domain of json-based web apis, and identify composition links among them. our approach allows developers to easily visualize what is behind apis and how they can be composed to be used in their applications.
universe polymorphism in coq. universes are used in type theory to ensure consistency by checking that definitions are well-stratified according to a certain hierarchy. in the case of the coq proof assistant, based on the predicative calculus of inductive constructions (pcic), this hierachy is built from an impredicative sort prop and an infinite number of predicative type_i universes. a cumulativity relation represents the inclusion order of uni- verses in the core theory. originally, universes were thought to be floating levels, and definitions to implicitly constrain these levels in a consistent manner. this works well for most theories, however the globality of levels and constraints precludes generic constructions on universes that could work at different levels. universe polymorphism extends this setup by adding local bindings of universes and constraints, supporting generic definitions over universes, reusable at different levels. this provides the same kind of code reuse facilities as ml-style parametric polymorphism. however, the structure and hierarchy of universes is more complex than bare polymorphic type variables. in this paper, we introduce a conservative extension of pcic supporting universe polymorphism and treating its whole hierarchy. this new design supports typical ambiguity and implicit polymorphic generalization at the same time, keeping it mostly transparent to the user. benchmarking the implementation as an extension of the coq proof assistant on real-world examples gives encouraging results.
emf views: dealing with several interrelated emf models. users only need to see some parts of an emf model, others have to get the full model extended with data from another model, and others simply access to a combination of information coming from different models. based on the unquestionable success and usefulness of database views to solve similar problems in the database world, emf views aims to bring the same concepts to the modeling world. this short talk is going to introduce the current version of emf views, notably showing different possible applications of model views such as software developer views, enterprise architect views, view querying or transformation.
improving the scalability of model driven web engineering approaches with runtime transformations. null
an adapter-based approach to co-evolve generated sql in model-to-text transformations. null
on the verification of uml/ocl class diagrams using constraint programming. assessment of the correctness of software models is a key issue to ensure the quality of the final application. to this end, this paper presents an automatic method for the verification of uml class diagrams extended with ocl constraints. our method checks compliance of the diagram with respect to several correctness properties including weak and strong satisfiability or absence of constraint redundancies among others. the method works by translating the uml/ocl model into a constraint satisfaction problem (csp) that is evaluated using state-of-the-art constraint solvers to determine the correctness of the initial model. our approach is particularly relevant to current mda and mdd methods where software models are the primary artifacts of the development process and the basis for the (semi-)automatic code-generation of the final application.
abstract accountability language. usual preventive security mechanisms are not adequate for a world where personal data can be exchanged on-line between different parties and/or stored at multiple jurisdictions. accountability becomes a necessary principle for future computer systems. this is specially critical for the cloud and web applications that collect personal and sensitive data from end users. accountability regards the responsibility and liability (including other attributes) for the data handling performed by a computer system on behalf of an organisation. in case of misconduct (e.g. security breaches, personal data leak, etc.), accountability should imply in remediation and redress actions, as in the real life. contrary to data privacy, which is already supported by several concrete languages, there is currently no language supporting accountability obligations representation. in this work, we provide an abstract language for accountability obligations representation. we analyze two use cases to illustrate the efficiency of our approach in representing accountability obligations in realistic situations.
test data generation for model transformations combining partition and constraint analysis. model-driven engineering (mde) is a software engineering paradigm where models play a key role. in a mde-based development process, models are successively transformed into other models and eventually into the final source code by means of a chain of model transformations. since writing model transformations is an error-prone task, mechanisms to ensure their reliability are greatly needed. one way of achieving this is by means of testing. a challenging aspect when testing model transformations is the generation of adequate input test data. most existing approaches generate test data following a black-box approach based on some sort of partition analysis that exploits the structural features of the source metamodel of the transformation. however, these analyses pay no attention to the ocl invariants of the metamodel or do it very superficially. in this paper, we propose a mechanism that systematically analyzes ocl constraints in the source metamodel in order to fine-tune this partition analysis and therefore, the generation of input test data. our mechanism can be used in isolation, or combined with other black-box or white-box test generation approaches.
formal verification of static software models in mde: a systematic review. context: model-driven engineering (mde) promotes the utilization of models as primary artifacts in all software engineering activities. therefore, mechanisms to ensure model correctness become crucial, specially when applying mde to the development of software, where software is the result of a chain of (semi)automatic model transformations that refine initial abstract models to lower level ones from which the final code is eventually generated. clearly, in this context, an error in the model/s is propagated to the code endangering the soundness of the resulting software. formal verification of software models is a promising approach that advocates the employment of formal methods to achieve model correctness, and it has received a considerable amount of attention in the last few years. objective: the objective of this paper is to analyze the state of the art in the field of formal verification of models, restricting the analysis to those approaches applied over static software models complemented or not with constraints expressed in textual languages, typically the object constraint language (ocl). method: we have conducted a systematic literature review (slr) of the published works in this field, describing their main characteristics. results: the study is based on a set of 48 resources that have been grouped in 18 different approaches according to their affinity. for each of them we have analyzed, among other issues, the formalism used, the support given to ocl, the correctness properties addressed or the feedback yielded by the verification process. conclusions: one of the most important conclusions obtained is that current model verification approaches are strongly influenced by the support given to ocl. another important finding is that in general, current verification tools present important flaws like the lack of integration into the model designer tool chain or the lack of efficiency when verifying large, real-life models.
k*(892)^0 and phi(1020) production in pb-pb collisions at sqrt(snn) = 2.76 tev. the yields of the k*(892) and phi(1020) resonances are measured in pb-pb collisions at sqrt(snn) = 2.76 tev through their hadronic decays using the alice detector. the measurements are performed in multiple centrality intervals at mid-rapidity (|y|&lt;0.5) in the transverse-momentum ranges 0.3.
characterization of the raw gases emitted during the thermal treatment of nanocomposites, and potential impacts on flue gas cleaning systems. as nanocomposites are increasingly used for a wide range of applications, they are expected to end up in waste treatment facilities. thus, it seems appropriate to study at a lab-scale the thermal treatment by incineration of various nanocomposites. our purpose is to identify the main mechanisms of thermal degradation involved when such materials are incinerated, and to fully characterize the raw gases emitted in the combustion chambers, prior implementing fluegas treatment processes (like filtration, etc). with experimental lab-devices, combustion tests are performed and coupled with raw gas analysis to provide a better understanding of the thermal degradation and emission mechanisms.
characterization of aerosols emitted during the incineration of nanocomposites. end-of-life nanocomposites are likely to undergo disposal through thermal treatment like incineration. data from literature indicate that nanocomposites present specific behaviours according to the conditions of thermal treatment [1]. it is thus relevant to understand the mechanisms of emission and thermal degradation of nanocomposites under incineration conditions. thence, the characterization of the effluents during the incineration of nanocomposites, collected downstream the incinerator furnace (or even upstream the flue-gas cleaning systems), provides an insight in the "nanosafety" of this technology [2], useful to cope with nanowastes (wastes containing nanomaterials). at a lab-scale, the emission mechanisms and the thermal degradation mechanisms were determined using various devices: a microcalorimeter pcfc and a fire propagation apparatus (fpa tewarson) coupled with a fourier transformed infrared (ftir); as well as a modified tubular furnace (fig. 1) coupled with a gas analyser. conditions of thermal degradation implemented in those laboratory devices were accurately characterized by evaluating and controlling (whenever possible) the key operational parameters that govern an incineration process, i.e. chiefly: temperature, residence time, air-excess and turbulence. electrical low pressure impactor (elpi) coupled with combustion devices and the morphology was determined using tem observation (transmission electronic microscopy) via a mps (mini-particle-sampler) [3]. our tests were performed on specimens of nanocomposites incorporating different mineral fillers (like sepiolite and halloysite nanoclays) and neat polymer matrices. the characterization of the combustion aerosols from the small-scale thermal degradation of nanocomposites is a first step for determining nano- objects potentially released during incineration process.
the nanofluegas project : characterization of nanoparticulate emissions from the incineration of wastes containing manufactured nanomaterials. the wide development of nanotechnologies requires a consideration of nanosafety during the whole life cycle of products containing nanomaterials. yet, there is neither specialized procedure for the waste management of nano-objects at the end of their life nor associated regulation. the nanofluegas project has been recently launched for three years (2011-2014) to investigate the safe incineration of wastes containing nanomaterials. the main objectives of this project supported by ademe are: (1) to provide a better understanding of the possible mechanisms of nanoparticles release during the combustion of nanowastes and (2) to evaluate the efficiency of existing processes. the initial works of the nanofluegas aim at: (1) identifying deposits containing nanomaterials; (2) selecting three representative nanowastes. the experimental part is twofold: (1) investigate emission mechanisms and characterization of nanomaterials in the smoke during the incineration at pilot and industrial scales; (2) evaluate the efficiency of main available pollution abatement processes, thus providing fully adapted recommendations on procedures and technological features for the whole processing of nanowastes.this communication describes the different steps of the project and presents preliminary results.
characterization of nanoparticulate emissions from the incineration of wastes containing manufactured nanomaterials. null
the nanofluegas project : characterization and reduction of particulate emissions from the incineration of wastes containing manufactured nanomaterials. in view of sustainable innovation, the development of nanotechnologies requires a deep insight in the nanosafety during the whole life cycle of products containing nanomaterials, from production to recycling and final destruction. no peculiar approved procedure exists for the waste management of nanoobjects at their end of life, due to the so far regulatorily unrecognized nano-specificity of such emerging products. the french nanofluegas project (2011-2014), supported by ademe and led by ineris, will examine nanosafety during the final destruction of nanomanufactured products by incineration, i) to understand possible mechanisms of nanoparticle release during combustion, and ii) to evaluate the efficiency of current pollution control devices. this project benefits from the know-how of both the ecole des mines de nantes gepea laboratory, a main academic partner specialized in engineering, and tredi-séché environnemen, a major expert of hazardous wastes incineration. the initial efforts focused on: i) identifying possible sources containing nanomaterials; ii) selecting three representative nanomaterials; and iii) providing small-scale incineration devices as close as possible to real scale installations. the experimental part is twofold. first, a deep insight is provided on the emission mechanisms and on the characterization of nanoparticles in the smokes (particle number, size, chemical composition, morphology...) during incineration at both pilotscale and on industrial units. secondly, the nanofluegas consortium will review the efficiency of main available pollution abatement processes, thus providing fully adapted recommendations on procedures and technological features for the whole processing of nanowastes. herein, we describe the different steps of the project, providing preliminary results.
direct photon measurements with alice. null
neo4emf, a scalable persistence layer for emf models. several industrial contexts require software engineering methods and tools able to handle large -size artifacts. the central idea of abstraction makes model-driven engineering (mde) a promising approach in such contexts, but current tools do not scale to very large models (vlms): already the task of storing and accessing vlms from a persisting support is currently ine cient. in this paper we propose a scalable persistence layer for the de-facto standard mde framework emf. the layer exploits the e ciency of graph databases in storing and accessing graph structures, as emf models are. a preliminary experimentation shows that typical queries in reverse-engineering emf models have good performance on such persistence layer, compared to le-based backends.
enforcing expressive accountability policies. accountability policies for the enforcement of the responsible stewardship of personal data have to support the gathering of information at all levels of the service stack and across different policy domains, for instance, for the retrospective enforcement of transparency and remediation properties. existing approaches to accountability, however, often do not meet these requirements and corresponding implementation support is generally lacking. in this paper we show how expressive policies can be defined in terms of properties that change across boundaries of policy domains, include access to data at different levels of the service stack, and support preventive and retrospective mechanisms for different accountability properties, notably transparency and remediability. furthermore, we present a notion of accountability schemes that support the constructive implementation of accountability policies. finally, we motivate and apply our approach in the context of real-world attacks to oauth-based authorization and authentication schemes.
unusual structure, bonding and properties in a californium borate. the participation of the valence orbitals of actinides in bonding has been debated for decades. recent experimental and computational investigations demonstrated the involvement of 6p, 6d and/or 5f orbitals in bonding. however, structural and spectroscopic data, as well as theory, indicate a decrease in covalency across the actinide series, and the evidence points to highly ionic, lanthanide-like bonding for late actinides. here we show that chemical differentiation between californium and lanthanides can be achieved by using ligands that are both highly polarizable and substantially rearrange on complexation. a ligand that suits both of these desired properties is polyborate. we demonstrate that the 5f, 6d and 7p orbitals are all involved in bonding in a cf(iii) borate, and that large crystal-field effects are present. synthetic, structural and spectroscopic data are complemented by quantum mechanical calculations to support these observations.
fuml as an assembly language for mda. null
pseudo jahn-teller effect, spin-orbit coupling, and electron correlation in the xf&lt;sub&gt;3&lt;/sub&gt; (x=cl, br, i, at) series. null
the v protein of tioman virus is incapable of blocking type i interferon signaling in human cells. the capacity of a virus to cross species barriers is determined by the development of bona fide interactions with cellular components of new hosts, and in particular its ability to block ifn-α/β antiviral signaling. tioman virus (tiov), a close relative of mumps virus (muv), has been isolated in giant fruit bats in southeast asia. nipah and hendra viruses, which are present in the same bat colonies, are highly pathogenic in human. despite serological evidences of close contacts between tiov and human populations, whether tiov is associated to some human pathology remains undetermined. here we show that in contrast to the v protein of muv, the v protein of tiov (tiov-v) hardly interacts with human stat2, does not degrade stat1, and cannot block ifn-α/β signaling in human cells. in contrast, tiov-v properly binds to human stat3 and mda5, and thus interferes with il-6 signaling and ifn-β promoter induction in human cells. because stat2 binding was previously identified as a host restriction factor for some paramyxoviridae, we established stat2 sequence from giant fruit bats, and binding to tiov-v was tested. surprisingly, tiov-v interaction with stat2 from giant fruit bats is also extremely weak and barely detectable. altogether, our observations question the capacity of tiov to appropriately control ifn-α/β signaling in both human and giant fruit bats that are considered as its natural host.
a framework for variable content document generation with multiple actors. context: advances in customization have highlighted the need for tools supporting variable content document management and generation in many domains. current tools allow the generation of highly customized documents that are variable in both content and layout. however, most frameworks are technology-oriented, and their use requires advanced skills in implementationrelated tools, which means their use by end users (i.e. document designers) is severely limited. objective: starting from past and current trends for customized document authoring, our goal is to provide a document generation alternative in which variants are specified at a high level of abstraction and content reuse can be maximized in high variability scenarios. method: based on our experience in document engineering, we identified areas in the variable content document management and generation field open to further improvement. we first classified the primary sources of variability in document composition processes and then developed a methodology, which we called dpl - based on software product lines principles - to support document generation in high variability scenarios. results: in order to validate the applicability of our methodology we implemented a tool - dplfw - to carry out dpl processes. after using this in different scenarios, we compared our proposal with other state-of-the-art tools for variable content document management and generation. conclusion: the dplfw showed a good capacity for the automatic generation of variable content documents equal to or in some cases surpassing other currently available approaches. to the best of our knowledge, dplfw is the only framework that combines variable content and document workflow facilities, easing the generation of variable content documents in which multiple actors play different roles.
reaction-diffusion approach in soft diffraction. we apply the reaction-diffusion (stochastic) approach to the numerical calculation of the elastic amplitude in the reggeon field theory (rft) and its single diffractive cut. fits to the total, integrated and differential elastic cross sections with account of all pomeron loops are reported together with all-loop calculation of the single difraction dissociation cross section.
vacuum structure in 3d supersymmetric gauge theories. this minireview (written on the basis of the talk that the author delivered at the pomeranchuk memorial conference in itep in june 2013 and his original papers written before on this subject) is devoted to the problem of vacuum dynamics in 3d supersymmetric yang-mills-chern-simons theories with and without extra matter multiplets. by analyzing the effective born-oppenheimer hamiltonian in a small spatial box, we calculate the number of vacuum states (the witten index) for these theories and analyze their structure. the results coincide with those derived by other methods.
beam energy dependence of moments of the net-charge multiplicity distributions in au+au collisions at rhic. we report the first measurements of the moments -mean ($m$), variance ($\sigma^{2}$), skewness ($s$) and kurtosis ($\kappa$) -of the net-charge multiplicity distributions at mid-rapidity in au+au collisions at seven energies, ranging from $\sqrt {{s_{\rm nn}}}$=7.7 to 200 gev, as a part of the beam energy scan program at rhic. the moments are related to the thermodynamic susceptibilities of net-charge, which are expected to diverge at the qcd critical point. we compare the products of the moments, $\sigma^{2}/m$, $s\sigma$ and $\kappa\sigma^{2}$ with the expectations from poisson and negative binomial distributions (nbd). the $s\sigma$ values deviate from poisson and are close to nbd baseline, while the $\kappa\sigma^{2}$ values tend to lie between the two. within the present uncertainties, our data do not show clear evidence of non-monotonic behavior as a function of collision energy.
dielectron azimuthal anisotropy at mid-rapidity in au+au collisions at $\sqrt{s_{_{nn}}} = 200$ gev. we report on the first measurement of the azimuthal anisotropy ($v_2$) of dielectrons at mid-rapidity from $\sqrt{s_{_{nn}}} = 200$ gev au+au collisions with the star detector at rhic, presented as a function of transverse momentum ($p_t$) for different invariant mass regions. in the mass region $m_{ee}/!&lt;1.1$ gev/$c^2$ the dielectron $v_2$ measurements are found to be consistent with expectations from $\pi^{0}$, $\eta$, $\omega$ and $\phi$ decay contributions. in the mass region $1.1/!.
beam-energy-dependent two-pion interferometry and the freeze-out eccentricity of pions measured in heavy ion collisions at the star detector. we present results of analyses of two-pion interferometry in au+au collisions at $\sqrt{s_{nn}}$ = 7.7, 11.5, 19.6, 27, 39, 62.4 and 200 gev measured in the star detector as part of the rhic beam energy scan program. the extracted correlation lengths (hbt radii) are studied as a function of beam energy, azimuthal angle relative to the reaction plane, centrality, and transverse mass ($m_{t}$) of the particles. the azimuthal analysis allows extraction of the eccentricity of the entire fireball at kinetic freeze-out. the energy dependence of this observable is expected to be sensitive to changes in the equation of state. a new global fit method is studied as an alternate method to directly measure the parameters in the azimuthal analysis. the eccentricity shows a monotonic decrease with beam energy that is qualitatively consistent with the trend from all model predictions and quantitatively consistent with a hadronic transport model.
methodology of the state approach control. this chapter deals with advanced control methodology based on h2 control design. the controlled system and the exogeneous signals as well are considered through state-space models. the associated h2 problem is non standard (not necessarily stabilizable or detectable). the solution of this extended problem is given, with new conditions making the riccati equation involved solvable. the continuous-time and exactly discretized problems are both considered.
discrete time systems. the chapter deals with some fundamentals about analysis and manipulation of discrete signals and systems, and about discretization of linear systems.
measurement of quarkonium production at forward rapidity in pp collisions at sqrt{s}= 7 tev. the inclusive production cross sections at forward rapidity of j/psi, psi(2s), upsilon(1s) and upsilon(2s) are measured in pp collisions at sqrt{s} = 7 tev with the alice detector at the lhc. the analysis is based in a data sample corresponding to an integrated luminosity of 1.35 pb^-1. quarkonia are reconstructed in the dimuon-decay channel and the signal yields are evaluated by fitting the mu+mu- invariant mass distributions. the differential production cross sections are measured as a function of the transverse momentum pt and rapidity y, over the ranges 0 &lt; pt &lt; 20 gev/c for j/psi, 0 &lt; pt &lt; 12 gev/c for all other resonances and for 2.5 &lt; y &lt; 4. the measured cross sections integrated over pt and y, and assuming unpolarized quarkonia, are: sigma_{j/psi} = 6.69 +- 0.04 +- 0.63 microbarn, sigma_{psiprime} = 1.13 +- 0.07 +- 0.14 microbarn, sigma_{upsilon(1s)} = 54.2 +- 5.0 +- 6.7 nb and sigma_{upsilon(2s)} = 18.4 +- 3.7 +- 2.2 nb, where the first uncertainty is statistical and the second one is systematic. the results are compared to measurements performed by other lhc experiments and to theoretical models.
open bottom states and the anti-b meson propagation in hadronic matter. the interaction and propagation of anti-b mesons with light mesons, n and delta baryons is studied within a unitarized approach based on effective models that are compatible with chiral and heavy-quark symmetries. we find several heavy-quark spin doublets in the open-bottom sectors, where anti-b and anti-b* mesons are present. in the meson sector we find several resonant states, among them, a b0 and a b1 with masses 5530 mev and 5579 mev as well as bs0* and bs1* narrow states at 5748 mev and 5799 mev, respectively. they form two doublets with no experimental identification yet, the first one being the bottom counterpart of the d0(2400) and d1(2430) states, and the second bottom doublet associated to the ubiquitous ds0* (2317) and the ds1 (2460). in the baryon sector, several lambda_b and sigma_b doublets are identified, among them the one given by the experimental lambda_b(5910) and lambda*_b(5921). moreover, one of our states, the sigma_b*(5904), turns out to be the bottom counterpart of the sigma*(1670) and sigma_c*(2549), which is a case for discovery. we finally analyze different transport coefficients for the anti-b meson in hot matter, such as formed in heavy-ion collisions at rhic and lhc. for rhic/lhc energies, the main contribution to the coefficients comes from the interaction of anti-b mesons with pions. however, we also include the effects of baryonic density which might be sizable at temperatures t &lt; 100 mev, as the chemical potential is expected to increase in the last stages of the expansion. we conclude that although the relaxation time decreases with larger baryonic densities, the anti-b meson does not thermalize at rhic/lhc energies, representing an ideal probe for the initial bottom distribution.
azimuthal emission patterns of $k^{+}$ and of $ k^{-} $ mesons in ni + ni collisions near the strangeness production threshold. azimuthal emission patterns of $k^\pm$ mesons have been measured in ni + ni collisions with the fopi spectrometer at a beam kinetic energy of 1.91 a gev. the transverse momentum $p_{t}$ integrated directed and elliptic flow of $k^{+}$ and $k^{-}$ mesons as well as the centrality dependence of $p_{t}$ - differential directed flow of $k^{+}$ mesons are compared to the predictions of hsd and iqmd transport models. the data exhibits different propagation patterns of $k^{+}$ and $k^{-}$ mesons in the compressed and heated nuclear medium and favor the existence of a kaon-nucleon in-medium potential, repulsive for $k^{+}$ mesons and attractive for $k^{-}$ mesons.
natural modeling: retrospective and perspectives an anthropological point of view. is extreme modeling so extreme? we advocate that natural modeling might be a better term. after all, the ultimate goal is to enable modelers to perform their job naturally. in the century of the "disappearing computer", it definitively makes sense to search for non invasive and flexible modeling technologies. this paper considers modeling from an anthropological point of view. a retrospective starting back to the prehistoric age leads to new perspectives for natural modeling in the information age. it is shown (1) that the need for compromises between flexibility and formality is "natural" rather than "extreme", (2) that the languages are emergent by nature, and (3) that natural interfaces should be provided to all stakeholders. we advocate that surface computing, tangible user-interfaces, collaborative modeling and emergent (meta)modeling are future research directions to be investigated in order to make "extreme" modeling just "natural". just as it should be.
isolated photon production measurement in p-p collisions at √s= 7 tev with the alice detector. the high transverse momentum photon production inproton-proton collisions (p-p) is described by perturbativequantum chromodynamics (pqcd). among thesephotons, those produced directly by an energetic partonicinteraction (called direct photons) are of great interestsince their measurement allows to test pqcdpredictions and it allows also the constraint of protonstructure functions. the work of this thesis aims atstudying and measuring direct photons produced in p-pcollisions at 7 tev with the alice detector. the aliceelectromagnetic calorimeter (emcal) is used to achievethis measurement which is based on an isolation procedurethat allows to reduce background coming fromother photon production modes (fragmentation, decay).multiple aspects like emcal data quality, photon identificationas well as spectrum correction and its normalizationare highlighted. finally, the first isolated promptphoton cross-section measured with alice detector ispresented, compared to theoretical predictions and tothe last results from other lhc experiments.
determination of absolute gas adsorption isotherms: simple method based on the potential theory for buoyancy effect correction of pure gas and gas mixtures adsorption. the absolute adsorption isotherms are necessary to correctly evaluate the selectivity of the adsorbent material or to design adsorption processes at high pressure (e.g., h2 purification from syngas processes, removal of acid gas from natural gas,...). the aim of this work is thus to propose an easy method to correct the buoyancy effect of the bulk phase on the adsorbed phase volume during both pure gas and gas mixtures adsorption for pressures up to 10 mpa. the potential theory of adsorption and the dubinin-radushkevich relation are adapted by introducing mixing parameters based on simple berthelot rules. the concept of internal pressure used to characterize the adsorbed phase is also adapted for mixtures. the method is then improved on a commercial activated carbon (ac), when adsorbing pure h2s and ch4, and their mixtures up to 5 mpa. the study points out the importance to carefully consider the buoyancy effect of the bulk phase on the adsorbed phase volume. its impact on the adsorbent material selectivity at high pressures could affect the design and the performances of psa or tsa processes. for example, only considering the excess adsorption data leads to an apparent selectivity 13 % greater than the absolute one for a concentration of 6 ppm of h2s in a ch4 matrix at 5 mpa (298 k) on the ac.
performance of the alice experiment at the cern lhc. alice is the heavy-ion experiment at the cern large hadron collider. the experiment continuously took data during the first physics campaign of the machine from fall 2009 until early 2013, using proton and lead-ion beams. in this paper we describe the running environment and the data handling procedures, and discuss the performance of the alice detectors and analysis methods for various physics observables.
probing the radio emission from air showers with polarization measurements. the emission of radio waves from air showers has been attributed to the so-called geomagnetic emission process. at frequencies around 50 mhz this process leads to coherent radiation which can be observed with rather simple setups. the direction of the electric field induced by this emission process depends only on the local magnetic field vector and on the incoming direction of the air shower. we report on measurements of the electric field vector where, in addition to this geomagnetic component, another component has been observed which cannot be described by the geomagnetic emission process. the data provide strong evidence that the other electric field component is polarized radially with respect to the shower axis, in agreement with predictions made by askaryan who described radio emission from particle showers due to a negative charge-excess in the front of the shower. our results are compared to calculations which include the radiation mechanism induced by this charge-excess process.
application of sensitivity analysis in building energy simulations: combining first- and second-order elementary effects methods. sensitivity analysis plays an important role in the understanding of complex models. it helps to identify the influence of input parameters in relation to the outputs. it can also be a tool to understand the behavior of the model and can then facilitate its development stage. this study aims to analyze and illustrate the potential usefulness of combining first and second-order sensitivity analysis, applied to a building energy model (esp-r). through the example of an apartment building, a sensitivity analysis is performed using the method of elementary effects (also known as the morris method), including an analysis of the interactions between the input parameters (second-order analysis). the usefulness of higher-order analysis is highlighted to support the results of the first-order analysis better. several aspects are tackled to implement the multi-order sensitivity analysis efficiently: interval size of the variables, the management of non-linearity and the usefulness of various outputs.
adding virtualization capabilities to the grid'5000 testbed. null
tio2 photocatalytic oxidation of indoor vocs at ppb levels in a multi-pass dynamic reactor: influence of vocs mixture on reaction intermediates concentrations. null
micro-ingredients carry-over during bucket elevator handling in feed industry: influence of process parameters. currently, carry-over level of feed production lines can be accurately defined, but the causes are not identified yet because of a misunderstanding of this phenomenon. on-site studies charged conveying equipment between mixer and pelleting press, and especially bucket elevator, to be responsible for carry-over level increasing. a series of experiments has been carried out on a bucket elevator test bench in order to determine how process parameters influence carry-over phenomenon. product deposits on equipment walls represent, on average, 3.0% of the initial batch mass and contain about 0.2% of micro-ingredient after one tracer batch passing and 0.1% after flushing batch passing. experimental design results showed that parameters acting on micro-ingredient deposited are different from those influencing collected product. therefore, optimal position of tested process parameters will depend on the batch objective regarding its position in the production schedule. practical applicationsthis work allows a new approach of understanding carry-over phenomenon at pilot scale. indeed, process parameters acting on micro-ingredient deposits by a batch n have been differentiated from those acting on micro-ingredient gathering by a batch n+1. by this way, the results of this experimental study brought industrial solutions to control micro-ingredient transfer, considering the batch position in the whole sequencing.
gas phase photocatalytic oxidation of decane at ppb levels: removal kinetics, reaction intermediates and carbon mass balance. this study focuses on the photocatalytic oxidation of decane at ppb levels, close to indoor air conditions. although these concentrations are typical of indoor air volatile organic compound (voc) levels, such conditions have been poorly investigated to date. decane conversion rates higher than 90% were reached within 15 h for the highest initial concentration tested in the operating conditions used. despite this high decane conversion, 18 reaction intermediates were detected in the gas phase. the main compounds, in terms of concentration level in the gas phase, were formaldehyde, acetaldehyde and propanal. the amounts of these compounds in the gas phase were linearly dependent on the initial decane concentration. a reaction pathway is proposed, based on reaction intermediate temporal profiles and the literature. it consists of six main steps describing the oxidation process from decane to co and co2. the influence of the relative humidity level on the diversity and amounts of reaction intermediates was also studied. it was shown that moisture tends to shift the equilibrium of intermediates adsorption toward desorption, resulting in a relative increase in intermediate quantities in the gas phase. however, the monitoring of co and co2 formation highlighted that, at the end of the reaction, a high mineralization rate could be obtained. finally, an overview of the photocatalytic reaction is given through the carbon mass balance determined for various reaction advancements. this approach gives an overall evaluation of the photocatalytic process performance, from primary compound removal to reaction intermediate and mineralization. (c) 2013 elsevier b.v. all rights reserved.
adsorption phenomena in photocatalytic reactions: the case of toluene, acetone and heptane. nowadays, with the increase in the thermal insulation of buildings, indoor air quality (iaq) has deteriorated, particularly because of the presence of volatile organic compounds (voc). to improve iaq photocatalytic processes can be used. photocatalytic reactions can be broken down into three steps: adsorption of pollutants, chemical reaction, and desorption of water, carbon dioxide and by-products. in this work, the accessibility of the pollutants to the reactive sites of a commercial tio(2)-photocatalyst is studied. firstly, adsorption mechanisms are investigated through toluene adsorption isotherms in batch reactors. in humid air conditions (relative humidity of 50% at 24 degrees c), the classical adsorption models cannot be applied. consequently, a new model, called the "langmuir-multi", is built. it fits the obtained experimental data properly. adsorption equilibrium constants are calculated based on this new model. to understand adsorption mechanisms better, adsorption isotherms are also performed in dry air conditions with toluene and in humid air with acetone and heptane. it is observed that water vapor plays a major role. secondly, photocatalytic reactions are carried out with toluene, acetone and heptane in humid air. the kinetic curves are well-represented by the langmuir-hinshelwood (l-h) equation so that reaction and equilibrium constants can be assessed. the l-h equilibrium constant appears to depend on the type of pollutant and on the affinity between the pollutant and the photocatalyst surface. no correlation is found between the equilibrium constant and light intensity. it is also shown that the calculated l-h equilibrium constants are not equal to those previously obtained in dark conditions and in batch reactors. (c) 2011 elsevier b.v. all rights reserved.
retention of iodide by the callovo-oxfordian formation: an experimental study. null
bayesian updating for road maintenance optimization. pavement structures are subject to several deterioration patterns classified in surface or structural failure modes. structural deteriorations are frequent and lead to heavy and costly maintenance. we restrict our study to the fatigue longitudinal cracks which arise in the underlying layers and growth up to the surface due to traffic repetitive tensile stresses. the characterization of the complete cracking process is very complex because of a large number of covariates and the strong randomness of the environment (climate and traffic loads). moreover, the current maintenance indicator is a cracking percentage of the road section surface. it gives only partial information onto the underlying racking. in such a context, a condition-based maintenance model on a single variable does not allow to guarantee an optimal maintenance decision. in [10], we have proposed a new model for the longitudinal cracking based on a bivariate stochastic process where the joint probability is a function of the current system state. it allows first to propose a new modeling of imperfect maintenance and then to differentiate maintenance according to their own cracking speed. nevertheless, one of the main limits is the difficulty of its implementation in operation. the objective of this work is to deepen the model in [10] for improving its applicability for road maintenance while keeping their theoretical properties and advantages. two directions are developed. first, a new definition of the bivariate deterioration process and the construction of the respective joint probability law based on classical results in bayesian theory are presented. then the derivation of the statistical framework for estimating the associated parameters will be proposed. the second direction is in the modeling of the uncertainty in the maintenance impact onto the cracking process.
improved lighthill fish swimming model for bio-inspired robots - modelling, computational aspects and experimental comparisons. the best known analytical model of swimming was originally developed by lighthill and is known as large amplitude elongated body theory (laebt). recently, this theory has been improved and adapted to robotics through a series of studies [boyer et al., 2008, 2010; candelier et al., 2011] ranging from hydrodynamic modelling to mobile multibody system dynamics. this article marks a further step towards the lighthill theory. the laebt is ap- plied to one of the best bio-inspired swimming robots yet built: the amphibot iii, a modular anguilliform swimming robot. to that end, we apply a newton-euler modelling approach and focus our attention on the model of hydrodynamic forces. this model is numerically in- tegrated in real time by using an extension of the newton-euler recursive forward dynamics algorithm for manipulators to a robot without a fixed base. simulations and experiments are compared on undulatory gaits and turning manoeuvres for a wide range of parameters. the discrepancies between modelling and reality do not exceed 16% for the swimming speed, while requiring only the one-time calibration of a few hydrodynamic parameters. since the model can be numerically integrated in real time, it has significantly superior accuracy com- pared with computational speed ratio, and is, to the best of our knowledge, one of the most accurate models that can be used in real-time. it should provide an interesting tool for the design and control of swimming robots. the approach is presented in a self contained manner, with the concern to help the reader not familiar with fluid dynamics to get insight both into the physics of swimming and the mathematical tools that can help its modelling.
a 4d-sequencing approach for air traffic management. the current air traffic system is forecasted to face strong challenges due to the continuous increase in air traffic demand. hence, there is a need for new types of organization permitting a more efficient air traffic management, with both a high capacity and a high level of safety, and possibly with a reduced environmental impact. in this article, we study a holistic approach, consisting in designing across europe a very organized air traffic system, as opposed to free flight, to reduce costs while maintaining safety. our work is based on the moving point paradigm, initially presented in prot et al, 2010 (using graph concepts to assess the feasibility of a sequenced air traffic flow with low conflict rate, european journal of operational research, 207 (1), pp 184-196). we give theoretical background to design conflict-free routes with high capacity and propose, based on these results, the allocation of aircraft to a new system of air routes, that include both lattices and orthodromic routes. the efficiency of the approach is assessed through simulations based on real data sets representing a full day of traffic over the whole european sky. the numerical results demonstrates a drastic reduction of the conflicts rate compared to the actual commercial routes, with a very limited fuel overconsumption.
model-based testing of global properties on large-scale distributed systems. large-scale distributed systems are becoming commonplace with the large popularity of peer-to-peer and cloud computing. the increasing importance of these systems contrasts with the lack of integrated solutions to build trustworthy software. a key concern of any large-scale distributed system is the validation of global properties, which cannot be evaluated on a single node. thus, it is necessary to gather data from distributed nodes and to aggregate these data into a global view. this turns out to be very challenging because of the system's dynamism that imposes very frequent changes in local values that affect global properties. this implies that the global view has to be frequently updated to ensure an accurate validation of global properties. in this paper, we present a model-based approach to define a dynamic oracle for checking global properties. our objective is to abstract relevant aspects of such systems into models. these models are updated at runtime, by monitoring the corresponding distributed system. we conduce real-scale experimental validation to evaluate the ability of our approach to check global properties. in this validation, we apply our approach to test two open-source implementations of distributed hash tables. the experiments are deployed on two clusters of 32 nodes. the experiments reveal an important defect on one implementation and show clear performance differences between the two implementations. the defect would not be detected without a global view of the system. testing global properties on distributed software consists of gathering data from different nodes and building a global view of the system, where properties are validated. this process requires a distributed test architecture and tools for representing and validating global properties. model-based techniques are an expressive mean for building oracles that validate global properties on distributed systems.
a language support for cloud elasticity management. elasticity is the intrinsic element that differentiates cloud computing from traditional computing paradigm, since it allows service providers to rapidly adjust their needs for resources to absorb the demand and hence guarantee a minimum level of quality of service (qos) that respects the service level agreements (slas) previously defined with their clients. however, due to non-negligible resource initiation time, network fluctuations or unpredictable workload, it becomes hard to guarantee qos levels and sla violations may occur. this paper proposes a language support for cloud elasticity management that relies on csla (cloud service level agreement). csla offers new features such as qos/functionality degradation and an advanced penalty model that allow providers to finely express contracts so that services self-adaptation capabilities are improved and sla violations minimized. the approach was evaluated with a real infrastructure and application testbed. experimental results show that the use of csla makes cloud services capable of absorbing more peaks and oscillations by trading-off the qos levels and costs due to penalties.
a cloud accountability policy representation framework. nowadays we are witnessing the democratization of cloud services. as a result, more and more end- users (individuals and businesses) are using these services for achieving their electronic transactions (shopping, administrative procedures, b2b transactions, etc.). in such scenarios, personal data is generally flowed between several entities and end-users need (i) to be aware of the management, processing, storage and retention of personal data, and (ii) to have necessary means to hold service providers accountable for the usage of their data. in fact, dealing with personal data raises several privacy and accountability issues that must be considered before to promote the use of cloud services. in this paper, we propose a framework for the representation of cloud accountability policies. such policies offer to end-users a clear view of the privacy and accountability obligations asserted by the entities they interact with, as well as means to represent their preferences. this framework comes with two novel accountability policy languages. an abstract one devoted for the representation of preferences/obligations in an human readable fashion. and a concrete one for the mapping to concrete enforceable policies. we motivate our solution with concrete use case scenarios.
towards improvement of natural gas-diesel dual fuel mode: an experimental investigation on performance and exhaust emissions. abstract the use of natural gas in compression ignition engines as supplement to liquid diesel in a dual fuel combustion mode is a promising technique. in this study, the effect of \df\ (dual fuel) operating mode on combustion characteristics, engine performances and pollutants emissions of an existing diesel engine using natural gas as primary fuel and neat diesel as pilot fuel, has been examined. at moderate and relatively high loads, the results show very interesting behavior of dual fuel operating mode in comparison to conventional diesel, both for engine performance and emissions. it showed a simultaneous reduction of soot and \nox\ species over a large engine operating area. moreover, it showed the possibility to obtain lower \bsfc\ (brake specific fuel consumption) than conventional diesel engine. however, this mode presents some deficits at low loads, especially concerning unburned hydrocarbons and carbon monoxide emissions. understanding those deficiencies is a key of such engines improvement. some suggestions for new measures towards \df\ mode improvement are deduced.
catalytic hydroliquefaction of charcoal \ccb\ (copper, chromium and boron)-treated wood for bio-oil production: influence of \ccb\ salts, residence time and catalysts. abstract thermochemical processes offer a feasible option for wood waste management and the recovery of a variety of useful chemicals. in this paper, hydroliquefaction with the use of catalysts was optimized to provide bio-oil from ccb-treated wood by reducing gaseous emissions of copper, chromium and boron (hazardous materials). in addition, the influence of \ccb\ salts, catalysts (al2o3, na2co3, mgo and caco3) and residence time on the hydroliquefaction process was investigated. for this, hydroliquefaction of charcoal obtained by slow pyrolysis of ccb-treated wood was conducted under hydrogen pressure in presence of tetralin. the results showed that \ccb\ salts and catalysts increase the yield of bio-oil compared to hydroliquefaction of charcoal from untreated wood. it was also observed, that the use of catalysts improves the residence time during the process. among the catalysts employed, al2o3 appears to be the most effective. furthermore na2co3 promotes the formation of gaseous species particularly ch4. analyses of hazardous materials in charcoal residue (coke) illustrate their transfer to the bio-oil with the increase of bio-oil yield and residence time except when al2o3 was using. the bio-oil obtained contains aromatic compounds.
speciation of technetium in acidic media : effect of α radiations. this project is part of the fundamental study of technetium speciation in highly acidic medium. the behaviour of technetium in htfms was carried out in the absence then in the presence of α irradiation. given these two different conditions, spectrophotometric results of tc(vii) reduction are similar. xas analysis indicates the formation of a cyclic dimer of tc(iv) complexed to triflate ligands and formulated astc₂o₂(cf₃so₃)₄(h₂o)₄. this compound is linearized to tciv-o-tciv with the increase of htfms concentration. at high concentration of htfms +98% (11.15 m), the protonated species tco₃(oh)(h₂o)₂ which is formed in the absence of external ionizing radiations, is reduced to the v oxidation state under α irradiation. structural characterization by exafs spectroscopy and dft calculations suggests the formation of monomer species of tc(v)-triflate complexes where [otc(f₃cso₃)₂(h₂o)₂]⁺ and [otc(f₃cso₃)₂(oh)₂]⁻ compounds were proposed. in concentrated h₂so₄ (ch₂so₄ ≥ 12 m), α-radiolysis experiments of tc(vii) were performed in order to compare the radiolytic behaviour of tc(vii) in both comparable media htfms and h₂so₄. xanes studies show that radiolytic reduction of tc(vii) leads to the formation of tc(v)-tc(vii) mixture in h₂so₄ 13 m and just tc(v) in 18 m of h₂so₄. the analysis of exafs spectra is consistent with the formation of [tco(hso₄)₃(h₂o)₂] and [tco(hso₄)₃(h2o)(oh)]⁻ monomer complexes in h₂so₄ 13 m and [tc(hso₄)₃(so₄)(h₂o)] and [tc(hso₄)₃(so₄)(oh)]⁻ species at 18 m of h₂so₄.
dmaas : syntactic, structural and semantic mediation for service composition. service composition is a major advance service-oriented computing brings to enable the development of distributed applications. however, the distributed nature of services hampers their composition with data heterogeneity problems. in this paper, we address these problems with a decentralized mediation-as-a-service architecture that solves data inconsistencies occurring during the composition of business services. as an extension to our previous work that focused on data interpretation problems, we present in this paper a solution to solve data inconsistencies at the syntactic, structural and semantic levels. we show how syntactic, structural and semantic mediation techniques can be combined, and how semantic mediation provides useful information that helps structural and syntactic mediation. we demonstrate how our architecture enables decentralized publication and discovery of mediation services. we motivate our work with a concrete scenario and validate our proposal with experiments.
a comprehensive study on performance, emission, and combustion characteristics of a dual-fuel engine fuelled with orange oil and jatropha oil. performance of a single-cylinder, water-cooled, direct-injection diesel engine on dual-fuel operation with jatropha oil (jo) as pilot fuel and orange oil as primary fuel was evaluated. constant load test at different power outputs was conducted at the rated speed of 1500 r/min with varying orange oil quantities. the loads were fixed as 20 per cent, 40 per cent, 60 per cent, 80 per cent, and 100 per cent. in dual-fuel operation with orange oil induction, the thermal efficiency of jo was increased mainly at high power outputs. maximum thermal efficiency with jo was found as 29 per cent at 31 per cent of orange oil induction at 100 per cent load. smoke was reduced significantly with all orange oil induction rates at all power outputs in dual-fuel operation with jo. it was reduced from 4.4 to 3.3 bsu (bosch smoke units) with jo at the maximum efficiency point at 100 per cent load. hc emissions were increased further at all power outputs in the dual-fuel mode with all rates of orange oil induction. dual-fuel operation increased the ignition delay of jo. however, peak pressure and energy release rates were improved in the dual-fuel operation with orange oil induction. in general, dual-fuel operation with orange oil as inducted fuel with jo as pilot fuel showed inferior performance and emissions at part loads. it is concluded that the jo as pilots fuel and orange oil as the inducted fuel could be used in diesel engines with reduced smoke levels and improved thermal efficiencies with no major detoriation in performance.
use of palm oil-based biofuel in the internal combustion engines: performance and emissions characteristics. palm oil (po) was treated using different methods in order to use and test it as fuel in compression ignition (cl) engines. the treatments include po preheated and preparation of po/diesel oil blends, using mixtures of po with waste cooking oil (wco), which are converted into esters by a transesterification process. the purpose of this study is to evaluate the potential of the palm oil-based biofuels to replace diesel oil in cl engines. tests were conducted in a single cylinder, four-stroke, air-cooled, direct injection diesel engine (no engine modifications were required). experiments were initially carried out with diesel oil for providing baseline data. all the tested fuels have a low heating value compared to diesel fuel. a high fraction of po in diesel fuel decreases the heating value of the blend. the brake thermal efficiency increases for the po/diesel blends. hc emissions for all those fuels except for the po/diesel blends are found lower, while co emissions rise for all types of fuels. no(x) emissions are higher at low load, but lower at full load, for the engine fueled with po and lower both at middle and full load for the engine fueled with the esters. (c) 2011 elsevier ltd. all rights reserved.
thermogravimetric investigation and thermal conversion kinetics of typical north african and middle eastern lignocellulosic wastes. the aim of this work was to thermally characterize the renewable lignocellulosic bioresources derived from palm trees in order to highlight their energy potential. pyrolysis and combustion behaviours of date stones (ds) agricultural by-products were tested by thermo-gravimetric analysis, and the main chemical compositions were analyzed. the work has also been conducted to identify their most important physical characteristics. the study of the sizes and heating rate effects constitute the first part of the experimental work. inert atmosphere and three heating rates: 10, 20, and 50 degrees c/min, were applied to various particle sizes of ds. in the second part, tests were carried out in an oxidizing atmosphere (21% o-2) by varying the size of the ds. the kinetic parameters such as pre-exponential factor and activation energy were determined. increasing the particle sizes and the heating rates didn't have an appreciable influence on the global weight losses. however, degradation rates were significant with the porous structure of the ds. weight losses in inert and oxidizing atmospheres were found to occur in two stages (drying and devolatilization) and in three stages (drying, devolatilization, and oxidation of the char).
performance and emissions of diesel engine using bio-fuel derived from waste fish oil. in the present work, waste fish fat from fish processing industry is considered as an energy source for diesel engines. in this regard, catalytic cracking process is considered for this present study. the physical and chemical properties of biofuel are very close to diesel fuel. the experiments were conducted in a single cylinder diesel engine to study the performance, emission and combustion characteristics of biofuel. as a result, fuel undergoes good combustion and hence there is significant improvement in performance and reduction in emissions. experimental results indicate a marginal increase in brake thermal efficiency at all loads compared to diesel fuel. the results show that despite of high nox and co2, the engine has lesser uhc, co and pm than standard diesel fuel. the premixed and diffusion combustion duration is decreased with biofuel compared to diesel fuel. the engine was running smooth at all load conditions with biofuel. it is concluded that the biofuel derived from waste fish fat can be consider as a substitute for diesel fuel.
experimental analysis of biofuel as an alternative fuel for diesel engines. the growth of energy demand and limited fossil fuel resources lead to renewable energy development such as vegetable oils and animal fats or their derivatives. in the present work, the valuation of waste fish fat by the pyrolysis technique with the presence of catalyst to produce biofuel for diesel engines. as a result, fuel undergoes good combustion and hence there is a significant improvement in performance and reduction in emissions. the brake thermal efficiency of neat biofuel is 32.4% at 80% load which is very high compared to neat diesel (29.98%). the combustion duration and ignition delay are decreased with neat biofuel due to high oxygen content and high cetane number of biofuel. the main problem with the use of neat biofuel in diesel engine is high nox emissions at all loads. addition of diesel with biofuel reduces the nox emissions significantly from 917 ppm to 889 ppm at 80% load with an optimum blend of b80d20. there is a slight decrease in brake thermal efficiency and increase in particulate emission with this blend. the overall results show that by adding small quantity of diesel with biofuel decreases the nox emissions significantly and approaches the performance of neat biofuel. (c) 2012 elsevier ltd. all rights reserved.
investigations on a compression ignition engine using animal fats and vegetable oil as fuels. biofuels are a promising alternative to petroleum-based fuels. this paper investigates the performance, combustion, and exhaust emissions of a single cylinder diesel engine operated on baseline diesel and biofuel produced by vegetable oil and processing animal fat. the vegetable oil is called podl20, which is a blend of palm oil and d-limonen in proportion of 80% and 20%, respectively. the second biofuel is synthesized from the animal fat wastes (waf) after transesterification process. both experimental and numerical investigations are achieved in this work. the experiments are conducted at constant engine speed mode (1800 rpm) with applied loads on a wide domain. the cfd code converge is used to simulate the in-cylinder combustion for all the tested fuels. comparative measures of brake thermal efficiency, break specific fuel consumption (bsfc), exhaust gas temperature, volumetric efficiency, and pollution (thc, co2, co, no, nox) are presented and discussed. also, a step is achieved with in-cylinder cfd simulation of biofuel combustion. the obtained results indicate that the combustion characteristics are slightly changed when comparing neat diesel to biofuels. some of the results obtained in this work indicate that waf fuel decreases the total unburned fuel as well as the nitrogen oxides (nox) emissions. the numerical results are in logic agreement with those obtained experimentally, which promotes more detailed investigations and combustion characteristics optimization in forthcoming works. [doi: 10.1115/1.4005660].
effects of biofuel from fish oil industrial residue - diesel blends in diesel engine. the present work aims to produce biofuel from fish oil industrial residue and to test the biofuel in diesel engine. a 4.5 kw at 1500 rpm single cylinder air cooled direct injection diesel engine was used for the present experimental work. the experimental results show that the brake thermal efficiency marginally increases with biofuel from 29.98% (neat diesel) to the maximum of 32.4% with biofuel at 80% of maximum load. also experiments were conducted with different blends of biofuel and diesel (b20 and 840). though the no emissions are high with neat biofuel and blends, the other emissions like co, hc and particulate matter (pm) are decreased. the pm emissions decrease when the percentage biofuel increases in the blend. it reduces from 8271 ng/s with neat diesel to 8137 ng/s with b40. it further reduces to the minimum of 7842 ng/s with neat biofuel. the cylinder peak pressure increases as the biofuel quantity increases in the blend. the rate of premixed combustion increases with neat biofuel and its blends than neat diesel. addition of biofuel with diesel decreases the combustion duration and ignition delay due to higher cetane number of biofuel. (c) 2012 elsevier ltd. all rights reserved.
combination of pyrolysis and hydroliquefaction of ccb-treated wood for energy recovery: optimization and products characterization. in this paper, pyrolysis and hydroliquefaction processes were successively used to convert ccb-treated wood into bio-oil with respect to environment. pyrolysis temperature has been optimized to produce maximum yield of charcoal with a high metal content (cu, cr, and b). the results obtained indicate that the pyrolysis at 300 degrees c and 30 min are the optimal conditions giving high yield of charcoal about 45% which contains up to 94% of cu, 100% of cr and 88% of b. after pyrolysis process, the charcoal has been converted into bio-oil using hydroliquefaction process. the optimization approach for the yield of bio-oil using a complete factorial design with three parameters: charcoal/solvent, temperature and hydrogen pressure was discussed. it is observed that the temperature is the most significant parameter and the optimum yield of bio-oil is around 82%. the metal analysis shows that the metals present in the bio-oil is very negligible. (c) 2012 elsevier ltd. all rights reserved.
influence of impregnation method on metal retention of ccb-treated wood in slow pyrolysis process. in the present work, the effects of copper, chromium and boron on the pyrolysis of wood and their distribution in the pyrolysis products were investigated. for this, the wood has been impregnated with chromium-copper-boron (ccb). in addition, to describe the effects of impregnation method, vacuum-pressure and dipping methods were also conducted. thermogravimetric analysis (tga) results show that an increase in the final residue and decrease in degradation temperature on both methods of treated wood compared to untreated wood. then, slow pyrolysis experiments were carried out in a laboratory reactor. the mass balance of pyrolysis products is confirmed by tga. furthermore, the concentration of metals in the final residue is measured by inductively coupled plasma mass spectroscopy (icp-ms). the results show that the final residue contains more than 45% of the initial amount of metal present in the treated wood. the phenomenon is more pronounced with vacuum-pressure treated wood. the heating values of pyrolysis products were analyzed. the heating value of charcoal obtained from treated and untreated wood is approximately same. but the heating value of tar from untreated wood is higher than the heating value of the tar from treated wood. (c) 2012 elsevier b.v. all rights reserved.
experimental investigations of a gamma stirling engine. the present work deals with the measurement and performance of a gamma stirling engine of 500?w of mechanical shaft power and 600?rpm of maximal revolutions per minute. series of measurements concerning the pressure distribution, temperature evolution, and brake power were performed. the study of the different functioning parameters such as initial charge pressure, engine velocity, cooling water flowrate, and temperature gradient (between the sources of heat) has been analyzed. the engine brake power increases with the initial charge pressure, with the cooling water flow, and with the engine revolutions per minute. the working fluid temperature measurements have been recorded in different locations symmetrically along both regenerator sides. the recorded temperature in regenerator side one is about 252 degrees c and about 174 degrees c in the opposite side (side two). it shows an asymmetric temperature distribution in the stirling engine regenerator; consequently, heat transfer inside this porous medium is deteriorated. copyright (c) 2011 john wiley &amp; sons, ltd.
assessment of liquid fuel (bio-oil) production from waste fish fat and utilization in diesel engine. increased acceptance of climate change induced by human activities and raising oil demand with unsecure deliverance compels the searching for alternative fuels. the problems with environmental degradation due to industrial wastes can be reduced by converting some of them into bio-oil. in the present work, the waste from fish processing industry is converted to bio-oil by catalytic cracking. experiments were conducted in a direct injection diesel engine of 4.5 kw at 1500 rpm. the different test fuels of diesel, fish oil at 75 c, bio-oil ud (undistilled bio-oil), b20d80 (20% bio-oil in fossil diesel), b80d20 (80% bio-oil in fossil diesel) and neat bio-oil were tested to assess the suitability in diesel engines through combustion, emission and performance characteristics. experimental results show that the brake thermal efficiency is marginally higher with neat bio-oil over other test fuels. it is lower with preheated fish oil and it is almost same for both bio-oil and bio-oil ud. nox, hc, co and pm emissions are higher with bio-oil ud compared to bio-oil. pm, co and hc emissions are lower with bio-oil over diesel. nox emissions are lower with bio-oil compared to bio-oil ud but it is still higher than diesel fuel. addition of diesel with bio-oil reduces the nox emissions marginally. intensity of premixed combustion is strong with bio-oil. ignition delay and combustion duration are reduced with bio-oil due to high cetane number and oxygen concentration. bio-oil from waste fish fat by catalytic cracking can be used as a fuel for diesel engines and also the waste to energy may reduce the environmental and climate change issues due to industrial wastes. (c) 2012 elsevier ltd. all rights reserved.
biodiesel production from biomass gasification tar via thermal/catalytic cracking. this paper is devoted to the study of valorization of tar from biomass gasification as a fuel for internal combustion engine. the methods selected were both thermal cracking and catalytic cracking in the presence of zeolite, magnesium oxide, and aluminum oxide catalyst. the chemical composition of the cracking product was analyzed by gas chromatography-mass spectrometry, together with the physico-chemical properties determination (density, viscosity, higher heating value, and acidic value). thermal cracking of biomass gasification tar gave a yield of bio-diesel 73.67 wt.% of feed. the cracking process in the presence of zeolite, magnesium oxide, and aluminum oxide catalysts gave a yield of biodiesel 62-75 wt.%, 55-66 wt.%, 67-71 wt.% respectively. the influence of the type and quantity of catalyst on production yield and properties of the produced bio-oil is highlighted. the produced bio-oil density and heating value were close to the conventional diesel fuel. the viscosity and acidic value were found to be slightly higher than that of conventional diesel fuel. (c) 2012 elsevier b.v. all rights reserved.
optimization of biodiesel production from animal fat residue in wastewater using response surface methodology. animal fat residues (afr) from waste water were used as feedstock to produce biodiesel by a two-step acid-catalyzed process. treatment of the afrs with 5.4% (w/w) of 17 m h2so4 at a methanol/afr ratio of 13:1 (50% w/w) at 60 degrees c converted more than 95% of the triglycerides into fatty acid methyl esters (fames) with an acid value (av) of 1.3 mg(koh)/g(biodiesel). response surface methodology indicated that a lower av cannot be reached using a one-step acid catalyzed process. thus a two-step acid catalyzed process was employed using 3.6% catalyst and 30% methanol for 5 h for the first step and 1.8% catalyst and 10% methanol for i h in the second step, resulting in a yield higher than 98% and an av of 0.3 mg(koh)/g(biodiesel). the product thus conforms to the european norm en14214 concerning biodiesel. (c) 2012 elsevier ltd. all rights reserved.
single zone combustion modeling of biodiesel from wastes in diesel engine. increasing interest in diesel engine technology and the continuous demand of finding alternate fuels and reducing emissions has motivated over the years for the development of numerical models, to provide qualitatively predictive tools for the designers. among the alternative fuels, biodiesel is considered suitable and the most promising fuel for diesel engine. the properties of biodiesel from waste oils are found similar to that of diesel. in this present work, a unique single zone combustion model for diesel fuel and biodiesel was implemented to predict the cylinder pressure for the better understanding of combustion characteristics of different fuels tested in a diesel engine and also to predict the combustion and performance characteristics of the same engine running on different fuels. the single zone model coupled with a triple-wiebe function was performed to simulate heat release between the period of ivc (inlet valve close) and evo (exhaust valve open). this model also includes the submodels of intake and exhaust gases through the valves, ignition delay, burned fuel during the cycle and heat losses through walls to simulate all phases of combustion. the model calibration was performed using data from experiments on diesel fuel and biodiesel from waste cooking oil. later the same model was used to simulate the combustion and the cylinder pressure of engine running on biodiesel derived from animal fat residues. finally, cylinder pressure traces predicted by using single-zone model are compared to experimental pressure traces obtained from a diesel engine fuelled with diesel fuel and biodiesel. (c) 2012 elsevier ltd. all rights reserved.
continuous production of water-in-oil emulsion using micromixers. the formation of emulsions is a critical application that interests many industrial fields. among the various interests to produce emulsions, this work focuses on the emulsification of water in fuels, in order to improve combustion and reduce emission of harmful gases. the manufacturing process of these emulsions must meet a number of constraints such as water fraction, mean droplet size, delivered flow rate and process energy consumption. among possible techniques, this study focuses on the implementation of crossing microchannels. for this purpose, two geometries of the cross section of the channels have been tested. the implementation of several flow configurations has also been investigated. other parameters were varied such as the variation of the ratio of flow rates of lipid phase and water, the nature and content of surfactant. in conclusion, obtaining emulsions of water in oil having a mean droplet size of about 4 lm was possible with several operating conditions. channel geometry and flow pattern have a significant influence on the possibility of forming this type of emulsions. (c) 2012 elsevier ltd. all rights reserved.
modelling of an indirectly heated fixed bed pyrolysis reactor of wood: transition from batch to continuous staged gasification. gasification is today a mature technology and staged processes know an important development. the design of this kind of reactor is still a sensitive work, and numerical simulation, as a flexible and economic tool, has to be developed. the present work concerns the first pyrolysis stage of a staged gasification process and is organised into two parts. the first part deals with the calibration of the effective thermal conductivity of the bed. this is the key parameter in the heat transfer modelling. the calibration is obtained by comparison between experimental and modelling results in batch reactor. the second part is the development of a transient two dimensional model of a continuous indirectly heated fixed bed pyrolysis reactor. the model uses a finite difference formulation to solve mass and energy balances in a cylindrical vessel. the assumptions of local thermal equilibrium, constant particle size and thermally thin particles are considered. the simulations are used to design a pyrolysis reactor in function of the type of biomass, the wall temperature and the direction of gas circulation. the parametric study is presented to highlight the limitations of the external heating regarding the wood conversion rate. (c) 2012 elsevier ltd. all rights reserved.
liquid hydrocarbon fuels from fish oil industrial residues by catalytic cracking. in the present work, catalytic cracking of fish oil industrial residue was investigated to study the effect of temperature, type of catalyst and the heating rate on the yield of organic liquid fraction (olf) and its acid value. the highest bio-oil yield of 72% (wt.) was obtained at temperature range of 300-500 degrees c and heating rate of 10 degrees c/min with the mixture of al2o3 and na2co3 as a catalyst. it was found that the mixture of na2co3 and mgso4 as a catalyst gives lowest acid value of 8.75 mgkoh/goil and 68.1% of olf yield. furthermore, the acid value is reduced to 0.36 mgkoh/goil using na2co3 as an absorbent. the results show that the catalytic cracking process represents a sustainable method to produce bio-oil from fish oil industrial residues with physicochemical characteristics similar to the diesel fuel. copyright (c) 2012 john wiley &amp; sons, ltd.
slow pyrolysis of ccb-treated wood for energy recovery: influence of chromium, copper and boron on pyrolysis process and optimization. this paper investigates the effect of copper, chromium and boron on the slow pyrolysis of ccb-treated wood. a mixture of softwood has been impregnated with several inorganic salts (individually cuso4, kcr(so4)(2), b4na2o7) and with salt of ccb (mixture of cuso4, kcr(so4)(2) and h3bo3). the weight loss of samples is identified by thermogravimetric analysis (tga). the results show a higher mass of final residue and a decrease in degradation temperature on treated wood compared to untreated wood. tga results also show that, the maximum degradation of ccb treated wood occurs at 300 degrees c. in addition, slow pyrolysis experiments were carried out in a laboratory scale reactor. the pyrolysis products were quantified using analytical balance whereas gases were analyzed by gas chromatography. the trends of weight loss obtained by tga and by laboratory scale pyrolysis are similar. furthermore, it is observed that ccb salts inhibit the formation of co and co2 but promote that of h-2 in the second part of this work, yield of solid pyolysis residue (charcoal) with a high metal content (cu, cr, b) has been carried out at 300 degrees c and 370 degrees c during residence time of 20 and 30 mm. metals in charcoal are analyzed using inductively coupled plasma mass spectrometry. taking into account the energy recovery of by-products, slow pyrolysis at 300 degrees c and 30 mm appears to be optimal conditions with a high yield of charcoal about 45% and the element recovery is up to 94% of cu, 100% of cr and 88% of b. (c) 2013 elsevier b.v. all rights reserved.
effect of free fatty acids and short chain alcohols on conversion of waste cooking oil to biodiesel. in this article, the transesterification of three types of waste cooking oil (wco) with methanol and ethanol was studied using alkali catalyzed process. the catalyst used in this study was sodium hydroxide. the effects of temperature, catalyst amount, alcohol to oil ratio, and the time of reaction on the yield were studied. the temperature and the catalyst amount were the most important factors affecting the yield of biodiesel. also the process exhibited some sensitivity to the level of free fatty acids (ffa) in the wco and to the type of alcohol. the yields of methyl esters varied from 97% with the lowest acidity (0.4% ffa wco) to 76% with the highest acidity (3.25% ffa wco). the ethyl esters yields were lower and the difference increased with the level of ffa in the oil, the maximum yield was 95% and 73% with the lowest and the medium acidities respectively and no reaction was registered with the highest one. the chromatographic analysis of the produced biodiesel showed high contents of fatty acid methyl esters varying from 96.5% to 98%. the physical-chemical characteristics of produced biodiesel were studied and compared to the european norm, en 14214.
background-independent measurement of θ13 in double chooz. the oscillation results published by the double chooz collaboration in 2011 and 2012 rely on background models substantiated by reactor-on data. in this analysis, we present a background-model-independent measurement of the mixing angle θ13 by including 7.53 days of reactor-off data. a global fit of the observed antineutrino rates for different reactor power conditions is performed, yielding a measurement of both θ13 and the total background rate. the results on the mixing angle are improved significantly by including the reactor-off data in the fit, as it provides a direct measurement of the total background rate. this reactor rate modulation analysis considers antineutrino candidates with neutron captures on both gd and h, whose combination yields sin2(2θ13)=0.102±0.028(stat.)±0.033(syst.). the results presented in this study are fully consistent with the ones already published by double chooz, achieving a competitive precision. they provide, for the first time, a determination of θ13 that does not depend on a background model.
radiolytic corrosion of uranium dioxide: role of molecular species. n.a.
direct and steering tilt robust control of narrow vehicles. narrow tilting vehicles (ntvs) are the convergence of a car and a motorcycle. they are expected to be the new generation of city cars considering their practical dimensions and lower energy consumption. however, due to their height to breadth ratio, in order to maintain lateral stability, ntvs should tilt when cornering. unlike the motorcycle, where the driver tilts the vehicle himself, the tilting of an ntv should be automatic. two tilting systems are available; direct and steering tilt control, the combined action of these two systems being certainly the key to improve considerably ntv dynamic performances. in this paper, multivariable control tools (h2 methodology) are used to design, in a systematic way, lateral assistance controllers driving dtc, stc or both dtc/stc systems. a three degrees of freedom model of the vehicle is used, as well as a model of the steering signal, leading to a two degrees of freedom low order controller with an efficient feedforward anticipative part. taking advantage of all the available measurements on ntvs, the lateral acceleration is directly regulated. finally, a gain-scheduling solution is provided to make the dtc, stc, and dtc/stc controllers robust to longitudinal speed variations.
fission of actinides through quasimolecular shapes. the potential energy of heavy nuclei has been calculated in the quasimolecular shape path from a generalized liquid drop model including the proximity energy, the charge and mass asymmetries and the microscopic corrections. the potential barriers are multiple-humped. the second maximum is the saddle-point. it corresponds to the transition from compact one-body shapes with a deep neck to two touching ellipsoids. the scission point lies at the end of an energy plateau well below the saddle-point and where the effects of the nuclear attractive forces between two separated fragments vanish. the energy on this plateau is the sum of the kinetic and excitation energies of the fragments. the shell and pairing corrections play an essential role to select the most probable fission path. the potential barrier heights agree with the experimental data and the theoretical half-lives follow the trend of the experimental values. a third peak and a shallow third minimum appear in asymmetric decay paths when one fragment is close to a double magic quasi-spherical nucleus, while the smaller one changes from oblate to prolate shapes.
scalable multi-dimensional resources scheduling constraints. constraint programming is an approach often used to solve combinatorial problems in different application areas. in this thesis we focus on the cumulative scheduling problems. a scheduling problem is to determine the starting dates of a set of tasks while respecting capacity and precedence constraints. capacity constraints affect both conventional cumulative constraints where the sum of the heights of tasks intersecting a given time point is limited, and colored cumulative constraints where the number of distinct colors assigned to the tasks intersecting a given time point is limited. a newly identified challenge for constraint programming is to deal with large problems, usually solved by dedicated algorithms and metaheuristics. for example, the increasing use of virtualized datacenters leads to multi dimensional placement problems of thousand of jobs. scalability is achieved by using a synchronized sweep algorithm over the different cumulative and precedence constraints that allows to speed up convergence to the fix point. in addition, from these filtering algorithms we derive greedy procedures that can be called at each node of the search tree to find a solution more quickly. this approach allows to deal with scheduling problems involving more than one million jobs and 64 cumulative resources. these algorithms have been implemented within choco and sicstussolvers and evaluated on a variety of placement and scheduling problems.
optiplace: designing cloud management with flexible power models through constraint programing. null
beam-energy dependence of directed flow of protons, antiprotons and pions in au+au collisions. rapidity-odd directed flow($v_1$) measurements for charged pions, protons and antiprotons near mid-rapidity ($y=0$) are reported in $\sqrt{s_{nn}} =$ 7.7, 11.5, 19.6, 27, 39, 62.4 and 200 gev au + au collisions as recorded by the star detector at the relativistic heavy ion collider (rhic). at intermediate impact parameters, the proton and net-proton slope parameter $dv_1/dy|_{y=0}$ shows a minimum between 11.5 and 19.6 gev. in addition, the net-proton $dv_1/dy|_{y=0}$ changes sign twice between 7.7 and 39 gev. the proton and net-proton results qualitatively resemble predictions of a hydrodynamic model with a first-order phase transition from hadronic matter to deconfined matter, and differ from hadronic transport calculations.
first demonstration of thgem/gapd-matrix optical readout in a two-phase cryogenic avalanche detector in ar. the multi-channel optical readout of a thgem multiplier coupled to a matrix of 3×3 geiger-mode apds (gapds) was demonstrated in a two-phase cryogenic avalanche detector (crad) in ar. the gapds recorded thgem-hole avalanches in the near infrared (nir) spectral range. at an avalanche charge gain of 160, the yield of the combined thgem/gapd-matrix multiplier amounted to ~80 photoelectrons per 20 kev x-ray absorbed in the liquid phase. a spatial resolution of 2.5 mm (fwhm) has been measured for the impinging x-rays. this technique has potential applications in coherent neutrino-nucleus scattering and in dark matter search experiments.
voltage regulation of a boost converter in discontinuous conduction mode: a simple robust adaptive feedback controller. ideal switches in power converters are typically implemented using unidirectional semiconductor devices that may lead to a new operation mode generically called discontinuous conduction mode (dcm). the dcm arises when the ripple, that is, sustained oscillations of small amplitude, is large enough to cause the polarity of the signal (current or voltage) applied to the switch to reverse. due to the presence of diodes, switches are assumed to operate unidirectionally, but in dcm this unidirectionality assumption is violated. in classicalconverter topologies, dcm appears very frequently in low load operating modes. more interestingly, to achieve high performance some new converters are purposely designed to operate all the time in dcm [1].
towards a unified description of evaporation-residue fusion cross-sections above the barrier. a meticulous study of nearly 300 fusion-evaporation cross-section data reveals that, when properly scaled, fusion excitation function complies with a universal homographic law which is, within experimental errors, reaction system independent. from such complete and summed complete and incomplete fusion excitation functions are extracted the limiting energy for the complete fusion and the main characteristics (onset, maximum and vanishing) of the incomplete fusion. the dywan microscopic transport model correctly predicts the incomplete fusion cross-section for incident energies $\gtrsim15a\ \text{mev}$ and suggests that the nuclear transparency is at the origin of fusion disappearance.
transport coefficients from the nambu-jona-lasinio model for $su(3)_f$. we calculate the shear $\eta(t)$ and bulk viscosities $\zeta(t)$ as well as the electric conductivity $\sigma_e(t)$ and heat conductivity $\kappa(t)$ within the nambu-jona-lasinio model for 3 flavors as a function of temperature as well as the entropy density $s(t)$, pressure $p(t)$ and speed of sound $c_s^2(t)$. we compare the results with other models such as the polyakov-nambu-jona-lasinio (pnjl) model and the dynamical quasiparticle model (dqpm) and confront these results with lattice qcd data whenever available. we find the njl model to have a limited predictive power for the thermodynamic variables and various transport coefficients above the critical temperature whereas the pnjl model and dqpm show acceptable results for the quantities of interest.
semi-classical approach to $j/\psi$ suppression in high energy heavy-ion collisions. we study the heavy quark/antiquark pair dynamics in strongly-coupled quark gluon plasma. a semi-classical approach, based on the wigner distribution and langevin dynamics, is applied to a color screened $c{\bar c}$ pair, in a hydrodynamically cooling fireball, to evaluate the total $j/\psi$ suppression at both rhic and lhc energies. although its limitation is observed, this approach results to a $j/\psi$ suppression of around 0.30 at rhic and 0.25 at lhc.
transverse-energy distributions at midrapidity in $p$$+$$p$, $d$$+$au, and au$+$au collisions at $\sqrt{s_{_{nn}}}=62.4$--200~gev and implications for particle-production models. measurements of the midrapidity transverse energy distribution, $d\et/d\eta$, are presented for $p$$+$$p$, $d$$+$au, and au$+$au collisions at $\sqrt{s_{_{nn}}}=200$ gev and additionally for au$+$au collisions at $\sqrt{s_{_{nn}}}=62.4$ and 130 gev. the $d\et/d\eta$ distributions are first compared with the number of nucleon participants $n_{\rm part}$, number of binary collisions $n_{\rm coll}$, and number of constituent-quark participants $n_{qp}$ calculated from a glauber model based on the nuclear geometry. for au$+$au, $\mean{d\et/d\eta}/n_{\rm part}$ increases with $n_{\rm part}$, while $\mean{d\et/d\eta}/n_{qp}$ is approximately constant for all three energies. this indicates that the two component ansatz, $de_{t}/d\eta \propto (1-x) n_{\rm part}/2 + x n_{\rm coll}$, which has been used to represent $e_t$ distributions, is simply a proxy for $n_{qp}$, and that the $n_{\rm coll}$ term does not represent a hard-scattering component in $e_t$ distributions. the $de_{t}/d\eta$ distributions of au$+$au and $d$$+$au are then calculated from the measured $p$$+$$p$ $e_t$ distribution using two models that both reproduce the au$+$au data. however, while the number-of-constituent-quark-participant model agrees well with the $d$$+$au data, the additive-quark model does not.
production of charged pions, kaons and protons at large transverse momenta in pp and pb-pb collisions at sqrt(snn) = 2.76 tev. transverse momentum spectra of pi+/-, k+/- and p(anti-p) up to pt = 20 gev/c at mid-rapidity, |y| ~&lt; 0.8, in pp and pb-pb collisions at sqrt(snn) = 2.76 tev have been measured using the alice detector at the lhc. at intermediate pt (2-8 gev/c) an enhancement of the proton-to-proton ratio, (p + anti-p)/(pi+ + pi-), with respect to pp collisions is observed and the ratio reaches ~0.80 in central pb-pb collisions. the measurement of the nuclear modification factors for pi+/-, k+/- and p(anti-p) indicates that within the systematic and statistical uncertainties they are the same at high pt (&gt; 10 gev/c), suggesting that the chemical composition of leading particles from jets in the medium is similar to that of vacuum jets.
highlights from the pierre auger observatory. the pierre auger observatory is the world's largest cosmic ray observatory. our current exposure reaches nearly 40,000 km$^2$ str and provides us with an unprecedented quality data set. the performance and stability of the detectors and their enhancements are described. data analyses have led to a number of major breakthroughs. among these we discuss the energy spectrum and the searches for large-scale anisotropies. we present analyses of our x$_{max}$ data and show how it can be interpreted in terms of mass composition. we also describe some new analyses that extract mass sensitive parameters from the 100% duty cycle sd data. a coherent interpretation of all these recent results opens new directions. the consequences regarding the cosmic ray composition and the properties of uhecr sources are briefly discussed.
oxygen dayglow observations on mars by spicam ir on mars-express. o2(1δg) dayglow at 1.27 μm reflects the ozone distribution in the martian atmosphere as a result of ozone photolysis by solar uv radiation. spicam ir on mars-express performed continuous observations of the o2 dayglow at limb and nadir from 2004 to 2012 with resolving power of 2200. the results of o2(1δg) observations have been compared with lmd gcm simulation [1-3] to study its seasonal variations and sensitivity to kinetic parameters.
rightcapacity: sla-driven cross-layer cloud elasticity management. cloud computing paradigm has become the solution to provide good service quality and exploit economies of scale. however, the management of such elastic resources, with different quality-of-service (qos) combined with on-demand self-service, is a complex issue. new challenges for elasticity management arise when people look deeper into the cloud characteristics such as non-ignorable instance initiation time and full hour billing model. the main challenge for a saas provider is to determine the best trade-off between profit and end-user satisfaction. this paper proposes rightcapacity, an approach driven by service level agreement (sla) for optimizing the cloud elasticity management (i.e., both elasticity at the application and at the infrastructure levels). we consider cross-layer (application-resource) cloud elasticity. we model cloud application using closed queueing network model taking into account the sla concept and the cloud economic model. our results show that rightcapacity successfully keeps the best trade-off between saas provider profit and end-user satisfaction. using rightcapacity, the cost saving of as much as 30% can be achieved while causing the minimum number of violations, as small as 1%.
estimated costs of implementation of membrane processes for on-site greywater recycling. greywater reuse inside buildings is a possible way to preserve water resources and face up to water scarcity. this study is focused on a technical-economic analysis of greywater treatment by a direct nanofiltration (nf) process or by a submerged membrane bioreactor (smbr) for on-site recycling. the aim of this paper is to analyse the cost of recycled water for two different configurations (50 and 500 inhabitants) in order to demonstrate the relevance of the implementation of membrane processes for greywater recycling, depending on the production capacity of the equipment and the price of drinking water. the first step was to define a method to access the description of the cost of producing recycled water. the direct costs were defined as a sum of fixed costs due to equipment, maintenance and depreciation, and variable costs generated by chemical products and electricity consumptions. they were estimated from an experimental approach and from data found in literature, enabling operating conditions for greywater recycling to be determined. the cost of treated water by a smbr unit with a processing capacity of 500 persons is close to 4.40 m-3, while the cost is 4.81 m-3 with a nf process running in the same conditions. these costs are similar to the price of drinking water in some european countries.
flauncher and dvms -- deploying and scheduling thousands of virtual machines on hundreds of nodes distributed geographically. although live migration of virtual machines has been an active area of research over the past decade, it has been mainly evaluated by means of simulations and small scale deployments. proving the relevance of live migration at larger scales is a technical challenge that requires to be able to deploy and schedule virtual machines. in the last year, we succeeded to tackle such a challenge by conducting experiments with flauncher and dvms, two frameworks that can respectively deploy and schedule thousands of virtual machines over hundreds of nodes distributed geographically across the grid'5000 testbed.
compositional reasoning about aspect interference. oliveira and colleagues recently developed a powerful model to reason about mixin-based composition of effectful components and their interference, exploiting a wide variety of techniques such as equational reasoning, parametricity, and algebraic laws about monadic effects. this work addresses the issue of reasoning about interference with effectful aspects in the presence of unrestricted quantification through pointcuts. while global reasoning is required, we show that it is possible to reason in a compositional manner, which is key for the scalability of the approach in the face of large and evolving systems. we establish a general equivalence theorem that is based on a few conditions that can be established, reused, and adapted separately as the system evolves. interestingly, one of these conditions, local harmlessness, can be proven by a translation to the mixin setting, making it possible to directly exploit previously established results about certain kinds of harmless extensions.
vr4d: an immersive and collaborative experience to improve the interior design process. ergonomics and spatial constraints are important issues to consider during the design process of limited spaces. in this paper, we present a user-centered methodology for designing a new vr tool for collaborative design and evaluation of limited spaces. the system is composed of two communicating tools: a sketch-based application, and a 3d immersive application. using this tool, two collaborating users can perform simultaneously two steps of the design process: the sketching phase, and the organization and evaluation of the 3d space. a preliminary evaluation session, conducted with expert designers to assess the usability and the utility of the system, shows its value.
a preliminary investigation of the isg glass vapor hydration. during the geological disposal of high-level waste, the nuclear glass is expected to be first hydrated in water vapor prior to liquid alteration. in the present work, we investigated the vapor hydration of the international simple glass (isg) at 175°c and different relative humidities (60%, 80% and 98%). the glass hydration was investigated by nuclear reaction analysis (nra) and fourier transform infra-red spectroscopy. the chemical and mineralogical compositions of the alteration products were studied using scanning electron microscopy/energy dispersive x-ray spectroscopy (sem-eds) and μ-raman spectroscopy, respectively. the nra results gave water diffusion coefficients of 2.31-7.34 × 10−21 m2/s, in good agreement with the literature data on borosilicate glasses altered in aqueous media. the glass hydration increased with relative humidity percentage and the sem-eds analysis showed a slight enrichment in si and loss of na in the hydrated glass layer compared with the pristine glass. the hydration rate of the isg glass was little higher than that of the french son68 glass hydrated using water vapor. the corrosion products were analcime, tobermorite, and calcite, which were typical of the son68 glass hydrated in similar conditions.
hazardous dichloromethane recovery in combined temperature and vacuum pressure swing adsorption process. organic vapors emitted from solvents used in chemical and pharmaceutical processes, or from hydrocarbon fuel storage stations at oil terminals, can be efficiently captured by adsorption onto activated carbon beds. to recover vapors after the adsorption step, two modes of regeneration were selected and could be possibly combined: thermal desorption by hot nitrogen flow and vacuum depressurization (vtsa). because of ignition risks, the conditions in which the beds operate during the adsorption and regeneration steps need to be strictly controlled, as well as optimized to maintain good performances. in this work, the optimal conditions to be applied during the desorption step were determined from factorial experimental design (fed), and validated from the process simulation results. the regeneration performances were compared in terms of bed regeneration rate, concentration of recovered volatile organic compounds (voc) and operating costs. as an example, this methodology was applied in case of dichloromethane. it has been shown that the combination of thermal and vacuum regeneration allows reaching 82% recovery of dichloromethane. moreover, the vacuum desorption ended up in cooling the activated carbon bed from 93°c to 63°c and so that it significantly reduces the cooling time before starting a new cycle.
sla-driven cloud elasticity anagement approach. cloud computing promises to completely revolutionize the way to manage resources. thanks to elasticity, resources can be provisioning within minutes to satisfy a required level of quality of service(qos) formalized by service level agreements (slas) between different cloud actors. the main challenge of service providers is to maintain its consumer's satisfaction while minimizing the service costs due to resources fees. for example, from the saas point of view, this challenge can be achieved in ad-hoc manner by allocating/releasing resources based on a set of predefined rules as amazon auto scaling implements it. however, doing it right -in a way that maintains end-users satisfaction while optimizing service cost- is not a trivial task. first, because of the difficulty to profile service performance,the accuracy of capacity planning may be compromised. second, several parameters should be taken into account such as multiple resource types, non-ignorable resource initiation time and iaas billing model. for that purpose, we propose a complete solution for cloud service level management. we first introduce csla (cloud service levelagreement), a specific language to describe sla for cloud services. it finely expresses sla violations via functionality/qos degradationand an advanced penalty model. then, we propose hybridscale, an auto-scaling framework driven by sla. it implements the cloud elasticity in a triple hybrid way : reactive-proactive management, vertical horizontal scaling at cross-layer (application-infrastructure). our solution is experimentally validated on amazon ec2.
supersymmetric field theory with benign ghosts. we construct a supersymmetric (1+1)-dimensional field theory involving extra derivatives and associated ghosts: the spectrum of the hamiltonian is not bounded from below, neither from above. in spite of that, there is neither classical, nor quantum collapse and unitarity is preserved.
collisional and radiative energy loss of heavy quarks. null
recent results on jet physics from alice at the lhc. null
towards the dynamical study of heavy-flavor quarks in the quark-gluon-plasma. within the aim of a dynamical study of on- and off-shell heavy quarks q in the quark gluon plasma (qgp) - as produced in relativistic nucleus-nucleus collisions - we study the heavy quark collisional scattering on partons of the qgp. the elastic cross sections $\sigma_{q,g-q}$ are evaluated for perturbative partons (massless on-shell particles) and for dynamical quasi-particles (massive off-shell particles as described by the dynamical quasi-particles model "dqpm") using the leading order born diagrams. we demonstrate that the finite width of the quasi-particles in the dqpm has little influence on the cross sections $\sigma_{q,g-q}$ except close to thresholds. we, furthermore, calculate the heavy quark relaxation time as a function of temperature t within the different approaches using these cross sections.
j/$\psi$ roduction in pb-pb collisions at =$\sqrt{s}$ = 2.76 tev in the alice experiment. null
observation and applications of single-electron charge signals in the xenon100 experiment. the xenon100 dark matter experiment uses liquid xenon in a time projection chamber (tpc) to measure xenon nuclear recoils resulting from the scattering of dark matter weakly interacting massive particles (wimps). in this paper, we report the observation of single-electron charge signals which are not related to wimp interactions. these signals, which show the excellent sensitivity of the detector to small charge signals, are explained as being due to the photoionization of impurities in the liquid xenon and of the metal components inside the tpc. they are used as a unique calibration source to characterize the detector. we explain how we can infer crucial parameters for the xenon100 experiment: the secondary-scintillation gain, the extraction yield from the liquid to the gas phase and the electron drift velocity.
measurement of transverse-single-spin asymmetries for midrapidity and forward-rapidity production of hadrons in polarized p+p collisions at $\sqrt{s}=$200 and 62.4 gev. measurements of transverse-single-spin asymmetries ($a_{n}$) in $p$$+$$p$ collisions at $\sqrt{s}=$62.4 and 200 gev with the phenix detector at rhic are presented. at midrapidity, $a_{n}$ is measured for neutral pion and eta mesons reconstructed from diphoton decay, and at forward rapidities, neutral pions are measured using both diphotons and electromagnetic clusters. the neutral-pion measurement of $a_{n}$ at midrapidity is consistent with zero with uncertainties a factor of 20 smaller than previous publications, which will lead to improved constraints on the gluon sivers function. at higher rapidities, where the valence quark distributions are probed, the data exhibit sizable asymmetries. in comparison with previous measurements in this kinematic region, the new data extend the kinematic coverage in $\sqrt{s}$ and $p_t$, and it is found that the asymmetries depend only weakly on $\sqrt{s}$. the origin of the forward $a_{n}$ is presently not understood quantitatively. the extended reach to higher $p_t$ probes the transition between transverse momentum dependent effects at low $p_t$ and multi-parton dynamics at high $p_t$.
selecting test sensitivity and specificity parameters to optimally maintain a degrading system. null
joint-optimization of inventory policies on a multi-product multi-echelon pharmaceutical system with batching and ordering constraints. null
phase transitions of iron sulphides formed by steel microbial corrosion. null
summary of the workshop on multi-parton interactions (mpi@lhc 2012). with short resumes and highlights the discussions in the different working groups of the workshop mpi@lhc 2012 is documented.
epos lhc : test of collective hadronization with lhc data. epos is a monte-carlo event generator for minimum bias hadronic interactions, used for both heavy ion interactions and cosmic ray air shower simulations. since the last public release in 2009, the lhc experiments have provided a number of very interesting data sets comprising minimum bias p-p, p-pb and pb-pb interactions. we describe the changes required to the model to reproduce in detail the new data available from lhc and the consequences in the interpretation of these data. in particular we discuss the effect of the collective hadronization in p-p scattering. a different parametrization of flow has been introduced in the case of a small volume with high density of thermalized matter (core) reached in p-p compared to large volume produced in heavy ion collisions. both parametrizations depend only on the geometry and the amount of secondary particles entering in the core and not on the beam mass or energy. the transition between the two flow regimes can be tested with p-pb data. epos lhc is able to reproduce all minimum bias.
mass transfer coefficients of styrene into water/silicone oil mixtures: new interpretation using the "equivalent absorption capacity" concept. the physical absorption of styrene into water/silicone oil systems at a constant flow rate for mixtures of different compositions (silicone oil volume fraction ϕ = 0%, 1%, 2%, 3%, 4%, 5%, 6%, 8%, and 10%) was investigated in laboratory-scale bubble reactors using a dynamic absorption method. experimental results previously analyzed assuming no contact between gas and silicone oil [10] were reconsidered by applying the "equivalent absorption capacity" concept characterized by the value of the styrene partition coefficient between air and the mixture (hmix). the results indicate that silicone oil addition slightly hinders the styrene mass transfer rate compared to the air/water system. moreover, a dramatic decrease in kla values due to silicone oil addition is observed. in comparison with similar measurements available in the literature, it is noted that this decrease in kla value could be related to the change in the partition coefficient ratio mr = hwater/hoil. two explanations concerning the relationship between the change in a hydrodynamic parameter (kla) and that in a thermodynamic parameter (mr) are proposed. finally, it appears questionable to study the change in kla in terms of silicone oil addition.
pseudo dynamic transitional modeling of building heating energy demand using artificial neural network. this paper presents the building heating demand prediction model with occupancy profile and operational heating power level characteristics in short time horizon (a couple of days) using artificial neural network. in addition, novel pseudo dynamic transitional model is introduced, which consider time dependent attributes of operational power level characteristics and its effect in the overall model performance is outlined. pseudo dynamic model is applied to a case study of french institution building and compared its results with static and other pseudo dynamic neural network models. the results show the coefficients of correlation in static and pseudo dynamic neural network model of 0.82 and 0.89 (with energy consumption error of 0.02%) during the learning phase, and 0.61 and 0.85 during the prediction phase respectively. further, orthogonal array design is applied to the pseudo dynamic model to check the schedule of occupancy profile and operational heating power level characteristics. the results show the new schedule and provide the robust design for pseudo dynamic model. due to prediction in short time horizon, it finds application for energy services company (escos) to manage the heating load for dynamic control of heat production system.
collisional processes of on-shell and off-shell heavy quarks in vacuum and in the quark-gluon-plasma. we study the heavy quark scattering on partons of the quark gluon plasma (qgp) being especially interested in the collisional (elastic) scattering processes of heavy quarks on quarks and gluons. we calculate the different cross sections for perturbative partons (massless on-shell particles in the vacuum) and for dynamical quasi-particles (off-shell particles in the qgp medium as described by the dynamical quasi-particles model "dqpm") using the leading order born diagrams. our results show clearly the effect of a finite parton mass and width on the perturbative elastic $(q(g) q \rightarrow q (g) q)$ cross sections which depend on temperature $t$, energy density $\epsilon$, the invariant energy $\sqrt{s}$ and the scattering angle $\theta$. our detailed comparisons demonstrate that the finite width of the quasi-particles in the dqpm - which encodes the multiple partonic scattering - has little influence on the cross section for $q q \rightarrow q q$ as well as $g q \rightarrow g q$ scattering except close to thresholds. thus when studying the dynamics of energetic heavy quarks in a qgp medium the spectral width of the degrees-of-freedom may be discarded. we have, furthermore, compared the cross sections from the dqpm with corresponding results from hard-thermal-loop (htl) approaches. the htl inspired models - essentially fixing the regulators by elementary vacuum cross sections and decay amplitudes instead of properties of the qgp at finite temperature - provide quite different results especially w.r.t. the temperature dependence of the $qq$ and $gq$ cross sections (in all settings). accordingly, the transport properties of heavy quarks will be very different as a function of temperature when compared to dqpm results.
mde 2.0 : pragmatical formal model verification and other challenges. this document presents a synthesis of the research results conducted in the eld of software veri cation for model-driven engineering (mde). mde is becoming one of the dominant software engineering paradigms in the industry. the main characteristic of mde is the use of software models and model manipulation operations as main artifacts in all software engineering activities. this change of perspective implies that correctness of models (and model manipulation operations) becomes a key factor in the quality of the nal software product. the problem of ensuring software correctness is still considered to be a grand challenge for the software engineering community. at the modellevel, we are still missing a set of tools and methods that helps in the detection of defects and smoothly integrates in existing mde-based tool-chains without an excessive overhead. characteristics of existing tools, which require designer interaction, deep knowledge of formal methods or extensive manual model annotations seriously impair its usability in practice. in this document, we present our pragmatic set of techniques for formal model veri cation to overcome these limitations. we call our techniques pragmatic because they try to nd the best trade-o between completeness of the veri cation and the usability of the process.
systematical calculation of alpha decay half-lives with a generalized liquid drop model. a systematic calculation of α decay half-lives is presented for even-even nuclei between te and z=118 isotopes. the potential energy governing α decay has been determined within a liquid drop model including proximity effects between the α particle and the daughter nucleus and taking into account the experimental q value. the α decay half-lives have been deduced from the wkb barrier penetration probability. the α decay half-lives obtained agree reasonably well with the experimental data.
heavy quark quenching from rhic to lhc: mc@hq generator compared to experiments. null
heavy-flavor azimuthal correlations of d mesons. observables of heavy-quark azimuthal correlations in heavy-ion collisions are a new and promising tool for the investigation of the in-medium energy loss. we explore the potential of these observables to discriminate the collisional and radiative contributions within a hybrid epos+mc@shq transport approach.
analysing radial flow features in p-pb and p-p collisions at several tev by studying identified particle production in epos3. experimental transverse momentum spectra of identified particles in p-pb collisions at 5.02 tev show many similarities to the corresponding pb-pb results, the latter ones usually being interpreted in term of hydrodynamic flow. we analyse these data using epos3, an event generator based on a 3d+1 viscous hydrodynamical evolution starting from flux tube initial conditions, which are generated in the gribov-regge multiple scattering framework. an individual scattering is referred to as pomeron, identified with a parton ladder, eventually showing up as flux tubes (or strings). each parton ladder is composed of a pqcd hard process, plus initial and final state linear parton emission. nonlinear effects are considered by using saturation scales $q_{s}$, depending on the energy and the number of participants connected to the pomeron in question. we compute transverse momentum ($p_{t}$) spectra of pions, kaons, protons, lambdas, and $\xi$ baryons in p-pb and p-p scattering, compared to experimental data and many other models. in this way we show in a quantitative fashion that p-pb data (and even p-p ones) show the typical ''flow effect'' of enhanced particle production at intermediate $p_{t}$ values, more and more visible with increasing hadron mass.
$j/\psi$ polarization in p+p collisions at $\sqrt{s}$ = 200 gev in star. we report on a polarization measurement of inclusive $j/\psi$ mesons in the di-electron decay channel at mid-rapidity at 2 $&lt; p_t &lt;$ 6 gev/c in p+p collisions at $\sqrt{s}$ = 200 gev. data were taken with the star detector at rhic. the $j/\psi$ polarization measurement should help to distinguish between the different models of the $j/\psi$ production mechanism since they predict different $p_t$ dependences of the $j/\psi$ polarization. in this analysis, $j/\psi$ polarization is studied in the helicity frame. the polarization parameter $\lambda_{\theta}$ measured at rhic becomes smaller towards high $p_t$, indicating more longitudinal $j/\psi$ polarization as $p_t$ increases. the result is compared with predictions of presently available models.
heavy-flavor electron-muon correlations in $p$$+$$p$ and $d$+au collisions at $\sqrt{s_{_{nn}}}$ = 200 gev. we report $e^\pm-\mu^\mp$ pair yield from charm decay measured between midrapidity electrons ($|\eta|&lt;0.35$ and $p_t&gt;0.5$ gev/$c$) and forward rapidity muons ($1.4&lt;\eta&lt;2.1$ and $p_t&gt;1.0$ gev/$c$) as a function of $\delta\phi$ in both $p$$+$$p$ and in $d$+au collisions at $\sqrt{s_{_{nn}}}=200$ gev. comparing the $p$$+$$p$ results with several different models, we find the results are consistent with a total charm cross section $\sigma_{c\bar{c}} =$ 538 $\pm$ 46 (stat) $\pm$ 197 (data syst) $\pm$ 174 (model syst) $\mu$b. these generators also indicate that the back-to-back peak at $\delta\phi = \pi$ is dominantly from the leading order contributions (gluon fusion), while higher order processes (flavor excitation and gluon splitting) contribute to the yield at all $\delta\phi$. we observe a suppression in the pair yield per collision in $d$+au. we find the pair yield suppression factor for $2.7&lt;\delta\phi&lt;3.2$ rad is $j_{da}$ = 0.433 $\pm$ 0.087 (stat) $\pm$ 0.135 (syst), indicating cold nuclear matter modification of $c\bar{c}$ pairs.
overview of mhz air shower radio experiments and results. in this paper, i present a review of the main results obtained in the last 10 years in the field of radio-detection of cosmic-ray air showers in the mhz range. all results from all experiments cannot be reported here so that i will focus on the results more than on the experiments themselves. modern experiments started in 2003 with codalema and lopes. in 2006, small-size autonomous prototypes setup were installed at the pierre auger observatory site, to help the design of the auger engineering radio array (aera). we will discuss the principal aspects of the radio data analysis and the determination of the primary cosmic ray characteristics: the arrival direction, the lateral distribution of the electric field, the correlation with the primary energy, the emission mechanisms and the sensitivity to the composition of the cosmic rays.
impact of water radiolysis on uranium dioxide corrosion. null
radiolytic corrosion of grain boundaries onto the uo2 triso particle surface. this work is dealing with the understandingof the corrosion mechanisms at solid/solutioninterface and taking into account for the4he2+ions irradiation effects on these mechanisms.these corrosion and4he2+ions radiolysis phenomena append at solid/solution interface andwill be studied at a μmetric scale by the raman spectroscopy. moreover, a4he2+ionsirradiation affects a small low volume and allowsus to control the irradiated area (solution,solid or interface). for the solid,the chemical species induced by4he2+ions radiolysis ofwater are reactive and are involved inclassical corrosion mechanisms of uo2. moreover, wewant to study the impact of the4he2+ions radiolysis of waterlayers physisorbed into thesurface onto corrosion mechanisms. that is thereason why we want touse a local irradiation,allowed by the4he2+ions ion beam provided by thearronax cyclotron (e = 64.7 mev).in this work an experimental apparatus will be performed in order to characterizesolid/solution interface at μmetric scale by raman spectroscopy under4he2+ions irradiationprovided by the cyclotron arronax facility.the leaching experiments under irradiationwill be performed for a short time in order tostudy the parameters during the fast instantrelease step. the grain boundaries effect will be studied by the comparison between onetriso particles set (solids with grain boundaries) and one triso particles set previouslywashed by one acid solution (solid without grain boundaries). the role of h2will be studiedby the comparison between experiments under ar or ar/h2atmosphere. the dose rate rangewill be between 0 and 100 gy/min by using the alpha ion beam which let us control the doseset down into the sample. for all these experiments, measurements will be performed by thein situraman spectroscopy during the irradiation in order to follow theformation/consumption of the secondary phases formed onto the solid. the sem will beperformed in order to characterize the grain boundaries and the secondary phases formed bythe leaching/irradiation experiments.the μgc is used to measure the ph2into the irradiationcell to follow the production/consumption of this gaseous species formed by the waterradiolysis and consumedby the leaching process.
role of bacteria on the release of cesium from illite. null
drifting tracers behavior into the groundwater of a french nuclear waste disposal. null
transport coefficients from the nambu-jona-lasinio model for su(3)f. we calculate the shear η(t) and bulk viscosities ζ(t) as well as the electric conductivity σe(t) and heat conductivity κ(t) within the nambu-jona-lasinio model for three flavors as a function of temperature as well as the entropy density s(t), pressure p(t), and speed of sound cs2(t). we compare the results with other models such as the polyakov-nambu-jona-lasinio (pnjl) model and the dynamical quasiparticle model (dqpm) and confront these results with lattice qcd data whenever available. we find the njl model to have a limited predictive power for the thermodynamic variables and various transport coefficients above the critical temperature, whereas the pnjl model and dqpm show acceptable results for the quantities of interest.
supply chain design and distribution planning under supply uncertainty : application to bulk liquid gas distribution. the distribution of liquid gazes (or cryogenic liquids) using bulks and tractors is a particular aspect of a fret distribution supply chain. traditionally, these optimisation problems are treated under certainty assumptions. however, a large part of real world optimisation problems are subject to significant uncertainties due to noisy, approximated or unknown objective functions, data and/or environment parameters. in this research we investigate both robust and stochastic solutions. we study both an inventory routing problem (irp) and a production planning and customer allocation problem. thus, we present a robust methodology with an advanced scenario generation methodology. we show that with minimal cost increase, we can significantly reduce the impact of the outage on the supply chain. we also show how the solution generation used in this method can also be applied to the deterministic version of the problem to create an efficient grasp and significantly improve the results of the existing algorithm. the production planning and customer allocation problem aims at making tactical decisions over a longer time horizon. we propose a single-period, two-stage stochastic model, where the first stage decisions represent the initial decisions taken for the entire period, and the second stage representing the recovery decision taken after an outage. we aim at making a tool that can be used both for decision making and supply chain analysis. therefore, we not only present the optimized solution, but also key performance indicators. we show on multiple real-life test cases that it isoften possible to find solutions where a plant outage has only a minimal impact.
cad-based approach for identification of elasto-static parameters of robotic manipulators. the paper presents an approach for the identification of elasto-static parameters of a robotic manipulator using the virtual experiments in a cad environment. it is based on the numerical processing of the data extracted from the finite element analysis results, which are obtained for isolated manipulator links. this approach allows to obtain the desired stiffness matrices taking into account the complex shape of the links, couplings between rotational/translational deflections and particularities of the joints connecting adjacent links. these matrices are integral parts of the manipulator lumped stiffness model that are widely used in robotics due to its high computational efficiency. to improve the identification accuracy, recommendations for optimal settings of the virtual experiments are given, as well as relevant statistical processing techniques are proposed. efficiency of the developed approach is confirmed by a simulation study that shows that the accuracy in evaluating the stiffness matrix elements is about 0.1%.
identification of geometrical and elastostatic parameters of heavy industrial robots. the paper focuses on the stiffness modeling of heavy industrial robots with gravity compensators. the main attention is paid to the identification of geometrical and elastostatic parameters and calibration accuracy. to reduce impact of the measurement errors, the set of manipulator configurations for calibration experiments is optimized with respect to the proposed performance measure related to the end-effector position accuracy. experimental results are presented that illustrate the advantages of the developed technique.
stiffness modeling of robotic manipulator with gravity compensator. the paper focuses on the stiffness modeling of robotic manipulators with gravity compensators. the main attention is paid to the development of the stiffness model of a spring-based compensator located between sequential links of a serial structure. the derived model allows us to describe the compensator as an equivalent non-linear virtual spring integrated in the corresponding actuated joint. the obtained results have been efficiently applied to the stiffness modeling of a heavy industrial robot of the kuka family.
practical identifiability of the manipulator link stiffness parameters. the paper addresses a problem of the manipulator stiffness modeling, which is extremely important for the precise manufacturing of contemporary aeronautic materials where the machining force causes significant compliance errors in the robot end-effector position. the main contributions are in the area of the elastostatic parameters identification. particular attention is paid to the practical identifiability of the model parameters, which completely differs from the theoretical one that relies on the rank of the observation matrix only, without taking into account essential differences in the model parameter magnitudes and the measurement noise impact. this problem is relatively new in robotics and essentially differs from that arising in geometrical calibration. to solve the problem, several physical and statistical model reduction methods are proposed. they are based on the stiffness matrix sparseness taking into account the physical properties of the manipulator elements and also on the heuristic selection of the practically non-identifiable parameters that employs numerical analyses of the parameter estimates. the advantages of the developed approach are illustrated by an application example that deals with the stiffness modeling of an industrial robot used in aerospace industry.
advanced robot calibration using partial pose measurements. the paper focuses on the calibration of serial industrial robots using partial pose measurements. in contrast to other works, the developed advanced robot calibration technique is suitable for geometrical and elastostatic calibration. the main attention is paid to the model parameters identification accuracy. to reduce the impact of measurement errors, it is proposed to use directly position measurements of several points instead of computing orientation of the end-effector. the proposed approach allows us to avoid the problem of non-homogeneity of the least-square objective, which arises in the classical identification technique with the full-pose information. the developed technique does not require any normalization and can be efficiently applied both for geometric and elastostatic identification. the advantages of a new approach are confirmed by comparison analysis that deals with the efficiency evaluation of different identification strategies. the obtained results have been successfully applied to the elastostatic parameters identification of the industrial robot employed in a machining work-cell for aerospace industry.
robust algorithm for calibration of robotic manipulator model. the paper focuses on the robust identification of geometrical and elastostatic parameters of robotic manipulator. the main attention is paid to the efficiency improvement of the identification algorithm. to increase the identification accuracy, it is proposed to apply the weighted least square technique that employs a new algorithm for assigning of the weighting coefficients. the latter allows taking into account variation of the measurement system precision in different directions and throughout the robot workspace. the advantages of the proposed approach are illustrated by an application example that deals with the elasto-static calibration of industrial robot.
efficiency improvement of measurement pose selection techniques in robot calibration. the paper deals with the design of experiments for manipulator geometric and elastostatic calibration based on the test-pose approach. the main attention is paid to the efficiency improvement of numerical techniques employed in the selection of optimal measurement poses for calibration experiments. the advantages of the developed technique are illustrated by simulation examples that deal with the geometric calibration of the industrial robot of serial architecture.
modelling of the gravity compensators in robotic manufacturing cells. the paper deals with the modeling and identification of the gravity compensators used in heavy industrial robots. the main attention is paid to the geometrical parameters identification and calibration accuracy. to reduce impact of the measurement errors, the design of calibration experiments is used. the advantages of the developed technique are illustrated by experimental results.
pierre auger observatory and telescope array: joint contributions to the 33rd international cosmic ray conference (icrc 2013). joint contributions of the pierre auger and telescope array collaborations to the 33rd international cosmic ray conference, rio de janeiro, brazil, july 2013: cross-calibration of the fluorescence telescopes, large scale anisotropies and mass composition.
the substitution principle in an object-oriented framework for web services: from failure to success. nowadays, services are more and more implemented by using object-oriented frameworks. in this context, two properties could be particularly required in the specification of these frameworks: (i) a loose coupling between the service layer and the object layer, allowing evolution of the service layer with a minimal impact on the object layer, (ii) an interoperability induced by the substitution principle associated to subtyping in the object layer, allowing to freely convert a value of a subtype into a supertype. however, experimenting with the popular cxf framework, we observed some undesirable coupling and interoperability issues, due to the failure of the substitution principle. therefore we propose a new specification of the data binding used to translate data between the object and service layers. we show that if the cxf framework followed the specification, then the substitution principle would be recovered, with all its advantages.
enabling large-scale testing of iaas cloud platforms on the grid'5000 testbed. almost ten years after its premises, the grid'5000 platform has become one of the most complete testbeds for designing or evaluating large-scale distributed systems. initially dedicated to the study of high performance computing, the infrastructure has evolved to address wider concerns related to desktop computing, the internet of services and more recently the cloud computing paradigm. in this paper, we present the latest mechanisms we designed to enable the automated deployment of the major open-source iaas cloudkits (i.e., nimbus, opennebula, cloudstack, and openstack) on grid'5000. providing automatic, isolated and reproducible deployments of cloud environments lets end-users study and compare each solution or simply leverage one of them to perform higher-level cloud experiments (such as investigating map/reduce frameworks or applications).
a strain-specific segment of the rna-dependent rna polymerase of grapevine fanleaf virus determines symptoms in nicotiana species. null
certified calibration of a cable-driven robot using interval contractor programming. in this paper, an interval based approach is proposed to rigorously identify the model parameters of a parallel cable-driven robot. the studied manipulator follows a parallel architecture having 8 cables to control the 6 dofs of its mobile platform. this robot is complex to model, mainly due to the cable behavior. to simplify it, some hypotheses on cable properties (no mass and no elasticity) are done.an interval approach can take into account the maximal error between this model and the real one. this allows us to work with a simplified although guaranteed interval model. in addition, a specific interval operator makes it possible to manage outliers. a complete experiment validates our method for robot parameter certified identification and leads to interesting observations.
dirac operator on complex manifolds and supersymmetric quantum mechanics. we explore a simple n=2 supersymmetric quantum mechanics (sqm) model describing the motion over complex manifolds in external gauge fields. the nilpotent supercharge q of the model can be interpreted as a (twisted) exterior holomorphic derivative, such that the model realizes the twisted dolbeault complex. the sum q+barq can be interpreted as the dirac operator: the standard dirac operator if the manifold is kähler and the dirac operator involving certain particular extra torsions for a generic complex manifold. focusing on the kähler case, we give new simple physical proofs of the two mathematical facts: (i) the equivalence of the twisted dirac and twisted dolbeault complexes and (ii) the atiyah-singer theorem.
vapor hydration of son68 glass from 90 degrees c to 200 degrees c: a kinetic study and corrosion products investigation. corrosion of nuclear waste glass in unsaturated conditions is expected to occur upon the closure of the repository galleries during disposal cell saturation in the proposed french disposal site. the objectives of the present work were to determine the alteration kinetics of the son68 reference in such conditions. vapor hydration tests were conducted using thin, polished son68 glass coupons contained in stainless steel autoclaves. temperatures ranged between 90 °c and 200 °c and the relative humidity (rh) was maintained at 91 ± 1%. additional experiments at 175 °c and 80, 85, 90 and 95% rh were also conducted to assess the role of rh on the glass corrosion rate. the nature and extent of corrosion have been determined by characterizing the reacted glass surface with scanning electron microscopy (sem), transmission electron microscopy (tem), and energy dispersive x-ray spectroscopy (eds). elemental profiling of the glass hydrated at 90 °c was studied by tof-sims. the chemical composition of the external layer depends on experimental conditions. the hydration rate at 90 °c (tof-sims analysis) is 10 × higher than the generally accepted final rate of son68 in water at 90 °c (~ 10− 4 g m− 2 d− 1). this may indicate that the glass hydration process cannot be simulated by experiments in aqueous solution with a high s/v ratio. subsequent leaching (corrosion in an aqueous solution) of samples weathered in water vapor showed dissolution rate values higher than those of pristine glass. this result indicates that mobile elements are trapped within the alteration products during the hydration step and it gives insight into mobility variations of the considered elements.
combining finite and continuous solvers towards a simpler solver maintenance. combining efficiency with reliability within cp systems is one of the main concerns of cp developers. this paper presents a simple and efficient way to connect choco and ibex, two cp solvers respectively specialised on finite and continuous domains. this enables to take advantage of the most recent advances of the continuous community within choco while saving development and maintenance resources, hence ensuring a better software quality.
the waveform digitiser of the double chooz experiment: performance and quantisation effects on photomultiplier tube signals. we present the waveform digitiser used in the double chooz experiment. we describe the hardware and the custom-built firmware specifically developed for the experiment. the performance of the device is tested with regards to digitising low light level signals from photomultiplier tubes and measuring pulse charge. this highlights the role of quantisation effects and leads to some general recommendations on the design and use of waveform digitisers.
toward the autonomous radio-detection of ultra high energy cosmic rays with codalema. the codalema experiment aims to study the radio-detection of ultra high energy cosmic rays in the energy range of 1017 ev. spread over an area of 0.25 km2, the original device hosted at nançay (france) has mainly benefited of an array of short dipoles, connected by cables up to a centralized acquisition room. since 2010, a major evolution has been initiated to add 60 autonomous radio-detection stations, covering a surface of 1.5 km2. this enlarged configuration should help refine the studies and serve as a bench test for the mastery of autonomous detection. the main characteristics of this new mode of operation is presented in the light of recent results obtained by the original codalema setup.
net-baryon-, net-proton-, and net-charge kurtosis in heavy-ion collisions within a relativistic transport approach. we explore the potential of net-baryon, net-proton and net-charge kurtosis measurements to investigate the properties of hot and dense matter created in relativistic heavy-ion collisions. contrary to calculations in a grand-canonical ensemble we explicitly take into account exact electric and baryon charge conservation on an event-by-event basis. this drastically limits the width of baryon fluctuations. a simple model to account for this is to assume a grand-canonical distribution with a sharp cut-off at the tails. we present baseline predictions of the energy dependence of the net-baryon, net-proton and net-charge kurtosis for central (b≤2.75 fm) pb+pb/au+au collisions from e lab=2a gev to snn−−−√=200 gev from the urqmd model. while the net-charge kurtosis is compatible with values around zero, the net-baryon number decreases to large negative values with decreasing beam energy. the net-proton kurtosis becomes only slightly negative for low snn−−−√ .
sorption of selenite in a multi-component system using the "dialysis membrane" method. abstract 79se is a potentially mobile long-lived fission product, which may make a dominant contribution to the long-term radiation exposure resulting from deep geological disposal of radioactive waste. its mobility is affected by sorption on minerals. selenium sorption processes have been studied mainly by considering interaction with a single mineral surface. in the case of multi-component systems (e.g. soils), it is difficult to predict the radioelement behaviour only from the mineral constituents. this study contributes to the understanding of multi-component controls of se concentrations towards predicting se behaviour in soils after migration from a disposal site. this goal was approached by measuring selenite sorption on mono and multi-phase systems physically separated by dialysis membranes. to the best of the authors' knowledge, very few studies have used dialysis membranes to study the sorption competition of selenite between several mineral phases. other workers have used this method to study the sorption of pesticides on montmorillonite in the presence of dissolved organic matter. indeed, this method allows measurement of individual kd in a system composed of several mineral phases. dialysis membranes allowed (i) determination of the competition of two mineral phases for selenite sorption (ii) and determination of the role of humic acids (has) on selenite sorption in oxidising conditions. experimental results at ph 7.0 show an average se(iv) sorption distribution coefficient (kd) of approximately 125 and 9410 l kg−1 for bentonite and goethite, respectively. the average kd for goethite decreases to 613 l kg−1 or 3215 l kg−1 in the presence of bentonite or ha, respectively. for bentonite, the average kd decreases slightly in the presence of goethite (60 l kg−1) and remains unchanged in the presence of ha. the experimental data were successfully modelled with a surface complexation model using the phreeqc geochemical code. the drastic decrease in se(iv) sorption on goethite in a multi-phase system is attributed to competition with dissolved silica released by bentonite. as with si the ha compete with se for sorption sites on goethite.
academic evaluation protocol for monitoring the usage modalities in automatic control laboratory: real vs. remote. this article describes an academic evaluation protocol (aep) that has been designed and implemented in order for monitoring different kinds of modalities that can be used to carry out an automatic control laboratory. for this proposed, these types of patterns lab-works have been related as: real laboratory (rl), remote laboratory (r@l) and real plus remote laboratory (rl+r@l). twelve (12) groups of students from a control course at the universidad de los andes have been selected to carry out the presented testing, at the same time that, they have been placed into the evaluated patterns lab-works trying to keep uniform the academic features of performance for each involved student-groups in order to reduce the uncertainly in the analysis of the results obtained from the applied aep. to estimate how a specific lab-work modality impact the development of an experimental practice, the parameter average utilization time and also the abet-indicators have been employed for this propose. the results from this pedagogical instrument are analyzed applying the anova test, a descriptive statistical technique and the wilcoxon testing, concluding that, the student-groups who developed the experimental practice on rl and rl+r@l modalities, perform better in the development of the automatic control laboratory than the student-groups who only work on the remote system.
a multi-user remote academic laboratory system. this article describes the development, implementation and preliminary operation assessment of multiuser network architecture to integrate a number of remote academic laboratories for educational purposes on automatic control. through the internet, real processes or physical experiments conducted at the control engineering laboratories of four universities are remotely operated. through an internet connection to the manager administration server, a remote experiment to design and test a modeling and control algorithm can be performed. the suggested network architecture is based on the singlet-server model and uses a database server to record important information that helps create a new remote experiment, including a graphical user interface (applet) developed with easy java simulation, which allows the simple integration of new processes to the manager administrator. results of a real-physical-process remote manipulation through the proposed network architecture are presented as well as results of an academic pilot test conducted to measure functional aspects related to the operation of the remote system when carrying out remote-laboratory work.
valid inequalities and dominance rules for speed-dating scheduling linear models. null
system-size dependence of open-heavy-flavor production in nucleus-nucleus collisions at $\sqrt{s_{_{nn}}}$=200 gev. the phenix collaboration at the relativistic heavy ion collider has measured open heavy flavor production in cu$+$cu collisions at $\sqrt{s_{_{nn}}}$=200 gev through the measurement of electrons at midrapidity that originate from semileptonic decays of charm and bottom hadrons. in peripheral cu$+$cu collisions an enhanced production of electrons is observed relative to $p$$+$$p$ collisions scaled by the number of binary collisions. in the transverse momentum range from 1 to 5 gev/$c$ the nuclear modification factor is $r_{aa}$$\sim$1.4. as the system size increases to more central cu$+$cu collisions, the enhancement gradually disappears and turns into a suppression. for $p_t&gt;3$ gev/$c$, the suppression reaches $r_{aa}$$\sim$0.8 in the most central collisions. the $p_t$ and centrality dependence of $r_{aa}$ in cu$+$cu collisions agree quantitatively with $r_{aa}$ in $d+$au and au$+$au collisions, if compared at similar number of participating nucleons $\langle n_{\rm part} \rangle$.
centrality, rapidity and transverse momentum dependence of j/psi suppression in pb-pb collisions at sqrt(snn)=2.76tev. the inclusive j/psi nuclear modification factor raa in pb-pb collisions at sqrt(snn)=2.76tev has been measured by alice as a function of centrality in the e+e- decay channel at mid-rapidity |y| &lt; 0.8 and as a function of centrality, transverse momentum and rapidity in the u+u- decay channel at forward-rapidity 2.5 &lt; y &lt; 4.the j/psi yields measured in pb-pb are suppressed compared to those in pp collisions scaled by the number of binary collisions.the raa integrated over a centrality range corresponding to 90% of the inelastic pb-pb cross section is 0.72 +- 0.06 (stat.) +- 0.10 (syst.) at mid-rapidity and 0.57 +- 0.01 (stat.) +- 0.09 (syst.) at forward-rapidity.at low transverse momentum, significantly larger values of raa are measured at forward-rapidity compared to measurements at lower energy.these features suggest that a contribution to the j/psi yield originates from charm quarks (re)combination in the deconfined partonic medium.
mineral resource assessment: compliance between emergy and exergy respecting odum's hierarchy concept. in this paper, authors suggest to combine the exergoecology and the emergy concept in order to evaluate mineral resources, taking into account their abundance, their chemical and physical properties and the impact of their extraction. the first proposition of this work is to consider that every group of mineral, dispersed in the earth's crust, is a co-product of the latter. the specific emergies of dispersed minerals are, then, inversely proportional to their abundance. the results comply with the material hierarchy as the specific emergy of a dispersed mineral rise with its scarcity. the second is an emergy evaluation model based on the chemical and concentration exergy of the mineral, its condition in the mine and its abundance. this model permits to assess the decline of mineral reserves and its impact on the ecosystem. the dispersed specific emergy of 42 main commercially used minerals has been calculated. furthermore, the emergy decrease of some australian mineral reserves has been studied, as well as the land degradation of us copper mines.
jets, bulk matter, and their interaction in heavy ion collisions at the lhc. null
status of the nucifer experiment. null
the neutron background of the xenon100 dark matter search experiment. null
two and three-pion quantum statistics correlations in pb-pb collisions at sqrt(snn)=2.76 tev at the lhc. correlations induced by quantum statistics are sensitive to the spatio-temporal extent as well as dynamics of particle emitting sources in heavy-ion collisions. in addition, such correlations can be used to search for the presence of a coherent component of pion production. two and three-pion correlations of same and mixed-charge are measured at low relative momentum to estimate the coherent fraction of charged pions in pb-pb collisions at sqrt(snn)= 2.76 tev at the lhc with alice. the genuine three-pion quantum statistics correlation is found to be suppressed relative to the two-pion correlation based on the assumption of fully chaotic pion emission.the suppression is observed to decrease with triplet momentum. the observed suppression at low triplet momentum may correspond to a coherent fraction in charged pion emission of 22% +- 12%.
erratum: silicone oil: an effective absorbent for the removal of hydrophobic volatile organic compounds. null
bathroom greywater characterization and potential treatments for reuse. with the emerging crisis of water, greywaters represent a significant resource of water if considering recycling for uses not requiring a drinking water quality. samples of greywaters were taken from a few households. their characterization led to results similar to those in literature. however, they showed a lack of phosphorus in c/n/p ratio. nevertheless, it was shown that, in our study, median was more appropriate than mean. the potential treatment steps studied during this work were sand bed filtration, adsorption onto granular activated carbon (gac), and sanitation by chlorine. the sand bed which was supplied with sequential feedings led to a very good removal of total suspended solids (tss; and consequently of turbidity) as well as to a 30% cod decrease. however, the organic matter withdrawal was more efficient by adsorption onto gac. the chlorination of greywaters was efficient to decrease the microbial population. therefore, following the reclaimed water quality which would be required treatment might imply all steps or just one or two. this kind of low-cost device could thus be implemented for reuse such as irrigation, agricultural need, or urban use.
voc absorption in a countercurrent packed-bed column using water/silicone oil mixtures: influence of silicone oil volume fraction. a calculation procedure to determine the influence of the silicone volume fraction on the physical absorption of vocs in water/silicone oil mixtures is presented (eta(silicone) oil = 5 mpa s). it is based on the ''equivalent absorption capacity'' concept previously developed by dumont et al. (2010)111 and applied to a countercurrent gas-liquid absorber. the calculation procedure is first applied to three vocs: dimethylsulphide (dms), dimethyldisulphide (dmds) and toluene, and then generalised to other vocs. the influence of voc partition coefficients (h(voc,water) and h(voc,solvent)) and the ratio mr = h(voc,water)/h(voc,solvent) is shown. for vocs having a much higher affinity for silicone oil than for water (m(r) &gt; 20 as for dmds and toluene). it is preferable to use pure silicone oil rather than water/silicone oil mixtures for absorption. (c) 2011 elsevier b.v. all rights reserved.
conversion of agricultural residues into activated carbons for water purification: application to arsenate removal. the conversion of two agricultural wastes, sugar beet pulp and peanut hulls, into sustainable activated carbons is presented and their potential application for the treatment of arsenate solution is investigated. a direct and physical activation is selected as well as a simple chemical treatment of the adsorbents. the material properties, such as bet surface areas, porous volumes, elemental analysis, ash contents and ph(pzc), of these alternative carbonaceous porous materials are determined and compared with a commercial granular activated carbon. an adsorption study based on experimental kinetic and equilibrium data is conducted in a batch reactor and completed by the use of different models (intraparticle diffusion, pseudo-second-order, langmuir and freundlich) and by isotherms carried out in natural waters. it is thus demonstrated that sugar beet pulp and peanut hulls are good precursors to obtain activated carbons for arsenate removal.
evolution of microbial aerosol behaviour in heating, ventilating and air-conditioning systems - quantification of staphylococcus epidermidis and penicillium oxalicum viability. the aim of this study was to develop an experimental set-up and a methodology to uniformly contaminate several filter samples with high concentrations of cultivable bacteria and fungi. an experimental set-up allows contaminating simultaneously up to four filters for range of velocities representative of heating, ventilating and air-conditioning systems. the test aerosol was composed of a microbial consortium of one bacterium (staphylococcus epidermidis) and one fungus (penicillium oxalicum) and aerosol generation was performed in wet conditions. firstly, the experimental set-up was validated in regards to homogeneity of the air flows. the bioaerosol was also characterized in terms of number and particle size distribution using two particle counters: optical particle counter grimm (r) 1.109 (optical diameters) and tsi aps 3321 (aerodynamic diameters). moreover, stabilities of the number of particles generated were measured. finally, concentrations of cultivable microorganisms were measured with biosamplers (skc) downstream of the four filters.
potential of aquatic macrophytes as bioindicators of heavy metal pollution in urban stormwater runoff. the concentrations of heavy metals in water, sediments, soil, roots, and shoots of five aquatic macrophytes species (oenanthe sp., juncus sp., typha sp., callitriche sp.1, and callitriche sp.2) collected from a detention pond receiving stormwater runoff coming from a highway were measured to ascertain whether plants organs are characterized by differential accumulations and to evaluate the potential of the plant species as bioindicators of heavy metal pollution in urban stormwater runoff. heavy metals considered for water and sediment analysis were cd, cr, cu, ni, pb, zn, and as. heavy metals considered for plant and soil analysis were cd, ni, and zn. the metal concentrations in water, sediments, plants, and corresponding soil showed that the studied site is contaminated by heavy metals, probably due to the road traffic. results also showed that plant roots had higher metal content than aboveground tissues. the floating plants displayed higher metal accumulation than the three other rooted plants. heavy metal concentrations measured in the organs of the rooted plants increased when metal concentrations measured in the soil increased. the highest metal bioconcentration factors (bcf) were obtained for cadmium and nickel accumulation by typha sp. (bcf = 1.3 and 0.8, respectively) and zinc accumulation by juncus sp. (bcf = 4.8). our results underline the potential use of such plant species for heavy metal biomonitoring in water, sediments, and soil.
assessment of the efficiency of photocatalysis on tetracycline biodegradation. the use of photocatalysis to improve the biodegradability of an antibiotic compound, tetracycline (tc) was investigated. the toxicity of tc and its degradation products were also examined. the sturm test was conducted to assess the biodegradability of by-products formed in the photocatalytic process. the toxicity of tetracycline and its by-products was evaluated using a dehydrogenase inhibition test, which showed a decrease in toxicity during photocatalysis. however, the sturm test results indicated that, like tetracycline, the by-products are not biodegradable. possible structures of these by-products were determined using liquid chromatography-electrospray ionization-tandem mass spectrometry (lc-esi-ms/ms). it was found that, during the photocatalytic process, the tc aromatic ring is not opened and the structure of the identified by-products is quite similar to that of tetracycline. a reaction pathway is proposed. (c) 2012 elsevier b.v. all rights reserved.
phosphate removal from synthetic and real wastewater using steel slags produced in europe. electric arc furnace steel slags (eaf-slags) and basic oxygen furnace steel slags (bof-slags) were used to remove phosphate from synthetic solutions and real wastewater. the main objective of this study was to establish an overview of the phosphate removal capacities of steel slags produced in europe. the influences of parameters, including ph, and initial phosphate and calcium concentrations, on phosphate removal were studied in a series of batch experiments. phosphate removal mechanisms were also investigated via an in-depth study. the maximum capacities of phosphate removal from synthetic solutions ranged from 0.13 to 0.28 mg p/g using eaf-slags and from 1.14 to 2.49 mg p/g using bof-slags. phosphate removal occurred predominantly via the precipitation of ca-phosphate complexes (most probably hydroxyapatite) according to two consecutive reactive phases: first, dissolution of cao-slag produced an increase in ca2+ and oh- ion concentrations; then the ca2+ and oh- ions reacted with the phosphates to form hydroxyapatite. it was found that the release of ca2+ from slag was not always enough to enable hydroxyapatite precipitation. however, our results indicated that the ca2+ content of wastewater represented a further source of ca2+ ions that were available for hydroxyapatite precipitation, thus leading to an increase in phosphate removal efficiencies. (c) 2012 elsevier ltd. all rights reserved.
influence of operating conditions on direct nanofiltration of greywaters: application to laundry water recycling aboard ships. the present study completes a previous work dedicated to the feasibility to implement, on-board ship, a direct nanofiltration process in order to treat laundry greywaters and recycle 80% to the inlets of the washing machines (guilbaud et al., 2010). at present, the study investigates the influence of nanofiltration operating conditions on chemical oxygen demand (cod) rejection rates and permeates fluxes. thus, the ph and temperature of greywater as well as transmembrane pressure have been fixed to 7 or 9, 25 or 40 degrees c and 35 or 40 bar, respectively. afc80 membranes show different cod rejection rates whereas permeate fluxes are quasi similar when the same greywater (ph 7) is nanofiltered at 35 bar and 25 degrees c. amongst all the tested operating conditions, the nanofiltration of greywater (ph 7) on afc80 membrane, at 35 bar and 25 degrees c, allows to obtain the highest cod rejection rate (around 93% at vrf5). the best permeate flux (85.5 l/h/m(2) at vrf5) has been obtained at 40 bar and 40 degrees c. an increase of temperature or pressure above 25 degrees c and 35 bar respectively leads to a drop of cod rejection rates. the ph should be maintained at a value of 7, the initial ph of raw greywater, in order to allow a good cod rejection rate. an economic evaluation of greywater nanofiltration has been investigated. (c) 2012 elsevier b.v. all rights reserved.
biological treatment of a mixture of gaseous sulphur reduced compounds: identification of the total bacterial community's structure. background: this study deals with the potential of biological processes combining biofiltration and a biotrickling filter to treat a mixture of sulphur reduced compounds (src) including dimethylsulfide (dms), dimethyldisulfide (dmds) and hydrogen sulphide (h2s). the first step in this work is to evaluate the influence of ph on src biodegradation in microcosms seeded with planktonic biomass from activated sludge of a rendering facility. in a second step, for each tested ph, evaluation of the influence of ph on total bacterial community diversity and structure has been investigated using denaturing gradient gel electrophoresis (dgge). results: at ph 7 and 5, h2s, dms and dmds were completely removed. in return, the abatement of dms and dmds is low, around 20%, for ph 3 and ph 1 microcosms. the selective pressure imposed in the microcosms (concentration and ph) is sufficient to influence strongly the community diversity and composition. the inoculum community seems to make a significant contribution to the 67-day community in the considered ph microcosms. conclusion: these results suggest that distinct communities from different inocula can achieve high and stable functionality. the results obtained provide relevant information for improving the treatment of different src using biotrickling filter/biofilter combination. copyright (c) 2012 society of chemical industry.
h2s biofiltration using expanded schist as packing material: performance evaluation and packed-bed tortuosity assessment. background: the aim of this work was to test an innovative packing material (expanded schist) for h2s biofiltration in order to determine the packing material performance in terms of elimination capacity, removal efficiency and pressure drop changes. additionally, the changes over time of bed characteristics, especially tortuosity, were evaluated according to porosity measurements. results: schist material can treat large loading rates (up to 30 g.m-3.h-1) with 100% efficiency at an empty bed residence time (ebrt) of 16 s, which is much better than most results reported in the literature. the porosity of the packed bed is around 40% (tortuosity estimated to range from 1.5 to 2.0) which leads to pressure drop measurements in the range of 1080 pa m-1. conclusion: schist is a good material for h2s biofiltration in terms of mechanical stability, removal efficiency and effective treatment of high h2s loading rates. schist is a material that provides the appropriate environment for micro-organisms by itself. this trend should be confirmed over a long period. copyright (c) 2012 society of chemical industry.
sustainable activated carbons from agricultural residues dedicated to antibiotic removal by adsorption. the objectives of this study are to convert at laboratory scale agricultural residues into activated carbons (ac) with specific properties, to characterize them and to test them in adsorption reactor for tetracycline removal, a common antibiotic. two new acs were produced by direct activation with steam from beet pulp (bp-h2o) and peanut hulls (ph-h2o) in environmental friendly conditions. bp-h2o and ph-h2o present carbon content ranged between 78% and 91%, similar bet surface areas (821 and 829 m(2).g(-1) respectively) and ph(pzc) values (9.8). their porosities are different: ph-h2o is mainly microporous (84%) with 0.403 cm(3).g(-1) of total porous volume, whereas bp-h2o develops a mesoporous volume of 0.361 cm(3).g-(1) representing 50% of the total porous volume. two other commercial granular ac carbons (gac1 and gac2) were also characterised and used for comparison. the adsorption study is conducted in batch reactors. two parts can be observed from kinetic decay curves: a very fast concentration decrease during the first 12 h, followed by a slow adsorption. an optimal contact time of 120 h is also deduced from these curves. it is shown as well that adsorption decreases with an increase of ph, indicating that the form preferentially adsorbed is probably the zwitterion form of the tetracycline. from equilibrium isotherms data, two adsorption models have been used: langmuir and freundlich. both of them lead to a very good description of the experimental data. maximum adsorption capacities deduced from the langmuir equation follow the sequence: gac2 (817 mg.g(-1))&gt;bp-h2o (288 mg.g(-1))&gt;gac1 (133 mg.g(-1))&gt;ph-h2o (28 mg.g(-1)). in real spring waters spiked with tc (tetracyclines), adsorption isotherms show that the maximum adsorption capacity of bp-h2o is slightly increased to 309 mg.g(-1) while it is decreased by one third to 550 mg.g(-1) in the case of gac2. this study demonstrates that the production of ac from agricultural residues, at lab-scale, is feasible and leads to genuine activated carbons with different intrinsic properties.
styrene absorption in water/silicone oil mixtures. the absorption of styrene in water/silicone oil systems at a constant flow rate for emulsion compositions ranging from 0% to 20% was investigated using a dynamic absorption method. it was found that the mass transfer for air/water/silicone oil systems is roughly double than those determined for air/water systems whatever the silicone oil percentage (phi). this result, mainly due to the high value of the partition coefficient ratio (m(r) = h-water/h-oil = 257), was in agreement with the enhancement factor models proposed in the literature. considering the volumetric mass transfer coefficient, it was shown that change in k(l)a versus phi depended on the mass transfer model used for its determination. by using the ''equivalent absorption capacity'' concept developed by dumont et al. [18] (cej 162 (2010) 927-934; doi:10.1016/j.cej.2010.06.045), a dramatic decrease in the k(l)a with increasing silicone oil volume fraction was observed in relation to the decrease in the value of the partition coefficient h-mix. conversely, considering a styrene mass transfer pathway in series (gas -&gt; water -&gt; oil), the k(l)a values for gas/water/silicone oil systems were roughly double the k(l)a for gas/water systems and did not depend on the mixture composition. the styrene mass transfer performances were also analyzed using the modeling framework proposed by hernandez et al. [17] (cej 172 (2011) 961-969; doi:10.1016/j.cej.2011.07.008). this model confirmed the ability of a water/silicone oil mixture to increase the styrene mass transfer by a factor of 2 and verified that the change in h-mix versus phi followed the trend predicted by the theory (although the h-mix values determined from this model were significantly lower than the theoretical values). (c) 2012 elsevier b.v. all rights reserved.
steel slag filters to upgrade phosphorus removal in constructed wetlands: two years of field experiments. electric arc furnace steel slag (eaf-slag) and basic oxygen furnace steel slag (bof-slag) were used as filter substrates in two horizontal subsurface flow filters (6 m(3) each) designed to remove phosphorus (p) from the effluent of a constructed wetland. the influences of slag composition, void hydraulic retention time (hrtv), temperature, and wastewater quality on treatment performances were studied. over a period of almost two years of operation, the filter filled with eaf-slag removed 37% of the inlet total p, whereas the filter filled with bof-slag removed 62% of the inlet total p. p removal occurred predominantly via cao-slag dissolution followed by ca phosphate precipitation. p removal efficiencies improved with increasing temperature and hrtv, most probably because this affected the rates of cao-slag dissolution and ca phosphate precipitation. it was observed that long hrtv (&gt;3 days) can cause high ph in the effluents (&gt;9) as a result of excessive cao-slag dissolution. however, at shorter hrtv (1-2 days), ph values were elevated only during the first five weeks and then stabilized below a ph of 9. the kinetics of p removal were investigated employing a first-order equation, and a model for filter design was proposed.
novel fe loaded activated carbons with tailored properties for as(v) removal: adsorption study correlated with carbon surface chemistry. novel fe loaded activated carbons have been prepared from sugar beet pulp (bp) agricultural residues by direct steam activation followed by iron impregnation with or without previous oxidation. the corresponding activated carbons were: bp-h2o, bp-h2o-fe, bp-h2o-h2o2-fe and bp-h2o-mno2-fe. the textural characterization of these tailored activated carbons was based on nitrogen adsorption/desorption isotherms leading to bet surface area values between 741 and 821 m(2)/g and total porous volumes between 0.58 and 0.79 cm(3)/g. elemental analysis and ash content showed that carbon content reached 78% for bp-h2o with 13.6% of ash and decreased to 50% in iron-based materials. bp-h2o and bp-h2o-fe revealed a basic nature with ph(pzc) values of 9.8 and 9 respectively while bp-h2o-h2o2-fe and bp-h2o-mno2-fe had acid ph(pzc) (5.1 and 3.6). their surface chemistry has been investigated by xps analysis and by the quantification of the surface chemical moieties based on boehm's approach. a clear relationship was found between the surface iron content and the strong acidic groups. arsenic (as(v)) adsorption isotherms were performed and langmuir, freundlich, redlich-peterson models were used to describe the experimental data by non-linear regression. it was found that redlich-peterson isotherm provided the best fit and the langmuir adsorption capacities confirmed that the iron-based activated carbons were highly attractive for as(v) removal with capacities up to 17 mg g(-1). finally, it has been shown that the surface iron content determined by xps analysis is very well correlated with langmuir q(m) values (r(2) = 0.982) and with the strong acidic moieties deduced from the boehm's method. (c) 2012 elsevier b.v. all rights reserved.
steady- and transient-state h2s biofiltration using expanded schist as packing material. the performances of three laboratory-scale biofilters (bf1, bf2, bf3) packed with expanded schist for h2s removal were studied at different empty bed residence times (ebrt = 35, 24 and 16 s) in terms of elimination capacity (ec) and removal efficiency (re). bf1 and bf2 were filled with expanded schist while bf3 was filled with both expanded schist and a nutritional material (up20; 12% vol). bf1 and bf3 were inoculated with activated sludge, whereas bf2 was not inoculated. a maximum ec of 42 g m(-3) h(-1) was recorded for bf3 at ebrt = 35 s demonstrating the ability of schist to treat high h2s loading rates, and the ability of up20 to improve h2s removal. michaelis-menten and haldane models were fitted to the experimental elimination capacities while biofilter responses to transient-state conditions in terms of removal efficiency during shock load events were also evaluated for bf1 and bf3.
performances of two macrophytes species in floating treatment wetlands for cadmium, nickel, and zinc removal from urban stormwater runoff. conventional stormwater detention ponds frequently show limitations for dissolved heavy metal removal. floating treatment wetlands (ftws), a variant of constructed wetlands, are considered as a promising technology to improve the quality of urban stormwater runoff. our study aimed at evaluating the treatment performances of ftws for cadmium, nickel, and zinc removal through a pot experiment. two macrophytes species, juncus effusus and carex riparia, were grown during 4 months under three metal concentrations (in micrograms per liter): high (cd, 200; ni, 500; zn, 2,000), low (cd, 10; ni, 10; zn, 40), and control (cd-ni-zn, 0). results showed that both macrophytes species significantly removed dissolved heavy metals. cadmium and nickel accumulations were greater in roots than in shoots for both species. under low metal concentration, maximum accumulation of 0.4 mu g g(-1) dry weight (dw) for cd was observed in the roots of j. effusus. under high metal concentration, accumulations of up to 5 mu g cdg(-1) dw and 62 mu g nig(-1) dw were observed in the roots of j. effusus and up to 73 mu g zng(-1) dw in the roots of c. riparia. although j. effusus and c. riparia are not recognized as metal hyperaccumulators, our study demonstrated that they can achieve high metal uptake when both roots and shoots are harvested.
biological characterization and treatment performances of a compact vertical flow constructed wetland with the use of expanded schist. the use of compact vertical flow constructed wetlands is becoming increasingly popular in france to treat raw domestic wastewater. this system enables the use of a single deep stage rather than two stages and therefore dramatically reduces the capital costs. a compact vertical flow constructed wetland known as ecophyltre (r) was investigated. this system was partially filled with a light expanded schist (mayennite (r)) designed to reduce the surface area to 1.2 m(2) pe(-1). the aim of this work was to highlight relationships between mayennite (r) characteristics with biological activity and treatment performances. after 12 months of operation, the total accumulated dry matter quantity was around 5 kg m(-2) (20% on the surface). between 70% and 80% of the microorganisms respiration was principally measured in the top layer of ecophyltre (r) during the first year of operation. however, biological activity was shared between the two mayennite (r) layers over time. results showed that mayennite (r) retention enabled to kept around 15% water in mass enhancing microorganisms growing. removal performances of this system met the french standards (35 mg tss l(-1); 25 mg bod1(-1); 125 mg cod1(-1)). crown copyright (c) 2012 published by elsevier b.v. all rights reserved.
volumetric mass transfer coefficients characterising voc absorption in water/silicone oil mixtures. the physical absorption of three volatile organic compounds (dimethyldisulphide (dmds), dimethylsulphide (dms) and toluene) in "water/silicone oil" systems at a constant flow rate for mixtures of different compositions (f = 0, 5, 10, 15, 20 and 100%) was investigated using a dynamic absorption method. the results indicate that silicone oil addition leads to a dramatic decrease in kla which can be related to the change in the partition coefficient (hmix). they confirm the results obtained for styrene absorption using another measurement technique [15]. the interpretation of the results using dimensionless ratios kla(f)/kla(f=0%) and kla(f)/kla(f=100%) versus f also confirms the importance of the partition coefficient ratio mr = hwater/hoil in the kla change. moreover, the results obtained for toluene absorption in "air/water/silicone oil" systems (f = 10, 15 and 20%) suggest that the mass transfer pathway is in the order gas→water→oil for these operating conditions.
uv-visible photocatalytic degradation of 17b-estradiol and estrogenic activity assessment. null
biofiltration using peat and a nutritional synthetic packing material: influence of the packing configuration on h2s removal. this study aims to evaluate the feasibility of using a nutritional synthetic material (up20) combined with fibrous peat as a packing material in treating h2s (up to 280ppmv). three identical laboratory-scale biofilters with different packing material configurations (peat only; peat+up20 in a mixture; peat+up20 in two layers) were used to determine the biofilter performances. the superficial velocity of the polluted gas on each biofilter was 65m/h (gas flow rate 0.5nm(3)/h) corresponding to an empty bed residence time=57 s. variations in elimination capacity, removal efficiency, temperature and ph were tracked during 111 d. a removal efficiency of 100% was obtained for loading rates up to 6g/m(3)/h for the biofilter filled with 100% peat, and up to 10g/m(3)/h for both biofilters using peat complemented with up20. for higher loading rates (up to 25.5g/m(3)/h), the configuration of peat-up20 in a mixture provided the best removal efficiencies (around 80% compared to 65% for the configuration of peat-up20 in two layers and 60% for peat only). microbial characterization highlighted that peat is able to provide sulfide-oxidizing bacteria. through kinetic analysis (ottengraf and michaelis-menten models were applied), it appeared that the configuration peat-up20 in two layers (80/20 v/v) did not show significant improvement compared with peat alone. although the configuration of peat-up20 in a mixture (80/20 v/v) offered a real advantage in improving h2s treatment, it was shown that this benefit was related to the bed configuration rather than the nutritional properties of up20.
photocatalytic degradation of endocrine disruptor compounds under simulated solar light. nanostructured titanium materials with high uv-visible activity were synthesized in the collaborative project clean water fp7. in this study, the efficiency of some of these catalysts to degrade endocrine disruptor compounds, using bisphenol a as the model compound, was evaluated. titanium dioxide p25 (aeroxide (r) tio2, evonik degussa) was used as the reference. the photocatalytic degradation was carried out under the uv part of a simulated solar light (280-400 nm) and under the full spectrum of a simulated solar light (200 nm-30 gm). catalytic efficiency was assessed using several indicators such as the conversion yield, the mineralization yield, by-product formation and the endocrine disruption effect of by-products. the new synthesized catalysts exhibited a significant degradation of bisphenol a, with the so-called ect-1023t being the most efficient. the intermediates formed during photocatalytic degradation experiments with ect-1023t as catalyst were monitored and identified. the estrogenic effect of the intermediates was also evaluated in vivo using a chgh-gfp transgenic medaka line. the results obtained show that the formation of intermediates is related to the nature of the catalyst and depends on the experimental conditions. moreover, under simulated uv, in contrast with the results obtained using p25, the by-products formed with ect-1023t as catalyst do not present an estrogenic effect. (c) 2013 elsevier ltd. all rights reserved.
influence of natural mobile organic matter on europium retention on bure clay rock. bure clay rock (cr) was chosen as host rock for the french high and intermediate level long lived radioactive waste repository. this choice is mostly explained by the retention ability of the callovo-oxfordian rock (cox). bure clay rock contains natural organic matter (om) that could have an influence on radionuclide retention. the aim of this work is to assess the influence of natural mobile om on the retention of eu on clay rock. eu was chosen as a chemical model for trivalent actinides contained in vitrified waste. three organic molecules were studied: suberic, sorbic and tiglic acids, small organic acids identified in cox pore water. all the experiments were carried out in an environment recreating cox water (ph=7.5 ; i=0.1 mol/l ; pco2 =10^(-2) bar).clay rock sample characterization showed that the sample used in this work was similar to those previously extracted from the area of interest and that it was necessary to maintain ph at 7.5 to avoid altering the clay rock. the eu-om system study indicated that organic acids had no influence on eu speciation in cox water. the eu-cr system experimental study confirmed that retention implied sorption on cr (ceu&lt;6.10^(-6)mol/l) and precipitation in cox water (ceu&gt;6.10-6mol/l). distribution coefficient rd (quantifying sorption) was estimated at 170 ± 30 l/g. this high value is consistent with literature values obtained on clay rocks. the ternary eu-om-cr system study showed a slight increase of sorption in the presence of organic matter. this synergistic effect is very satisfactory in terms of storage security: the presence of small organic acids in clay rock does not question retention properties with respect to europium and trivalent actinides.
anomalous centrality evolution of two-particle angular correlations from au-au collisions at $\sqrt{s_{\rm nn}}$ = 62 and 200 gev. we present two-dimensional (2d) two-particle angular correlations on relative pseudorapidity $\eta$ and azimuth $\phi$ for charged particles from au-au collisions at $\sqrt{s_{\rm nn}} = 62$ and 200 gev with transverse momentum $p_t \geq 0.15$ gev/$c$, $|\eta| \leq 1$ and $2\pi$ azimuth. observed correlations include a {same-side} (relative azimuth $&lt; \pi/2$) 2d peak, a closely-related away-side azimuth dipole, and an azimuth quadrupole conventionally associated with elliptic flow. the same-side 2d peak and away-side dipole are explained by semihard parton scattering and fragmentation (minijets) in proton-proton and peripheral nucleus-nucleus collisions. those structures follow n-n binary-collision scaling in au-au collisions until mid-centrality where a transition to a qualitatively different centrality trend occurs within a small centrality interval. above the transition point the number of same-side and away-side correlated pairs increases rapidly {relative to} binary-collision scaling, the $\eta$ width of the same-side 2d peak also increases rapidly ($\eta$ elongation) and the $\phi$ width actually decreases significantly. those centrality trends are more remarkable when contrasted with expectations of jet quenching in a dense medium. observed centrality trends are compared to {\sc hijing} predictions and to the expected trends for semihard parton scattering and fragmentation in a thermalized opaque medium. we are unable to reconcile a semihard parton scattering and fragmentation origin for the observed correlation structure and centrality trends with heavy ion collision scenarios which invoke rapid parton thermalization. on the other hand, if the collision system is effectively opaque to few-gev partons the observations reported here would be inconsistent with a minijet picture.
first test of lorentz violation with a reactor-based antineutrino experiment. we present a search for lorentz violation with 8249 candidate electron antineutrino events taken by the double chooz experiment in 227.9 live days of running. this analysis, featuring a search for a sidereal time dependence of the events, is the first test of lorentz invariance using a reactor-based antineutrino source. no sidereal variation is present in the data and the disappearance results are consistent with sidereal time independent oscillations. under the standard-model extension (sme), we set the first limits on fourteen lorentz violating coefficients associated with transitions between electron and tau flavor, and set two competitive limits associated with transitions between electron and muon flavor.
supercharges in the hyper-kähler with torsion supersymmetric sigma models. we construct explicitly classical and quantum supercharges satisfying the standard n=4supersymmetryalgebra in the supersymmetric sigma models describing the motion over hyper-kähler with torsion manifolds. one member of the family of superalgebras thus obtained is equivalent to the superalgebra derived and formulated earlier in purely mathematical framework.
the distributed slow control system of the xenon100 experiment. the xenon100 experiment, in operation at the laboratori nazionali del gran sasso (lngs) in italy, was designed to search for evidence of dark matter interactions inside a volume of liquid xenon using a dual-phase time projection chamber. this paper describes the slow control system (scs) of the experiment with emphasis on the distributed architecture as well as on its modular and expandable nature. the system software was designed according to the rules of object-oriented programming and coded in java, thus promoting code reusability and maximum flexibility during commissioning of the experiment. the scs has been continuously monitoring the xenon100 detector since mid 2008, remotely recording hundreds of parameters on a few dozen instruments in real time, and setting emergency alarms for the most important variables.
on ocl-based imperative languages. the object constraint language (ocl) is a well-accepted ingredient in model-driven engineering and accompanying modeling languages such as uml (unified modeling language) and emf (eclipse modeling framework) that support object-oriented software development. among various possibilities, ocl offers the formulation of class invariants and operation contracts in form of pre- and postconditions, and side effect-free query operations. much research has been done on ocl and various mature implementations are available for it. ocl is also used as the foundation for several modeling-specific programming and transformation languages. however, an intrusive way of embedding ocl into these language hampers us when we want to benefit from the existing achievements for ocl. in response to this shortcoming, we propose the language soil (simple ocl-like imperative language), which we implemented in the uml and ocl modeling tool use to amend its declarative model validation features. the expression sub-language of soil is identical to ocl. soil adds imperative constructs for programming in the domain of models. thus by employing ocl and soil, it is possible to describe any operation in a declarative way and in an operational way on the modeling level without going into the details of a conventional programming language. in contrast to other similar approaches, the embedding of ocl into soil is done in a careful, non-intrusive way so that purity of ocl is preserved.
investigation of eu(iii) immobilization on γ-al2o3 surfaces by combining batch technique and exafs analyses: role of contact time and humic acid. aluminum (hydr)oxides play an important role in the regulation of the composition of soil/water, sediment/water and other natural water systems. in this study, the interactions among eu(iii), humic acid (ha) and γ-al2o3 were investigated using a combination of batch and extended x-ray absorption fine structure (exafs) techniques. experiments were performed with varying contact times (2, 15, 60 and 180 d) at a ph of 6.5 for both the binary γ-al2o3/eu(iii) and the ternary γ-al2o3/ha/eu(iii) systems. in addition, two representative ph values (ph 6.5 for a near-neutral condition and ph 8.5 for an alkalescence condition) were selected to determine the sequestration mechanisms of eu(iii) in the ternary γ-al2o3/ha/eu(iii) systems. to verify the specific binding modes and corresponding chemical species, a coordination geometry calculation and a quantitative comparison between the ha binding site concentration and the initial eu(iii) concentration were conducted along with exafs data analysis. the microstructure and thermodynamic stability of the formed eu(iii) species were dependent on various environmental parameters. for the binary γ-al2o3/eu(iii) systems, quantitative analysis results of exafs spectra suggested the presence of two eu(iii) species within a contact time of 15 d. using a coordination geometry calculation, the reu-al values at ∼3.28 å and ∼3.99 å corresponded to the formation of edge-shared and corner-shared surface complexes, respectively. for samples reacted longer than 15 d, the appearance of an additional eu-eu shell at ∼3.50 å was indicative of a structural rearrangement process, leading to the formation of thermodynamically stable surface polynuclear complexes. for the ternary γ-al2o3/ha/eu(iii) systems, the exafs-derived structural parameters indicated the formation of 1:1 type b ternary complexes and binary corner-shared complexes at ph 6.5 after 2 d. in contrast, the eu(iii) sequestration mechanisms at ph 8.5 were mainly attributed to the formation of 1:2 type a ternary complexes and binary edge-shared complexes. considering the high proton dissociation constant of strong ha phenolic sites (8.8) and the high metal loading in the present study, the weak ha carboxylic sites are predominantly involved in eu(iii) complexation at ph 6.5 and 8.5. the time-dependent variation tendency of the eu(iii) chemical species formed in the ternary systems may arise from eu(iii)-induced ha agglomeration, binding of eu(iii) ions on stronger ha binding sites and migration of eu(iii) ions to less sterically accessible sites in the ha macromolecule structures. the adsorbed ha could accelerate eu(iii) immobilization at the γ-al2o3/water interfaces and could enhance the thermodynamic stability of the formed chemical species. the findings presented in this study could provide important microcosmic information for the prediction of the long-term behaviors of eu(iii) and the relevant ln/an(iii) in a geological environment rich in aluminum hydr(oxides).
what heavy ion can teach us about strange particles and what strange particles can teach us about heavy ions?. we show that heavy ion collisions can reveal properties of k+ in matter, here demonstrated for the kn optical potential, and that at the same time the k+ yield is sensitive to nuclear matter properties, here demonstrated for the hadronic equation of state.
in-medium effects on strangeness production. we discuss the strangeness production close to threshold in heavy-ion collisions based on two independent microscopic transport approaches - hsd and iqmd - employing different in-medium scenarios for the modification of particle properties (strange mesons and hyperons) in the dense and hot medium following either from the chiral models or from a coupled-channel g-matrix approach using a meson-exchange model for strange mesons. the comparison of available kaon, antikaon and λ data with the hsd and iqmd models shows a good agreement for the large majority of observables when incorporating the in-medium effects. the investigation of the reactions with help of transport models reveals the complicated multiple interactions of the strange particles with hadronic matter which shows that strangeness production in heavy-ion collisions is very different from that in elementary interactions. we discuss how a variety of strange particle observables can be used to study the different facets of this interaction (production, rescattering and potential interaction) which finally merge into a comprehensive understanding of these reactions.
global statistical predictor model for characteristic adsorption energy of organic vapors-solid interaction: use in dynamic process simulation. adsorption of volatile organic compounds (vocs) is one of the best remediation techniques for controlling industrial air pollution. in this paper, a quantitative predictor model for the characteristic adsorption energy (e) of the dubinin-radushkevich (dr) isotherm model has been established with r(2) value of 0.94. a predictor model for characteristic adsorption energy (e) has been established by using multiple linear regression (mlr) analysis in a statistical package minitab. the experimental value of characteristic adsorption energy was computed by modeling the isotherm equilibrium data (which contain 120 isotherms involving five vocs and eight activated carbons at 293, 313, 333, and 353 k) with the gauss-newton method in a statistical package r-stat. the mlr model has been validated with the experimental equilibrium isotherm data points, and it will be implemented in the dynamic adsorption simulation model prosim. by implementing this model, it predicts an enormous range of 1200 isotherm equilibrium coefficients of dr model at different temperatures such as 293, 313, 333, and 353k (each isotherm has 10 equilibrium points by changing the concentration) just by a simple mlr characteristic energy model without any experiments.
recovery comparisons--hot nitrogen vs steam regeneration of toxic dichloromethane from activated carbon beds in oil sands process. the regeneration experiments of dichloromethane from activated carbon bed had been carried out by both hot nitrogen and steam to evaluate the regeneration performance and the operating cost of the regeneration step. factorial experimental design (fed) tool had been implemented to optimize the temperature of nitrogen and the superficial velocity of the nitrogen to achieve maximum regeneration at an optimized operating cost. all the experimental results of adsorption step, hot nitrogen and steam regeneration step had been validated by the simulation model prosim. the average error percentage between the simulation and experiment based on the mass of adsorption of dichloromethane was 2.6%. the average error percentages between the simulations and experiments based on the mass of dichloromethane regenerated by nitrogen regeneration and steam regeneration were 3 and 12%, respectively. from the experiments, it had been shown that both the hot nitrogen and steam regeneration had regenerated 84% of dichloromethane. but the choice of hot nitrogen or steam regeneration depends on the regeneration time, operating costs, and purity of dichloromethane regenerated. a thorough investigation had been made about the advantages and limitations of both the hot nitrogen and steam regeneration of dichloromethane.
different families of volatile organic compounds pollution control by microporous carbons in temperature swing adsorption processes. in this research work, the three different vocs such as acetone, dichloromethane and ethyl formate (with corresponding families like ketone, halogenated-organic, ester) are recovered by using temperature swing adsorption (tsa) process. the vapors of these selected vocs are adsorbed on a microporous activated carbon. after adsorption step, they are regenerated under the same operating conditions by hot nitrogen regeneration. in each case of regeneration, factorial experimental design (fed) tool had been used to optimize the temperature, and the superficial velocity of the nitrogen for achieving maximum regeneration efficiency (r(e)) at an optimized operating cost (op(€)). all the experimental results of adsorption step and hot nitrogen regeneration step had been validated by the simulation model prosim. the average error percentage between the simulation and experiment based on the mass of adsorption of dichloromethane was 3.1%. the average error percentages between the simulations and experiments based on the mass of dichloromethane regenerated by nitrogen regeneration were 4.5%.
dynamics modeling of compliant locomotion : application to flapping flight bio-inspired by insects. the objective of the present work is to model the locomotion dynamics of "soft robots", i.e. compliant mobile multi-body systems. these compliances can be either localized and treated as passive joints of the system, or introduced by distributed flexibilities along the bodies. the dynamics of these systems is modeled in a lagrangian approach based on the mathematical tools developed by the american school of geometric mechanics. from the algorithmic viewpoint, the computation of these dynamic models is based on a recursive and efficient newton-euler algorithm which is extended here to the case of robots equipped with compliant organs. the proposed algorithm is compatible with control, fast simulation and real time robotic applications. it is able to solve the direct external dynamics as well as the inverse internal torque dynamics. the modeling tools and algorithms developed in this thesis are applied to one of the most advanced cases of compliante locomotion i.e. the flapping flight mavs bio-inspired by insects. the nonlinear equations governing the passive deformations of the wing are derived using two different methods. in the first method, we separate the wing movement into a rigid component (which corresponds to the movements of a "floating frame"), and a deformation component. the latter one is parameterized in the floating frame using the assumed modes approach where the wing is considered as an euler-bernoulli beam undergoing flexion and torsion deformations. regarding the second method, the wing movements are no longer separated but directly parameterize dusing rigid finite absolute transformations of a cosserat beam. this method is called galilean or "geometrically exact" because it does not require any approximation apart from the unavoidable spatial and temporal discretizations imposed by numerical resolution of the flight dynamics. in both cases, the aerodynamic forces are taken into account through a simplified analytical model. the resulting models and algorithms are used in the context of the collaborative project (anr) eva to develop a flight simulator, and to design wing prototype.
a generically well-posed h2 control problem for a one shot feedforward and feedback synthesis. in control design, a particular method consists in two main steps. firstly, a global model is defined by embodying both, the system to be controlled and an exosystem (regrouping reference and disturbances models). secondly, a standard h2 (or h1) optimization problem is defined. formerly named the disturbance modeling method in the lqg context, the control goal is to minimize a quadratic index by weighting the output reference error in one side, and the control input in the other. in the case where the exosystem is unstable because e.g. persistent disturbances, this index is not bounded. it is then possible to change the criterion, weighting the ad hoc control input and output deviation to make the problem well posed (comprehensively stabilizable). the current paper deals with the same framework for stable exosystems and unstable ones, and proposes a systematic way to get a well-posed control optimization problem leading to a 2 degree of freedom controller.
describing and generating solutions for the edf unit commitment problem with the modelseeker. null
a parametric propagator for discretely convex pairs of sum constraints. null
a synchronized sweep algorithm for the $k$-dimensional cumulative constraint. null
the energetic reasoning checker revisited. energetic reasoning (er) is a powerful filtering algorithm for the cumulative constraint. unfortunately, er is generally too costly to be used in practice. one reason of its bad behavior is that many intervals are considered as relevant by the checker of er, although most of them should be ignored. in this paper, we provide a sharp characterization that allows to reduce the number of intervals by a factor seven. our experiments show that associating this checker with a time-table filtering algorithm leads to promising results.
j/ψ production at low pt in au + au and cu + cu collisions at snn−−−−√=200 gev with the star detector. the $j/\psi$ $p_t$ spectrum and nuclear modification factor ($r_{\textit{aa}}$) are reported for $p_t &lt; 5$ gev/c and $|y|&lt;1$ from 0-60% central au+au and cu+cu collisions at $\sqrt{s_{_{nn}}} =200$ gev at star. a significant suppression of $p_t$-integrated $j/\psi$ production is observed in central au+au events, with less suppression observed in cu+cu. the $p_t$ dependence of the $r_{\textit{aa}}$ is observed to increase at a higher $p_t$ region. the data are compared with the previously published rhic results. comparing with model calculations, it is found that the invariant yields at low $p_t$ are significantly above hydrodynamic flow predictions but are consistent with models that include color screening and regeneration.
views and program transformations for modular maintenances. maintenance consumes a large part of the cost of software development which makes the optimization of that cost among the important issues in the world of software engineering. in this thesis we aim to optimize this cost by making these maintenances modular. to achieve this goal, we define transformations of program architectures that allow to transform a program to maintain into an architecture that facilitates the maintenance tasks required. we focus on transformation between architectures having dual modularity properties such as composite and visitor designpatterns. in this context, we define an automatic and reversible transformation based on refactoring between a program structured according to the composite structure and its corresponding visitor structure. this transformation is validated by generating a precondition which guarantees statically its success. it is also adapted to take into account the transformation of four variations of composite pattern and it is then applied to jhotdraw program in which these four variations occur. we define also a reversible transformation in the singleton pattern to benefit from optimization by introducing this pattern and flexibility by its suppression according to the requirements of the software user.
aspectual session types. multiparty session types allow the definition of distributed processes with strong communication safety properties. a global type is a choreographic specification of the interactions between peers, which is then projected locally in each peer. well-typed processes behave accordingly to the global protocol specification. multiparty session types are however monolithic entities that are not amenable to modular extensions. also, session types impose conservative requirements to prevent any race condition, which prohibit the uni- form application of extensions at different points in a protocol. in this paper, we describe a means to support modular extensions with aspectual session types, a static pointcut/advice mechanism at the session type level. to support the modular definition of crosscut- ting concerns, we augment the expressivity of session types to al- low harmless race conditions. we formally prove that well-formed aspectual session types entail communication safety. as a result, aspectual session types make multiparty session types more flexible, modular, and extensible.
execution levels for aspect-oriented programming: design, semantics, implementations and applications. in aspect-oriented programming (aop) languages, advice evaluation is usually considered as part of the base program evaluation. this is also the case for certain pointcuts, such as if pointcuts in aspectj, or simply all pointcuts in higher-order aspect languages like aspectscheme. while viewing aspects as part of base level computation clearly distinguishes aop from reflection, it also comes at a price: because aspects observe base level computation, evaluating pointcuts and advice at the base level can trigger infinite regression. to avoid these pitfalls, aspect languages propose ad-hoc mechanisms, which increase the complexity for programmers while being insufficient in many cases. after shed- ding light on the many facets of the issue, this paper proposes to clarify the situation by introducing levels of execution in the programming language, thereby allowing aspects to observe and run at specific, possibly different, levels. we adopt a defensive default that avoids infinite regression, and gives advanced programmers the means to override this default using level-shifting operators. we then study execution levels both in practice and in theory. first, we study the relevance of the issues addressed by execution levels in existing aspect-oriented programs. we then formalize the semantics of execution levels and prove that the default semantics is indeed free of a certain form of infinite regression, which we call aspect loops. finally, we report on existing implementations of execution levels for aspect-oriented extensions of scheme, javascript and java, discussing their implementation techniques and current applications.
effective aspects: a typed monadic embedding of pointcuts and advice. aspect-oriented programming(aop) aims to enhance modularity and reusability in software systems by offering an abstraction mechanism to deal with crosscutting concerns. however, in most general-purpose aspect languages aspects have almost unrestricted power, eventually conflicting with these goals. in this work we present effective aspects: a novel approach to embed the point- cut/advice model of aop in a statically-typed functional programming language like haskell. our work extends effectiveadvice, by oliveira, schrijvers and cook; which lacks quantification, and explores how to exploit the monadic setting in the full pointcut/advice model. type soundness is guaranteed by exploiting the underlying type system, in particular phantom types and a new anti-unification type class. aspects are first-class, can be deployed dynamically, and the pointcut language is extensible, therefore combining the flexibility of dynamically-typed aspect languages with the guarantees of a static type system. monads enables us to directly reason about computational effects both in aspects and base programs using traditional monadic techniques. using this we extend aldrich's notion of open modules with effects, and also with protected pointcut interfaces to external advising. these restrictions are enforced statically using the type system. also, we adapt the techniques of effectiveadvice to reason about and enforce control flow properties. moreover, we show how to control effect interference us- ing the parametricity-based approach of effectiveadvice. however this approach falls short when dealing with interference between multiple aspects. we propose a different approach using monad views, a recently developed technique for han- dling the monad stack. finally, we exploit the properties of our monadic weaver to enable the modular construction of new semantics for aspect scoping and weaving. these semantics also benefit fully from the monadic reasoning mechanisms present in the language. this work brings type-based reasoning about effects for the first time in the pointcut/advice model, in a framework that is both expressive and extensible; thus allowing development of robust aspect-oriented systems as well as being a useful research tool for experimenting with new aspect semantics.
rheological characterization of a thermally unstable bioplastic in injection molding conditions. poly(hydroxy-alcanoates) bioplastics have an unusually small temperature processing window due to their thermal instability, which affects both molecular weight distribution and rheological behavior. the use of a rheometrical device mounted on an injection molding machine has been shown to allow the study of dynamic viscosity variations for very short residence times in the melt (down to a few tens of seconds). moreover, dynamic viscosity can be obtained over a wide range of shear rates (up to 50000 s−1) with both bagley and rabinowitsch corrections. the rheological data obtained for a poly(3-hydroxybutyrate-co-3-hydroxyvalerate), phbv copolymer shows that the reduction in viscosity associated with degradation is very fast and is detectable even at temperatures close to the melting point. size exclusion chromatography analysis of selected samples shows that, except for residence times below 15s, molecular mass distribution is substantially affected, shifting toward lower masses, and with an increase in oligomer compound content. these oligomers may act as plasticizers of the bioplastic. the results also suggest that important thermomechanical degradation takes place during the plasticization/feeding step of injection molding.
visualization of the exothermal voc adsorption in a fixed-bed activated carbon adsorber. activated carbon fixed beds are classically used to remove volatile organic compounds (vocs) present in gaseous emissions. in such use, an increase of local temperature due to exothermal adsorption has been reported; some accidental fires in the carbon bed due to the removal of high concentrations of ketones have been published. in this work, removal of vocs was performed in a laboratory-scale pilot unit. in order to visualize the increase in local temperature, the adsorption front was tracked with a flame ionization detector and the thermal wave was simultaneously visualized with an infrared camera. in extreme conditions, fire in the adsorber and the combustion of activated carbon was achieved during ketone adsorption. data have been extracted from these experiments, including local temperature, front velocity and carbon bed combustion conditions.
three generalizations of the focus constraint. the focus constraint expresses the notion that solutions are concentrated. in practice, this constraint suffers from the rigidity of its semantics. to tackle this issue, we propose three generalizations of the focus constraint. we provide for each one a complete filtering algorithm as well as discussing decompositions.
photon and eta production in p+pb and p+c collisions at sqrt{snn} = 17.4 gev. measurements of direct photon production in p+pb and p+c collisions at $\sqrt{s_\mathrm{nn}} = 17.4\mathrm{gev}$ are presented. upper limits on the direct photon yield as a function of $p_\mathrm{t}$ are derived and compared to the results for pb+pb collisions at $\sqrt{s_\mathrm{nn}} = 17.3$ gev. the production of the $\eta$ meson, which is an important input to the direct photon signal extraction, has been determined in the $\eta \rightarrow 2\gamma$ channel for p+c collisions at $\sqrt{s_\mathrm{nn}} = 17.4\mathrm{gev}$.
branch-and-price approach for the multi-skill project scheduling problem. this work introduces a procedure to solve the multi-skill project scheduling problem (mspsp) (néron and baptista, international symposium on combinatorial, optimization (co'2002), 2002). the mspsp mixes both the classical resource constrained project scheduling problem and the multi-purpose machine model. the aim is to find a schedule that minimizes the completion time (makespan) of a project, composed of a set of activities. in addition, precedence relations and resources constraints are considered. in this problem, resources are staff members that master several skills. thus, a given number of workers must be assigned to perform each skill required by an activity. practical applications include the construction of buildings, as well as production and software development planning. we present a column generation approach embedded within a branch-and-price (b&amp;p) procedure that considers a given activity and time-based decomposition approach. obtained results show that the proposed b&amp;p procedure is able to reach optimal solutions for several small and medium sized instances in an acceptable computational time. furthermore, some previously open instances were optimally solved.
two-phase cryogenic avalanche detectors with thgem and hybrid thgem/gem multipliers operated in ar and ar+n2. two-phase cryogenic avalanche detectors (crads) with gem and thgem multipliers have become an emerging potential technique for charge recording in rare-event experiments. in this work we present the performance of two-phase crads operated in ar and ar+n2. detectors with sensitive area of 10 × 10 cm2, reaching a litre-scale active volume, yielded gains of the order of 1000 with a double-thgem multiplier. higher gains, of about 5000, have been attained in two-phase ar crads with a hybrid triple-stage multiplier, comprising of a double-thgem followed by a gem. the performance of two-phase crads in ar doped with n2 (0.1-0.6%) yielded faster signals and similar gains compared to the operation in two-phase ar. the applicability to rare-event experiments is discussed.
identifying clouds over the pierre auger observatory using infrared satellite data. we describe a new method of identifying night-time clouds over the pierre auger observatory using infrared data from the imager instruments on the goes-12 and goes-13 satellites. we compare cloud identifications resulting from our method to those obtained by the central laser facility of the auger observatory. using our new method we can now develop cloud probability maps for the 3000 km2 of the pierre auger observatory twice per hour with a spatial resolution of ∼2.4 km by ∼5.5 km. our method could also be applied to monitor cloud cover for other ground-based observatories and for space-based observatories.
reactivity of htco4 with methanol in sulfuric acid: tc-sulfate complexes revealed by xafs spectroscopy and first principles calculations. the reaction between htco4 and meoh in 13 m h2so4 was investigated by 99tc nmr, uv-visible and x-ray absorption fine structure (xafs) spectroscopy. experimental results and first principles calculations show the formation of tc(+5) sulfate complexes. the results expand the fundamental understanding of tc in high acid solutions.
structure, energetics, and dynamics of smectite clay interlayer hydration: molecular dynamics and metadynamics investigation of na-hectorite. this paper presents a classical molecular dynamics (md) and metadynamics investigation of the relationships between the structure, energetics, and dynamics of na-hydroxyhectorite and serves to provide additional, molecular-scale insight into the interlayer hydration of this mineral. the computational results support a model for interlayer h2o structure and dynamics based on 2h nmr spectroscopy and indicate that h2o molecules undergo simultaneous fast librational motions about the h2o c2 symmetry axis and site hopping with c3 symmetry with respect to the surface normal. hydration energy minima occur at one-, one-and-one-half-, and two-water-layer hydrates, which for the composition modeled correspond to 3, 5.5, and 10 h2o/na+, respectively. na+ ions are coordinated by basal o atoms (omin) at the lowest hydration levels and by h2o molecules (oh2o) in the two-layer hydrate, and h2o molecules have an average of three h-bonds at the greatest hydration levels. the metadynamics calculations yield activation energies for site hopping of h2o molecules of 6.0 kj/mol for the one-layer structure and 3.3 kj/mol for hopping between layers in the two-layer structure. computed diffusion coefficients for water and na+ are substantially less than in bulk liquid water, as expected in a nanoconfined environment, and are in good agreement with previous results.
on the nature of heptavalent technetium in concentrated nitric and perchloric acid. the speciation of tc(+7) was performed in hclo4 and hno3 by 99-tc nmr, uv-vis and xafs spectroscopy. the speciation of tc(+7) depends on the concentration and strength of the acid. pertechnetic acid, htco4, forms above 8 m hclo4 while in concentrated hno3, [tco4]− is still the predominant species. exafs spectroscopy shows that the structure of htco4 in hclo4 is similar to the one in h2so4. the reactivity of tc(+7) was analyzed in the frame of the partial charge model. the partial charge calculated on the tc atoms (δtc) indicates that htco4 (δtc = +057) is more electrophilic than [tco4]− (δtc = +0.52). the difference in the oxidizing properties between [tco4]− and htco4 is given from the reaction of these species with 12 m hcl(aq). in 13 m sulfuric acid htco4 is reduced to tc(+5) while [tco4]− is not reduced in 6 m h2so4.
aerial pollutants in swine buildings: a review of their characterization and methods to reduce them. the swine industry follows a large increase of meat production since the 1950s causing the development of bigger swine buildings which involves a raise of pollutants emissions. due to recent anthropological pressures concerning the animal welfare, the limitation of neighborhood disturbances and atmospheric pollutions limitations, the livestock farming has to adapt their management methods to reduce or treat the aerial pollutants emissions. through the diversity of livestock barns configurations, their climatic location, their size, and their management, we thus propose hereafter a critical review of the characterizations of these aerial pollutants. this is realized by distinguishing both solids and gaseous emissions and by referencing the measurements methods mainly used to analyze and quantify airborne particles, odorants, and gaseous compounds in the atmosphere of swine buildings. the origins of these pollutants are focused and the sturdiest techniques for concentration measurements are highlighted. finally, we discuss pollutants abatement techniques criticizing their implementation in swine buildings and emphasizing the use of biological ways such as biofiltration for gases and odors treatment.
engaging end-users in the collaborative development of domain-speci c modelling languages. domain-speci c modelling languages (dsmls) are high-level languages specially designed to perform tasks in a particular domain. when developing dsmls, the participation of end-users is normally limited to providing domain knowledge and testing the resulting language prototypes. language developers, which are perhaps not domain experts, are therefore in control of the language development and evolution. this may cause misinterpretations which hamper the development process and the quality of the dsml. thus, it would be bene cial to promote a more active participation of end-users in the development process of dsmls. while current dsml workbenches are mono-user and designed for technical experts, we present a process and tool support for the example-driven, collaborative construction of dsmls in order to engage end-users in the creation of their own languages.
management of stateful firewall misconfiguration. firewall configurations are evolving into dynamic policies that depend on protocol states. as a result, stateful configurations tend to be much more error prone. some errors occur on configurations that only contain stateful rules. others may affect those holding both stateful and stateless rules. such situations lead to configurations in which actions on certain packets are conducted by the firewall, while other related actions are not. we address automatic solutions to handle these problems. permitted states and transitions of connection-oriented protocols (in essence, on any layer) are encoded as automata. flawed rules are identified and potential modifications are provided in order to get consistent configurations. we validate the feasibility of our proposal based on a proof of concept prototype that automatically parses existing firewall configuration files and handles the discovery of flawed rules according to our approach.
towards an access-control metamodel for web content management systems. out-of-the-box web content management systems (wcmss) are the tool of choice for the development of millions of enterprise web sites but also the basis of many web applications that reuse wcms for important tasks like user registration and authentication. this widespread use highlights the importance of their security, as wcmss may manage sensitive information whose disclosure could lead to monetary and reputation losses. however, little attention has been brought to the analysis of how developers use the content protection mechanisms provided by wcmss, in particular, access-control (ac). indeed, once configured, knowing if the ac policy provides the required protection is a complex task as the specificities of each wcms need to be mastered. to tackle this problem, we propose here a metamodel tailored to the representation of wcms ac policies, easing the analysis and manipulation tasks by abstracting from vendor-specific details.
model-driven extraction and analysis of network security policies. firewalls are a key element in network security. they are in charge of filtering the traffic of the network in compliance with a number of access-control rules that enforce a given security policy. in an always-evolving context, where security policies must often be updated to respond to new security requirements, knowing with precision the policy being enforced by a network system is a critical information. otherwise, we risk to hamper the proper evolution of the system and compromise its security. unfortunately, discovering such enforced policy is an error-prone and time consuming task that requires low-level and, often, vendor-specific expertise since firewalls may be configured using different languages and conform to a complex network topology. to tackle this problem, we propose a model-driven reverse engineering approach able to extract the security policy implemented by a set of firewalls in a working network, easing the understanding, analysis and evolution of network security policies.
mde support for enterprise architecture in an industrial context: the teap framework experience. model driven engineering (mde) is often applied to support software engineering processes (i.e., from reverse to forward engineering, including maintenance and/or evolution tasks). however, as promoted by the model driven organization (mdo) initiative, it can also be relevant in more business-oriented and strategic decision-making activities such as enterprise architecture (ea). ea is the process of translating business vision and strategy into effective change by better describing the enterprise's future state and thus enable its evolution. even if several approaches have already proposed different kinds of support to deal with the company's ea, an integrated mde framework combining ea data federation, ea standard adaptation and multiple viewpoint support is still missing. this paper reports on our ongoing experience of building the teap mde framework (based on the togaf standard and smartea tooling) notably addressing these three challenges in an industrial ea context.
artist methodology and framework: a novel approach for the migration of legacy software on the cloud. nowadays cloud computing is considered as the ideal environment for engineering, hosting and provisioning applications. a continuously increasing set of cloud-based solutions is available to application owners and developers to tailor their applications exploiting the advanced features of this paradigm for elasticity, high availability and performance. even though these offerings provide many benefits to new applications, they often incorporate constrains to the modernization and migration of legacy applications by obliging the use of specific development technologies and explicit architectural design approaches. the modernization and adaptation of legacy applications to cloud environments is a great challenge for all involved stakeholders, not only from the technical perspective, but also in business level with the need to adapt the the business processes and models of the modernized application that will be offered from now on, as a service. in this paper we present a novel model-driven approach for the migration of legacy applications in modern cloud environments which covers all aspects and phases of the migration process, as well as an integrated framework that supports all migration process.
parallel execution of atl transformation rules. industrial environments that make use of model-driven engineering (mde) are starting to see the appearance of very large models, made by millions of elements. such models are produced automatically (e.g., by reverse engineering complex systems) or manually by a large number of users (e.g., from social networks). the success of mde in these application scenarios strongly depends on the scalability of model manipulation tools. while parallelization is one of the traditional ways of making computation systems scalable, developing parallel model transformations in a general-purpose language is a complex and error-prone task. in this paper we show that rule-based languages like atl have strong parallelization properties. transformations can be developed without taking into account concurrency concerns, and a transformation engine can automatically parallelize execution. we describe the implementation of a parallel transformation engine for the current version of the atl language and experimentally evaluate the consequent gain in scalability.
migrating legacy software to the cloud with artist. as cloud computing allows improving the quality of software and aims at reducing costs of operating software, more and more software is delivered as a service. however, moving from a software as a product strategy to delivering software as a service hosted in cloud environments is very ambitious. this is due to the fact that managing software modernization is still a major challenge; especially when paradigm shifts, such as moving to cloud environments, are targeted that imply fundamental changes to how software is modernized, delivered, and sold. thus, in addition to technical aspects, business aspects need also to be considered. artist proposes a comprehensive software modernization approach covering business and technical aspects. in particular, artist employs model-driven engineering (mde) techniques to automate the reverse engineering of legacy software and forward engineering of cloud-based software in a way that modernized software truly benefits from targeted cloud environments. therewith, artist aims at reducing the risks, time, and costs of software modernization and lowers the barriers to exploit cloud computing capabilities and new business models.
a research roadmap towards achieving scalability in model driven engineering. null
extracting business rules from cobol: a model-based tool. this paper presents a business rule extraction tool for cobol systems. starting from a cobol program, we derive a model-based representation of the source code and we provide a set of model transformations to identify and visualize the embedded business rules. in particular, the tool facilitates the definition of an application vocabulary and the identification of relevant business variables. in addition, such variables are used as starting point to slice the code in order to identify business rules, that are finally represented by means of textual and graphical artifacts. the tool has been developed as an eclipse plug-in in collaboration with ibm france.
extracting business rules from cobol: a model-based framework. organizations rely on the logic embedded in their information systems for their daily operations. this logic implements the business rules in place in the organization, which must be continuously adapted in response to market changes. unfortunately, this evolution implies understanding and evolving also the underlying software components enforcing those rules. this is challenging because, first, the code implementing the rules is scattered throughout the whole system and, second, most of the time documentation is poor and out-of-date. this is specially true for older systems that have been maintained and evolved for several years (even decades). in those systems, it is not even clear which business rules are enforced nor whether rules are still consistent with the current organizational policies. in this sense, the goal of this paper is to facilitate the comprehension of legacy systems (in particular cobol-based ones) by providing a model driven reverse engineering framework able to extract and visualize the business logic embedded in them.
automating inference of ocl business rules from user scenarios. user scenarios have been advocated as an effective means to capture requirements by describing the system-to-be at the instance or example level. this instance-level information is then used to infer a possible software specification consistent with the provided valid and invalid scenarios. so far existing approaches have often focused on the generation of static models but have omitted the inference of business rules that could complement the static models and improve the precision of the software specification. in this sense this paper provides a first set of invariant inference patterns that are applied on valid and invalid snapshots in order to generate ocl~(object constraint language) integrity constraints that the system should always satisfy. we strengthen the confidence of inferred results based on the user's feedback of generated examples and counterexamples for the considered constraint. the approach is realized with a prolog-based tool that could support the designer to effectively define ocl integrity constraints in a semi-automatic way.
extracting uml/ocl integrity constraints and derived types from relational databases. relational databases usually enforce relevant organizational business rules. this aspect is ignored by current database reverse engineering approaches which only focus on the extraction of the structural part of the conceptual schema. other database elements like triggers, views, column constraints, etc. are not considered by those methods. as a result, the generated conceptual schema is incomplete since integrity constraints and derivation rules enforced by the database are not represented.
hydrophobic voc absorption in two-phase partitioning bioreactors; influence of silicone oil volume fraction on absorber diameter. a methodology to determine the diameter of an absorber contacting a gas phase and two immiscible liquid phases (water/silicone oil mixture) is presented. the methodology is applied to a countercurrent gas/liquid randomly packed column for the absorption of three vocs (toluene, dimethyl sulfide, or dimethyl disulfide). whatever the silicone oil volume fraction, eckert's generalized pressure drop correlation was used. the results present the change in the column diameter through the change in the dimensionless ratio d(ϕ)/d(ϕ=1) versus the silicone oil volume fraction for the same operating conditions. for toluene and dimethyl disulfide, characterized by hvoc,silicone oil values equal to 2.3 and 3.4 pa m3 mol−1, respectively, it is highlighted that it is unwise to use water/silicone oil mixtures for mass transfer. in these cases, the contact between the polluted air and pure silicone oil requires roughly the same amount of silicone oil as for a (90/10 v/v) water/silicone oil mixture, but enables a 2-fold decrease in the column diameter. for dimethyl sulfide, which is characterized by a larger partition coefficient value (hvoc,silicone oil=17.7 pa m3 mol−1), the mass transfer operation should not be considered because large amounts of silicone oil are required (whatever the silicone oil volume fraction), which is not acceptable from an economic point of view. the feasibility of using a bioscrubber for the treatment of hydrophobic pollutants depends mainly on the partition coefficient hvoc,silicone oil. voc absorption in tppb should therefore be restricted to pollutants characterized by a hvoc,silicone oil value of around 3 to 4 pa m3 mol−1 or less. in this case, absorption can be efficiently carried out in a biphasic air/silicone oil system.
optimization of the volume fraction of the napl, silicon oil, and biodegradation kinetics of toluene and dmds in a tppb. the volume ratio between non-aqueous phase liquid (napl) and water was optimised in order to remove the two hydrophobic volatile organic compounds (vocs), toluene and dimethyldisulfide (dmds). biological oxygen demand after 5 days (bod5) measurements showed that ratios ranging from 20 to 30% of silicone oil in water enabled optimal removal of both selected substrates, certainly due to a high adhesion of microorganisms at the interface and a high o2 consumption to metabolize pollutants. the removal of these compounds was then efficiently carried out in batch cultures in a two-phase partitioning bioreactor (tppb) containing 25% silicone oil. if compared to the results obtained in the absence of napl, its presence led to 62 and 107% improvement of the biodegradation rate (global removal rate) for toluene and dmds respectively. in addition silicone oil was a more efficient napl than di-ethylhexyladipate (deha), since biodegradation rates were 0.17 and 0.95 g m3 h−1 for toluene in 25% deha and silicone oil respectively. the use of silicone oil as napl led to removal yields close to 90 and 75% for toluene and dmds respectively, and hence to the complete removal of residual voc if losses by stripping were taken into account, showing the efficiency of tppb containing silicone oil for the removal of these compounds.
efficiency of biological activator formulated material (bafm) for volatile organic compounds removal--preliminary batch culture tests with activated sludge. during biological degradation, such as biofiltration of air loaded with volatile organic compounds, the pollutant is passed through a bed packed with a solid medium acting as a biofilm support. to improve microorganism nutritional equilibrium and hence to enhance the purification capacities, a biological activator formulated material (bafm) was developed, which is a mixture of solid nutrients dissolving slowly in a liquid phase. this solid was previously validated on mineral pollutants: ammonia and hydrogen sulphide. to evaluate the efficiency of such a material for biodegradation of some organic compounds, a simple experiment using an activated sludge batch reactor was carried out. the pollutants (sodium benzoate, phenol, p-nitrophenol and 2-4-dichlorophenol) were in the concentration range 100 to 1200 mg l(-1). the positive impact of the formulated material was shown. the improvement of the degradation rates was in the range 10-30%. this was the consequence of the low dissolution of the nutrients incorporated during material formulation, followed by their consumption by the biomass, as shown for urea used as a nitrogen source. owing to its twofold interest (mechanical resistance and nutritional supplementation), the biological activator formulated material seems to be a promising material. its addition to organic or inorganic supports should be investigated to confirm its relevance for implementation in biofilters.
propagation engine prototyping with a domain specific language. constraint propagation is at the heart of constraint solvers. two main trends co-exist for its implementation: variable-oriented propagation engines and constraint-oriented propagation engines. those two approaches ensure the same level of local consistency but their efficiency (computation time) can be quite different depending on the instance solved. however, it is usually accepted that there is no best approach in general, and modern constraint solvers implement only one. in this paper, we would like to go a step further providing a solver independent language at the modeling stage to enable the design of propagation engines. we validate our proposal with a reference implementation based on the choco solver and the minizinc constraint modeling language.
when is it worthwhile to propagate a constraint? a probabilistic analysis of alldifferent. this article presents new work on analyzing the behaviour of a constraint solver, with a view towards optimization. in constraint programming, the propagation mechanism is one of the key tools for solving hard combinatorial problems. it is based on specific algorithms: propagators, that are called a large number of times during the resolution process. but in practice, these algorithms may often do nothing: their output is equal to their input. it is thus highly desirable to be able to recognize such situations, so as to avoid useless calls. we propose to quantify this phenomenon in the particular case of the alldifferent constraint (bound consistency propagator). our first contribution is the definition of a probabilistic model for the constraint and the variables it is working on. this model then allows us to compute the probability that a call to the propagation algorithm for alldifferent does modify its input. we give an asymptotic approximation of this probability, depending on some macroscopic quantities related to the variables and the domains, that can be computed in constant time. this reveals two very different behaviors depending of the sharpness of the constraint. first experiments show that the approximation allows us to improve constraint propagation behaviour.
coefficients of different macro-microscopic mass formulae from the ame2012 atomic mass evaluation. the coefficients of different possible macro-microscopic mass formulae previously proposed have been adjusted on 2264 experimental atomic masses extracted from the ame2012 atomic mass evaluation [1] assuming n,z⩾8 and the one standard deviation uncertainty on the mass lower than 150 kev. all the formulae include the volume and surface energies, the coulomb energy, the diffuseness correction to the sharp radius coulomb energy, the shell and pairing energies and take into account or not the curvature energy, different forms of the wigner term, a free charge radius, the experimental equivalent rms charge radius or a fixed short central radius. masses of 976 more exotic nuclei are extrapolated from the most accurate formula.
adapting workﬂows using generic schemas: application to the security of business processes. existing approaches to the adaptation of workflows over web services fall short in two respects. first, they only provide, if ever, limited means for taking into account the execution history of a workflow. second, they do not support adaptations that require modifications not only at the service composition level but also at the levels of interceptors and service implementations. this is particular problematic for the enforcement of security properties over workflows: enforcing authorization properties, for instance, frequently requires execution contexts to be defined and modifications to be applied at all these abstraction levels of web services. we present two main contributions in this context. first, we introduce workflow adaptation schemas (was), a new notion of generic protocol-based workflow adapters. was enable the declarative definition of adaptations involving complex service compositions and implementations. second, we present two real-world security issues related to the use of oauth 2.0, a recent and widely used framework for the authorization of resource accesses. as we motivate, these security issues require history-based adaptations over different abstraction levels of services. we then show how to resolve these issues using was.
contraction analysis of nonlinear random dynamical systems. in order to bring contraction analysis into the very fruitful and topical fields of stochastic and bayesian systems, we extend here the theory describes in \cite{lohmiller98} to random differential equations. we propose new definitions of contraction (almost sure contraction and contraction in mean square) which allow to master the evolution of a stochastic system in two manners. the first one guarantees eventual exponential convergence of the system for almost all draws, whereas the other guarantees the exponential convergence in $l_2$ of to a unique trajectory. we then illustrate the relative simplicity of this extension by analyzing usual deterministic properties in the presence of noise. specifically, we analyze stochastic gradient descent, impact of noise on oscillators synchronization and extensions of combination properties of contracting systems to the stochastic case. this is a first step towards combining the interesting and simplifying properties of contracting systems with the probabilistic approach.
energy dependence of moments of net-proton multiplicity distributions at rhic. we report the beam energy (\sqrt s_{nn} = 7.7 - 200 gev) and collision centrality dependence of the mean (m), standard deviation (\sigma), skewness (s), and kurtosis (\kappa) of the net-proton multiplicity distributions in au+au collisions. the measurements are carried out by the star experiment at midrapidity (|y| &lt; 0.5) and within the transverse momentum range 0.4 &lt; pt &lt; 0.8 gev/c in the first phase of the beam energy scan program at the relativistic heavy ion collider. these measurements are important for understanding the quantum chromodynamic (qcd) phase diagram. the products of the moments, s\sigma and \kappa\sigma^{2}, are sensitive to the correlation length of the hot and dense medium created in the collisions and are related to the ratios of baryon number susceptibilities of corresponding orders. the products of moments are found to have values significantly below the skellam expectation and close to expectations based on independent proton and anti-proton production. the measurements are compared to a transport model calculation to understand the effect of acceptance and baryon number conservation, and also to a hadron resonance gas model.
sr-82 purification procedure using chelex-100 resin. 82rb is a positron-emitting radionuclide widely used in nuclear cardiology. one great advantage is its availability through a generator loaded with 82sr. 82sr can be produced in a high energy cyclotron by irradiating rubidium chloride target with proton beam. in this paper, we present an extensive study (elution profiles, effect of the elution flow rate) on the use of chelex-100 resin and ammonia buffer. no significant effect of flow rate was evidenced between 1 and 10 ml/min leading us to propose a purification process which can be easily automated.
an international initiative on long-term behavior of high-level nuclear waste glass. nations using borosilicate glass as an immobilization material for radioactive waste have reinforced the importance of scientific collaboration to obtain a consensus on the mechanisms controlling the long-term dissolution rate of glass. this goal is deemed to be crucial for the development of reliable performance assessment models for geological disposal. the collaborating laboratories all conduct fundamental and/or applied research using modern materials science techniques. this paper briefly reviews the radioactive waste vitrification programs of the six participant nations and summarizes the current state of glass corrosion science, emphasizing the common scientific needs and justifications for on-going initiatives.
studies of di-jets in au+au collisions using angular correlations with respect to back-to-back leading hadrons. jet-medium interactions are studied via a multi-hadron correlation technique (called "2+1"), where a pair of back-to-back hadron triggers with large transverse momentum is used as a proxy for a di-jet. this work extends the previous analysis for nearly-symmetric trigger pairs with the highest momentum threshold of trigger hadron of 5 gev/$c$ with the new calorimeter-based triggers with energy thresholds of up to 10 gev and above. the distributions of associated hadrons are studied in terms of correlation shapes and per-trigger yields on each trigger side. in contrast with di-hadron correlation results with single triggers, the associated hadron distributions for back-to-back triggers from central au+au data at $\sqrt{s_{nn}}$=200 gev show no strong modifications compared to d+au data at the same energy. an imbalance in the total transverse momentum between hadrons attributed to the near-side and away-side of jet-like peaks is observed. the relative imbalance in the au+au measurement with respect to d+au reference is found to increase with the asymmetry of the trigger pair, consistent with expectation from medium-induced energy loss effects. in addition, this relative total transverse momentum imbalance is found to decrease for softer associated hadrons. such evolution indicates the energy missing at higher associated momenta is converted into softer hadrons.
evaluation of discrepancies in tetravalent oxide solubility values by isotopic exchange and its impact on the safety assessment. null
electrolocation sensors in conducting water bio-inspired by electric fish. this article presents the first research into designing an active sensor inspired by electric fish. it is notable for its potential for robotics underwater navigation and exploration tasks in conditions where vision and sonar would meet difficulty. it could also be used as a complementary omnidirectional, short range sense to vision and sonar. combined with a well defined engine geometry, this sensor can be modeled analytically. in this article, we focus on a particular measurement mode where one electrode of the sensor acts as a current emitter and the others as current receivers. in spite of the high sensitivity required by electric sense, the first results show that we can obtain a detection range of the order of the sensor length, which suggests that this sensor principle can be used for robotics obstacle avoidance as it is illustrated at the end of the article.
how to take into account potential change of deterioration mode in condition-based maintenance decision rule. classical results in maintenance optimization are based on the characterization of an average system degradation behaviour. the main reasons of such approaches are the robustness of the decisions with respect to the system state and the time horizon. by cons, the ''smoothing'' effect in existing degradation models and the ''generic'' aspect of the decision affect the decision quality as these models do not take into account potential changes in deterioration modes. such consideration can be particularly valuable in a safety context. we propose here a condition-based maintenance approach which adapts the maintenance decision according to the degradation behaviour updated using current state observations. first, we propose to extend the system state definition, usually limited to an indicator of observable degradation by adding information related to the speed of deterioration referred hereafter as potential of degradation or deterioration growth rate. moreover, we propose to take benefit of current observations as well as performed actions to update the degradation laws. more than the model fitness improvement, another advantage of our approach is the proposition of a more realistic modelling of the maintenance impact onto the future behaviour of the system. however, the introduction of the potential of degradation as a new decision parameter does not prejudge an intuitive structure for the decision policy. moreover, as the classical concept of the failure rate, the potential of degradation is non observable. which implies more efforts in both optimization modelling and solution procedure. we highlight the structural properties of the optimization problem which ensure an optimal control limit policy. we will conclude our communication by the illustration of the results of our maintenance model in a pavement management context.
jescala: modular coordination with declarative events and joins. advanced concurrency abstractions overcome the drawbacks of low-level techniques such as locks and monitors, freeing programmers that implement concurrent applications from the burden of concentrating on low-level details. however, with current approaches the coordination logic involved in complex coordination schemas is fragmented into several pieces including join patterns, data emissions triggered in different places of the application, and the application logic that implicitly creates dependencies among channels, hence indirectly among join patterns. we present jescala, a language that captures coordination schemas in a more expressive and modular way by leveraging a seamless integration of an advanced event system with join abstractions. we validate our approach with case studies and provide a first performance assessment.
methodology to estimate methane surface emission in a landfill site. null
note on the swimming of an elongated body in a non-uniform flow. this paper presents an extension of lighthill's large-amplitude elongated-body theory of fish locomotion which enables the effects of an external weakly non-uniform potential flow to be taken into account. to do so, the body is modelled as a kirchhoff beam, made up of elliptical cross-sections whose size may vary along the body, undergoing prescribed deformations consisting of yaw and pitch bending. the fluid velocity potential is decomposed into two parts corresponding to the unperturbed potential flow, which is assumed to be known, and to the perturbation flow. the laplace equation and the corresponding neumann's boundary conditions governing the perturbation velocity potential are expressed in terms of curvilinear coordinates which follow the body during its motion, thus allowing the boundary of the body to be considered as a fixed surface. equations are simplified according to the slenderness of the body and the weakness of the non-uniformity of the unperturbed flow. these simplifications allow the pressure acting on the body to be determined analytically using the classical bernoulli equation, which is then integrated over the body. the model is finally used to investigate the passive and the active swimming of a fish in a kármán vortex street.
a hybrid dynamic model for bio-inspired soft robots - application to a flapping-wing micro air vehicle. the paper deals with the dynamic modeling of bio-inspired robots with soft appendages such as flying insect-like or swimming fish-like robots. in order to model such soft systems, we propose to use the mobile multibody system framework introduced in [1][2][3]. in such a framework, the robot is considered as a tree-like structure of rigid bodies where the evolution of the position of the joints is governed by stress-strain laws or control torques. based on the newton-euler formulation of these systems, we propose a new algorithm able to compute at each step of a time loop both the net and passive joint accelerations along with the control torques supplied by the motors. to illustrate, based on previous work [4], the proposed algorithm is applied to the simulation of the hovering flight of a soft flapping-wing insect-like robot (see the attached video).
using the execo toolbox to perform automatic and reproducible cloud experiments. on cloud testbeds, conducting reproducible and verifiable experiments cannot be done by hand. this paper describes execo, a library providing easy and efficient control of local or remote, standalone or parallel, processes execution, as well as tools designed for scripting distributed computer science experiments or tests on any computing platform, in particular on the grid'5000 testbed. execo usage is illustrated by two experiments exploring the impact on performances of virtual machines collocation and the performances of virtual machines migrations.
adding a live migration model into simgrid, one more step toward the simulation of infrastructure-as-a-service concerns. virtual machine (vm) placement problem has been active research area over the past decade. the research community needs an open simulation framework that can accurately and scalably simulate virtual machine operations including live migrations. however, existing cloud simulation frameworks cannot reproduce live migration behaviors correctly. a naive migration model, not considering memory update operations nor resource sharing contention, can drastically underestimate the duration of a live migration and the size of migration traffic. in this paper, we propose a simulation framework of virtualized distributed systems with the first class support of live migration operations. we developed a resource share calculation mechanism for vms and a live migration model implementing the precopy migration algorithm of qemu/kvm. we extended a widely-used simulation toolkits, simgrid, which allows users to simulate large-scale distributed systems by using user friendly programming api. through experiments, we confirmed that our simulation framework correctly reproduced live migration behaviors of the real world under various conditions. through a first use case, we also confirmed that it is possible to conduct large scale simulations of complex virtualized workloads upon hundred thousands of vms upon thousands of physical machines (pms).
adding virtual machine abstractions into simgrid, a first step toward the simulation of infrastructure-as-a-service concerns. as real systems become larger and more complex, the use of simulator frameworks grows in our research community. by leveraging them, users can focus on the major aspects of their algorithm, run in-siclo experiments (i.e., simulations), and thoroughly analyze results, even for a large-scale environment without facing the complexity of conducting in-vivo studies (i.e., on real testbeds). since nowadays the virtual machine (vm) technology has become a fundamental building block of distributed computing environments, in particular in cloud infrastructures, our community needs a full-fledged simulation framework that enables us to investigate large-scale virtualized environments through accurate simulations. to be adopted, such a framework should provides easy-to-use apis, close to the real ones and preferably fully compatible with those of an existing popular simulation framework. in this paper, we present the current implementation status of a highly-scalable and versatile simulation framework supporting vm environments, extending a widely-used, open-source frame- work, simgrid. our simulation framework allows users to launch hundreds of thousands of vms on their simulation programs and control vms in the same manner as in the real world (e.g., suspend/resume and migrate). users can execute computation and communication tasks on physical machines (pms) and vms through the same simgrid api, which will provide a seamless migration path to iaas simulations for thousands of simgrid users. preliminary validations showed that the resource sharing mechanism of the vm support worked correctly.
a shared " passengers &amp; goods " city logistics system. many strategic planning models have been developed to help decision making in city logistics. such models do not take into account, or very few, the flow of passengers because the considered unit does not have the same nature (a person is active and a good is passive). however, it seems fundamental to gather the goods and the passengers in one model when their respective transports interact with each other. in this context, we suggest assessing a shared passengers &amp; goods city logistics system where the spare capacity of public transport is used to distribute goods toward the city core. we model the problem as a vehicle routing problem with transfers and give a mathematical formulation. then we propose an adaptive large neighborhood search (alns) to solve it. this approach is evaluated on data sets generated following a field study in the city of la rochelle in france.
heavy-quarkonium suppression in p-a collisions from parton energy loss in cold qcd matter. the effects of parton energy loss in cold nuclear matter on heavy-quarkonium suppression in p-a collisions are studied. it is shown from first principles that at large quarkonium energy e and small production angle in the nucleus rest frame, the medium-induced energy loss scales as e. using this result, a phenomenological model depending on a single free parameter is able to reproduce j/psi and upsilon suppression data in a broad xf-range and at various center-of-mass energies. these results strongly support energy loss as the dominant effect in heavy-quarkonium suppression in p-a collisions. predictions for j/psi and upsilon suppression in p-pb collisions at the lhc are made. it is argued that parton energy loss scaling as e should generally apply to hadron production in p-a collisions, such as light hadron or open charm production.
the influence of the openness of an e-learning situation on adult learners'self-regulation. this article presents an empirical research conducted with french speaking adult studying for a diploma. their training took place mainly in e-learning. the goal of this research was to identify and explain the processes of influence existing between two specific dimensions: the degree of openness of the components of the e-learning situation and students' self-regulated behaviors in the management of these components (2). this research was based on the socio-cognitive theory of self-regulation (bandura, 1986; schunk &amp; zimmerman, 2007; zimmerman, 2002) and on a theoretical definition of the notion of "openness" (jézégou (2005). it applied the "actantial model" (greimas, 1966; hiernaux, 1977) for analyzing data collected while using a specific validated instrument of assessment of openness (jézégou, 2010b). the main results of this empirical work are the role played by three psychological dimensions in the influence processes identified. more empirical study is required to confirm their explanatory and validity.
tactical production planning under system availability constraint. null
a preliminary integrated model for optimizing tactical production planning and condition-based maintenance. the inclusion of production and maintenance objectives in an overall decision-making approach remains as well an academic challenge as a response to a real industrial need. based on the observation of mutual dependence between production planning and the stochastic variability of the system production performance due to degradation, we propose in this paper a strategy for optimizing both tactical production planning and maintenance under feasibility constraint. the strategy is driven by an iterative two-unit algorithm. the first unit performs the master production schedule optimization whereas the second unit provides an estimation of the feasibility indicators via a stochastic simulation-based approach. mathematical framework and the proposed iterative algorithm are directly integrated in friendly-user interface software for optimizing the methodology usability in an industrial context.
discrete particle swarm optimization for the multi-level lot-sizing problem. this paper presents a discrete particle swarm optimization (dpso) approach for the multi-level lot-sizing problem (mllp), which is an uncapacitated lot sizing problem dedicated to materials requirements planning (mrp) systems. the proposed dpso approach is based on cost modification and uses pso in its original form with continuous velocity equations. each particle of the swarm is represented by a matrix of logistic costs. a sequential approach heuristic, using wagner-whitin algorithm, is used to determine the associated production planning. the authors demonstrate that any solution of the mllp can be reached by particles. the sequential heuristic is a subjective function from the particles space to the set of the production plans, which meet the customer's demand. the authors test the dpso scheme on benchmarks found in literature, more specifically the unique dpso that has been developed to solve the mllp.
neutral pion cross section and spin asymmetries at intermediate pseudorapidity in polarized proton collisions at sqrt{s} = 200 gev. the differential cross section and spin asymmetries for neutral pions produced within the intermediate pseudorapidity range 0.8 &lt; {\eta} &lt; 2.0 in polarized proton-proton collisions at sqrt{s} = 200 gev are presented. neutral pions were detected using the endcap electromagnetic calorimeter in the star detector at rhic. the cross section was measured over a transverse momentum range of 5 &lt; p_t &lt; 16 gev/c and is found to be within the scale uncertainty of a next-to-leading order perturbative qcd calculation. the longitudinal double-spin asymmetry, a_ll, is measured in the same pseudorapidity range. this quantity is sensitive to the gluonic contribution to the proton spin, {\delta}g(x), at low bjorken-x (down to x approx 0.01), where it is less constrained by measurements at central pseudorapidity. the measured a_ll is consistent with model predictions. the parity-violating asymmetry, a_l, is also measured and found to be consistent with zero. the transverse single-spin asymmetry, a_n, is measured within a previously unexplored kinematic range in feynman-x and p_t. such measurements may aid our understanding of the on-set and kinematic dependence of the large asymmetries observed at more forward pseudorapidity ({\eta} approx 3) and their underlying mechanisms. the a_n results presented are consistent with a twist-3 model prediction of a small asymmetry within the present kinematic range.
taming the zoo of supersymmetric quantum mechanical models. we show that in many cases nontrivial and complicated supersymmetric quantum mechanical (sqm) models can be obtained from the simple model describing free dynamics in flat complex space by two operations: (i) hamiltonian reduction and (ii) similarity transformation of the complex supercharges. we conjecture that it is true for any sqm model.
centrality and pt dependence of j/psi suppression in proton-nucleus collisions from parton energy loss. the effects of parton energy loss and pt-broadening in cold nuclear matter on the pt and centrality dependence, at various rapidities, of j/psi suppression in p-a collisions are investigated. calculations are systematically compared to e866 and phenix measurements. the very good agreement between the data and the theoretical expectations further supports pt-broadening and the associated medium-induced parton energy loss as dominant effects in j/psi suppression in high-energy p-a collisions. predictions for j/psi (and upsilon) suppression in p-pb collisions at the lhc are given.
response of the xenon100 dark matter detector to nuclear recoils. results from the nuclear recoil calibration of the xenon100 dark matter detector installed underground at the laboratori nazionali del gran sasso (lngs), italy are presented. data from measurements with an external 241ambe neutron source are compared with a detailed monte carlo simulation which is used to extract the energy dependent charge-yield qy and relative scintillation efficiency leff. a very good level of absolute spectral matching is achieved in both observable signal channels - scintillation s1 and ionization s2 - along with agreement in the 2-dimensional particle discrimination space. the results confirm the validity of the derived signal acceptance in earlier reported dark matter searches of the xenon100 experiment.
calculation of the gibbs free energy of solvation and dissociation of hcl in water via monte carlo simulations and continuum solvation models. the gibbs free energy of solvation and dissociation of hydrogen chloride in water is calculated through a combined molecular simulation/quantum chemical approach at four temperatures between t = 300 and 450 k. the gibbs free energy is first decomposed into the sum of two components: the gibbs free energy of transfer of molecular hcl from the vapor to the aqueous liquid phase and the standard-state gibbs free energy of acid dissociation of hcl in aqueous solution. the former quantity is calculated using gibbs ensemble monte carlo simulations using either kohn-sham density functional theory or a molecular mechanics force field to determine the system's potential energy. the latter gibbs free energy contribution is computed using a continuum solvation model utilizing either experimental reference data or micro-solvated clusters. the predicted combined solvation and dissociation gibbs free energies agree very well with available experimental data.
a large neighborhood search for the shift design personnel task scheduling problem with equity. null
filtering atmostnvalue with difference constraints: application to the shift minimisation personnel task scheduling problem. this paper introduces a propagator which filters a conjunction of difference constraints and an atmostnvalue constraint. this propagator is relevant in many applications such as the shift minimisation personnel task scheduling problem, which is used as a case study all along this paper. extensive experiments show that it significantly improves a straightforward cp model, so that it competes with best known approaches from operational research.
a branch-and-cut-and-price approach for the pickup and delivery problem with shuttle routes. the pickup and delivery problem with shuttle routes (pdps) is a special case of the pickup and delivery problem with time windows (pdptw) where the trips between the pickup points and the delivery points can be decomposed into two legs. the first leg visits only pickup points and ends at some delivery point. the second leg is a direct trip - called a shuttle - between two delivery points. this optimization problem has practical applications in the transportation of people between a large set of pickup points and a restricted set of delivery points. this paper proposes three mathematical models for the pdps and a branch- and-cut-and-price algorithm to solve it. the pricing subproblem, an elementary shortest path problem with resource constraints (espprc) is solved with a la- beling algorithm enhanced with efficient dominance rules. three families of valid inequalities are used to strengthen the quality of linear relaxations. the method is evaluated on generated and real-world instances containing up to 193 transportation requests. instances with up to 87 customers are solved to optimality within a computation time of one hour.
the dial-a-ride problem with transfers. the dial-a-ride problem with transfers (darpt) consists in defining a set of routes that satisfy transportation requests of users between a set of pickup points and a set of delivery points, in the presence of ride time constraints. users may change vehicles during their trip. this change of vehicle, called a transfer, is made at specific locations called transfer points. solving the darpt involves modeling and algorithmic difficulties. in this paper we provide a solution method based on an adaptive large neighborhood search (alns) metaheuristic and explain how to check the feasibility of a request insertion. the method is evaluated on real-life and generated instances. experiments show that savings due to transfers can be up to 8\% on real-life instances.
underwater reflex navigation in confined environment based on electric sense. this paper shows how a sensor inspired by an electric fish could be used to help navigate in confined environments. exploiting the morphology of the sensor, the physics of electric interactions, as well as taking inspiration from passive electrolocation in real fish, a set of reactive control laws encoding simple behaviors, such as avoiding any electrically contrasted object, or seeking a set of objects while avoiding others according to their electric properties, is proposed. these reflex behaviors are illustrated on simulations and experiments carried out on a setup dedicated to the study of electric sense. the approach does not require a model of the environment and is quite cheap to implement.
impact of a sulphidogenic environment on the corrosion behavior of carbon steel at 90 °c. the influence of sulphide ion concentration on the behavior of carbon steel in a synthetic solution at 90 °c has been investigated using the methods of weight loss, scanning electron microscopy/energy dispersive x-ray spectroscopy (sem/eds), confocal micro-raman spectroscopy and x-ray diffraction (xrd). corrosion batch experiments were conducted at 90 °c for 1 month with steel coupons immersed in na2s solutions. weight loss measurements revealed that the corrosion layer resistance is strongly dependent on both the sulphide concentration and the physicochemical properties of the corrosion products. in the absence of sulphide ions, a magnetite (fe3o4) corrosion product layer was formed on the steel coupon, while in presence of sulphide ions (1 mg l−1), we observed the formation of a less protective mackinawite corrosion layer. at higher sulphide concentrations (5-15 mg l−1), highly protective pyrrhotite and pyrite are formed, inhibiting the steel corrosion process. thus, it has been suggested that the formation of pyrite and/or pyrrhotite could be a promising strategy to protect against carbon steel corrosion in sulphidogenic media.
modeling and analysis of networked control algorithms using descriptor models. this paper deals with the problem of controllers/filters implementation over networked control systems (ncs). the implementation of a given controller/filter over ncs is not unique, depending on its realization and how it is distributed over several cpu. the point here is to consider a dedicated descriptor model, allowing the essential features of the implementation to be represented. this macroscopic model accounts for the inherent delays or packet losses occurring during communication between subsystems. it makes possible to consider in the same framework, deteriorations induced by implementation of whatever nature: i) computation using a finite word length arithmetic and ii) communication delays over the network. focusing on the last point, the paper shows how to evaluate the resilience of a given realization against internal delays. finally, a filtering problem is considered to show the importance on how the filter is implemented, by using the modeling and analyzing tools proposed.
radiometals for combined imaging and therapy. null
emissivity and conductivity of parton-hadron matter. we investigate the properties of the qcd matter across the deconfinement phase transition. in the scope of the parton-hadron string dynamics (phsd) transport approach, we study the strongly interacting matter in equilibrium as well as the out-of equilibrium dynamics of relativistic heavy-ion collisions. we present here in particular the results on the electromagnetic radiation, i.e. photon and dilepton production, in relativistic heavy-ion collisions and the relevant correlator in equilibrium, i.e. the electric conductivity. by comparing our calculations for the heavy-ion collisions to the available data, we determine the relative importance of the various production sources and address the possible origin of the observed strong elliptic flow $v_2$ of direct photons.
j/psi production and nuclear effects in p-pb collisions at sqrt(snn)=5.02 tev. inclusive j/psi production has been studied with the alice detector in p-pb collisions at sqrt(snn) = 5.02 tev at the cern lhc, in the rapidity domains 2.03 &lt; ycms &lt; 3.53 and -4.46 &lt; ycms &lt; -2.96, down to zero transverse momentum. the j/psi measurement is performed in the muon spectrometer through the mu+ mu- decay mode. in this paper, the j/psi production cross section and the nuclear modification factor r(ppb) for the rapidities under study are presented. while at forward rapidity, corresponding to the proton direction, a suppression of the j/psi yield with respect to binary-scaled pp collisions is observed, in the backward region no suppression is present. the ratio of the forward and backward yields is also shown differentially in rapidity and transverse momentum. theoretical predictions based on nuclear shadowing, as well as on models including, in addition, a contribution from partonic energy loss, are in fair agreement with the experimental results.
beyond the cloud, how should next generation utility computing infrastructures be designed?. to accommodate the ever-increasing demand for utility computing (uc) resources, while taking into account both energy and economical issues, the current trend consists in building larger and larger data centers in a few strategic locations. although such an approach enables to cope with the actual demand while continuing to operate uc resources through centralized software system, it is far from delivering sustainable and efficient uc infrastructures. we claim that a disruptive change in uc infrastructures is required: uc resources should be managed differently, considering locality as a primary concern. we propose to leverage any facilities available through the internet in order to deliver widely distributed uc platforms that can better match the geographical dispersal of users as well as the unending demand. critical to the emergence of such locality-based uc (luc) platforms is the availability of appropriate operating mechanisms. in this paper, we advocate the implementation of a unified system driving the use of resources at an unprecedented scale by turning a complex and diverse infrastructure into a collection of abstracted computing facilities that is both easy to operate and reliable. by deploying and using such a luc operating system on backbones, our ultimate vision is to make possible to host/operate a large part of the internet by its internal structure itself: a scalable and nearly infinite set of resources delivered by any computing facilities forming the internet, starting from the larger hubs operated by isps, government and academic institutions to any idle resources that may be provided by end-users. unlike previous researches on distributed operating systems, we propose to consider virtual machines (vms) instead of processes as the basic element. system virtualization offers several capabilities that increase the flexibility of resources management, allowing to investigate novel decentralized schemes.
multi autonomic management for optimizing energy consumption in cloud infrastructures. as a direct consequence of the increasing popularity of internet and cloud computing services, data centers are amazingly growing and hence have to urgently face energy consumption issues. paradoxically, cloud computing allows infrastructure and applications to dynamically adjust the provision of both physical resources and software services in a pay-per-use manner so as to make the infrastructure more energy efficient and applications more quality of service (qos) compliant. however, optimization decisions taken in isolation at a certain level may indirectly interfere in (or even neutralize) decisions taken at another level, e.g. an application requests more resources to keep its qos while part of the infrastructure is being shutdown for energy reasons. hence, it becomes necessary not only to establish a synergy between cloud layers but also to make these layers flexible and sensitive enough to be able to react to runtime changes and thereby fully benefit from that synergy. this thesis proposes a self-adaptation approach that considers both application internals (architectural elasticity) and infrastructure (resource elasticity) to reduce the energy footprint in cloud infrastructures. each application and the infrastructure are equipped with their own autonomic manager, which allows them to autonomously optimize their execution. in order to get several autonomic managers working together, we propose an autonomic model for coordination and synchronization of multiple autonomic managers. the approach is experimentally validated through two studies: a qualitative (qos improvements and energy gains) and a quantitative one (scalability).
eval-pdu: urban traffic and its environmental impacts modelling to assess urban mobility master plan : conception of a methodology based on the nantes case. the eval-pdu research, funded by french national research agency' program sustainable city, aims at testing methodologies for strategic environmental assessment of urban mobility master plans with an application to nantes metropolis. the general approach used for this interdisciplinary research consists in combining several modelling of flows: first vehicles flows in the city, then the flows of atmospheric pollutants and noise pollution resulting from the traffic. in front of phenomena that are quite impossible to be precisely measured considering their micro-local diversity, modelling seems the only solution to estimate the effect of transport policies on the urban environment (in comparison with a reference situation where they are not applied) and thus to enlighten collective action in order to minimize negative externalities.
nuclear waste disposal: i. laboratory simulation of repository properties. after more than 30 years of research and development, there is a broad technical consensus that geologic disposal will provide for the safety of humankind, now and far into the future. safety analyses have demonstrated that the risk, measured as the exposure to radiation, will be of little consequence. still, there is not yet an operating geologic repository for highly radioactive waste, and there remains substantial public concern about the long-term safety of geologic disposal. in the two linked papers we argue for a stronger connection between the scientific data (this paper i) and the safety analysis, particularly in the context of societal expectations (paper ii). in the present paper i, we use new experimental data on the properties of clay formations simulating geological disposal conditions to illustrate how one can understand the ability of clay to isolate radionuclides. the data include percolation tests on various intact clay–rock cores with different calcite contents. for the first time, hydrodynamic parameters (anion and cation accessible porosities, permeability, dispersion and diffusion coefficients), as well as retention parameters (sorption behavior of iodine, cesium) and materials interaction parameters (glass dissolution rates, etc.) have been obtained for a series of clay–rock samples of varying mineralogy. increased calcite content leads to lower permeability and porosity, but the difference between anion and cation accessible porosity diminished. the data confirm very slow radionuclide migration, and a direct extrapolation to repository geometry yields isolation times, for a 70 m clay–rock formation, of many hundreds of thousands of years, even for the most mobile radionuclides such as iodine-129 and chlorine 36 and complete retention for the more radiotoxic, less mobile radionuclides such as the actinides or cesium-137.in order to assess the meaning of the technical results and derived models for long-term safety, paper ii addresses model validity and credibility not only from a technical perspective, but in a much broader historical, epistemological and societal context. safety analysis is treated in its social and temporal dimensions. this approach provides new insights into the societal dimension of scenarios and risk, and it shows that there is certainly no direct link between increased scientific understanding and a public position for or against different strategies of nuclear waste disposal.
role of structural effects on the collective transverse flow and the energy of vanishing flow in nuclear collisions. we address the question of why so far most of the simulation approaches to find the energy of vanishing flow (evf) in light systems have failed to reproduce the experimental data. by investigating systematically the dependence of the evf on the initial setup of the nuclei in these approaches we find out that for light systems a small variation of this setup can create large differences in the evf whereas for large systems the dependence is weak. these studies have been done with the isospin-dependent quantum molecular dynamics model.
limits on spin-dependent wimp-nucleon cross sections from 225 live days of xenon100 data. we present new experimental constraints on the elastic, spin-dependent wimp-nucleon cross section using recent data from the xenon100 experiment, operated in the laboratori nazionali del gran sasso in italy. an analysis of 224.6 live days x 34 kg of exposure acquired during 2011 and 2012 revealed no excess signal due to axial-vector wimp interactions with 129-xe and 131-xe nuclei. this leads to the most stringent upper limits on wimp-neutron cross sections for wimp masses above 6 gev, with a minimum cross section of 3.5 x 10^{-40} cm^2 at a wimp mass of 45 gev, at 90% confidence level.
application of a bivariate deterioration model for a pavement management optimization. this work is a part of a research project for the development of new condition-based approaches for road maintenance optimization. the degradation pattern on interest is here the longitudinal cracking process due to cumulative fatigue (the traffic repetition leads to the occurrence of cracks in the road basement which grows up to the road surface). in this context, we have proposed a new theoretical deterioration model based on two dependent indicators -the observable deterioration measurement and the potential deterioration growth. even if the construction of the model is based on practical considerations, its application in an operational context remains difficult. the objective of this communication is to study the applicability of such a model and propose some improvements. after analyzing the original model to highlight its strengths and limitations, we propose to revisit the definitions of the decision parameters while specifying the construction of the associated functions. a statistical inference procedure is discussed. a numerical example based on the original maintenance model is presented to illustrate the benefits of this approach that we will present as some best practices for future road pavement management.
influence of an inhomogeneous and expanding medium on signals of the qcd phase transition. according to a fluid dynamic expansion of the fireball we investigate how the inhomogeneity of the system influences the chiral phase transition of qcd. we compare the averaged values of the order parameter in equilibrium with that of a homogeneous system. if the temperature is averaged over a certain region of the fireball the corresponding correlation length does not diverge in an expansion with a critical point.
occurrence of natural organic chlorine in soils for different land uses. consideration of natural formation of organochlorine compounds in soils is necessary in radioecology in order to understand chlorine radioisotope (36cl) cycling in various environments for safety assessment purposes, but also in ecotoxicology because certain chlorinated organics in soils are toxic compounds. on the other hand, occurrence of organic chlorine in soils is poorly documented, especially in non-forest ecosystems. we measured total and organic chlorine concentrations in 51 french surface soils sampled from grassland, arable and forest sites on a national scale (french soil quality monitoring network) in order to characterize the variability of organic chlorine concentrations for these different land uses. while previous studies reported that the chlorination of soil organic matter is responsible for chlorine retention in temperate forest ecosystems, this study shows that the non-extractable organohalogen pool accounts for the majority (&gt;80 % on an average) of the total measurable chlorine in grassland and agricultural soils. this suggests that natural chlorination is a widespread phenomenon in all kinds of soils. a multiple linear regression analysis performed on the dataset indicated that retention of organochlorine in soils is related to the organic carbon content, cl input and soil ph.
the pickup and delivery problem with shuttle routes. the main objective of this communication is to show how realistic-sized instances of the pickup and delivery problem with transfers (pdpt) can be solved to optimality with a branch-and-cut-and-price method, under realistic hypotheses. we introduce the pickup and delivery problem with shuttle routes (pdp-s) which is a special case of the pdpt relying on a structured network with two categories of routes. {\em pickup routes} visit a set of pickup points independently of their delivery points and end at one delivery point. {\em shuttle routes} are direct trips between two delivery points. we propose a path based formulation of the pdp-s and solve it with a branch-and-cut-and-price algorithm. we show that this approach is able to solve real-life instances with up to 87 transportation requests.
energetic and exergetic assessment of solar and wind potentials in europe. this paper deals with a physics-based assessment of renewable energy potential in europe, particularly solar and wind energy sources, using two literature models. a sensibility analysis with the weather data is first done. actual temperature, pressure, relative humidity, global radiation and wind speed data are employed to develop energy and exergy maps for europe, based on iso-areas of land-use. these maps are compared with similar existing ones. a good agreement is obtained. a paradoxical result is found for wind exergy efficiency. the yearly average exergy efficiency where wind speed is less than 5 m/s is greater than that where wind speed is greater than 7 m/s. this can be explained by the 'dome' shape of wind exergy efficiency. a solar efficiency map for europe is also developed to serve as a useful guide for choosing a renewable energy form based on yearly energy production.
multiplicity dependence of pion, kaon, proton and lambda production in p--pb collisions at sqrt(s_nn) = 5.02 tev. in this letter, comprehensive results on {\pi}+-, k+-, k^0_s, p(antiproton) and {\lambda} anti-{\lambda} production at mid-rapidity (0 &lt; y_cms &lt; 0.5) in p-pb collisions at sqrt(s_nn) = 5.02 tev, measured by the alice detector at the lhc, are reported. the transverse momentum distributions exhibit a hardening as a function of event multiplicity, which is stronger for heavier particles. this behavior is similar to what has been observed in pp and pb--pb collisions at the lhc. the measured pt distributions are compared to results at lower energy and with predictions based on qcd-inspired and hydrodynamic models.
simulation of the 3γ imaging using liquid xenon compton telescope. nuclear medical 3γ imaging is an innovative technique which is studied at the subatech laboratory. it isbased on the three-dimensional localization of a (β+, γ) radioisotope emitter, the 44sc, by using a liquid xenon compton telescope. the position of the disintegration of this radioisotope is obtained by the intersection of the line of response, built by the detection of two 511 kevphotons from the annihilation of a positron, and the cone determined by the third photon. a small prototype xemis1 (xenon medical imaging system) was developed to demonstrate experimentally the feasibility of 3γ imaging. the results of this prototype are quite encouraging in terms of energy resolution, purity of liquid xenon and electronic noise. the monte carlo simulation is an indispensable tool to support the r&amp;d and to evaluate the new proposed technique of imaging ; this thesis work is to develop the simulation of 3γ imaging system by using gate (geant4 application for tomographic emission). new functionalities have been added to gate to simulate a tpc (time projection chamber) detector. we performed a simulation of xemis1 prototype and obtained results in good agreement with our experimental data. the next step of the project is to build a full liquid xenon cylindrical camera for the small animal imaging. the results presented in this thesis of the simulations of this camera demonstrate the ability to locate every decayalong the line of response with very good accuracy and good detection sensitivity. the first direct images of simple phantoms, realized event by event, and after tomographic reconstruction are also presented.
multi-strange baryon production at mid-rapidity in pb-pb collisions at sqrt(s_nn) = 2.76 tev. the alice experiment at the lhc has measured the production of {\xi}- and {\omega}- baryons and their anti-particles in pb-pb collisions at sqrt(s_nn) = 2.76 tev. the transverse momentum spectra at mid-rapidity (|y| &lt; 0.5) for charged {\xi} and {\omega} hyperons have been studied in the range 0.6 &lt; pt &lt; 8.0 gev/c and 1.2 &lt; pt &lt; 7.0 gev/c, respectively, and in several centrality intervals (from the most central 0-10% to the most peripheral 60-80% collisions). these spectra have been compared with the predictions of recent hydrodynamic models. in particular, the krak{ó}w and epos models give a satisfactory description of the data, with the latter covering a wider pt range. mid-rapidity yields, integrated over pt, have been determined. the hyperon-to-pion ratios are similar to those at rhic: they rise smoothly with centrality up to ~ 150 and saturate thereafter. the enhancements (yields per participant nucleon relative to p-p collisions) increase both with the strangeness content of the baryon and with centrality, but are less pronounced than at lower energies.
k^0_s and {\lambda} production in pb-pb collisions at sqrt(snn) = 2.76 tev. the alice measurement of k^0_s and {\lambda} production at mid-rapidity in pb-pb collisions at sqrt(snn) = 2.76 tev is presented. the transverse momentum (pt) spectra are shown for several collision centrality intervals and in the pt range from 0.4 gev/c (0.6 gev/c for {\lambda}) to 12 gev/c. the pt dependence of the {\lambda}/k^0_s ratios exhibits maxima in the vicinity of 3 gev/c, and the positions of the maxima shift towards higher pt with increasing collision centrality. the magnitude of these maxima increases by almost a factor of three between most peripheral and most central pb-pb collisions. this baryon excess at intermediate pt is not observed in pp interactions at sqrt(s) = 0.9 tev and at sqrt(s) = 7 tev. qualitatively, the baryon enhancement in heavy-ion collisions is expected from radial flow. however, the measured pt spectra above 2 gev/c progressively decouple from hydrodynamical-model calculations. for higher values of pt, models that incorporate the influence of the medium on the fragmentation and hadronization processes describe qualitatively the pt dependence of the {\lambda}/k^0_s ratio.
gluon radiation by heavy quarks at intermediate energies. employing scalar qcd we study the gluon emission of heavy quarks created by the interaction with light quarks. we develop approximation formulas for the high energy limit and study when the full calculation reaches this high energy limit. for zero quark masses and in the high energy limit our model reproduces the gunion-bertsch results. we justify why scalar qcd represents a good approximation to the full qcd approach for the energy loss of heavy quarks. in the regime of accessible phenomenology we observe that the emission at small transverse momentum (dead cone effect) is less suppressed than originally suggested. we also investigate the influence of a finite gluon mass on the discussed results.
reference monitors for security and interoperability in oauth 2.0. the oauth 2.0 protocol is a recent ietf standard devoted to providing authorization to clients requiring access to specific resources over http. it was recently adopted by major internet players like google, facebook, and microsoft. it has been pointed out that this protocol is potentially subject to security issues, as well as difficulties concerning the interoperability between protocol participants and application evolution. as we show in this paper, there are indeed multiple reasons that make this protocol hard to implement and impede interoperability in the presence of different kinds of clients. our main contribution consists in a framework that harnesses a type based policy language and aspect-based support for protocol adaptation through flexible reference monitors in order to handle security, interoperability and evolution issues of oauth 2.0. we apply our framework in the context of three scenarios that make explicit variations in the protocol and show how to handle those issues.
medium modification of jet fragmentation in au+au collisions at sqrt(s_nn)=200 gev measured in direct photon-hadron correlations. the jet fragmentation function is measured with direct photon-hadron correlations in p+p and au+au collisions at sqrt(s_nn)=200 gev. the p_t of the photon is an excellent approximation to the initial p_t of the jet and the ratio z_t=p_t^h/p_t^\gamma is used as a proxy for the jet fragmentation function. a statistical subtraction is used to extract the direct photon-hadron yields in au+au collisions while a photon isolation cut is applied in p+p. i_ aa, the ratio of jet fragment yield in au+au to that in p+p, indicates modification of the jet fragmentation function. suppression, most likely due to energy loss in the medium, is seen at high z_t. the fragment yield at low z_t is enhanced at large angles. such a trend is expected from redistribution of the lost energy into increased production of low-momentum particles.
evidence for flow in ppb collisions at 5 tev from v2 mass splitting. we show that a fluid dynamical scenario describes quantitatively the observed mass splitting of the elliptical flow coefficients v2 for pions, kaons, and protons. this provides a strong argument in favor of the existence of a fluid dynamical expansion in ppb collisions at 5tev.
long-range angular correlations of pi, k and p in p--pb collisions at sqrt(s_nn) = 5.02 tev. angular correlations between unidentified charged trigger particles and various species of charged associated particles (unidentified particles, pions, kaons, protons and antiprotons) are measured by the alice detector in p-pb collisions at a nucleon--nucleon centre-of-mass energy of 5.02 tev in the transverse-momentum range 0.3 &lt; p_t &lt; 4 gev/c. the correlations expressed as associated yield per trigger particle are obtained in the pseudorapidity range |eta_lab|&lt;0.8. fourier coefficients are extracted from the long-range correlations projected onto the azimuthal angle difference and studied as a function of $\pt$ and in intervals of event multiplicity. in high-multiplicity events, the second-order coefficient for protons, v_2^p, is observed to be smaller than that for pions, v_2^pi, up to about p_t = 2 gev/c. to reduce correlations due to jets, the per-trigger yield measured in low-multiplicity events is subtracted from that in high-multiplicity events. a two-ridge structure is obtained for all particle species. the fourier decomposition of this structure shows that the second-order coefficients for pions and kaons are similar. the v_2^p is found to be smaller at low p_t and larger at higher p_t than v_2^pi, with a crossing occurring at about 2 gev. this is qualitatively similar to the elliptic-flow pattern observed in heavy-ion collisions. a mass ordering effect at low transverse momenta is consistent with expectations from hydrodynamic model calculations assuming a collectively expanding system.
insight into the mechanism of carbon steel corrosion under aerobic and anaerobic conditions. we particularly focused our study on identifying the corrosion products formed at 30 °c on carbon steel under aerobic and anaerobic conditions and on following their evolution with time due to enhanced microbial activity under environmental and geological conditions. the nature and structural properties of corrosion products were investigated by scanning electron microscopy/energy dispersive x-ray spectroscopy (sem/eds), x-ray diffraction (xrd) and confocal micro-raman spectroscopy. structural characterisation clearly showed the formation of iron oxides (magnetite and maghemite) under aerobic conditions. under anaerobic conditions, the first corrosion product formed on the steel surface was nanocrystalline mackinawite, which was then followed by a fast transformation process into the pyrrhotite phase, and the raman spectrum of monoclinic pyrrhotite was proposed for the first time. finally, this study also shows that in the context of geological disposal of radioactive waste, the corrosion of carbon steel containers in anoxic and sulphidogenic environments sustained by sulphate-reducing bacteria may not be a problem notably due to the formation of a passive layer on the steel surface.
multiplicity dependence of two-particle azimuthal correlations in pp collisions at the lhc. we present the measurements of particle pair yields per trigger particle obtained from di-hadron azimuthal correlations in pp collisions at sqrt(s) = 0.9, 2.76, and 7 tev recorded with the alice detector. the yields are studied as a function of the charged particle multiplicity. taken together with the single particle yields the pair yields provide information about parton fragmentation at low transverse momenta, as well as on the contribution of multiple parton interactions to particle production. data are compared to calculations using the pythia6, pythia8, and phojet event generators.
chapter 10. dynamics models for deformable manipulators. null
reduced locomotion dynamics with passive internal dofs: application to non-holonomic and soft robotics. this article proposes a general modelling approach for locomotion dynamics of mobile multibody systems (mms) containing passive internal degrees of freedom (dofs) concentrated into (ideal or not) joints and/or distributed along deformable bodies of the system. the approach embraces the case of non-holonomic mobile multibody systems with passive wheels, the pendular climbers and the locomotion systems bio-inspired by animals that exploit the advantages of soft appendages such as the fish swimming with their caudal fin or the moths using the softness of their flapping wings to improve flight performance. the article proposes a general structured modelling approach of mms with tree-like structures along with efficient computational algorithms of the resulting equations. the approach is illustrated through non-trivial examples such as the 3d bicycle and a compliant version of the snake-board.
early cosmic ray research in france. the french research on cosmic rays in the first half of the 20th century is summarized. the main experiments are described as the discovery of air cosmic ray showers by pierre auger. the results obtained at the french altitude laboratories like the "pic du midi de bigorre" are also briefly presented.
multiplicity dependence of the average transverse momentum in pp, p-pb, and pb-pb collisions at the lhc. the average transverse momentum versus the charged-particle multiplicity n_ch was measured in p-pb collisions at a collision energy per nucleon-nucleon pair sqrt(s_nn) = 5.02 tev and in pp collisions at collision energies of sqrt(s) = 0.9, 2.76, and 7 tev in the kinematic range 0.15 &lt; p_t &lt; 10.0 gev/c and |eta| &lt; 0.3 with the alice apparatus at the lhc. these data are compared to results in pb-pb collisions at sqrt(s_nn) = 2.76 tev at similar charged-particle multiplicities. in pp and p-pb collisions, a strong increase of with n_ch is observed, which is much stronger than that measured in pb-pb collisions. for pp collisions, this could be attributed, within a model of hadronizing strings, to multiple-parton interactions and to a final-state color reconnection mechanism. the data in p-pb and pb-pb collisions cannot be described by an incoherent superposition of nucleon-nucleon collisions and pose a challenge to most of the event generators.
energy dependence of the transverse momentum distributions of charged particles in pp collisions measured by alice. differential cross sections of charged particles in inelastic pp collisions as a function of p_t have been measured at sqrt(s) = 0.9, 2.76 and 7 tev at the lhc. the p_t spectra are compared to nlo-pqcd calculations. though the differential cross section for an individual sqrt(s) cannot be described by nlo-pqcd, the relative increase of cross section with sqrt(s) is in agreement with nlo-pqcd. based on these measurements and observations, procedures are discussed to construct pp reference spectra at sqrt(s) = 2.76 and 5.02 tev up to p_t = 50 gev/c as required for the calculation of the nuclear modification factor in nucleus-nucleus and proton-nucleus collisions.
effect of h2 produced through steam methane reforming on chp plant efficiency. in situ hydrogen production is carried out by a catalytic reformer kit set up into exhaust gases for a chp plant based on spark ignition engine running under lean conditions. an overall auto-thermal reforming process is achieved. hydrogen production is mainly dependent on o2 content in exhaust gases. experiments are conducted at constant speed at 2 air/fuel ratios and 4 additional natural gas flow rates. h2 content varies in the range 6-10% in vol. h2 content effect is analysed with respect to performance and emissions. comparing with egr shows an increasing of electrical efficiency of 1% whilst heat recovery decreases by 1%. no and hc decrease by 18% and 12%, but co increases by 14%, respectively. the results show that: (i) graphite joints were destroyed under effect of h2 and high temperature; (ii) a cold spot appeared in the rgr line, and condensation has as consequence a carbon deposit; and (iii) no back-fire or knock occurred. © 2011, hydrogen energy publications, llc. published by elsevier ltd. all rights reserved.
recycling flows in emergy evaluation: a mathematical paradox?. this paper is a contribution to the emergy evaluation of systems involving recycling or reuse of waste. if waste exergy (its residual usefulness) is not negligible, wastes could serve as input to another process or be recycled. in cases of continuous waste recycle or reuse, what then is the role of emergy? emergy is carried by matter and its value is shown to be the product of specific energy with mass flow rate and its transformity. this transformity (τ) given as the ratio of the total emergy input and the useful available energy in the product (exergy) is commonly calculated over a specific period of time (usually yearly) which makes transformity a time dependent factor. assuming a process in which a part of the non-renewable input is an output (waste) from a previous system, for the waste to be reused, an emergy investment is needed. the transformity of the reused or recycled material should be calculated based on the pathway of the reused material at a certain time (t) which results in a specific transformity value (τ). in case of a second recycle of the same material that had undergone the previous recycle, the material pathway has a new time (t+t 1) which results in a transformity value (τ 1). recycling flows as in the case of feedback is a dynamic process and as such the process introduces its own time period depending on its pathway which has to be considered in emergy evaluations. through the inspiration of previous emergy studies, authors have tried to develop formulae which could be used in such cases of continuous recycling of material in this paper. the developed approach is then applied to a case study to give the reader a better understanding of the concept. as a result, a 'factor' is introduced which could be included on emergy evaluation tables to account for subsequent transformity changes in multiple recycling. this factor can be used to solve the difficulties in evaluating aggregated systems, serve as a correction factor to up-level such models keeping the correct evaluation and also solve problems of memory loss in emergy evaluation. the discussion deals with the questions; is it a pure mathematical paradox in the rules of emergy? is it consistent with previous work? what were the previous solutions to avoid the cumulative problem in a reuse? what are the consequences?. © 2011 elsevier b.v.
entropy production and field synergy principle in turbulent vortical flows. the heat transfer in turbulent vortical flows is investigated by three different physical approaches. vortical structures are generated by inclined baffles in a turbulent pipe flow, of three different configurations. in the first, the vortex generators are aligned and inclined in the flow direction (called the reference geometry); in the second, a periodic 45° rotation is applied to the tab arrays (alternating geometry); the third is the reference geometry used in the direction opposite to the flow (reversed geometry). the effect of the flow structure on the temperature distribution in these different configurations is analyzed. the conventional approach based on heat-transfer analysis using the nusselt number and the enhancement factor is used to determine the efficiency of these geometries relative to other heat exchangers in the literature. the effect of vorticity on the nusselt number is clearly demonstrated, and so as to highlight the respective roles of the coherent structures and the turbulence, a new parameter is defined as the ratio of the vortex circulation to the turbulent viscosity. the relative contribution of the radial convection to heat transfer appears to increase with reynolds number. the effect of mixing performance on the temperature distribution is investigated by the field synergy method. a global parameter, namely the intersection angle between the velocity and temperature gradient, is defined in order to compare performances. finally, an analysis of energetic efficiency by entropy production, involving both heat transfer and pressure losses, is carried out to determine the overall performance of the heat exchangers. all these approaches lead to the same conclusion: that the reversed geometry presents the best heat transfer coefficient and the best energetic efficiency. the reference geometry shows the worst performance, and the alternating array has intermediate performance. © 2011 elsevier masson sas. all rights reserved.
exact computation of emergy based on a mathematical reinterpretation of the rules of emergy algebra. the emergy algebra is based on four rules, the use of which is sometimes confusing or reserved only to the experts of the domain. the emergy computation does not obey conservation logic (i.e. emergy computation does not obey kirchoff-like circuit law). in this paper the authors propose to reformulate the emergy rules into three axioms which provide (i) a rigourous mathematical framework for emergy computation and (ii) an exact recursive algorithm to compute emergy within a system of interconnected processes at steady state modeled by an oriented graph named the emergy graph.because emergy algebra follows a logic of memorization, the evaluation principles deal with paths in the emergy graph. the underlying algebraic structure is the set of non-negative real numbers operated on by three processes, the maximum (max), addition (+) and multiplication (*). the maximum is associated with the co-product problem. addition is linked with the split problem or with the independence of two emergy flows. and multiplication is related to the logic of memorization. the axioms describe how to use the different operators max, + and * to combine flows without any confusion or ambiguity. the method is tested on five benchmark emergy examples. © 2012 elsevier b.v.
effects on chp plant efficiency of h 2 production through partial oxydation of natural gas over two group viii metal catalysts. blending h 2 with natural gas in spark ignition engines can increase for electric efficiency. in-situ h 2 production for spark ignition engines fuelled by natural gas has therefore been investigated recently, and reformed exhaust gas recirculation (rgr) has been identified a potentially advantageous approach: rgr uses the steam and o 2 contained in exhaust gases under lean combustion, for reforming natural gas and producing h 2, co, and co 2. in this paper, an alternative approach is introduced: air gas reforming circulation (agrc). agrc uses directly the o 2 contained in air, rendering the chemical pathway comparable to partial oxidation. formulations based on palladium and platinum have been selected as potential catalysts. with agrc, the concentrations of the constituents of the reformed gas are approximately 25% hydrogen, 10% carbon monoxide, 8% unconverted hydrocarbons and 55% nitrogen. experimental results are presented for the electric efficiency and exhaust gas (co and hc) composition of the overall system (si engine equipped with agrc). it is demonstrated that the electric efficiency can increase for specific ratios of air to natural gas over the catalyst. although the electric efficiency gain with agrc is modest at around 0.2%, agrc can be cost effective because of its straightforward and inexpensive implementation. misfiring and knock were both not observed in the tests reported here. nevertheless, technical means of avoiding knock are described by adjusting the main flow of natural gas and the additional flow of agrc. copyright © 2012, hydrogen energy publications, llc. published by elsevier ltd. all rights reserved.
impact of building material recycle or reuse on selected emergy ratios. while the emergy evaluation method has been used successfully in recycling processes, this area of application still requires further development. one of such is developing emergy ratios or indices that reflect changes depending on the number of times a material is recycled. some of these materials may either have been recycled or reused continuously as inputs to a building, for example, and thus could have various impacts on the emergy evaluation of the building. the paper focuses on reuse building materials in the context of environmental protection and sustainable development. it presents the results of an emergy evaluation of a low-energy building (leb) in which a percentage of input materials are from recycled sources. the corresponding impacts on the emergy yield ratio (eyr b) and the environmental loading ratio (elr b) are studied. the eyr which is the total emergy used up per unit of emergy invested, is a measure of how much an investment enables a process to exploit local resources in order to further contribute to the economy. the elr however, is the total nonrenewable and imported emergy used up per unit of local renewable resource and indicates the stress a process exhibits on the environment. the evaluation provides values for the selected ratios based on different recycle times. results show that values of the emergy indices vary, even more, when greater amounts of material is recycled with higher amount of additional emergy required for recycling. this provides relevant information prioritizing the selection of materials for recycling or reuse in a building, and the optimum number of reuse or recycle times of a specific material. © 2012 elsevier b.v. all rights reserved.
carbon footprint and emergy combination for eco-environmental assessment of cleaner heat production. the aim of this paper is to study via environmental indicators to which extent, replacing fossil fuel with biomass for heating is an environmentally friendly solution. the environmental impact of using biomass depends mostly on the transportation process. authors define the notion of maximum supply distance, beyond which biomass transportation becomes too environmentally intensive compared to a fossil fuel fired heating system. in this work a carbon footprint analysis and an emergy evaluation, has been chosen to study the substitution of wood for natural gas. the comparative study seeks to examine, via the two approaches, two heating systems: one is fired with wood, transported by trucks and the other one is fired with natural gas transported by pipelines. the results are expressed in terms of maximum supply distance of wood. in the emergy evaluation it represents the maximum supply distance permitting wood to be more emergy saving than natural gas. in the carbon footprint analysis, it represents the maximum supply distance permitting wood to be a carbon saving alternative to natural gas. furthermore, the unification of carbon footprint and emergy evaluation permits to define, for both approaches, the minimum theoretical wood burner first law efficiency that allows, co 2 or emergy to be saved, when there is no wood transport. in order to identify the impacts of the main parameters of the study a sensitivity analysis has been carried out. the case study investigated in this paper shows that there is a large gap between the results. the maximum supply distances calculated via carbon footprint and emergy evaluation are about 5000 km and 1000 km, respectively, anthe minimum theoretical wood burner efficiencies are about 5% and 54%, respectively. © 2012 elsevier ltd. all rights reserved.
a rigourous mathematical framework for computing a sustainability ratio: the emergy. the computational problem of emergy within a general system of interconnected processes at steady state is a subject of interest in literature today. when there is no co-product the proposed method coincides with the track summing method of tennenbaum which was developed precisely for interconnected networks with feedbacks and splits of emergy. as the underlying algebraic structure of the tennenbaum's method is the linear algebra, it is not well-suited to account for the co-product problem which induces the idempotent operator max. thus, authors have chosen another underlying algebraic structure which is the idempotent semiring structure (i.e. a semiring equipped with an idempotent addition). this method is divided into two parts. the first part is the emergy flow enumeration, where paths from an emergy source to the input of a given process are enumerated avoiding double counting of emergy assignation. this part is a path-finding problem which is a slight modification of gerbier of null square approach to find elementary/simple paths in a graph. the second part evaluates the emergy flowing between two components of the system. it is a quantitative part in which the problem of avoiding double counting split and co-product flows are dealt with by introducing a way to mark splits and co-products flows. the method is partially parallelizable. however, the method enumerates paths on a graph thus, in worst cases, its complexity is not polynomial. this paper provides a rigorous framework based on an axiomatic basis to conduct the emergy evaluation of an emergy graph. © 2012 iseis all rights reserved.
how does the solvation unveil ato+ reactivity?. the ato+ molecular ion, a potential precursor for the synthesis of radiotherapeutic agents in nuclear medicine, readily reacts in aqueous solution with organic and inorganic compounds, but at first glance, these reactions must be hindered by spin restriction quantum rules. using relativistic quantum calculations, coupled to implicit solvation models, on the most stable ato+(h2o)6 clusters, we demonstrate that specific interactions with water molecules of the first solvation shell induce a spin change for the ato+ ground state, from a spin state of triplet character in the gas phase to a kramers-restricted closed-shell configuration in solution. this peculiarity allows rationalization of the ato+ reactivity with closed-shell species in aqueous solution and may explain the differences in astatine reactivity observed in 211at production protocols based on "wet" and "dry" processes.
refactoring composite to visitor and inverse transformation in java. we describe how to use refactoring tools to transform a java program conforming to the composite design pattern into a program conforming to the visitor design pattern with the same external behavior. we also describe the inverse transformation. we use the refactoring tool provided by intellij idea.
a framework for the coordination of multiple autonomic managers in cloud environments. null
asynchronous forward bounding revisited. null
estimating the power consumption of an idle virtual machine. power management has become one of the main challenges for data center infrastructures. currently, the cost of powering a server is approaching the cost of the server hardware itself, and, in a near future, the former will continue to increase, while the latter will go down. in this context, virtualization is used to decrease the number of servers, and increase the efficiency of the remaining ones. if virtualization can be used to positively impact on the data center energy consumption, this new abstraction layer disconnects user services (hosted on a virtual machine) from their operating cost. in this paper, we propose an approach and a model to estimate the total power consumption of a virtual machine, by taking into account its static (e.g. memory) and dynamic (e.g. cpu) consumption of resources. this model permits to reconnect each vm to its corresponding operating cost, and provides more information to virtual infrastructure providers and users to optimize their infrastructure/applications. it can be observed from results of experiments that the proposed method outperforms the methods found in the literature that only consider the dynamic consumption of resources.
formation of superheavy elements in the capture of very heavy ions at high excitation energies. the potential barriers governing the reactions 58fe+244pu, 238u+64ni, and 238u+72ge have been determined from a liquid-drop model taking into account the proximity energy, shell energies, rotational energy, and deformation of the incoming nuclei in the quasimolecular shape valley. double-humped potential barriers appear in these entrance channels. the external saddle-point corresponds to two touching ellipsoidal nuclei when the shell and pairing effects are taken into account, while the inner barrier is due to the shell effects at the vicinity of the spherical shape of the composite system. between them, a large potential pocket exists and persists at very high angular momenta allowing the capture of very heavy ions at high excitation energies.
the "ridge" in proton-proton scattering at 7 tev. one of the most important experimental results for proton-proton scattering at the lhc is the observation of a so-called "ridge" structure in the two particle correlation function versus the pseudorapidity difference $\delta\eta$ and the azimuthal angle difference $\delta\phi$. one finds a strong correlation around $\delta\phi=0$, extended over many units in $\delta\eta$. we show that a hydrodynamical expansion based on flux tube initial conditions leads in a natural way to the observed structure. to get this result, we have to perform an event-by-event calculation, because the effect is due to statistical fluctuations of the initial conditions, together with a subsequent collective expansion. this is a strong point in favour of a fluid-like behavior even in $pp$ scattering, where we have to deal with length scales of the order of 0.1 fm.
considerations concerning the fluctuation of the ratios of two observables. we discuss several possible caveats which arise with the interpretation of measurements of fluctuations in heavy ion collisions. we especially focus on the ratios of particle yields, which has been advocated as a possible signature of a critical point in the qcd phase diagram. we conclude that current experimental observables are not well-defined and are without a proper quantitative meaning.
mass dependence of balance energy for different n/z ratio. we present the study for the mass dependence of e$_{bal}$ for various n/z ratios covering pure symmetric systems to highly neutron-rich ones.
role of asymmetry of the reaction and momentum dependent interactions on the balance energy for neutron rich nuclei. null
effect of isospin degree of freedom on the counterbalancing of collective transverse in-plane flow. here we aim to understand the effect of isospin dependence of cross section and coulomb repulsion on the counterbalancing of collective flow.
effect of isospin dependence of cross section on symmetric and neutron rich systems. we aim to explore the effect of isospin dependence of cross section on symmetric and neutron rich system. we also aim to explore whether the analysis is affacted if one discusses in terms of "$e_{bal}$ as a function of n/z or n/a" of the system.
observable to explore high density behaviour of symmetry energy. we aim to see the sensitivity of collective transverse in-plane flow to symmetry energy at low as well as high densities and also to see the effect of different density dependencies of symmetry energy on the same.
a dynamical description of neutron star crusts. neutron stars are natural laboratories where fundamental properties of matter under extreme conditions can be explored. modern nuclear physics input as well as many-body theories are valuable tools which may allow us to improve our understanding of the physics of those compact objects. in this work the occurrence of exotic structures in the outermost layers of neutron stars is investigated within the framework of a microscopic model. in this approach the nucleonic dynamics is described by a time-dependent mean field approach at around zero temperature. starting from an initial crystalline lattice of nuclei at subnuclear densities the system evolves toward a manifold of self-organized structures with different shapes and similar energies. these structures are studied in terms of a phase diagram in density and the corresponding sensitivity to the isospin-dependent part of the equation of state and to the isotopic composition is investigated.
dynamical description of exotic structures at subnuclear densities. the dynamics of infinite nuclear matter in the conditions of density and temperature expected in the outermost layers of neutron stars is studied in the framework of a microscopic time-dependent mean-field approach around zero temperature. dynamical processes in inhomogeneous nuclear matter are studied using a large number of nucleons in numerical simulations without any assumptions on the morphology of nuclear matter. the occurrence of exotic structures when varying internal conditions as densities, nuclear species and elementary cell symmetries is investigated. the corresponding structures are studied in terms of a phase diagram in density space evidencing some sensitivity to the isospin-dependent part of the equation of state.
the physics of epos. null
stresscloud: an infrastructure stresser for virtual machine managers. null
generalized liquid drop model and fission, fusion, alpha and cluster radioactivity and superheavy nuclei. a particular version of the liquid drop model taking into account both the mass and charge asymmetries, the proximity energy, the rotational energy, the shell and pairing energies and the temperature has been developed to describe smoothly the transition between one and two-body shapes in entrance and exit channels of nuclear reactions. in the quasi-molecular shape valley where the proximity energy is optimized, the calculated l-dependent fusion and fission barriers, alpha and cluster radioactivity half-lives as well as actinide half-lives are in good agreement with the available experimental data. in this particular deformation path, double-humped potential barriers begin to appear even macroscopically for heavy nuclear systems due to the influence of the proximity forces and, consequently, quasi-molecular isomeric states can survive in the second minimum of the potential barriers in a large angular momentum range.
how to determine experimentally the k+ nucleus potential and the k+ n rescattering cross section in a hadronic environment?. comparing k+ spectra at low transverse momenta for different symmetric collision systems at beam energies around 1 agev allows for a direct determination of both the strength of the k+ nucleus potential as well as of the k+n rescattering cross section in a hadronic environment. other little known or unknown quantities which enter the k+ dynamics, like the production cross sections of k+ mesons or the hadronic equation of state, do not spoil this signal as they cancel by using ratios of spectra. this procedure is based on transport model calculations using the isospin quantum molecular dynamics (iqmd) model which describes the available data quantitatively.
alice potential for direct photon measurements in p-p and pb-pb collisions. the production of direct photons, not coming from hadron decays, at large transverse momentum pt &gt; 2 gev/c in proton-proton collisions at the lhc, is an interesting process to test the predictions of perturbative quantum chromodynamics at the highest energies ever and to put constraints on the gluon density in the proton. furthermore, they provide a baseline reference for quark-gluon-plasma studies in pb-pb collisions. we will present the experimental capabilities of the alice electromagnetic calorimeter emcal to reconstruct the direct and isolated photon spectra in p-p and pb-pb collisions.
study of the electromagnetic background in the xenon100 experiment. the xenon100 experiment, located at the laboratori nazionali del gran sasso, aims to directly detect dark matter in the form of weakly interacting massive particles via their elastic scattering off xenon nuclei. we present a comprehensive study of the predicted electronic recoil background coming from radioactive decays inside the detector and shield materials and intrinsic radioactivity in the liquid xenon. based on geant4 monte carlo simulations using a detailed geometry together with the measured radioactivity of all detector components, we predict an electronic recoil background in the energy region of interest and 30 kg fiducial mass of less than 10-2 events*kg-1*day-1*kev-1, consistent with the experiment's design goal. the predicted background spectrum is in very good agreement with the data taken during the commissioning of the detector in fall 2009.
coherent cherenkov radiation from cosmic-ray-induced air showers. very energetic cosmic rays entering the atmosphere of earth will create a plasma cloud moving with almost the speed of light. the magnetic field of earth induces an electric current in this cloud which is responsible for the emission of coherent electromagnetic radiation. we propose to search for a new effect: because of the index of refraction of air, this radiation is collimated in a cherenkov cone. to express the difference from usual cherenkov radiation, i.e., the emission from a fast-moving electric charge, we call this magnetically induced cherenkov radiation. we indicate its signature and possible experimental verification.
photon signals from quarkyonic matter. we calculate the bremsstrahlung photon spectrum emitted from dynamically evolving quarkyonic matter, and compare this spectrum with that of a high chemical potential quark-gluon plasma as well as to a hadron gas. we find that the transverse momentum distribution and the harmonic coefficient is markedly different in the three cases. the transverse momentum distribution of quarkyonic matter can be fit with an exponential, but is markedly steeper than the distribution expected for the quark-gluon plasma or a hadron gas, even at the lower temperatures expected in the critical point region. the quarkyonic elliptic flow coefficient fluctuates randomly from event to event, and within the same event at different transverse momenta. the latter effect, which can be explained by the shape of quark wavefunctions within quarkyonic matter, might be considered as a quarkyonic matter signature, provided initial temperature is low enough that the quarkyonic regime dominates over deconfinement effects, and the reaction-plane flow can be separated from the fluctuating component.
recent results on heavy quark quenching in ultrarelativistic heavy ion collisions: the impact of coherent gluon radiation. we present a model for radiative energy loss of heavy quarks in quark gluon plasma which incorporates coherence effects. we then study its consequences on the radiation spectra as well as on the nuclear modification factor of open heavy mesons produced in ultrarelativistic heavy ion collisions.
measurement of j/{psi} azimuthal anisotropy in au+au collisions at {sqrt{s_{nn}}} = 200 gev. the measurement of j/{psi} azimuthal anisotropy is presented as a function of transverse momentum for different centralities in au+au collisions at {sqrt{s_{nn}}} = 200 gev. the measured j/{psi} elliptic flow is consistent with zero within errors for transverse momentum between 2 and 10 gev/c. our measurement suggests that j/{psi} with relatively large transverse momentum are not dominantly produced by coalescence from thermalized charm quarks, when comparing to model calculations.
chiral fluid dynamics with explicit propagation of the polyakov loop. we present a fully dynamical model to study nonequilibrium effects in both the chiral and the deconfinement phase transition. the sigma field and the polyakov loop as the corresponding order parameters are propagated by langevin equations of motion. the locally thermalized background is provided by a fluid of quarks and antiquarks. allowing for an exchange of energy and momentum through dissipative and stochastic processes we ensure that the total energy of the coupled system remains conserved. we study its relaxational dynamics in different quench scenarios and are able to observe critical slowing down as well as the enhancement of long wavelength modes at the critical point. during the fluid dynamical expansion of a hot plasma fireball typical nonequilibrium effects like supercooling and domain formation occur when the system evolves through the first order phase transition.
fluctuations and correlations in polyakov loop extended chiral fluid dynamics. we study nonequilibrium effects at the qcd phase transition within the framework of polyakov loop extended chiral fluid dynamics. the quark degrees of freedom act as a locally equilibrated heat bath for the sigma field and a dynamical polyakov loop. their evolution is described by a langevin equation with dissipation and noise. at a critical point we observe the formation of long-range correlations after equilibration. during a hydrodynamical expansion nonequilibrium fluctuations are enhanced at the first order phase transition compared to the critical point.
the air shower maximum probed by cherenkov effects from radio emission. radio detection of cosmic-ray-induced air showers has come to a flight the last decade. along with the experimental efforts, several theoretical models were developed. the main radio-emission mechanisms are established to be the geomagnetic emission due to deflection of electrons and positrons in earth's magnetic field and the charge-excess emission due to a net electron excess in the air shower front. it was only recently shown that cherenkov effects play an important role in the radio emission from air showers. in this article we show the importance of these effects to extract quantitatively the position of the shower maximum from the radio signal, which is a sensitive measure for the mass of the initial cosmic ray. we also show that the relative magnitude of the charge-excess and geomagnetic emission changes considerably at small observer distances where cherenkov effects apply.
dynamic scalability of a consolidation service. in the coming years, cloud environments will increasingly face energy saving issues. while consolidating the virtual machines running in a cloud is a well-accepted solution to reduce the energy consumption, ensuring the scalability of the consolidation service remains a challenging issue. in this paper, we propose an elastic consolidation service that scales according to the dynamic needs of the cloud environment. our proposition is based on (i) virtualizing the consolidation manager, (ii) partitioning the consolidation work and (iii) regulating the consolidation scalability through an autonomic control loop. our proposition has been tested and validated through several experiments.
relativistic langevin dynamics in expanding media. we study the consequences of different realizations of diffusion processes in relativistic langevin simulations. we elaborate on the ito-stratonovich dilemma by showing how microscopically calculated transport coefficients as obtained from a boltzmann/fokker-planck equation can be implemented to lead to an unambiguous realization of the langevin process. pertinent examples within the pre-point (ito) and post-point (hänggi-klimontovich) langevin prescriptions are worked out explicitly. deviations from this implementation are shown to generate variants of the boltzmann distribution as the stationary (equilibrium) solutions. finally, we explicitly verify how the lorentz invariance of the langevin process is maintained in the presence of an expanding medium, including the case of an "elliptic flow" transmitted to a brownian test particle. this is particularly relevant for using heavy-flavor diffusion as a quantitative tool to diagnose transport properties of qcd matter as created in ultrarelativistic heavy-ion collisions.
azimuthal correlations of heavy quarks in pb + pb collisions at s√=2.76 tev at the cern large hadron collider. in this paper we study the azimuthal correlations of heavy quarks in pb+pb collisions with $\sqrt{s}=2.76$ tev at lhc. due to the interaction with the medium heavy quarks and antiquarks are deflected from their original direction and the initial correlation of the pair is broadened. we investigate this effect for different transverse momentum classes. low-momentum heavy-quark pairs lose their leading order back-to-back initial correlation, while a significant residual correlation survives at large momenta. due to the larger acquired average deflection from their original directions the azimuthal correlations of heavy-quark pairs are broadened more efficiently in a purely collisional energy loss mechanism compared to including radiative corrections. this discriminatory feature survives when next-to-leading order production processes are included.
hypernuclear spectroscopy of products from 6li projectiles on a carbon target at 2 agev. a novel experiment, aiming at demonstrating the feasibility of hypernuclear spectroscopy with heavy-ion beams, was conducted. using the invariant mass method, the spectroscopy of hypernuclear products of 6li projectiles on a carbon target at 2 agev was performed. signals of the \lambda-hyperon and 3\lambda h and 4\lambda h hypernuclei were observed for final states of p+\pi^-, 3he+\pi^- and 4he+\pi^-, respectively, with significance values of 6.7, 4.7 and 4.9\sigma. by analyzing the proper decay time from secondary vertex distribution with the unbinned maximum likelihood fitting method, their lifetime values were deduced to be $262 ^{+56}_{-43} \pm 45$ ps for \lambda, $183 ^{+42}_{-32} \pm 37$ ps for 3\lambda h, and $140 ^{+48}_{-33}\pm 35 $ ps for 4\lambda h.
influence of hadronic bound states above $t_c$ on heavy-quark observables in pb+pb collisions at lhc. we investigate how the possible existence of hadronic bound states above the deconfinement transition temperature $t_c$ affects heavy-quark observables like the nuclear modification factor, the elliptic flow and azimuthal correlations. lattice qcd calculations suggest that above $t_c$ the effective degrees of freedom might not be exclusively partonic but that a certain fraction of hadronic degrees of freedom might already form at higher temperatures. this is an interesting questions by itself but also has a strong influence on other probes of the strongly interacting matter produced in ultrarelativistic heavy-ion collisions. a substantial fraction of hadronic bound states above $t_c$ reduces on average the interaction of the heavy quarks with colored constituents of the medium. we find that all studied observables are highly sensitive to the active degrees of freedom in the quark-gluon plasma.
directed flow of charged particles at mid-rapidity relative to the spectator plane in pb-pb collisions at sqrt{s_nn}=2.76 tev. the directed flow of charged particles at mid-rapidity is measured in pb-pb collisions at sqrt{s_nn}=2.76 tev relative to the collision plane defined by the spectator nucleons. both, the rapidity odd (v_1^odd) and even (v_1^even) directed flow components are reported. the v_1^odd component has a negative slope as a function of pseudorapidity similar to that observed at the highest rhic energy, but with about a three times smaller magnitude. the v^even component is found to be non-zero and independent of pseudorapidity. both components show little dependence on the collision centrality and change sign at transverse momenta around 1.2-1.7 gev/c for midcentral collisions. the shape of v_1^even as a function of transverse momentum and a vanishing transverse momentum shift along the spectator deflection for v_1^even are consistent with dipole-like initial density fluctuations in the overlap zone of the nuclei.
beta decay of fission products for the non-proliferation and decay heat of nuclear reactors. today, nuclear energy represents a non-negligible part of the global energy market, most likely a rolling wheel to grow in the coming decades. reactors of the future must face the criteria including additional economic but also safety, non-proliferation, optimized fuel management and responsible management of nuclear waste. in the framework of this thesis, studies on non-proliferation of nuclear weapons are discussed in the context of research and development of a new potential tool for monitoring nuclear reactors, the detection of reactor antineutrinos, because the properties of these particles may be of interest for the international agency of atomic energy (iaea), in charge of the verification of the compliance by states with their safeguards obligations as well as on matters relating to international peace and security. the iaea encouraged its member states to carry on a feasibility study. a first study of non-proliferation is performed with a simulation, using a proliferating scenario with a candu reactor and the associated antineutrinos emission. we derive a prediction of the sensitivity of an antineutrino detector of modest size for the purpose of the diversion of a significant amount of plutonium. a second study was realized as part of the nucifer project, an antineutrino detector placed nearby the osiris research reactor. the nucifer antineutrino detector is dedicated to non-proliferation with an optimized efficiency, designed to be a demonstrator for the iaea. the simulation of the osiris reactor is developed here for calculating the emission of antineutrinos which will be compared with the data measured by the detector and also for characterizing the level of background noises emitted by the reactor detected in nucifer. in general, the reactor antineutrinos are emitted during radioactive decay of fission products. these radioactive decays are also the cause of the decay heat emitted after the shutdown of a nuclear reactor of which the estimation is an issue of nuclear safety. in this thesis, we present an experimental work which aims to measure the properties of beta decay of fission products important to the non-proliferation and reactor decay heat. first steps using the technique of total absorption gamma-ray spectroscopy (tags) were carried on at the radioactive beam facility of the university of jyvaskyla. we will present the technique used, the experimental setup and part of the analysis of this experiment.
transformations between composite and visitor implementations in java. basic automated refactoring operations can be chained to perform complex structure transformations. this is useful for recovering the initial architecture of a source code which has been degenerated with successive evolutions during its maintenance lifetime. this is also useful for changing the structure of a program so that a maintenance task at hand becomes modular when it would be initially crosscutting. we focus on programs structured according to composite and visitor design patterns, which have dual properties with respect to modularity. we consider a refactoring-based round-trip transformation between these two structures and we study how that transformation is impacted by four variations in the implementation of these patterns. we validate that study by computing the smallest preconditions for the resulting transformations. we also automate the transformation and apply it to jhotdraw, where the studied variations occur.
performance of the alice vzero system. alice is an lhc experiment devoted to the study of strongly interacting matter in proton--proton, proton--nucleus and nucleus--nucleus collisions at ultra-relativistic energies. the alice vzero system, made of two scintillator arrays at asymmetric positions, one on each side of the interaction point, plays a central role in alice. in addition to its core function as a trigger, the vzero system is used to monitor lhc beam conditions, to reject beam-induced backgrounds and to measure basic physics quantities such as luminosity, particle multiplicity, centrality and event plane direction in nucleus-nucleus collisions. after describing the vzero system, this publication presents its performance over more than four years of operation at the lhc.
automated verification of model. transformations in the automotive industry. many companies have adopted mdd for developing their software systems. several studies have reported on such industrial experiences by discussing the effects of mdd and the issues that still need to be addressed. however, only a few studies have discussed using automated verification of industrial model transformations. we previously demonstrated how transformations can be used to migrate gm legacy models to autosar models. in this study, we investigate using automated verification for such industrial transformations. we report on applying an automated verification approach to the gm-to-autosar transformation that is based on checking the satisfiability of a relational transformation representation, or a transformation model, with respect to well-formedness ocl constraints. an implementation of this approach is available as a prototype for the atl language. we present the verification results of this transformation and discuss the practicality of using such tools on industrial size problems.
optimal control with preview for lateral steering of a passenger car: design and test on a driving simulator. this chapter is dedicated to studying the characteristics of the optimal preview control for lateral steering of a passenger vehicle. such control is known to guarantee improved performance when the near future of the exogenous signal, here the road curvature, is known. the synthesis is performed in continuous time and leads to a two-degrees of freedom feedback and feedforward controller, whose feedforward part is a finite impulse response filter. the controller has been implemented on the scanertm driving simulator available at irccyn, whose steering column is electrically powered. a methodology for choosing the weighting matrices in the quadratic index and the preview time are finally proposed. the obtained experimental results are discussed as well.
real-time optimization of reactive technician tours. null
real-time optimization of technician tours in dynamic environment. null
open source erp adoption: a technological innovation perspective. this research-in-progress aims to indentify the salient factors explaining adoption of open source software (oss), as a technological innovation. the theoretical background of the paper is based on the technological innovation literature. we choose to focus on the open erp case, as it is considered as a promising innovation for firms - especially medium firms - but open erp also faces numerous challenges. the paper provides a framework and a method for investigation that has to be implemented.
knowledge sharing in the age of the web 2.0: a social capital perspective. null
new complexity results for parallel identical machine scheduling problems with preemption, release dates and regular criteria. in this paper, we are interested in parallel identical machine scheduling problems with preemption and release dates in case of a regular criterion to be minimized. we show that solutions having a permutation flow shop structure are dominant if there exists an optimal solution with completion times scheduled in the same order as the release dates, or if there is no release date. we also prove that, for a subclass of these problems, the completion times of all jobs can be ordered in an optimal solution. using these two results, we provide new results on polynomially solvable problems and hence refine the boundary between p and np for these problems.
a database for quarkonium and open heavy-flavour production in hadronic collisions with hepdata. we report on the creation of a database for quarkonium and open heavy-flavour production in hadronic collisions. this database, made as a collaboration between hepdata and the retequarkonii network of the integrating activity i3hp2 of the 7th framework programme, provides an up-to-date review on quarkonia and open heavy-flavour existing data. we first present the physics motivation for this project, which is connected to the aim of the retequarkonii network, studies of open heavy-flavour hadrons and quarkonia in nucleus-nucleus collisions. then we give a general overview of the database and describe the hepdata database for particle physics, which is the framework of the quarkonia database. finally we describe the functionalities of the database with as example the comparison of the production cross section for the j/$\psi$ meson at different energies.
study of the prospective usefulness of the detection of antineutrinos to monitor nuclear power plants for non proliferation purpose. the field of applied neutrino physics has shown new developments in the last decade. indeed, the antineutrinos (ne) emitted by a nuclear power plant depend on the composition of the fuel : thus their detection could be exploited for determining the isotopic composition of the reactor fuel. the international atomic energy agency (iaea) has expressed its interest in the potentialities of this detection as a new safeguard tool and has created an ad hoc working group devoted to this study. our aim is to determine on the one hand, the current sensitivity reached with the ne detection and on the other hand, the sensitivity required to be useful to the iaea and then we deduce the required performances required for a cubic meter detector.we will first present the physics on which our feasibility study relies : the neutrinos, the b-decay of the fission products (fp) and their conversion into ne spectra. we will then present our simulation tools : we use a package called mcnp utility for reactor evolution (mure), initially developed by cnrs/in2p3 labs to study generation iv reactors. thanks to mure coupled with nuclear data bases, we build the ne spectra by summing the fp contributions. the method is the only one that allows the ne spectra calculations associated to future reactors : we will present the predicted spectra for various innovative fuels. we then calculate the emitted ne associated to various concepts of current and future nuclear reactors in order to determine the sensitivity of the ne probe to various diversion scenarios, taking neutronics into account. the reactor studied are canadian deuterium, pebble bed reactor and fast breeder reactor.
biogas as a renewable energy source : hydrogen sulfide and siloxanes separation. this work presents a study of the siloxanes and hydrogen sulfide separation process applied to the biogas treatment. a bibliography review shows the interest in the development of new technologies of low cost, to integrate them into an overall process of biogas up-grading. one part of this study is focused on the possible separation process by gas-liquid transfer into oils. this technology is compared with a more classical treatment process by adsorption into activated carbons. the results showed that both technologies are complementary, the absorption into oils used primarily to the abatement of high concentrations and the adsorption into the activated carbon as a finishing process. a second part of this study is focused on the hydrogen sulfide treatment. the requirements of abatement are very low, thus the approach is focused on a finishing process to complement the more classical methods. thus a system by physic adsorption into pre-humidified activated carbon cloth was studied. the filter was regenerated in situ by direct electric heating under vacuum pressure. the study of the operating conditions allowed establishing the regeneration parameters and the process sustainability. the interest here is focused on the soft conditions of temperature and vacuum used to achieve the regeneration.
rotating hyperdeformed quasi-molecular states formed in capture of light nuclei and in collision of very heavy ions. within a rotational liquid drop model including the nuclear proximity energy the l-dependent potential barriers governing the capture reactions of light nuclei and of very heavy ions have been determined. rotating quasi-molecular hyperdeformed states appear at high angular momenta. the energy range of these very deformed high spin states is given for light systems. the same approach explains the observation of ternary cluster decay from56ni and 60zn through hyperdeformed shapes at angular momenta around 45 . the apparently observed superheavy nuclear systems in the u+ni and u+ge reactions at high excitation energy might correspond to these rotating isomeric states formed at very high angular momenta even though the shell effects vanish.
harvesting models from web 2.0 databases. data rather than functionality are the sources of competitive advantage for web2.0 applications such as wikis, blogs and social networking websites. this valuable information might need to be capitalized by third-party applications or be subject to migration or data analysis. model-driven engineering (mde) can be used for these purposes. however, mde ﬁrst requires obtaining models from the wiki/blog/website database (a.k.a. model harvesting). this can be achieved through sql scripts embedded in a program. however, this approach leads to laborious code that exposes the iterations and table joins that serve to build the model. by contrast, a domain-speciﬁc language (dsl) can hide these "how" concerns, leaving the designer to focus on the "what", i.e. the mapping of database schemas to model classes. this paper introduces schemol, a dsl tailored for extracting models out of databases which considers web2.0 specifics. web2.0 applications are often built on top of general frameworks (a.k.a. engines) that set the database schema (e.g.,mediawiki, blojsom). hence, table names offer little help in automating the extraction process. in addition, web2.0 data tend to be annotated. user-provided data (e.g., wiki articles, blog entries) might contain semantic markups which provide helpful hints for model extraction. unfortunately, these data end up being stored as opaque strings. therefore, there exists a considerable conceptual gap between the source database and the target metamodel. schemol offers extractive functions and view-like mechanisms to confront these issues. examples using blojsom as the blog engine are available for download.
non-functional requirements in architectural decision making. null
latitudinal and seasonal variations of o2 and d/h on mars using herschel/hifi. as a non-condensible species, molecular oxygen on mars is expected to show spatial and temporal variations, but these measurements have not been performed yet. in addition, mapping the d/h ratio and recording its seasonal variations is a key diagnostic for understanding the past history of water on mars, as well as surface/atmosphere exchange in the water cycle (montmessin et al. jgr 110, e3, citeid e03006, 2005). we have been using hifi abord herschel to study the latitudinal variations of o2 and d/h on mars for two different seasons, ls = 47° (dec. 23, 2011) and ls = 108-115° (may 09-25, 2012). three sets of transitions have been recorded : h218o and hdo around 1630 ghz, o2 and hdo around 1815 ghz, and 13co and co18 around 1870 ghz. the diameter of mars was 8.3 arcsec on dec. 23, 2011, and 8-9 arcsec in may 2012. the herschel field of view is 11.3 arcsec at 1870 ghz and 9.8 arcsec at 1630 ghz. for each period, three observations were successively recorded, centered along the central meridian, at the south limb, the center and the north limb. the total observing time, over the two periods, was 26 hours. a preliminary reduction indicates a mean o2 abundance in agreement with previous measurements (1400 ppm, hartogh et al. aa 521, id.l49, 2010). no significant variation is observed in o2 and co between north and south for ls = 47°, as expected in the vicinity of equinox (forget et al. lpi-1494, 2009). an analysis of the two data sets will be presented.
d meson elliptic flow in non-central pb-pb collisions at sqrts(s_nn) = 2.76tev. azimuthally anisotropic distributions of d0, d+ and d*+ mesons were studied in the central rapidity region (|y|&lt;0.8) in pb-pb collisions at a centre-of-mass energy sqrts(s_nn) = 2.76tev per nucleon-nucleon collision, with the alice detector at the lhc. the second fourier coefficient v2 (commonly denoted elliptic flow) was measured in the centrality class 30-50% as a function of the d meson transverse momentum pt, in the range 2-16gev/c$. the measured v2 of d mesons is comparable in magnitude to that of light-flavour hadrons. it is positive in the range 2 &lt; pt &lt; 6 gev/c with 5.7 sigma significance, based on the combination of statistical and systematic uncertainties.
bounds on the density of sources of ultra-high energy cosmic rays from the pierre auger observatory. we derive lower bounds on the density of sources of ultra-high energy cosmic rays from the lack of significant clustering in the arrival directions of the highest energy events detected at the pierre auger observatory. the density of uniformly distributed sources of equal intrinsic intensity was found to be larger than $\sim (0.06 - 5) \times 10^{-4}$ mpc$^{-3}$ at 95% cl, depending on the magnitude of the magnetic deflections. similar bounds, in the range $(0.2 - 7) \times 10^{-4}$ mpc$^{-3}$, were obtained for sources following the local matter distribution.
practical use of static composition of refactoring operations. refactoring tools are commonly used for remodularization tasks. basic refactoring operations are combined to perform complex program transformations, but the resulting composed operations are rarely reused, even partially, because popular tools have few support for composition. in this paper, we recast two calculus for static composition of refactorings in a type system framework and we discuss their use for inferring useful properties. we illustrate the value of support for static composition in refactoring tools with a complex remodularization use case: a round-trip transformation between programs conforming to the composite and visitor patterns.
charmonium and e+e- pair photoproduction at mid-rapidity in ultra-peripheral pb-pb collisions at sqrt(snn) = 2.76 tev. the alice collaboration at the lhc has measured the j/psi and psi' photoproduction at mid-rapidity in ultra-peripheral pb-pb collisions at sqrt(snn) = 2.76 tev. the charmonium is identified via its leptonic decay for events where the hadronic activity is required to be minimal. the analysis is based on an event sample corresponding to an integrated luminosity of about 23 {\mu}b^{-1}. the cross section for coherent and incoherent j/psi production in the rapidity interval -0.9 &lt; y &lt; 0.9, are d{\sigma}_{j/{\psi}}^{coh}/dy = 2.38^{+0.34}_{-0.24}(sta+sys) mb and d{\sigma}_{j/{\psi}}^{inc}/dy = 0.98^{+0.19}_{-0.17}(sta+sys) mb, respectively. the results are compared to theoretical models for j/{\psi} production and the coherent cross section is found to be in good agreement with those models which include nuclear gluon shadowing consistent with eps09 parametrization. in addition the cross section for the process {\gamma}{\gamma} -&gt; e+e- has been measured and found to be in agreement with the starlight monte carlo predictions.
mid-rapidity anti-baryon to baryon ratios in pp collisions at sqrt(s) = 0.9, 2.76 and 7 tev measured by alice. the ratios of yields of anti-baryons to baryons probes the mechanisms of baryon-number transport. results for anti-p/p, anti-\lambda/\lambda, anti-\xi+/\xi- and anti-\omega+/\omega- in pp collisions at sqrt(s) = 0.9, 2.76 and 7tev, measured with the alice detector at the lhc, are reported. within the experimental uncertainties and ranges covered by our measurement, these ratios are independent of rapidity, transverse momentum and multiplicity for all measured energies. the results are compared to expectations from event generators, such as pythia and hijing/b, that are used to model the particle production in pp collisions. the energy dependence of anti-p/p, anti-\lambda/\lambda, anti-\xi+/\xi- and anti-\omega+/\omega-, reaching values compatible with unity for sqrt(s) = 7tev, complement the earlier anti-p/p measurement of alice. these dependencies can be described by exchanges with the regge-trajectory intercept of \alpha_j ~ 0.5, which are suppressed with increasing rapidity interval \delta y. any significant contribution of an exchange not suppressed at large \delta y (reached at lhc energies) is disfavoured.
correlated background and impact on the measurement of theta_13 with the double chooz detector. the double chooz experiment uses antineutrinos emitted from the chooz nuclear power plant (france) to measure the oscillation mixing parameter θ13. by using two detectors at different baselines, a precise measurement of antineutrinos disappearance is anticipated. the far detector has been taking physics data since april 2011, while the near detector is under construction. data from april 13th 2011 to march 30th 2012 taken with the far detector only have been analyzed and an indication for antineutrino disappearance, consistent with the current neutrino oscillation hypothesis, has been found. the best fit value for the neutrino mixing parameter sin2(2θ13) is 0.109 ± 0.030(stat.) ± 0.025(syst.). this thesis present an accurate description of the double chooz experiment, with particular emphasis on the far detector and its acquisition system. the main focus of the thesis is the accurate study of the correlated background affecting the double chooz antineutrinos sample and its impact on the measurement of the mixing parameter θ13. a general overview of the current experimental scenario which aim to the characterization of the neutrino oscillation is also provided, focusing on the recent results obtained in this field.
the impact of dissipation and noise on fluctuations in chiral fluid dynamics. we investigate the nonequilibrium evolution of the sigma field coupled to a fluid dynamic expansion of a hot fireball to model the chiral phase transition in heavy-ion collisions. the dissipative processes and fluctuations are allowed under the assumption that the total energy of the coupled system is conserved. we use the linear sigma model with constituent quarks to investigate the effects of the chiral phase transition on the equilibration and excitation of the sigma modes. the quark fluid acts as a heat bath in local thermal equilibrium and the sigma field evolves according to a semiclassical stochastic langevin equation of motion. the effects of supercooling and reheating of the fluid in a first order phase transition are observed via the delayed relaxation of the sigma field to a new equilibrium state. at the first order phase transition the nonequilibrium fluctuations are strongly enhanced.
experimental study of anguilliform swimming : application to a biomimetic robot. in order to improve the performance of the submarine robots, the robotics community has been considered a new approach known as the biomimetic. it consist on the study of a living systems such, fish, to design and construct a bio-inspired robot. in this context, recently was took place an european project called angels, in which the objective is to design and construct a fish-like robot inspired from the swimming of the eel. this thesis takes place in this project and is dedicate to the study of the swimming of the robot. experiments were carried out in a hydrodynamic test bed designed and entirely set up for this study. at first,the kinematic shapes (i.e. deformation of the body) adopted by living eel during its swimming against or slantwise a uniform flow, were characterized by mean ofan image processing analysis technique. this study has allowed the establishing of a mathematical correlative model, describing the deformation of the eel's body in these swimming conditions. secondly, we studied the effects of the body's deformation on the lateral flow produced during swimming. piv experiences were carried out on different elliptic cylinder shapes. these experiments have allowed the understanding and the validation of a theoretical approach, concerning the swimming dynamic of the fish, used to obtain the propulsion force produced in reply of the body deformation during swimming. finally, experiments were carried out during the anguilliform swimming in a non-uniform flow such as, avon-kàrmàn vortex street. the goal was to study the hydrodynamics interactions and in particular the mechanisms of the exploited vortices adopted by fish. these experiences were realized on the swimming of a living eel and an anguilliform robot. experiments led on the robot show that under certain conditions, the propulsive force of the robot swimming in a von-kàrmàn vortex street can be increased of about 30 comparison to its swimming in a uniform flow. experiments with eel have allowed the highlighting of a particular shape of its body deformation formed when it's swimming in a reverse von-kàrmàn vortex street. the qualitative analysis realized on this kinematic observation led us to propose a mechanism adopted by the eel to exploited energy from altered flow.
toward cooperative management of large-scale virtualized infrastructures : the case of scheduling. the increasing need in computing power has been satisfied by federating more and more computers (called nodes) to build the so-called distributed infrastructures. over the past few years, system virtualization has been introduced in these infrastructures (the software is decoupled from the hardware by packaging it in virtual machines), which has lead to the development of software managers in charge of operating these virtualized infrastructures. most of these managers are highly centralized (management tasks are performed by a restricted set of dedicated nodes). as established, this restricts the scalability of managers, in other words their ability to be reactive to manage large-scale infrastructures, that are more and more common. during this ph.d., we studied how to mitigate these concerns ; one solution is to decentralize the processing of management tasks, when appropriate. our work focused in particular on the dynamic scheduling of virtual machines, resulting in the dvms (distributed virtual machine scheduler) proposal. we implemented a prototype, that was validated by means of simulations (especially with the simgrid tool) and with experiments on the grid'5000 test bed. we observed that dvms was very reactive to schedule tens of thousands of virtual machines distributed over thousands of nodes. we then took an interest in the perspectives to improve and extend dvms. the final goal is to build a full decentralized manager. this goal should be reached by the discovery initiative,that will leverage this work.
anisotropic flow of charged hadrons, pions and (anti-)protons measured at high transverse momentum in pb-pb collisions at root s(nn)=2.76 tev. the elliptic, v2v2, triangular, v3v3, and quadrangular, v4v4, azimuthal anisotropic flow coefficients are measured for unidentified charged particles, pions, and (anti-)protons in pb-pb collisions at view the mathml sourcesnn=2.76 tev with the alice detector at the large hadron collider. results obtained with the event plane and four-particle cumulant methods are reported for the pseudo-rapidity range |η|&lt;0.8|η|&lt;0.8 at different collision centralities and as a function of transverse momentum, ptpt, out to pt=20 gev/cpt=20 gev/c. the observed non-zero elliptic and triangular flow depends only weakly on transverse momentum for pt&gt;8 gev/cpt&gt;8 gev/c. the small ptpt dependence of the difference between elliptic flow results obtained from the event plane and four-particle cumulant methods suggests a common origin of flow fluctuations up to pt=8 gev/cpt=8 gev/c. the magnitude of the (anti-)proton elliptic and triangular flow is larger than that of pions out to at least pt=8 gev/cpt=8 gev/c indicating that the particle type dependence persists out to high ptpt. the goal of ultra-relativistic nucleus-nucleus collisions is to study nuclear matter under extreme conditions. for non-central collisions, in the plane perpendicular to the beam direction, the geometrical overlap region, where the highly lorentz contracted nuclei intersect and where the initial interactions occur, is azimuthally anisotropic. this initial spatial asymmetry is converted via interactions into an anisotropy in momentum space, a phenomenon referred to as transverse anisotropic flow (for a review see [1]). anisotropic flow has become a key observable for the characterization of the properties and the evolution of the system created in a nucleus-nucleus collision. identified particle anisotropic flow provides valuable information on the particle production mechanism in different transverse momentum, ptpt, regions [1]. for pt&lt;2-3 gev/cpt&lt;2-3 gev/c, the flow pattern of different particle species is qualitatively described by hydrodynamic model calculations [2]. at intermediate ptpt, 3.
location of concentration warehouses in a pooled distribution network in the retail sector. null
analyzing flowgraphs with atl. this paper presents a solution to the flowgraphs case study for the transformation tool contest 2013 (ttc 2013). starting from java source code, we execute a chain of model transformations to derive a simplifi ed model of the program, its control flow graph and its data flow graph. finally we develop a model transformation that validates the program flow by comparing it with a set of flow specifi cations written in a domain speci c language. the proposed solution has been implemented using atl.
enabling the collaborative definition of dsmls. software development processes are collaborative in nature. neglecting the key role of end-users leads to software that does not satisfy their needs. this collaboration becomes specially important when creating domain-specific modeling languages (dsmls), which are (modeling) languages specifically designed to carry out the tasks of a particular domain. while end-users are actually the experts of the domain for which a dsml is developed, their participation in the dsml specification process is still rather limited nowadays. in this paper we propose a more community-aware language development process by enabling the active participation of all community members (both developers and end-users of the dsml) from the very beginning. our proposal is based on a dsml itself, called collaboro, which allows representing change proposals on the dsml design and discussing (and tracing back) possible solutions, comments and decisions arisen during the collaboration.
behind the scenes in sante: a combination of static and dynamic analyses. while the development of one software verification tool is often seen as a difficult task, the realization of a tool combining various verification techniques is even more complex. this paper presents an innovative tool for verification of c programs called sante (static analysis and testing). we show how several tools based on heterogeneous techniques such as abstract interpretation, dependency analysis, program slicing, constraint solving and test generation can be combined within one tool. we describe the integration of these tools and discuss particular aspects of each underlying tool that are beneficial for the whole combination.
advanced validation of the dvms approach to fully distributed vm scheduling. the holy grail for infrastructure as a service (iaas) providers is to maximize the utilization of their infrastructure while ensuring the quality of service (qos) for the virtual machines they host. although the frameworks in charge of managing virtual machines (vm) on pools of physical ones (pm) have been significantly improved, enabling to manage large- scale infrastructures composed of hundreds of pms, most of them do not efficiently handle the aforementioned objective. the main reason is that advanced scheduling policies are subject to important and hard scalability problems, that become even worse when vm image transfers have to be considered. in this article, we provide a new validation of the distributed vm scheduler approach (dvms) in a twofold manner. first, we provide a formal proof of the algorithm based on temporal logic. second, we discuss large-scale evaluations involving up to 4.7k vms distributed over 467 nodes of the grid'5000 testbed. as far as we know, these experiments constitute the largest in vivo validation that has been performed so far with decentralized vm schedulers. these results show that a cooperative approach such as ours permits to fix overload problems in a reactive and scalable way.
nogood-based asynchronous forward checking algorithms. we propose two new algorithms for solving distributed constraint satisfaction problems (discsps). the first algorithm, afc-ng, is a nogood-based version of asynchronous forward checking (afc). besides its use of nogoods as justification of value removals, afc-ng allows simultaneous backtracks going from different agents to different destinations. the second algorithm, asynchronous forward checking tree (afc- tree), is based on the afc-ng algorithm and is performed on a pseudo-tree ordering of the constraint graph. afc-tree runs simultaneous search processes in disjoint problem subtrees and exploits the parallelism inherent in the problem. we prove that afc-ng and afc-tree only need polynomial space. we compare the performance of these algorithms with other discsp algorithms on random discsps and instances from real benchmarks: sensor networks and distributed meeting scheduling. our experiments show that afc-ng improves on afc and that afc-tree outperforms all compared algorithms, particularly on sparse problems.
a generic formulation and a memetic algorithm for the hub location-routing problem. in many logistic systems for less than truckload (ltl) shipments, transportation of goods from one origin to its destination is made through collection tours to a hub and delivery tours from the same or another hub, while the goods are shipped between two hubs using full truckload (ftl) shipments. the hub location-routing problem (hlrp), consists in determining the location of the hubs, the allocation of non-hub nodes, and the optimal collection and delivery routes within the network. the proposed mathematical model is a mip based on the following assumptions: each supplier and customer is allocated to one hub; each vehicle route begins and ends at the same hub; vehicles are capacitated; and collection/delivery routes are distinguished. based on different hypothesis for the hubs, we develop several variations of the model: fixed cost hlrp and p-hlrp with and without capacity. in these models, there are three main variables to determine: hub location variables, inter-hub flow variables and routing variables. the objective function minimizes the total fixed cost and transportation costs. we propose a memetic algorithm composed of a genetic algorithm (ga) combined with some local searches. computational experiments are presented based on some randomly generated data sets and australian post data sets.
a state of the art and a general formulation model of hub location-routing problems for ltl shipments. in many logistic systems for less than truckload (ltl) shipments, transportation of goods from one origin to its destination is made through collection tours to a hub and delivery tours from the same or another hub, while the goods are shipped between two hubs using full truckload (ftl) shipments. therefore, managers need to determine the location of the hubs, the allocation of non-hub nodes, and the optimal collection and delivery routes within the network. this problem, known as the hub location-routing problem (hlrp), is related to both the hub location problem (hlp) and the location-routing problem (lrp). the hlp involves the location of hub facilities concentrating flows in order to take advantage of economies of scale and through which flows are to be routed from origins to destinations. the objective of the hlrp is to minimize the total costs including hub costs, inter-hub transportation costs, and collection/distribution routing costs. based on the literatures review, the aims of this paper are to analyze the state of the art, propose some generic mathematical models for the hlrp and implement some tests using a mip solver.
tactical planning of procurement of a supply chain with environmental concern. in this paper, we study the economic and environmental aspects of the tactical procurement planning of an industrial plant within a supply chain, corresponding to a real case of a european company. a procurement plan is obtained by solving an integer linear programming model. two objectives are used: the first is to minimize the total cost including transportation, handling, and inventory holding cost, while the second is to minimize the co2 emissions from transport taking into account the distance and the loads of the vehicles. experimentations performed on the basis of real industrial data validate the approach and allow the analysis of tradeoffs between the two objectives.
an exact algorithm and a metaheuristic for the multi-vehicle covering tour problem with a constraint on the number of vertices. the multi-vehicle covering tour (m-ctp) involves finding a minimum-length set of vehicle routes passing through a subset of vertices, subject to capacity constraints on the length of each route and the number of vertices that it contains, such that each vertex not included in any route lies within a given distance of a route. this paper tackles a particular case of m-ctp where only the restriction on the number of vertices is considered, i.e., the constraint on the length is relaxed. the problem is solved by a branch-and-cut algorithm and a metaheuristic. to develop the branch-and-cut algorithm, we use a new integer programming formulation based on a two-commodity flow model. the metaheuristic is based on the evolutionary local search (els) method proposed in [20]. computational results are reported for a set of test problems derived from the tsplib.
exact and hybrid methods for the multi-period field service routing problem. this article deals with a particular class of routing problem, consisting of the planning and routing of technicians in the eld. this problem has been identi ed as a multiperiod, multidepot uncapacitated vehicle routing problem with speci c constraints that we call the multiperiod field service routing problem (mpfsrp). we propose a set covering formulation of the problem for the column generation technique and we develop an exact branch and price solution method for small-sized instances. we also propose several heuristic versions for larger instances. we present the results of experiments on realistic data adapted from an industrial application.
radio-détection of ultra-high energy cosmic rays. analysis, simulation and interpretation. despite the use of giant detectors suitable for low flux beyond 1018 ev, the origin of ultra energy cosmic rays, remains unclear. in the 60', the radiodetection of air shower is proposed as a complementary technique to the ground particle detection and to the fluorescence method. a revival of this technique took place in the 2000s in particular with codalema experiment. the first results show both a strong dependence of the signal to the geomagnetic field and a strong correlation between energy estimated by the radiodetectors and by particle detectors. the new generation of autonomous detectors created by the codalema collaboration indicates that it is now possible to detect air showers autonomously. due to the expected performances (a nearly 100% duty cycle, a signal generated by the complete shower, simplicity and low cost of a detector), it is possible to consider to deploy this technique for the future large arrays. in order to interpret experimental data, a simulation tool, selfas, is developed in this wok. this simulation code allowed us to highlight the existence of a second radioemission mechanism. a first interpretation of the longitudinal profile as an observable of a privileged instant of the shower development is also proposed, which could give an estimation of the nature of the primary.
initiating a benchmark for uml and ocl analysis tools. the object constraint language (ocl) is becoming more and more popular for model-based engineering, in particular for the development of models and model transformations. ocl is supported by a variety of analysis tools having different scopes, aims and technological corner stones. the spectrum ranges from treating issues concerning formal proof techniques to testing approaches, from validation to verification, and from logic programming and rewriting to sat-based technologies. this paper is a first step towards a well-founded benchmark for assessing validation and verification techniques on uml and ocl models. the paper puts forward a set of uml and ocl models together with particular questions for these models roughly characterized by the notions consistency, independence, consequences, and reachability. the paper sketches how these questions are handled by two ocl tools, use and emftocsp. the claim of the paper is not to present a complete benchmark right now. the paper is intended to initiate the development of further uml and ocl models and accompanying questions within the uml and ocl community. the ocl community is invited to check the presented uml and ocl models with their approaches and tools and to contribute further models and questions which emphasize the possibilities offered by their own tools.
checking model transformation refinement. refinement is a central notion in computer science, meaning that some artefact s can be safely replaced by a refinement r, which preserves s's properties. having available techniques and tools to check transformation refinement would enable (a) the reasoning on whether a transformation correctly implements some requirements, (b) whether a transformation implementation can be safely replaced by another one (e.g. when migrating from qvt-r to atl), and (c) bring techniques from stepwise refinement for the engineering of model transformations. in this paper, we propose an automated methodology and tool support to check transformation refinement. our procedure admits heterogeneous specification (e.g. pamomo, tracts, ocl) and implementation languages (e.g. atl, qvt), relying on their translation to ocl as a common representation formalism and on the use of model finding tools.
lightweight string reasoning in model finding. models play a key role in assuring software quality in the model-driven approach. precise models usually require the definition of well-formedness rules to specify constraints that cannot be expressed graphically. the object constraint language (ocl) is a de-facto standard to define such rules. techniques that check the satisfiability of such models and find corresponding instances of them are important in various activities, such as model-based testing and validation. several tools for these activities have been developed, but to our knowledge, none of them supports ocl string operations on scale that is sufficient for, e.g., model-based testing. as, in contrast, many industrial models do contain such operations, there is evidently a gap. we present a lightweight solver that is specifically tailored to generate large solutions for tractable string constraints in model finding, and that is suitable for directly express the main operations of the ocl datatype string. it is based on constraint logic programming (clp) and constraint handling rules (chr), and can be seamlessly combined with other constraint solvers in clp. we have integrated our solver into the emftocsp model finder, and we show that our implementation efficiently solves several common string constraints on a large instances.
ultra-high energy cosmic rays: analysis of extensive air showers and their associated electromagnetic signal in the mhz domain. null
model-driven standardization of public authority data interchange. in the past decade, several electronic data exchange processes between public authorities have been established by the german public administration. in the context of various legacy systems and numerous suppliers of software for public authorities, it is crucial that these interfaces are open and precisely and uniformly defined, in order to foster free competition and interoperability. a community of such projects and specifications for various public administration domains has arisen from an early adopter project in the domain of data interchange between the 5,400 german municipal citizen registers. a central coordination office provides a framework for these projects that is put into operation by a unified model-driven method, supported by tools and components, involving uml profiles, model validation, and model-to-text transformations into several technical domains. we report how this model-driven approach has already proven to be effective in a number of projects, and how it could contribute to the development of standardized e-government specifications in various ways.
on spectral assignment for neutral type systems. for a large class of linear neutral type systems the problem of eigenvalues and eigenvectors assignment is investigated, i.e. finding the system which has the given spectrum and almost all, in some sense, eigenvectors.
a constraint-based approach for the shift design personnel task scheduling problem with equity. null
approximations for the two-machine cross-docking flow shop problem. we consider in this article the two-machine cross-docking flow shop problem, which is a special case of scheduling with typed tasks, where we have two types of tasks and one machine per type. precedence constraints exist between tasks, but only from a task of the first type to a task of the second type. the precedence relation is thus a directed bipartite graph. minimizing the makespan is strongly np-hard even with unit processing times, but any greedy method yields a 2-approximation solution. in this paper, we are interested in establishing new approximability results for this problem. more specifically, we investigate three directions: list scheduling algorithms based on the relaxation of the resources, the decomposition of the problem according to the connected components of the precedence graph, and finally the search of the induced balanced subgraph with a bounded degree.
energy recovery of ccb-treated wood using thermo chemical processes (pyrolysis and hydroliquefaction) : application to ccb-treated wood. the amount of treated-wood waste was estimated to 27% of the deposit of hazardous waste in france. these wastes are incinerated in specials incineration plants "installations classées pour la protection de l'environnement". however, incineration produces harmful residues and contaminated gases released into the atmosphere inevitably. in this context, this work aims to develop and implement other ways of energy recovery from treated-wood waste using thermo-chemical processes. for this, the pyrolysis and hydroliquefaction processes were performed for energy recovery from ccb treated wood waste (copper-chromium-boron) representing 8750 t / year in france. natural wood were impregnated with salts of ccb in our laboratories according to industrial processing to control the balance of metals in pyrolysis and hydroliquefaction products. a preliminary study was carried out by thermogravimetric analysis in order to determine the temperature range for effective mass degradation of ccb treatedwood. in this temperature range, the experimental parameters of slow pyrolysis have been optimized to concentrate metals in charcoal. subsequently, a parametric study was conducted by the method of experimental design for the conversion of coal into bio-oil. in addition, the optimization of the conversion hydroliquefaction treated wood into bio-oil was performed. the results show that the metals initially present in the treated wood are divided between the bio-oil and coke whatever the processes energy recovery used (hydroliquefaction / pyrolysis + hydroliquefaction). however, the immediate characteristics of bio-oil and biodiesel are quite similar. the use of catalyst during charcoal conversion improves the quality of the bio-oil but also the energy balance of the process.
an iterated local search heuristic for multi-capacity bin packing and machine reassignment problems. this paper proposes an efficient multi-start iterated local search for packing problems (ms-ils-pps) metaheuristic for multi-capacity bin packing problems (mcbpp) and machine reassignment problems (mrp). the mcbpp is a generalization of the classical bin-packing problem in which the machine (bin) capacity and task (item) sizes are given by multiple (resource) dimensions. the mrp is a challenging and novel optimization problem, aimed at maximizing the usage of available machines by reallocating tasks/processes among those machines in a cost-efficient manner, while fulfilling several capacity, conflict, and dependency-related constraints. the proposed ms-ils-pp approach relies on simple neighborhoods as well as problem-tailored shaking procedures. we perform computational experiments on mrp benchmark instances containing between 100 and 50,000 processes. near-optimum multi-resource allocation and scheduling solutions are obtained while meeting specified processing-time requirements (on the order of minutes). in particular, for 9/28 instances with more than 1000 processes, the gap between the solution value and a lower bound measure is smaller than 0.1%. our optimization method is also applied to solve classical benchmark instances for the mcbpp, yielding the best known solutions and optimum ones in most cases. in addition, several upper bounds for non-solved problems were improved.
a non optimization-based method for reconstructing wind instruments bore shape. the presented work is concerned with a prospective method towards the reconstruction of the bore shape of wind instruments. one contemplated application is to the study of ancient wind instruments. the so-called 'horn equation' is well-known for modelling propagation in variable section bores of wind instruments. apart from strict modelling purposes, it has been used together with several optimization methods for the reconstruction of bore shapes of wind instruments. this work investigates properties of a quadratic invariant of second order linear differential equations of the sturm-liouville type. the question of reconstructing the potential appearing in such an equation thanks to this invariant is then applied to the horn wave equation, amounting to the bore shape reconstruction from external measurements that at this stage, are supposed to be feasible. contrary to most of the approaches known to the author, the method presented here does not rely on optimization. on another side, one pending question, thus critical to the method, is that of the measurements that should be made in order to make this approach effective in practice. this should motivate discussions within the conference audience. as this research is prospective, no case study will be presented.
ultrahigh energy neutrinos at the pierre auger observatory. the observation of ultrahigh energy neutrinos (uheνs) has become a priority in experimental astroparticle physics. uheνs can be detected with a variety of techniques. in particular, neutrinos can interact in the atmosphere (downward-going ν) or in the earth crust (earth-skimming ν), producing air showers that can be observed with arrays of detectors at the ground. with the surface detector array of the pierre auger observatory we can detect these types of cascades. the distinguishing signature for neutrino events is the presence of very inclined showers produced close to the ground (i.e., after having traversed a large amount of atmosphere). in this work we review the procedure and criteria established to search for uheνs in the data collected with the ground array of the pierre auger observatory. this includes earth-skimming as well as downward-going neutrinos. no neutrino candidates have been found, which allows us to place competitive limits to the diffuse flux of uheνs in the eev range and above.
optimization of nitrogen and phosphorus removal in constructed wetland. nutrient (phosphorus and nitrogen) discharges from wastewater lead to water quality degradation (74 % of the total french territory in 2006). according to the actual situation, the french government has adopted a water framework on december 30, 2006 to achieve a"good ecological status of water" in 2015. therefore,more stringent standards on nutrients removal for raw wastewater treatment plants are expected (down to 15mg tn.l-1 and 2 mg tp.l-1). however, standards actually remain under the responsibility of departmental authorities according to the water quality of the receiving environment. since 1990s, vertical flow constructed wetlands(vfcws) have been more and more popular in treating raw domestic wastewater for small collectivities of less than 2000 person equivalent (&gt; 2500 units in 2012). however, nitrogen and phosphorus removal are limited in vfcws (70-80 mg tn.l-1; &gt; 10 mg tp.l-1) according to the new legislation. the aim of this work was on the one hand to implement a recirculation of treated effluent on a vfcw for better nitrogen removal and on the other to use reactive materials to improve phosphorus removal. the experimental scientific approach consisted in monitoring both laboratory and field pilot scale systems under process conditions during 2 years. treatment and hydraulic performances were monitored over time. two vfcws (2,5 m²) filled with expanded schist (mayennite®) were fed with raw domestic wastewater. the effect of a saturated layer and of the recirculation of treated effluent were studied. results showed that 38 % of a saturated layer and 100 % of recirculation enabled to improve nitrogen treatment performances (&lt; 20 mgtkn.l-1; &lt; 45 mg tn.l-1) and to meet the french standard d4. electric arc furnace slags were selected as reactive materials to improve phosphorus treatment performance of the vfcws in laboratory and field pilot scale systems. five laboratory-scale column experiments (6l) were fed with a phosphorus synthetic effluent then with a secondary effluent. four horizontal subsurface flow active filters (0,3 m²; 34 l) were fed with a secondary effluent. the main results showed : (i)differences exist between laboratory and pilot scale regarding treatment performance (&lt; 2 mg p.l-1 during20 months in the laboratory ; seasonal variations at pilot scale) and removal mechanisms (adsorption/precipitation ca-p in laboratory ; ca-p + fe-p at pilot scale), (ii) an increase of temperature (&gt; 15°c) and/or hydraulic retention time (more than 2 days) improved the kinetics of phosphorus removal at pilot scale, (iii) the active filter implementing is limited by the discharge standard required (&gt; 3 mg ptot.l-1) and the distance to the steel factory (transport costs).
taming aspects with monads and membranes. when a software system is developed using several aspects, special care must be taken to ensure that the resulting behavior is correct. this is known as the aspect interference problem, and existing approaches essentially aim to detect whether a system exhibits problematic interferences of aspects. in this paper we describe how to control aspect interference by construction by relying on the type system. more precisely, we combine a monadic embedding of the pointcut/advice model in haskell with the notion of membranes for aspect-oriented programming. aspects must explicitly declare the side effectsa nd the context they can act upon. allowed patterns of control flow interference are declared at the membrane level and statically enforced. finally, computational interference between aspects is controlled by the membrane topology. to combine independent and reusable aspects and monadic components into a program specification we use monad views, a recent technique for conveniently handling the monadic stack.
advances in quark gluon plasma. in the last 20 years, heavy-ion collisions have been a unique way to study the hadronic matter in the laboratory. its phase diagram remains unknown, although many experimental and theoretical studies have been undertaken in the last decades. the relativistic heavy ion collider (rhic) at bnl was the first ever built heavy-ion collider. rhic delivered its first collisions in june 2000 boosting the heavy-ion community. impressive amount of experimental results has been obtained. in november 2010, the large hadron collider (lhc) at cern delivered lead-lead collisions at unprecedented center-of-mass energies, 14 times larger than that at rhic. needless to say that the heavy-ion programs at rhic and lhc promise fascinating and exciting results in the next decade. in the second part, a historical approach will be adopted, starting with the notion of limiting temperature of matter introduced by hagedorn in the 60's and the discovery of the qcd asymptotic freedom in the 70's. the phase diagram of hadronic matter, conceived as nowadays, will be shown together with the most important predictions of lattice qcd calculations at finite temperature. in the third part, the heavy-ion collisions at ultra-relativistic energies will be proposed as a unique experimental method to study qgp in the laboratory, as suggested by the bjorken model. in the last part of these lectures, i will present my biased review of the numerous experimental results obtained in the last decade at rhic which lead to the concept of strong interacting qgp, and the first results obtained at lhc with the 2010 and 2011 pbpb runs. finally, the last section is devoted to refer to other lectures about quark gluon plasma and heavy ion physics.
third harmonic flow of charged particles in au+au collisions at sqrtsnn = 200 gev. we report measurements of the third harmonic coefficient of the azimuthal anisotropy, v_3, known as triangular flow. the analysis is for charged particles in au+au collisions at $\sqrtsnn = 200$ gev, based on data from the star experiment at the relativistic heavy ion collider. two-particle correlations as a function of their pseudorapidity separation are fit with narrow and wide gaussians. measurements of triangular flow are extracted from the wide gaussian, from two-particle cumulants with a pseudorapidity gap, and also from event plane analysis methods with a large pseudorapidity gap between the particles and the event plane. these results are reported as a function of transverse momentum and centrality. a large dependence on the pseudorapidity gap is found. results are compared with other experiments and model calculations.
observation of an energy-dependent difference in elliptic flow between particles and anti-particles in relativistic heavy ion collisions. elliptic flow ($v_{2}$) values for identified particles at mid-rapidity in au+au collisions, measured by the star experiment in the beam energy scan at rhic at $\sqrt{s_{nn}}=$ 7.7--62.4 gev, are presented. a beam-energy dependent difference of the values of $v_{2}$ between particles and corresponding anti-particles was observed. the difference increases with decreasing beam energy and is larger for baryons compared to mesons. this implies that, at lower energies, particles and anti-particles are not consistent with the universal number-of-constituent-quark (ncq) scaling of $v_{2}$ that was observed at $\sqrt{s_{nn}}=$ 200 gev.
elliptic flow of identified hadrons in au+au collisions at $\sqrt{s_{nn}}=$ 7.7--62.4 gev. measurements of the elliptic flow, $v_{2}$, of identified hadrons ($\pi^{\pm}$, $k^{\pm}$, $k_{s}^{0}$, $p$, $\bar{p}$, $\phi$, $\lambda$, $\bar{\lambda}$, $\xi^{-}$, $\bar{\xi}^{+}$, $\omega^{-}$, $\bar{\omega}^{+}$) in au+au collisions at $\sqrt{s_{nn}}=$ 7.7, 11.5, 19.6, 27, 39 and 62.4 gev are presented. the measurements were done at mid-rapidity using the time projection chamber and the time-of-flight detectors of the star experiment during the beam energy scan program at rhic. a significant difference in the $v_{2}$ values for particles and the corresponding anti-particles was observed at all transverse momenta for the first time. the difference increases with decreasing center-of-mass energy, $\sqrt{s_{nn}}$ (or increasing baryon chemical potential, $\mu_{b}$) and is larger for the baryons as compared to the mesons. this implies that particles and anti-particles are no longer consistent with the universal number-of-constituent quark (ncq) scaling of $v_{2}$ that was observed at $\sqrt{s_{nn}}=$ 200 gev. however, for the group of particles ncq scaling at $(m_{t}-m_{0})/n_{q}&gt;$ 0.4 gev/$c^{2}$ is not violated within $\pm$10%. the $v_{2}$ values for $\phi$ mesons at 7.7 and 11.5 gev are approximately two standard deviations from the trend defined by the other hadrons at the highest measured $p_{t}$ values.
system-size dependence of transverse momentum correlations at √snn=62.4 and 200 gev at the bnl relativistic heavy ion collider. we present a study of the average transverse momentum ($p_t$) fluctuations and $p_t$ correlations for charged particles produced in cu+cu collisions at midrapidity for $\sqrt{s_{nn}} =$ 62.4 and 200 gev. these results are compared with those published for au+au collisions at same energies, to explore the system size dependence. in addition to the collision energy and system size dependence, the $p_t$ correlations results have been studied as functions of the collision centralities, the ranges in $p_t$, the pseudo-rapidity $\eta$, and the azimuthal angle $\phi$, for which the correlations are measured. the square root of the measured $p_t$ correlations when scaled by mean-$p_t$ are found to be independent of both colliding beam energy and system size studied. the transport based model calculations are found to have a better quantitative agreement with the measurements compared to models which incorporate only jet-like correlations.
freeze-out dynamics via charged kaon femtoscopy in sqrt(snn)=200 gev central au+au collisions. we present measurements of three-dimensional correlation functions of like-sign low transverse momentum kaon pairs from sqrt(snn)=200 gev au+au collisions. a cartesian surface-spherical harmonic decomposition technique was used to extract the kaon source function. the latter was found to have a three-dimensional gaussian shape and can be adequately reproduced by therminator event generator simulations with resonance contributions taken into account. compared to the pion one, the kaon source function is generally narrower and does not have the long tail along the pair transverse momentum direction. the kaon gaussian radii display a monotonic decrease with increasing transverse mass m_t over the interval of 0.55&lt;=m_t&lt;=1.15 gev/c^2. while the kaon radii are adequately described by the m_t-scaling in the outward and sideward directions, in the longitudinal direction the lowest m_t value exceeds the expectations from a pure hydrodynamical model prediction.
fluctuations of charge separation perpendicular to the event plane and local parity violation in sqrt(snn)=200 gev au+au collisions at rhic. recent experimental results from the star collaboration suggest event-by-event charge separation fluctuations perpendicular to the event plane in non-central heavy-ion collisions. here we present the correlator previously used split into its two component parts to reveal correlations parallel and perpendicular to the event plane. the results are from a high statistics 200 gev au+au collisions data set collected by the star experiment at rhic. we explicitly count units of charge separation from which we find clear evidence for more charge separation fluctuations perpendicular than parallel to the event plane. we also employ a modified correlator to study the possible p-even background in same and opposite charge correlations.
centrality dependence of the pseudorapidity density distribution for charged particles in pb-pb collisions at sqrt(snn) = 2.76 tev. we present the first wide-range measurement of the charged-particle pseudorapidity density distribution, for different centralities (the 0-5%, 5-10%, 10-20%, and 20-30% most central events) in pb-pb collisions at sqrt(snn) = 2.76 tev at the lhc. the measurement is performed using the full coverage of the alice detectors, -5.0 &lt; eta &lt; 5.5, and employing a special analysis technique based on collisions arising from lhc 'satellite' bunches. we present the pseudorapidity density as a function of the number of participating nucleons as well as an extrapolation to the total number of produced charged particles nch = 17165 +/- 772 for the 0-5% most central collisions). from the measured dnch/deta distribution we derive the rapidity density distribution, dnch/dy, under simple assumptions. the rapidity density distribution is found to be significantly wider than the predictions of the landau model, which reproduce data well at rhic energies. we assess the validity of longitudinal scaling by comparing to lower energy results from rhic. finally the mechanisms of the underlying particle production are discussed based on a comparison with various theoretical models.
radiative energy loss of relativistic charged particles in absorptive media. we determine the energy loss spectrum per time-interval of a relativistic charge traversing a dispersive medium. polarization and absorption effects in the medium are modelled via a complex index of refraction. we find that the spectrum amplitude becomes exponentially damped due to absorption mechanisms. taking explicitly the effect of multiple scatterings on the charge trajectory into account, we confirm results obtained in a previous work.
molecular dynamics description of an expanding $q$/$\bar{q}$ plasma with the nambu--jona-lasinio model and applications to heavy ion collisions at rhic and lhc energies. we present a relativistic molecular dynamics approach based on the nambu--jona-lasinio lagrangian. we derive the relativistic time evolution equations for an expanding plasma, discuss the hadronization cross section and how they act in such a scenario. we present in detail how one can transform the time evolution equation to a simulation program and apply this program to study the expansion of a plasma created in experiments at rhic and lhc. we present first results on the centrality dependence of $v_2$ and of the transverse momentum spectra of pions and kaons and discuss in detail the hadronisation mechanism.
system size and energy dependence of dilepton production in heavy-ion collisions at sis energies. we study the dilepton production in heavy-ion collisions at energies of 1-2 agev as well as in proton induced pp, pn, pd and p+a reactions from 1 gev up to 3.5 gev. for the analysis we employ three different transport models - the microscopic off-shell hadron-string-dynamics (hsd) transport approach, the isospin quantum molecular dynamics (iqmd) approach as well as the ultra-relativistic quantum molecular dynamics (urqmd) approach. we confirm the experimentally observed enhancement of the dilepton yield (normalized to the multiplicity of neutral pions $n_{\pi^0}$) in heavy-ion collisions with respect to that measured in $nn = (pp+pn)/2$ collisions. we identify two contributions to this enhancement: a) the $pn$ bremsstrahlung which scales with the number of collisions and not with the number of participants, i.e. pions; b) the dilepton emission from intermediate $\delta$'s which are part of the reaction cycles $\delta \to \pi n ; \pi n \to \delta$ and $nn\to n\delta; n\delta \to nn$. with increasing system size more generations of intermediate $\delta$'s are created. if such $\delta$ decays into a pion, the pion can be reabsorbed, however, if it decays into a dilepton, the dilepton escapes from the system. thus, experimentally one observes only one pion (from the last produced $\delta$) whereas the dilepton yield accumulates the contributions from all $\delta$'s of the cycle. we show as well that the fermi motion enhances the production of pions and dileptons in the same way. furthermore, employing the off-shell hsd approach, we explore the influence of in-medium effects like the modification of self-energies and spectral functions of the vector mesons due to their interactions with the hadronic environment.
spontaneous fission half-lives of heavy and superheavy nuclei within a generalized liquid drop model. we systematically calculate the spontaneous fission half-lives for heavy and superheavy nuclei between u and fl isotopes. the spontaneous fission process is studied within the semi-empirical wkb approximation. the potential barrier is obtained using a generalized liquid drop model, taking into account the nuclear proximity, the mass asymmetry, the phenomenological pairing correction, and the microscopic shell correction. macroscopic inertial-mass function has been employed for the calculation of the fission half-life. the results reproduce rather well the experimental data. relatively long half-lives are predicted for many unknown nuclei, sufficient to detect them if synthesized in a laboratory.
charm quark energy loss in proton-proton collisions at lhc energies. null
impact of gluon damping on heavy-quark quenching. in this conference contribution, we discuss the influence of gluon-bremsstrahlung damping in hot, absorptive qcd matter on the heavy-quark radiation spectra. within our monte-carlo implementation for the description of the heavy-quark in-medium propagation we demonstrate that as a consequence of gluon damping the quenching of heavy quarks becomes significantly affected at higher transverse momenta.
j/psi elliptic flow in pb-pb collisions at sqrt(snn) = 2.76 tev. we report on the first measurement of inclusive j/psi elliptic flow, v2, in heavy-ion collisions at the lhc. the measurement is performed with the alice detector in pb-pb collisions at sqrt(snn) = 2.76 tev in the rapidity range 2.5 &lt; y &lt; 4.0. the dependence of the j/psi v2 on the collision centrality and on the j/psi transverse momentum is studied in the range 0 &lt;= pt &lt; 10 gev/c. for semi-central pb-pb collisions at sqrt(snn) = 2.76 tev, an indication of non-zero v2 is observed with a maximum value of v2 = 0.116+/-0.046(stat.)+/-0.029(syst.) for j/psi in the transverse momentum range 2 &lt;= pt &lt; 4 gev/c. the elliptic flow measurement complements the previously reported alice results on the inclusive j/psi nuclear modification factor and favors the scenario of a significant fraction of j/psi production from charm quarks in a deconfined partonic phase.
techniques for measuring aerosol attenuation using the central laser facility at the pierre auger observatory. the pierre auger observatory in malargüe, argentina, is designed to study the properties of ultra-high energy cosmic rays with energies above 1018 ev. it is a hybrid facility that employs a fluorescence detector to perform nearly calorimetric measurements of extensive air shower energies. to obtain reliable calorimetric information from the fd, the atmospheric conditions at the observatory need to be continuously monitored during data acquisition. in particular, light attenuation due to aerosols is an important atmospheric correction. the aerosol concentration is highly variable, so that the aerosol attenuation needs to be evaluated hourly. we use light from the central laser facility, located near the center of the observatory site, having an optical signature comparable to that of the highest energy showers detected by the fd. this paper presents two procedures developed to retrieve the aerosol attenuation of fluorescence light from clf laser shots. cross checks between the two methods demonstrate that results from both analyses are compatible, and that the uncertainties are well understood. the measurements of the aerosol attenuation provided by the two procedures are currently used at the pierre auger observatory to reconstruct air shower data.
investigation of astatine(iii) hydrolyzed species: experiments and relativistic calculations. this work aims to resolve some controversies about astatine(iii) hydroxide species present in oxidant aqueous solution. ato+ is the dominant species existing under oxidizing and acidic ph conditions. this is consistent with highperformance ion-exchange chromatography data showing the existence of one species holding one positive charge. a change in speciation occurs as the ph changes from 1 to 4, while remaining under oxidizing conditions. dynamic experiments with ion-exchange resins evidence the existence of a neutral species witnessed by its elution in the void volume. batch-experiments using a competition method show the exchange of one proton indicating the formation of the ato(oh) species. the hydrolysis thermodynamic constant, extrapolated to zero ionic strength, was determined to be 10−1.9. this value is supported by two-component relativistic quantum calculations and therefore allows disclosing unambiguously the structure of the formed species.
on state feedback h_infinity control for discrete-time singular systems. this paper deals with the state feedback hinf control problem for linear time-invariant discrete-time singular systems. relied on the use of auxiliary matrices and a positive scalar, a novel necessary and sufficient condition for the bounded real lemma is derived for discrete-time singular systems. the characterization is reduced to a strict linear matrix inequality (lmi) when the scalar is fixed, and the resulting lmi is non- conservative as long as the scalar is chosen sufficiently large. moreover, the result is further expanded to hinf controller design, and a numerically efficient and reliable design procedure is given. since no particular restriction is imposed on the auxiliary matrices, the proposed result outperforms the existing methods in the literature. numerical examples are included to illustrate the effectiveness of the present result.
univalence for free. we present an internalization of the 2-groupoid interpretation of the calculus of construction that allows to realize the univalence axiom, proof irrelevance and reasoning modulo. as an example, we show that in our setting, the type of church integers is equal to the inductive type of natural numbers.
feasibility algorithms for two pickup and delivery problems with transfers. this presentation follows the phd thesis of renaud masson [1] on the pickup and delivery problem with transfers (pdpt). the motivating application is a dial-a-ride problem in which a passenger may be transferred from the vehicle that picked him/her up to another vehicle at some predetermined location, called transfer point. both the pdpt and the dial-a-ride problem with transfers (darpt) were investigated. an adaptive large neighborhood search has been developed to solve the pdpt [2] and also adapted to the darpt [3]. in both algorithms, multiple insertions of requests in routes are tested. e ciently evaluating their feasibility with respect to the temporal constraints of the problem is a key issue.
modeling energy demand of buildings at urban scale. urban scale is now considered as one of the most relevant scales to face energy and climate challenges. specific needs for knowledge, decision making tools and evaluation are identified at urban scale. modelling energy demand from residential buildings is one key aspect, priorto energy retrofitting of existing building asset or to valorisation of local energy sources. diversity of local contexts, stake holder goals and data availability lead to search flexible models, with ability to produce information for different applications, from alternative input data sets, combining different types of basic models (namely both physical and statistical ones), according to user needs. the present work is exploring the potential of bottom-up approaches, based on engineering models, developed originally for isolated buildings. these models are extrapolated for the complete set of buildings in a city or neighbourhood, based on building archetypes. two key questions tackled are the selection of suitable archetypes and the reconstitution of relevant input data, statistically representative for the area of interest sensitivity analysis techniques have been applied to a thermal simulation programme (esp-r), particularly the morris elementary effects method. a non-linear response of the model has been emphasized, caused by scattering of input parameters and interaction effects. the most influencing and interacting parameters have been identified. they concern the buildings themselves, their environment and the inhabitants. data collection or statistical reconstitution must be concentrated in priority to these main parameters. a model of the heat demand at a neighbourhood scale has been developed and tested on the sector st-félix in nantes. it is called medus (modelling energy demand at urban scale). application is based on three building archetypes. census data (insee) available at the sector scale are the main input data. results are analyzed both to check archetype relevancy and to study a possible application for evaluating actions at sector scale, such as energy retrofitting.
from object-oriented programming to service-oriented computing: how to improve interoperability by preserving subtyping. the object-oriented paradigm is increasingly used in the implementation and the use of web services. however, the mismatch between objects and document structures in the wire has a negative impact over interoperability, more particularly when subtyping is involved. in this paper, we discuss how to improve interoperability in this context by preserving the subsumption property associated to subtyping. first we show the weaknesses of existing web service frameworks used for serialization and deserialization. second we propose new foundations for serialization and deserialization, which leads to the specification of a new data binding between objects and document structures, compatible with subtyping.
an adaptive large neighborhood search for the pickup and delivery problem with transfers. the pickup and delivery problem (pdp) consists in defining a set of routes that satisfy transportation requests between a set of pickup points and a set of delivery points. this paper addresses a variant of the pdp where requests can change vehicle during their trip. the transshipment is made at specific locations called "transfer points". the corresponding problem is called the pickup and delivery with transfers (pdpt). solving the pdpt leads to new modeling and algorithmic difficulties. we propose new heuristics capable of efficiently inserting requests through transfer points. these heuristics are embedded into an adaptive large neighborhood search. we evaluate the method on generated instances and apply it to the transportation of people with disabilities. on these real-life instances we show that the introduction of transfer points generally brings non-negligible improvements (up to 9%). experiments on related instances from the literature show clearly that the method is competitive with existing method.
dissipative performance control with output regulation for continuous-time descriptor systems. this paper is concerned with the problem of dissipative performance control under output regulation constraints for continuous-time descriptor systems. in this problem, an output is to be regulated asymptotically with presence of an infinite-energy exo-system, while a specific dissipative performance from a finite external disturbance to a tracking error has also to be satisfied. based on a generalized sylvester equation, the asymptotical regulation objective is achieved and a specific structure of the resulting controller is deduced. using this structure, the solution to the defined multi-objective control problem is characterized in terms of a set of linear matrix inequalities (lmis).
the neutrons for science facility at spiral-2. the neutrons for science (nfs) facility is a component of spiral-2 laboratory under construction at caen (france). spiral-2 is dedicated to the production of high intensity radioactive ions beams (rib). it is based on a high-power linear accelerator (linag) to accelerate deuterons beams in order to produce neutrons by breakup reactions on a c converter. these neutrons will induce fission in 238u for production of radioactive isotopes. additionally to the rib production, the proton and deuteron beams delivered by the accelerator will be used in the nfs facility. nfs is composed of a pulsed neutron beam and irradiation stations for cross-section measurements and material studies. the beams delivered by the linag will allow producing intense neutron beams in the 100 kev-40 mev energy range with either a continuous or quasi-mono-energetic spectrum. at nfs available average fluxes will be up to 2 orders of magnitude higher than those of other existing time-of-flight facilities in the 1 mev - 40 mev range. nfs will be a very powerful tool for fundamental physics and application related research in support of the transmutation of nuclear waste, design of future fission and fusion reactors, nuclear medicine or test and development of new detectors. the facility and its characteristics are described, and several examples of the first potential experiments are presented.
measurement of charge multiplicity asymmetry correlations in high energy nucleus-nucleus collisions at 200 gev. a study is reported of the same- and opposite-sign charge-dependent azimuthal correlations with respect to the event plane in au+au collisions at 200 gev. the charge multiplicity asymmetries between the up/down and left/right hemispheres relative to the event plane are utilized. the contributions from statistical fluctuations and detector effects were subtracted from the (co-)variance of the observed charge multiplicity asymmetries. in the mid- to most-central collisions, the same- (opposite-) sign pairs are preferentially emitted in back-to-back (aligned on the same-side) directions. the charge separation across the event plane, measured by the difference, $\delta$, between the like- and unlike-sign up/down $-$ left/right correlations, is largest near the event plane. the difference is found to be proportional to the event-by-event final-state particle ellipticity (via the observed second-order harmonic $v_2^{\rm obs}$, where $\delta=(1.3\pm1.4({\rm stat})^{+4.0}_{-1.0}({\rm syst}))\times10^{-5}+(3.2\pm0.2({\rm stat})^{+0.4}_{-0.3}({\rm syst}))\times10^{-3}v_2^{\rm obs}$ for 20-40% au+au collisions. the implications for the proposed chiral magnetic effect (\cme) are discussed.
separating jets from bulk matter in heavy ion collisions at the lhc. null
a realistic treatment of geomagnetic cherenkov radiation from cosmic ray air showers. null
centrality dependence of pi, k, p production in pb-pb collisions at sqrt(snn) = 2.76 tev. in this paper measurements are presented of pi+, pi-, k+, k-, p and antiproton production at mid-rapidity &lt; 0.5, in pb-pb collisions at sqrt(snn) = 2.76 tev as a function of centrality. the measurement covers the transverse momentum pt range from 100, 200, 300 mev/c up to 3, 3, 4.6 gev/c, for pi, k, and p respectively. the measured pt distributions and yields are compared to expectations based on hydrodynamic, thermal and recombination models. the spectral shapes of central collisions show a stronger radial flow than measured at lower energies, which can be described in hydrodynamic models. in peripheral collisions, the pt distributions are not well reproduced by hydrodynamic models. ratios of integrated particle yields are found to be nearly independent of centrality. the yield of protons normalized to pions is a factor ~1.5 lower than the expectation from thermal models.
hydrodynamical evolution in heavy ion collisions and pp scatterings a the lhc ridges in aa and pp scattering. null
electron-d$^0$ correlations in p+p and au+au collisions at 200 gev with the star experiment at rhic. null
heavy flavour at rhic. null
environment reconstruction and navigation with electric sense based on kalman filter. electric fish sense the perturbations of a self generated electric field through their electro- receptive skin. this sense allows them to navigate and reconstruct their environment in conditions where vision and sonar cannot work. in this article, we use a sensor inspired by this sense to address the problem of locating and reconstructing small objects (electrolocation) and navigating in a tank. based on a kalman filter, any small object in the surroundings of the motion controlled sensor can be modeled as an equivalent sphere whose location is well estimated by the filter. as a first application to the problem of navigation, the filter is included into a closed feedback loop in order to achieve wall following in a tank. our experimental results demonstrate the feasibility of this approach.
cumulative scheduling with overloads of resource : global constraint and decompositions. constraint programming is an interesting approach to solve scheduling problems. in cumulative scheduling, activities are defined by their starting date, their duration and the amount of resource necessary for their execution. the total available resource at each point in time (the capacity) is fixed. in constraint programming, the cumulative global constraint models this problem. in several practical cases, the deadline of theproject is fixed and can not be delayed. in this case, it is not always possible to find a schedule that does not lead to an overload of the resource capacity. it can be tolerated to relax the capacity constraint, in a reasonable limit, to obtain a solution. we propose a new global constraint : the softcumulative constraint that extends the cumulative constraint to handle these overloads. we illustrate its modeling power on several practical problems, and we present various filtering algorithms. in particular, we adapt the sweep and edge-finding algorithms to the softcumulative constraint. we also show that some practical problems require to impose overloads to satisfy business rules, modelled by additional constraints. we present an original filtering procedure to deal with these imposed overloads. we complete our study by an approach by decomposition. at last, we test and validate our different resolution techniques through a series of experiments.
shared steering control between a driver and an automation: stability in the presence of driver behaviour uncertainty. this paper presents an advanced driver assistance system (adas) for lane keeping, together with an analysis of its performance and stability with respect to variations in driver behavior. the automotive adas proposed is designed so as to share control of the steering wheel with the driver in the best possible way. its development was derived from a h2-preview optimization control problem, which is based on the global driver-vehicle-road (dvr) system. the dvr model makes use of a cybernetic driver model so as to take into account any driver-vehicle interactions. such a formulation allows to: i) consider driver-assistance cooperation criteria in the control synthesis, ii) improve the performance of the assistance as a cooperative copilot, and iii) analyze the stability of the whole system in the presence of driver model uncertainty. the results have been validated experimentally with one participant using a fixed-base driving simulator. the developed assistance system improved lane-keeping performance and reduced the risk of a lane departure accident. good results were obtained using several criteria for human-machine cooperation. poor stability situations were successfully avoided thanks to the robustness of the whole system in spite of a large range of driver model uncertainty.
new methods for the multi-skills project scheduling problem. in this phd thesis we introduce several procedures to solve the multi-skill project scheduling problem (mspsp). the aim is to find a schedule that minimizes the completion time (makespan) of a project, composed of a set of activities. precedence relations and resource constraints are considered. in this problem, resources are staff members that master several skills. thus, a given number of workers must be assigned to perform each skill required by an activity. furthermore, we give a particula rimportance to exact methods for solving the multi-skill project scheduling problem (mspsp), since there are still several instances for which optimality is still to be proven. nevertheless, with the purpose of solving big sized instances we also developed and implemented a heuristic approach.
study of nuclear energy systems and double strata scenarios for minor actinides transmutation in ads. the french law of 28th june 2006 regarding advanced nuclear waste management requires a scientific assessment to define future industrial strategies. the present phd thesis was carried in this framework and concerns specifically the research axis of minoractinides transmutation. a high power accelerator driven system (ads) concept is developed at subatech for this purpose. a 1 gev proton beam feeds three liquid lead-bismuth spallation targets. the multiple spallation target (must) ads reaches the thermal powers up to 1 gw with a high specific power. a nuclear reactor dimensioning method has been developed and applied to different double strata scenarios. in these scenarios, sfr (sodium fastreactors) or pwr (pressurized water reactors) power reactors produce minor actinides that will be transmuted into ads. in each core (sfr and ads), the plutonium multi-reprocessing strategy is performed while ads subcritical core also multi-reprocesses minor actinides. to limit the core reactivity and improve the fuel thermal conductivity, the minor actinides fuel is mixed with mgo inert matrix. nuclear branches with lead and sodium coolants for the ads, have been studied for different irradiation times and two transmutation strategies have been assessed : whether whole minor actinides, whether americium only is tranmuted. the thesis presents precisely the must ads design methodology and the calculations to get a fuel composition at equilibrium. then a one cycle evolution is performed and analysed for the fuel and the multiplication factor. radiotoxicity and thermal power of the waste produced are then compared. finally, the study of double strata scenarios is performed to analyse the plutonium and minor actinides inventories in cycle and also the waste produced according to the transmutation strategies applied and the first stratum evolution.
efficient feasibility testing for request insertion in the pickup and delivery problem with transfers. the pickup and delivery problem with transfers (pdpt) consists of defining a set of minimum cost routes in order to satisfy a set of transportation requests, allowing them to change vehicles at specific locations. in this problem, routes are strongly interdependent due to request transfers. then it is critical to efficiently check if inserting a request into a partial solution is feasible or not. in this article, we present a method to perform this check in constant time.
beyond cross-functional teams: knowledge integration during organizational projects and the role of social capital. large organizational projects must integrate the specific and dispersed knowledge of many individuals and groups to succeed. thus, frequent exchanges between the project team and the organization's members are required. in this context, understanding of the knowledge integration process during cross-functional projects can be enhanced through the conceptual framework of social capital. a qualitative investigation of a french small firm conceptualizes knowledge integration as a three-phase model: collection, interpretation, and assimilation. the case shows that the integration process is cyclical with overlaps and inter-dependencies among the phases. this study leads to refinement of the social capital role in knowledge integration and reveals the dynamics of internal and external facets of social capital. that is, internal and external social capital play differentiated roles depending on the three phases of the knowledge integration process. finally, the study reveals the co-evolution of social capital and knowledge integration as a resulting long-term effect.
active lateral acceleration control of a narrow tilting vehicle. narrow tilting vehicles (ntv) are the convergence of a car and a motorcycle. one meter wide, these vehicles are designed for one or two people sitting the one in front the other. the idea behind the conception of ntv is the minimization of traffic congestion, energy consumption and pollutant emission. but because of their dimensions, these cars would have to lean into corners in order to compensate for the lateral acceleration and maintain their stability. the tilting should be automatic, and can be achieved by a tilting torque generated by a dedicated tilting actuator (dtc) or by modifying the steering angle (stc) or both (sdtc). in this thesis, we first propose a methodology for the design of an output feedback structured regulator, minimizing the h2 norm of a well-posed problem, built to optimize the lateral acceleration of the ntv, considering dtc and sdtc systems.the designed controllers, with the longitudinal velocity as a parameter, lead to the minimization of the tilting torque and of the lateral acceleration perceived by the driver, and have good performances as well as good robustness properties. furthermore, the tuning methodology allows the comparison of a pure dtc solution and a mixed sdtc alternative. compared to the literature, the originalities in this thesis are the direct control of the measured value of the lateral acceleration (instead of the tilting angle), and the anticipation of the tilt, thanks to the use of the steering angle and angular velocity. furthermore, the sdtc solution allows to drive both the stc and dtc systems in a coordinated manner. the design strategies are based on a preliminary study of vehicle models, and a design model with 5dof was developed. we demonstrated that the model has the nice property to be flat, and in the last section of the thesis, used this property to initiate the design of a non-linear robust controller, which can a priori lead to better performances in case of "large motions".
control of a lower mobility dual arm system. this paper studies the kinematic modeling and control of two cooperative manipulators. the system is composed of the two arms of the humanoid nao robot of aldebaran. the serial structure of each arm has five degrees of freedom, in the closed chain formulation when transporting a common object, it has 4-dof. the kinematic and dynamics representing the closed chain system is studied. a new control scheme demonstrates how the cooperative task space can be combined with a minimum representation of the task to control the 4 dof of object. furthermore by modeling the object grasp as a passive joint, we show that all 6-dof of the object can be controlled.
automata and constraint programming for personnel scheduling problems. as soon as a structure is organized, the ability to put the right people at the right time is critical to satisfy the need of a department, a school or a company. we define personnel scheduling problems as the process of building, in an optimized manner, the personnel schedules. the aims of this thesis are to propose a mean to express those problems in a simple and automatic way, avoiding the user to interact with the technical aspects of the resolution. for that matter, we propose to mix the modeling power of automata with the efficiency and modularity of constraint programming for complex problem solving. thus, we use the expressiveness of the finite multi-valued automata to model complex scheduling rules. then, to make use of those built automata, we introduce a new filtering algorithm for multi-valued finite automata based on lagrangian relaxation : multicost-regular. we also introduce a soft version of this constraint that has the ability to penalize violated rules defined by the automaton : soft-multicost-regular. the constraint model is automatically built. it is solved using the constraint library choco and the whole modeling-solving process has been tested on realistic instances from asap and nrp10 libraries. the solution search is finally improved using specialized regret-based heuristics using the structure of multicost-regular and soft-multicost-regular.
biological treatment of malodorous gaseous compounds stemming from the industrial sector of waste management : study of a biotrickling filter/biofilter combination. waste treatment industries generate gaseous emissions that may induce odor annoyance to the surrounding populations. these gaseous effluents contain a large variety of volatile compounds such as oxygenated (volatile fatty acids, ketones, aldehydes and alcohols), nitrogen and sulphur compounds (hydrogen sulphide (h2s), dimethylsulphide (dms), dimethyldisulfide (dmds) and methanethiol (mt). these gaseous emissions are controlled by using an adequate system such as biotechniques. nevertheless, because of their very low odor thresholds, complete elimination of sulphur compounds has to be assessed, as the residual concentration can induce an odorous impact on neighbourhood populations. the aim of this study is to improve these bioprocesses performances by carrying out an adequate system strategy. the originality of this work is to evaluate the removal efficiency of a mixture of sulphur compounds by implementing a combination of two bioprocesses and more precisely a biotrickling filter and biofilter.the first step of this phd. work consisted of evaluating the ph impact on the biodegradation activity of a mixture of sulphur compounds (h2s, dms and dmds) by using microcosms. the ph has an impact on the removal efficiency of dms and dmds. the total removal of these compounds is observed for a ph range between 5 and 7. the performances of the coupling have been compared with those reached by implementing control biofilters (duplicated). after an acclimatization period, stable performances are maintained under constant operating conditions. the efficiency of the coupling have been highlighted, the dms and dmds abatement levels are superior (around 20%) for the bioprocesses combination.the microbiological component has been investigated within all biofilters by estimating the densities of two populations involved in the biodegradation of sulphur compounds (hyphomicrobium and thiobacillus thioparus), by using qpcr. the obtained results highlighted the presence of both populations at high level (104 copies of dnar-16s gene/ng extracted dna for thiobacillus thioparus and 104-106 copies of dnar-16s gene/ng extracted dna for hyphomicrobium). the repartition of these two bacterial populations is similar in both cases (coupling system and reference biofilters). under transient shock load conditions, the robustness of the coupling has been revealed. the efficiency levels before the shock load are recovered 48 hours after the perturbation off. finally, the monitoring of an on- site pilot (rendering facility) has been carried out during three months. the laboratory results have been confirmed and the suitability of such a system has been showed under industrial gas variability.
microbial aerosol behavior in hvac system. microbial indoor air quality is an important issue in particular in the professional sector. this thesis aims to investigate the conditions leading to microbial development on to fibrous filters and to microbial release down stream of filters that could decrease air quality. the first part of the thesis was realized on laboratory and consisted in the filtration of a microbial consortium composed with staphylococcus epidermidis (bacterium specie) and penicillium oxalicum (fungi specie). the effects of three parameters on the microbial behavior were studied : the relative humidity (rh) of the air, the filter material, the airflow presence/absence. whatever conditions, s. epidermidis did not grow up. however, p. oxalicum has demonstrated its ability to develop itself when rh was close to 100% and some p. oxalicumspores were released downstream of filter after growth, when ventilation was restarted. the second part of the thesis consisted in working with a semi-urban outdoor air. two air handling unit (ahu) have operated during 5 months. the ventilation of one ahu was stopped each week-end and restarted each beginning of week. temperature and rh of the air, filters pressure drop and total concentration of pm in air before filtration were monitored. concentration of total cultivable microorganisms upstream and downstream of both filters was also measured each week, in particular at the restart of ventilation for one ahu. according to seasonal variations of microbial concentrations, results have revealed for instance that the filtration efficiency of cultivable bacteria was particularly weak, and sometimes negative, for the ahu operating continuously.
presence in e-learning: theoretical model and perspectives for research. this article proposes a model of presence in e-learning that has some similarities with but also some important distinctions from the model of community of inquiry in e-learning (garrison &amp; anderson, 2003). it addresses the notion of presence from a different angle, characterizes and specifies it differently. the author outlines the epistemological referents of the model proposed. then, she describes the interaction processes at work in each of the three dimensions of the model: socio-cognitive presence (1), socio-affective presence (2) and pedagogical presence (3). she also provides a schematic representation of this model. then, she shows how its three dimensions can be related to each another and presents the main hypotheses that result from these relations. in conclusion, the author outlines that theoretical and empirical research is needed to confirm the relevance of the model proposed, to identify its strengths and to suggest axes for improvement.
flexible and expressive aspect-based control over service compositions in the cloud. accountability properties (e.g., security and privacy proper- ties for trustworthy data stewardship) are becoming increas- ingly important for cloud applications. frequently, they have to be enforced on large-scale service-based legacy ap- plications. in this paper we argue that real-world service in- frastructures are best modeled in terms of three abstraction levels and that (partially invasive) adaptations involving all levels are needed to handle the corresponding evolution sce- narios. in this paper, we motivate these issues for the case of apache cxf, a popular service infrastructure, and secure logging as a basic accountability property. we propose an initial version of a dsl for flexible and expressive control over the execution of service compositions on three levels: service, interceptor and implementation. we also present a corresponding prototype tool and infrastructure we have implemented over cxf. finally, we show how our method can be applied to enable secure logging.
modeling and solving the generalized routing problems. the routing problem is one of the most popular and challenging combinatorial optimization problems. it involves finding the optimal set of routes for fleet of vehicles in order to serve a given set of customers. in the classic transportation problems, each customer is normally served by only one node (or arc). therefore, there is always a given set of required nodes (or arcs) that have to be visited or traversed, and we just need to find the solution from this set of nodes (or arcs). but in many real applications where a customer can be served by from more than one node (or arc), the generalized resulting problems are more complex. the primary goal of this thesis is to study three generalized routing problems. the first one, the close-enough arc routing problem(cearp), has an interesting real-life application to routing for meter reading while the others two, the multi-vehicle covering tour problem (mctp) and the generalized vehicle routing problem(gvrp), can model problems concerned with the design of bilevel transportation networks. the problems are solved by exact methods as well as metaheuristics. to develop exact methods, we formulate each problem as a mathematical program, and then develop branch-and-cut algorithms. the metaheuristics are based on the evolutionary local search (els) method et on the greedy randomized adaptive search procedure (grasp) method. the extensive computational experiments show the performance of our methods.
interpretation of the depths of maximum of extensive air showers measured by the pierre auger observatory. to interpret the mean depth of cosmic ray air shower maximum and its dispersion, we parametrize those two observables as functions of the first two moments of the $\ln a$ distribution. we examine the goodness of this simple method through simulations of test mass distributions. the application of the parameterization to pierre auger observatory data allows one to study the energy dependence of the mean $\ln a$ and of its variance under the assumption of selected hadronic interaction models. we discuss possible implications of these dependences in term of interaction models and astrophysical cosmic ray sources.
towards qos-oriented sla guarantees for online cloud services. cloud computing provides a convenient means of remote on-demand and pay-per-use access to computing re- sources. however, its ad-hoc management of quality-of-service and sla poses significant challenges to the performance, dependability and costs of online cloud services. the paper precisely addresses this issue and makes a threefold contribu- tion. first, it introduces a new cloud model, the slaaas (sla aware service) model. slaaas enables a systematic integration of qos levels and sla into the cloud. it is orthogonal to other cloud models such as saas or paas, and may apply to any of them. second, the paper introduces csla, a novel language to describe qos-oriented sla associated with cloud services. third, the paper presents a control-theoretic approach to provide performance, dependability and cost guarantees for online cloud services, with time-varying workloads. the proposed approach is validated through case studies and extensive experiments with online services hosted in clouds such as amazon ec2. the case studies illustrate sla guarantees for various services such as a mapreduce service, a cluster-based multi-tier e-commerce service, and a low-level locking service.
fate of emerging pollutants during photochemical or photocatalytic treatment under solar irradiation. industrialisation, the use of numerous chemical products in domestic activities and the use of medicine drugs have led to the release in the environment of various substances named "emerging pollutants". the existing wastewater treatments are not designed to eliminate this kind of pollution and then these pollutants are released into the natural aquatic media. to limit the release of these compounds by waste water treatment plant effluent, a solution could be the use of additional treatment processes such as advanced oxidation processes. in this context, the european project clean water has started in 2009. clean water involves 7 entities including the gepea laboratory-ecole des mines de nantes. the aim of the clean water project is to develop sustainable and cost effective water treatment and detoxification processes using tio2 nanomaterials with uv-visible light response under solar light. these processes act to remove emerging contaminants such as endocrine disruptors and pharmaceuticals. in this program, thegepea laboratory is concerned with the evaluation of the efficiency of novel photocatalysts under uv and visible irradiations for the elimination of emerging pollutants. for this purpose, an experimental methodology was established to express the efficiency of the tested catalysts in terms of degradation kinetic constants, pollutants conversion and mineralisation and also in terms of the intermediate products formed. the efficiency of photocatalysts is also evaluated in terms of intermediates biodegradability, toxicity and endocrine disruption effects. first, the experimental methodology was tested on the degradation of tetracycline with a reference catalyst. then, it was applied to the degradation of bisphenol a and estradiol respectively with the reference catalyst and the catalysts developed within the clean water project. the results obtained on the tetracycline degradation have showed that: i) tetracycline intermediate products are less toxic than tetracycline ii) the intermediates structure is similar to that of tetracycline, this can explain the low biodegradability observed for these intermediates. for the degradation of bisphenol a and estradiol, the results showed that: i) the photocatalysts are efficient under simulated solar irradiation. however, the catalyst photocatalytic efficiency depends on the compound to be degraded ii) the nature of the bisphenol a reaction intermediates identified depends on the catalyst used iii)the estrogenic effect of the estradiol treated solution persists during the photocatalytic treatment.
study of in vivo generators pb-212/bi--212 and u-230/th-226 for alpha radioimmunotherapy. alpha-radioimmunotherapy is a promising cancer therapy that uses a-particles vectorized by monoclonal antibody to break down cancerous tumors. the notion of in vivo generator was introduced in 1989 by leonard mausner. the concept involves labeling of various molecular carriers (antibodies, peptides, etc) with intermediate half-life generator parents, which after accumulation in the desired tissue generate much shorter half-life daughter radionuclide. this thesis focuses on the study of two in vivo generators potentially interesting for alpha-radioimmunotherapy: pb-212 / bi-212 generator and u-230 / th-226 generator. the first part of this work presents the pb-212 / bi-212 generator, two approaches allowing the vectorisation. chelation approach on a protein and an approach by encapsulation in liposomes have been proposed. this last approach appears to be the most interesting. in vitro stability studies have been performed on these labeling. the second part of this work presents the u-230 / th-226 generator. studies have first been made to achieve a theoretical model to describe the speciation of th(iv) in human serum. the efficacy of dtpa as chelating agent for complexation of th(iv) in human serum could thus be estimated.
hydrogen production from anaerobic co-digestion of coffee mucilage and swine manure. this research investigates an alternative approach to the use of two wastes from agricultural and livestock activities developed in colombia. swinemanure and coffee mucilage were used to evaluatean anaerobic co-digestion process focused on hydrogen production. in addition, the aims covered a further stage in order to close the cycle of the both wastes. the thesis was conducted in three phases : 1. evaluation of hydrogen production from the co-digestion of coffee mucilage and swine manure during dark fermentation ; 2. trends over retention time through the monitoring of microorganisms by quantitative pcr and other parameters incluiding ph, oxidation reduction potential, and hydrogen partial pressure ; 3. treatment of the effluent from hydrogen production process by anaerobic digestion with methane production. the experimental results showed that mixtures of both wastes are able to produce hydrogen. a substrate ratio of 5:5, which was associated with a c/n ratio of 53, was suitable for hydrogen production. moreover, the stability and optimization of the process were evaluated by increasing the influent organic load rate. this wasthe best experimental condition in terms of average cumulative hydrogen volume, production rate and yield which were 2661 nml, 760 nmlh2/lwd and 43 nml h2/gcod, respectively. this performance was preserved over time, which was verified through the repetitive batch cultivation during 43 days. two trends were identified over retention time associated with similar cumulative hydrogen, but with differences in lag-phase time and hydrogen production rate. t.thermosaccharolyticum was the dominating genus during the short trend related to the shortest lag phase time and highest hydrogen production rate. the long trends were associated with a decrease of bacillus sp. concentration at the beginning of the experiments and with the possible competition for soluble substrates between t.thermosaccharolyticum and clostridium sp. the third phase showed that the use of a second stage to produce methane was useful enhancing the treatment of both wastes. finally, the overall energy produced for both biofuels (hydrogen andmethane) showed similar levels with other process. however, hydrogen was around the 10% of the overall energy produced in the process. in addition, both gases could be mixed to produce biohythane which improves the properties of biogas.
centrality determination of pb-pb collisions at sqrt(snn) = 2.76 tev with alice. this publication describes the methods used to measure the centrality of inelastic pb-pb collisions at a center-of-mass energy of 2.76 tev per colliding nucleon pair with alice. the centrality is a key parameter in the study of the properties of qcd matter at extreme temperature and energy density, because it is directly related to the initial overlap region of the colliding nuclei. geometrical properties of the collision, such as the number of participating nucleons and number of binary nucleon-nucleon collisions, are deduced from a glauber model with a sharp impact parameter selection, and shown to be consistent with those extracted from the data. the centrality determination provides a tool to compare alice measurements with those of other experiments and with theoretical calculations.
accountability for cloud and other future internet services. presentation of current accountability problems and tracks for future work.
charge correlations using the balance function in pb-pb collisions at sqrt{s_{nn}} = 2.76 tev. in high-energy heavy-ion collisions, the correlations between the emitted particles can be used as a probe to gain insight into the charge creation mechanisms. in this article, we report the first results of such studies using the electric charge balance function in the relative pseudorapidity \delta\eta and azimuthal angle \delta\phi in pb-pb collisions at sqrt{s_{nn}} = 2.76 tev with the alice detector at the large hadron collider. the width of the balance function decreases with growing centrality (i.e. for more central collisions) in both projections. this centrality dependence is not reproduced by hijing, while ampt, a model which incorporates strings and parton rescattering, exhibits qualitative agreement with the measured correlations in \delta\phi but fails to describe the correlations in \delta\eta. a thermal blast wave model incorporating local charge conservation and tuned to describe the p_t spectra and v_2 measurements reported by alice, is used to fit the centrality dependence of the width of the balance function and to extract the average separation of balancing charges at freeze-out. the comparison of our results with measurements at lower energies reveals an ordering with sqrt{s_{nn}}: the balance functions become narrower with increasing energy for all centralities. this is consistent with the effect of larger radial flow at the lhc energies but also with the late stage creation scenario of balancing charges. however, the relative decrease of the balance function widths in \delta\eta and \delta\phi with centrality from the highest sps to the lhc energy exhibits only small differences. this observation cannot be interpreted solely within the framework where the majority of the charge is produced at a later stage in the evolution of the heavy-ion collision.
measurement of the inclusive differential jet cross section in pp collisions at sqrt{s} = 2.76 tev. the alice collaboration at the cern large hadron collider reports the first measurement of the inclusive differential jet cross section at mid-rapidity in pp collisions at sqrt(s) = 2.76 tev, with integrated luminosity of 13.6 nb^-1. jets are measured over the transverse momentum range 20 to 125 gev/c and are corrected to the particle level. calculations based on next-to-leading order perturbative qcd are in good agreement with the measurements. the ratio of inclusive jet cross sections for jet radii r = 0.2 and r = 0.4 is reported, and is also well reproduced by a next-to-leading order perturbative qcd calculation when hadronization effects are included.
first measurement of theta_13 from delayed neutron capture on hydrogen in the double chooz experiment. the double chooz experiment has determined the value of the neutrino oscillation parameter $\theta_{13}$ from an analysis of inverse beta decay interactions with neutron capture on hydrogen. this analysis uses a three times larger fiducial volume than the standard double chooz assessment, which is restricted to a region doped with gadolinium (gd), yielding an exposure of 113.1 gw-ton-years. the data sample used in this analysis is distinct from that of the gd analysis, and the systematic uncertainties are also largely independent, with some exceptions, such as the reactor neutrino flux prediction. a combined rate- and energy-dependent fit finds $\sin^2 2\theta_{13}=0.097\pm 0.034(stat.) \pm 0.034 (syst.)$, excluding the no-oscillation hypothesis at 2.0 \sigma. this result is consistent with previous measurements of $\sin^2 2\theta_{13}$.
the radioelements at subatech. null
direct measurement of backgrounds using reactor-off data in double chooz. double chooz is unique among modern reactor-based neutrino experiments studying ν̅ e disappearance in that data can be collected with all reactors off. in this paper, we present data from 7.53 days of reactor-off running. applying the same selection criteria as used in the double chooz reactor-on oscillation analysis, a measured background rate of 1.0±0.4 events/day is obtained. the background model for accidentals, cosmogenic β-n-emitting isotopes, fast neutrons from cosmic muons, and stopped-μ decays used in the oscillation analysis is demonstrated to be correct within the uncertainties. kinematic distributions of the events, which are dominantly cosmic-ray-produced correlated-background events, are provided. the background rates are scaled to the shielding depths of two other reactor-based oscillation experiments, daya bay and reno.
steel slag filters to upgrade phosphorus removal in small wastewater treatment plants. this thesis aimed at developing the use of electric arc furnace steel slag (eaf-slag) and basic oxygen furnace steel slag (bof-slag) in filters designed to upgrade phosphorus (p) removal in small wastewater treatment plants. an integrated approach was followed, with investigation at different scales: (i) batch experiments were performed to establish an overview of the p removal capacities of steel slag produced in europe, and then to select the most suitable samples for p removal; (ii)continuous flow column experiments were performed to investigate the effect of various parameters including slag size and composition, and column design on treatment and hydraulic performances of lab-scale slag filters; (iii)finally, field experiments were performed to investigate hydraulic and treatment performances of demonstration scale slag filters designed to remove p from the effluent of a constructed wetland. the experimental results indicated that the major mechanism of p removal was related tocao-slag dissolution followed by precipitation of caphosphate and recrystallisation into hydroxyapatite (hap).over 100 weeks of continuous feeding of a synthetic psolution (mean inlet total p 10.2 mg p/l), columns filled with small-size slag (6-12 mm bof-slag; 5-16 mm eafslag)removed &gt;98% of inlet total p, whereas columnsfilled with big-size slag (20-50 mm bof-slag and 20-40mm eaf-slag) removed 56 and 86% of inlet total p,respectively. most probably, the smaller was the size ofslag, the greater was the specific surface for cao-slagdissolution and adsorption of ca phosphate precipitates.field experiments confirmed that eaf-slag and bof-slagare efficient substrate for p removal from the effluent of aconstructed wetland (mean inlet total p 8.3 mg p/l). overa period of 85 weeks of operation, eaf-slag removed 36%of inlet total p, whereas bof-slag removed 59% of inlettotal p. p removal efficiencies increased with increasing temperature and void hydraulic retention time (hrtv),most probably because the increase in temperature and hrtv affected the rate of cao dissolution and caphosphate precipitation. however, it was found that longhrtv (&gt;3 days) may produce high ph of the effluents(&gt;9), as the result of excessive cao-slag dissolution. however, the results of field experiments demonstrated that at shorter hrtv (1-2 days), slag filters produced ph that were elevated only during the first 5 weeks of operation, and then stabilized below a ph of 9. finally, a dimensioning equation based on the experimental results was proposed.
nuclear graphite waste's behaviour under disposal conditions : study of the release and repartition of organic and inorganic forms of carbon 14 and tritium in alkaline media. 23000 tons of graphite wastes will be generated during dismantling of the first generation of french reactors (9 gas cooled reactors). these wastes are classified as long lived low level wastes (llw-ll). as requested by the law, the french national radioactive waste management agency (andra) is studying concepts of low-depth disposals.in this work we focus on carbon 14, the main long-lived radionuclide in graphite waste (5730y), but also on tritium, which is the main contributor to the radioactivity in the short term. carbon 14 and tritium may be released from graphite waste in many forms in gaseous phase (14co2, ht...) or in solution (14co32-, hto...). their speciation will strongly affect their migration from the disposal site to the environment. leaching experiments, in alkaline solution (0.1 m naoh simulating repository conditions) have been performed on irradiated graphite, from saint-laurent a2 and g2 reactors, in order to quantify their release and characterize their speciation. the studies show that carbon 14 exists in both gaseous and aqueous phases. in the gaseous phase, release is weak (&lt;0.1%) and corresponds to oxidizable species. carbon 14 is mainly released into liquid phase, as both inorganic and organic species. 65% of released fraction is inorganic and 35% organic carbon. two tritiated species have been identified in gaseous phase: hto and ht/organically bond tritium. more than 90% of tritium in that phase corresponds to ht/obt. but release is weak (&lt;0.1%). hto is mainly in the liquid phase.
molecular models of natural organic matter and its colloidal aggregation in aqueous solutions: challenges and opportunities for computer simulations (keynote talk). null
structure and dynamics of co2, h2co3, hco3-, co32- in aqueous solutions: ab initio molecular dynamics simulations. null
structure and h-bonding of aqueous carbonate species from ab initio md simulation (invited talk). null
hybrid and nonlinear control of power converters. switched electronic systems are used in a huge number of everyday domestic and industrial utilities: liquid crystal displays, home appliances, lighting, personal computers, power plants, transportation vehicles and so on. efficient operations of all such applications depend on the essential "hidden work" done by switched electronic systems, whose behavior is determined by a suitable interconnection and control of analog and digital devices. as a motivation of this work, we consider the dc-dc power converters. this thesis contributes to provide hybrid and nonlinear control problem solutions to several types of power converters. in the first part we are interested in the problem of voltage regulation of power converters operating in discontinuous conducting mode. two power converters are considered: the boost converter and the buck-boost converter. the system does not admit a (continuous--time) average model approximation, hence is a hybrid system where the control objective is the generation of a periodic orbit and the actuator commands are switching times. our main contribution is a simple robust algorithm that gives explicit formulas for the switching times without approximations. simulation and experimental results that illustrate the robustness of the scheme to parameter uncertainty, as well as performance comparisons with current practice, are presented. in the second part a class of power converters that can be globally stabilized with an output-feedback pi controller has been identified. moreover, we will prove that the i&amp;i observer can be combined with the pi controller preserving the gas properties of the closed-loop. the class is characterized by a simple linear matrix inequality. the new controller is illustrated with the widely-popular, and difficult to control, single-ended primary inductor converter, for which simulation and experimental results are presented.
charged kaon femtoscopic correlations in pp collisions at $\sqrt{s}=7$ tev. correlations of two charged identical kaons (kch kch) are measured in pp collisions at sqrt{s}=7 tev by the alice experiment at the large hadron collider (lhc). one-dimensional kch kch correlation functions are constructed in three multiplicity and four transverse momentum ranges. the kch kch femtoscopic source parameters r and lambda are extracted. the kch kch correlations show a slight increase of femtoscopic radii with increasing multiplicity and a slight decrease of radii with increasing transverse momentum. these trends are similar to the ones observed for pi pi and ks0 ks0 correlations in pp and heavy-ion collisions. however, the observed one dimensional correlation radii for charged kaons are larger at high multiplicities than those for pions in contrast to what was observed in heavy-ion collisions at rhic.
fluid-fluid phase separation under metamorphic conditions: md simulations of a generalized composition h2o-co2-nacl (invited talk). null
hydrogen bonding and molecular ordering of water at mineral-solution interfaces (keynote talk). null
molecular dynamics simulations on the structure and dynamics of h2o, k+, nh4+ on hydrated muscovite surface. the investigation of water-mineral interfaces plays a pivotal role in the study of various environmental and geochemical problems. the prime reason for exploring the ion-water-mineral interactions lies in the fact that it determines several phenomena ranging from transport of elements, swelling, dispersion, and mineral alteration and so on. these interactions would be different for different surface cations and at different degrees of hydration. recent experimental studies support the notion that the k+ ions on the hydrated muscovite surface can be exchanged for the hydronium ions. similarly, surface k+ ions can also be exchanged for ammonium in the aqueous solution (e.g., in tobelite). we have investigated the structure and dynamics of these ionic species at the hydrated surface of muscovite by molecular dynamics (md) computer simulations using clayff force field. in addition to replacing the interlayer cations, a series of md simulations have been performed with varying amounts of water on the muscovite surface at ambient conditions for each of the exchanged cations. atomic density profiles as functions of the distance from the muscovite surface were calculated for all atoms types present in the simulation. the comparison of hydration states and the topological details of the h-bonding network around the three ions on the surface help to interpret the structure and dynamics of the ions in the confined geometries. the angular distributions of the h2o, h3o+, and nh4+ molecules with respect to the muscovite surface have been studied. the dynamics of water molecules were further examined by self-diffusion coefficients calculated from the mean square displacement of oxygen (or nitrogen) atoms and by exchange/reorientation time correlation functions of the surface molecules. results obtained from the simulations are compared with the available experimental data and other previous simulations and provide reliable molecular-scale view of the structure and dynamics of ions and water on the muscovite surface.
computational molecular modeling for nuclear waste management and other radiochemical applications (invited talk). safe and sustainable management of nuclear energy poses major scientific and engineering challenges, one of which is the necessity to make the environmental impacts of the long-term nuclear waste storage as small as possible. this requires significant improvements in our understanding of the behaviour of radionuclides and their retention mechanisms in geological formations of nuclear waste repositories over the ranges of time and distance spanning many orders of magnitude. detailed molecular scale knowledge of the complex chemical and physical processes controlling the interaction of radionuclides with clay and cementitious materials is crucial for building better predictive models of their adsorption and mobility in natural and engineered barriers of the nuclear waste repositories. the presence of natural organic matter (nom) in clayey formations and its complexation with metal ions in aqueous solutions has significant effect on the transport properties of the radionuclides. in this presentation, we will overview our current efforts to apply computational molecular modeling techniques to address these problems on the fundamental molecular level. we use classical molecular dynamics (md) simulations for detailed quantitative studies of the structural, energetic and dynamic aspects of interactions between radionuclides, organic matter and clay particles. structural and thermodynamic parameters are obtained by studying different processes such as hydration, adsorption, complexation, and intercalation. the complexation mechanisms of organic molecules with aqueous metal ions will then be presented using the free energy calculations. metal cations can strongly associate with negatively charged functional groups of organic molecules and with negatively charged clay surfaces. this allows us to predict that cationic bridging could be the most probable mechanism responsible for the controlling effects of organics on the behaviour of radionuclides is clays and other repository materials. our most recent results demonstrating how the nature of the adsorbed cations affects the structural and dynamic properties of the mineral-water interface and also the effect of disordered substitution in on the adsorption and swelling behaviour of clay minerals will then be discussed briefly.
molecular structure and dynamics of nano-confined aqueous solutions: computer simulations of clay, cement, and polymer membranes. molecular-scale knowledge of the thermodynamic, structural, and transport properties of water confined by interfaces and nano-pores of various materials is crucial for quantitative understanding and prediction of many natural and technological processes, including carbon sequestration, water desalination, nuclear waste storage, cement chemistry, fuel cell technology, etc. experimental nanoscale studies of such systems are not always feasible, and their results often require considerable interpretation in the efforts to extract surface- and confinement-specific quantitative information from the measurements. computational molecular modeling significantly complements such efforts and provides invaluable molecular-scale background for better understanding of the specific effects of the substrate structure and composition on the structure, dynamics and reactivity of interfacial and nano-confined aqueous solutions. based on the successful development and implementation of the clayff force field (cygan et al., 2004), we have recently performed a series of molecular dynamics simulations of aqueous interfaces with several representative inorganic and organic nanoporous materials (ahn et al., 2008; kalinichev et al., 2007, 2010; wang et al., 2006, 2009) in order to better understand and quantify the effects of the substrate composition and structure on the properties of interfacial and nano-confined aqueous solutions. individual h2o molecules and hydrated ions at interfaces simultaneously participate in several dynamic processes, which can be characterized by different, but equally important time- and length- scales. the first molecular layer of interfacial water at all substrates is often highly ordered, indicating reduced translational and orientational mobility of the h2o molecules. however, this ordering can not be simply described as "ice-like", but rather resembles the behavior of supercooled water or amorphous ice, although with significant substrate-specific variations. these results help to interpret experimental nmr, ir, x-ray, and neutron scattering measurements performed for the same systems. ahn, w.-y., kalinichev, a.g., clark, m.m. (2008) effects of background cations on the fouling of polyethersulfone membranes by natural organic matter: experimental and molecular modeling study. j.membr.sci., 309,128-140. cygan r.t., liang j.-j., kalinichev a.g. (2004) molecular models of hydroxide, oxyhydroxide, and clay phases and the development of a general force field. journal of physical chemistry b, 108, 1255-1266. kalinichev a.g., wang, j., kirkpatrick r. j. (2007) molecular dynamics modeling of the structure, dynamics and energetics of mineral-water interfaces: application to cement materials. cement and concrete res., 37, 337-347. kalinichev, a.g., kumar, p., kirkpatrick, r.j. (2010) effects of hydrogen bonding on the properties of layered double hydroxides intercalated with organic acids: mdf computer simulations. philos. mag., 90, 2475-2488. wang j., kalinichev a.g., kirkpatrick r.j. (2006) effects of substrate structure and composition on the structure, dynamics and energetics of water on mineral surfaces: md modeling study. geochim. cosmochim. acta, 70, 562-582. wang j., kalinichev a.g., kirkpatrick r.j. (2009) asymmetric hydrogen bonding and orientational ordering of water at hydrophobic and hydrophilic surfaces: a comparison of water/vapor, water/talc, and water/mica interfaces. j.phys.chem.c, 113, 11077-11085.
structure and energetics of smectite interlayer hydration: molecular dynamics investigations of na- and ca hectorite. molecular-scale interactions present at mineral-water interfaces and in clay interlayer galleries control numerous environmental processes, including chemical interactions in soils and transport of nutrients and pollutants through them.[1-4] understanding these processes requires accurate knowledge of the structure, energetics, and dynamics of the interaction among the mineral substrate, ions, and water molecules.[5, 6] challenges to this objective include experimental difficulties in probing these interfaces and interlayers at the molecular scale; fully characterizing the mineral substrate; and identifying how the mineral surface, ions, and water molecules each contribute to the overall structure, energetics, and dynamics of these systems.[6] linked computational molecular dynamics (md) simulations and experimental nuclear magnetic resonance (nmr) studies are particularly effective in addressing these issues.[7-9] here we focus on md studies of na- and ca-smectite (hectorite) interlayer galleries to provide a molecular-scale picture of the structure and dynamics of their hydration[9, 10] and to complement our earlier nmr investigations of these systems.[7-9] classical md simulations were undertaken in the npt and nvt ensembles to determine the structural and energetic changes with increasing hydration with focus on the single- and double-layer hydrates. the results show substantial changes in the hydration of the interlayer cations, the orientations of the water molecules, the hydrogen bond network involving the water molecules and basal oxygen atoms, and the resulting potential energies as the interlayer gallery expands. [1] scheidegger et al. (1996) soil science 161 813-831. [2] stumm (1997) colloids and surfaces a-physicochemical and engineering aspects 120 143-166. [3] o'day (1999) reviews of geophysics 37 249-274. [4] koretsky (2000) journal of hydrology 230 127-171. [5] wang et al. (2001) chemistry of materials 13 145-150. [6] wang et al. (2006) geochimica et cosmochimica acta 70 562-582. [7] bowers et al. (2008) journal of physical chemistry c 112 6430-6438. [8] bowers et al. (2011) journal of physical chemistry c 115 23395-23407. [9] bowers et al. (2012), unpublished. [10] morrow et al. (2012) journal of physical chemistry c, submitted.
molecular models of natural organic matter and its colloidal aggregation in aqueous solutions: challenges and opportunities for computer simulations. natural organic matter (nom) is ubiquitous in soil and groundwater and its aqueous complexation with various inorganic and organic species can strongly affect the speciation, solubility and toxicity of many elements in the environment. despite significant geochemical, environmental and industrial interest, the molecular-scale mechanisms of the physical and chemical processes involving nom are not yet fully understood. recent molecular dynamics (md) simulations using relatively simple models of nom fragments are used here to illustrate the challenges and opportunities for the application of computational molecular modeling techniques to the structural, dynamic, and energetic characterization of metal-nom complexation and colloidal aggregation in aqueous solutions. the predictions from large-scale md simulations are in good qualitative agreement with available experimental observations, but also point out to the need for simulations at much larger time and length scales with more complex nom models in order to fully capture the diversity of molecular processes involving nom.
computational molecular modeling of mineral-water interfaces for geochemical and environmental applications (invited lecture). null
slow diffusional dynamics of water in cement nanopores: multiscale challenges for atomistic modeling (topic leader). molecular modeling of the properties of aqueous solutions confined in the nanopores and at the interfaces of cementitious materials is complicated by the significant structural and compositional heterogeneity of these phases and also by the fact that many of the important processes span several orders of magnitude both in time and in length. here we present an attempt to quantify the diffusional dynamics of 0.25 m kcl aqueous solution in contact with a model c-s-h binding phase (tobermorite) on the basis of molecular dynamics computer simulations. at the (001) surface of tobermorite, two types of h2o molecules can be effectively distinguished: the ones that spend most of their time within channels between the drierketten chains of silica on the tobermorite surface, and the more mobile adsorbed molecules that reside right above the interface. within the channels, h2o molecules donate h-bonds to both the bridging and non-bridging oxygens of the si-tetrahedra as well as to other h2o. some of these molecules form particularly strong h-bonds persisting well over 100 ps, but many others undergo frequent librational motions and occasional diffusional jumps from one surface site to another. the average diffusion coefficients of the surface-associated h2o molecules that spend most of their time in the channels and those that lie above the nominal interface differ by about one order of magnitude (dh2o[internal]=5.0×10−11 m2/s and dh2o[external]=6.0×10−10 m2/s, respectively). the average diffusion coefficient for all surface-associated h2o molecules is about 1.0×10−10 m2/s. all of these values are significantly less than the value of 2.3×10−9 m2/s, characteristic of h2o self-diffusion in bulk liquid water. the md simulations provided an opportunity to further quantify these relatively slow diffusional motions of h2o at the tobermorite interface on the longer time- and length- scale in terms of the van hove self-correlation function (vhscf). the emerging picture is in surprisingly good agreement with available experimental data on the dynamics of surface-associated water in similar cement materials obtained by 1h nmr [1,2]. 1. korb j.p., monteilhet l., mcdonald p.j., mitchell j., microstructure and texture of hydrated cement-based materials: a proton field cycling relaxometry approach. cement and concrete research, 37, 2007, 295-302. 2. korb j.p., nmr and nuclear spin relaxation of cement and concrete materialscurrent opinion in colloid &amp; interface science, 14, 2009, 192-202.
mineral-fluid interactions on the molecular scale: computational atomistic modeling of clay-related materials for geochemical and environmental applications. null
molecular dynamics simuation of cs+ on the hydrated muscovite surface: local structural environment and dynamics. adsorption of metal cations on mineral surfaces often controls their distribution in both natural and technological environments. the callovo-oxfordian (cox) formation, consisting largely of clay minerals like illite and smectite, is the location investigated in france as a site for geological nuclear waste disposal and storage. the uptake of radionuclides by layered clay minerals is the principal retention process for their diffusion. hence, detailed molecular-scale understanding of the adsorption mechanisms of radionuclides on silicate minerals is essential, because it can significantly influence their mobility under the conditions of nuclear waste repositories. cs+ ion is one of the important components of nuclear waste that is highly soluble in water and migrates easily in surface and sub-surface environments. the atomically smooth surface of muscovite mica, kal2(si3al)o10(oh)2 is often used as an accurate model of illite clay. experimental studies suggest that cs+ ion adsorbs directly at the muscovite surface as an "inner sphere complex" [1]. we have investigated the structure and dynamics of cs+ (exchanged for k+) and h2o molecules at the surface of muscovite at two different hydration levels by molecular dynamics (md) computer simulations using fully flexible clayff force field [2]. at the muscovite (001) surface, water molecules can donate 2 hydrogen bonds (to other h2o and/or to the surface o atoms) and accept 2 h-bonds (from other h2o). water molecules can also partially replace surface cations, because their hydrogens bear some positive charge. such surface-adsorbed h2o molecules have their negatively charged oxygen atoms exposed to the fluid phase and accessible for either h-bond acceptance from other h2os or for their coordination of surface cations in the inner-sphere or outer-sphere configuration. atomic density profiles of the surface species evidently support the presence of cs+ as inner sphere complexes at the muscovite interface. angular distributions of h2o molecular orientations with respect to the muscovite surface have also been studied, as well as the dynamical behaviour of surface species in terms of their self-diffusion coefficients, h-bonding time correlation functions, and residence times. the comparison of the surface behaviour at two different hydration states and the topological details of the interfacial h-bonding network provide new insight into the structure and dynamics of hydrated cs+ at confined geometries. the md simulation results are compared with available experimental data and the results of previous molecular simulations [3] to provide reliable molecular view of the hydrated cs+ ions at the surface of illite. references [1] kim, y., kirkpatrick, j.r., cygan, r.t. geochim. cosmochim acta, 60, 4059-4074 (1996). [2] cygan, r.t., liang, j.j., kalinichev, a.g. j. phys. chem. b, 108, 1255-1266 (2004). [3] wang, j.w., kalinichev, a.g., kirkpatrick, r.j., cygan, r.t. j. phys. chem. b, 109, 15893-15905 (2005).
molecular modeling of the swelling properties and interlayer structure of cs, and k-montmorillonites: effects of charge distribution in the clay layers. reliable prediction of the behaviour of radionuclides and their transport and retention in clayey formations at nuclear waste repositories requires detailed molecular scale understanding of these complex multicomponent systems. as the first step in our study of the effects of organic molecules on the adsorption and transport of radionuclides in hydrated clay systems we have investigated the effects of the ordering in charge distributions on the swelling behavior of montmorillonite (a smectite clay). montmorillonite layered structure consists of aluminum-oxygen octahedral sheet sandwiched between two opposing silicon-oxygen tetrahedral sheets giving rise to a 2:1 clay mineral. isomorphic substitutions in the tetrahedral and octahedral sheets are responsible of the negative layer charge of montmorillonite clay minerals having the chemical composition (si8-xxx)(al4-yyy)o20(oh)4 where x = al3+, y = mg2+, fe2+...[1]. the montmorillonite models for our study are based on a pyrophillite unit cell structure (5.16å×8.966å×9.347å) obtained from the crystallographic data of lee et al. [2]. the 4×4×2 simulation supercells were built and substitutions were made in the pyrophillite structure in order to approximate as close as possible the chemical composition of wyoming montmorillonite [1] m24(si248al8)(al112mg16)o640(oh)128, where m is either cs+, or k+. we have explored three different models of the substitution distributions. in the first model, the substitutions were uniformly and orderly distributed within the tetrahedral and octahedral sheets. in the second model, the substituted positions were kept ordered in the octahedral sheets but made disordered in the tetrahedral one. in the third model, the substituted positions of the octahedral sites were additionally made disordered. in order to study the swelling behavior of these montmorillonites, npt-ensemble molecular dynamics (md) simulations were run at t = 298 k and p = 1 bar for each of the three different substitution models and with 23 different hydration states ranging from 0 to 700 mgwater/gclay (from 0 to 42 h2o molecules per one monovalent cation). all md runs were performed for a total of 2 ns using the clayff force field [3]. after the system reached equilibrium, the last 1ns of each md trajectory was used to compute the clay basal spacing and the swelling thermodynamic properties: hydration energy, immersion energy, isosteric heat of adsorption. these calculations indicate that in addition to the commonly observed 1-layer and 2-layer hydrates, stable hydration states corresponding to 3-layer and 4-layer hydrates can also be distinguished. these stable states (minima of hydration and immersion energies) were then selected to run further 500 ps nvt-ensemble md simulations at the same temperature. the equilibrium parts of these nvt-simulated trajectories were then used to calculate radial distribution functions and atomic density profiles of the interlayer species in hydrated montmorillonites. references [1] tsipursky, s.i., drits, v.a. clay minerals, 19, 177-193 (1984). [2] lee, j.h. and guggenheim, s. american mineralogist, 66, 350-357 (1981). [3] cygan, r.t., liang, j.j., kalinichev, a.g. journal of physical chemistry b, 108, 1255-1266 (2004).
effects of surface cations on the structure and dynamics of the hydrogen-bonding network at the illite-water interface: a molecular dynamics simulation study. safe and sustainable management of nuclear energy poses major scientific and engineering challenges, one of which is the necessity to make the environmental impacts of the long-term nuclear waste storage as small as possible. this requires detailed understanding and prediction of the behaviour of radionuclides and their migration and retention properties in the geological formations of nuclear waste repositories. the callovo-oxfordien rock formation of the french nuclear repository site is mainly composed of clay minerals (illite, smectite and interstratified illite/smectite), quartz, calcite, with some non-negligible amount of organic matter. the adsorption of water can change the properties of mineral surfaces, including protonation state, surface charge, structure, and reactivity [1]. similarly, the properties of interfacial water are strongly affected by the mineral substrate structure and composition. recent advances in experimental techniques such as ftir [2], ellipsometry [3], synchrotron x-ray scattering [4], sum-frequency vibrational spectroscopy [5] are capable of probing the properties of mineral-water interfaces at different levels of hydration. however, the surface-specific results of these experiments are often difficult to quantitatively interpret without having a reliable molecular scale picture of the underlying physical and chemical processes. molecular computer simulations have become one of the most important tools in the study of such interfacial systems and phenomena by providing invaluable atomistic information on the underlying chemical and physical processes. the present study is aimed at investigating the structural and dynamics effects of three different cations (k+, nh4+, and h3o+) exchanged at the hydrated surface of muscovite mica, which is taken here as a model illite. molecular dynamics computer simulations were performed using the clayff force field [6] to investigate the important differences of the h-bonding configurations formed by the sorbed species, including h2o, h3o+, and nh4+, in contrast to the behavior of spherical metal ions, such as k+. at the muscovite (001) surface, h2o can donate 2 h-bonds (to other h2o and/or to the surface o atoms) and accept 2 h-bonds (from other h2o), but it can also partially replace surface k+, because the hydrogens of h2o bear some positive charge. this behavior was observed in previous md simulations of the mica surface [7]. such surface-adsorbed h2o molecules have their negatively charged oxygen atoms exposed to the fluid phase and accessible for either h-bond acceptance from other h2os or hydration of metal cations in outer-sphere coordination. for surface h3o+, the charges on the hydrogens are slightly higher than those of h2o, but the oxygen atom of hydronium is now almost hydrophobic, and cannot participate in a h-bond network (e.g., [8]). in contrast, nh4+ can equally well donate h-bonds to the surface o atoms and to the neighboring h2o molecules, but it cannot participate in the hydration shell of a displaced metal cation. thus, three similar species (h2o - two hb donors and 2 hb acceptors; h3o+ - 3 hb donors and no acceptors; nh4+ - 4 hb donors, no acceptors) can provide for three greatly different structural, energetic, and dynamical situations at the muscovite-water interface. since the hydrogen-bonding network in any aqueous media provides a natural mechanism of forming low-barrier reaction paths for proton transfer in such systems, it is also an important phenomenon controlling the surface reactivity under various ph conditions. in addition, a detailed study of the structural characteristics of surface-adsorbed nh4+ provides a way for better understanding of the mechanisms of adsorption for organic molecules having amino-groups in their structure, which is quite common for natural organic matter (e.g., [9]). each of the three systems was simulated at 7 different hydration states providing information on the structure and dynamics of the adsorbed water film in a wide range of relative humidity conditions. the atomic density profiles of water show significant layering at all hydration levels and the layering strongly depends upon the nature of the ionic species present on the surface. our studies support the fact that the h3o+ ion is less strongly bound when compared to k+ on the muscovite surface as observed in earlier studies [7]. at muscovite surfaces, both nh4+ and h3o+ cations establish strong hydrogen bonds with the surface bridging oxygen atoms and also with the neighbouring h2o molecules at all hydration levels. however, we observed that the interactions are different for both species at low hydration levels (&lt;&lt; molecular monolayer). at low hydration levels, h3o+ prefers to strongly bind as 3-cordinated species to the surface than with the neighbouring waters molecules. however, as the hydration levels increase, h3o+ binds as 2-cordinated species with the surface as is indicated by hydrogen bonding analysis (figure 1). in contrast, irrespective of the hydration levels, nh4+ ion strongly interacts with the surface as 3-cordinated species because of its tetrahedral geometry. at the same time, we observe from hydrogen bond analysis that the hydrogen bonding network of water has been strongly influenced by the nature of the surface cations present at the mineral-water interface. the dynamics of water molecules were examined by self-diffusion coefficients from the mean square displacement of water oxygen. the diffusion mechanism is similar for k+ and nh4+ but was different for h3o+, in particular at the low hydration states. furthermore, the spatial and orientation distributions of h2o and ions at the muscovite-water surface are analyzed in quantitative detail. all the simulation results are compared with available experimental data and the results of previous molecular simulations to provide reliable molecular view of the ions and water at the muscovite surface. references [1] henderson, m. a. surf. sci. rep. 2002, 46, 5-308. [2] cantrell and g. e. ewing. j. phys. chem. b, 2001, 105, 5434-5439. [3] beaglehole, d and christenson, h. k. j. phys. chem, 1992, 96, 3395-3403. [4] fenter, p. and sturchio, n. c. progress in surface science, 2004, 77, 171-258. [5] shen, y. r. and ostroverkhov, v. chem. rev., 2006, 106, 1140-1154. [6] cygan, r.t., liang, j.j., and kalinichev, a.g. j. phys. chem. b, 2004, 108, 1255-1266. [7] wang, j., kalinichev, a., kirkpatrick, r., cygan, r. j. phys. chem b., 2005, 109, 15893-15905. [8] petersen, p.b. and saykally, r.j. j. phys. chem. b, 2005, 109, 7976-7980. [9] leenheer, j.a. annals of environmental science, 2009, 3, 1-130.
molecular modeling of the swelling properties and interlayer structure of cs, na, k-montmorillonite: effects of charge distribution in the clay layers. safe and sustainable management of nuclear waste poses major scientific challenges to make the environmental footprint of nuclear energy as small as possible for very long periods of time. as many other countries, france is considering the deep geological disposal (in the callovo-oxfordian (cox) argillite formations of the paris basin) as a reliable way of storing high-level radioactive waste in order to provide adequate protection for humans and the environment. in addition to being proven geologically stable for million years, the natural and engineered clay barriers can benefit from many favorable properties, such as low permeability, high sorption capacity, etc. the mineralogical composition of the callovo-oxfordian argillite shows about 41% of clay minerals (23% of interstratified illite/smectite, 14% of illite-type minerals, 2% kaolinite and 2% chlorite) [1,2]. a non-negligible amount of organic matter is also present (~1%) [3], and it is known that the interaction of natural organic matter (nom) with radionuclides and clays can affect the solubility and toxicity of trace elements in natural aqueous environments [4,5]. reliable prediction of the behaviour of radionuclides and their transport and retention in clayey formations at nuclear waste repositories requires detailed molecular scale understanding of these complex multicomponent systems. computational molecular modelling has already become an important tool in the study of thermodynamic, structural and transport properties of hydrated clays (e.g., [6-8]). as the first step in our study of the effects of organic molecules on the adsorption and transport of radionuclides in hydrated clay systems we have investigated the effects of the ordering in charge distributions on the swelling behavior of simulated clays. montmorillonite was chosen as a model of smectite clay. montmorillonite structure consists of aluminum-oxygen octahedral sheet sandwiched between two opposing silicon-oxygen tetrahedral sheets giving rise to a 2:1 clay mineral. isomorphic substitutions in the tetrahedral and octahedral sheets are responsible of the negative layer charge of montmorillonite clay minerals having the chemical composition (si8-xxx)(al4-yyy)o20(oh)4 where x = al, y = mg, fe...[9]. the montmorillonite models for our study are based on a pyrophillite unit cell structure (5.16å×8.966å×9.347å) obtained from the crystallographic data of lee et al. [10]. the 4×4×2 simulation supercells were built and substitutions were made in the pyrophillite structure in order to approximate as close as possible the chemical composition of wyoming montmorillonite m24(si248al8)(al112mg16)o640(oh)128, where m is either cs+, na+, or k+ [9]. we explored three different models of substitution distributions. in the first model, the substitutions were uniformly and orderly distributed within the tetrahedral and octahedral sheets. in the second model, the substituted positions were kept ordered in the octahedral sheets but made disordered in the tetrahedral one. in the third model, the substituted positions of the octahedral sites were additionally made disordered. in order to study the swelling behavior of these montmorillonites, molecular dynamics (md) simulations were run in the npzt statistical ensemble (t = 298 k, pz = 1 bar) for each of the three different substitution models and with 22 different hydration states ranging from 0 to 700 mgwater/gclay (from 0 to 42 h2o molecules per one monovalent cation). all md runs were performed for a total of 2 ns using the clayff force field [11]. at the beginning of the simulations, the cations were placed at the midplane of the clay interlayer space and water molecules were added randomly. after the system reached equilibrium, the last 1ns of each md trajectory was used to compute the clay basal spacing and the swelling thermodynamic properties: hydration energy, immersion energy, isosteric heat of adsorption. the md simulation results indicate that in addition to the commonly observed 1-layer and 2-layer hydrates, stable hydration states corresponding to 3-layer and 4-layer hydrates can also be distinguished. the stable states corresponding to the minima of hydration energy were then selected to run further 500 ps nvt-ensemble md simulations at the same temperature and with the volume fixed at the average value resulting from the corresponding previous npzt simulation. the equilibrium parts of these nvt-simulated trajectories were then used to calculate the structural (radial distribution functions, atomic density profiles) and dynamical (diffusion coefficient) properties of the hydrated montmorillonite. references [1] erm (1997) echantillons d'argiles du forage est104 : etude minéralogique approfondie. rapport andra n° d.rp.0erm.97.008 [2] erm (1996b) caractérisation d'échantillons d'argiles du forage est103. rapport andra n° b.rp.0erm.96.003 [3] andra (2005) dossier 2005 argile, référentiel du site de meuse haute marne. c.r.p.ads.04.0022 andra : paris [4] buffle, j. (1988) complexation reactions in aquatic systems: an analytical approach; ellis horwood ltd.:chichester, p 692. [5] tipping, e. (2002) cation binding by humic substances, cambridge university press: cambridge, p 434. [6] smith, d.e., langmuir, 14, 5959-5967 (1998). [7] rotenberg, b., marry, v., vuilleumier, r., malikova, n., simon, c., turq, p., geochim. cosmochim. acta, 71, 5089-5101 (2007). [8] liu, x.d., lu, x.c., wang, r.c., zhou, h.q. geochim. cosmochim. acta, 72, 1837-1847 (2008). [9] tsipursky, s.i., drits, v.a. clay minerals, 19, 177-193 (1984). [10] lee, j.h. and guggenheim, s. american mineralogist, 66, 350-357 (1981). [11] cygan, r.t., liang, j.j., kalinichev, a.g. journal of physical chemistry b, 108, 1255-1266 (2004).
on the hydrogen bonding structure at the aqueous interface of ammonium-substituted mica: a molecular dynamics simulation. molecular dynamics (md) computer simulations were performed for an aqueous film of 3nm thickness adsorbed at the (001) surface of ammonium-substituted muscovite mica. the results provide a detailed picture of the near-surface structure and topological characteristics of the interfacial hydrogen bonding network. the effects of d/h isotopic substitution in n(h/d)4+ on the dynamics and consequently on the convergence of the structural properties have also been explored. unlike many earlier simulations, a much larger surface area representing 72 crystallographic unit cells was used, which allowed for a more realistic representation of the substrate surface with a more disordered distribution of al/si isomorphic substitutions in muscovite. the results clearly demonstrate that under ambient conditions both interfacial ammonium ions and the very first layer of water molecules are h-bonded only to the basal surface of muscovite, but do not form h-bonds with each other. as the distance from the surface increases, the h-bonds donated to the surface by both n(h/d)4+ and h2o are gradually replaced by the h-bonds to the neighboring water molecules, with the ammonia ions experiencing one reorientational transition region, while the h2o molecules experiencing three such distinct consecutive transitions. the hydrated n(h/d)4+ ions adsorb almost exclusively as inner-sphere surface complexes with the preferential coordination to the basal bridging oxygen atoms surrounding the al/si substitutions.
consistency of the french white certificates evaluation system with the framework proposed for the european energy services. according to the directive on energy end-use efficiency and energy services (esd), the european member states shall adopt a national indicative energy savings target of 9% (or beyond) in 2016. the issue of the energy savings evaluation is crucial for its implementation. the french white certificates (fwc) scheme is one of the important measures for france to fulfill its esd target. however, the accountings of energy savings in the fwc scheme and in the esd are different. therefore, an analysis of the consistency of the two systems is needed. a concrete example of actions on residential buildings is used to illustrate the challenges for policy marker and stakeholders to set harmonized evaluation rules. the fwc and esd calculations appear to be consistent from a physics point of view, as long as calculations are well-documented. but due to differences in the policy objectives, calculation routines may be necessary to convert national energy savings unit (e.g., kwh cumac) into supranational energy savings unit (e.g., esd kwh). finally, the work done to establish a transparent evaluation system brings additional benefits (e.g., increased visibility and quality of the actions), which will improve the results of the energy efficiency policies on long term.
forcing in coq. null
constraints on the origin of cosmic rays above 10^18 ev from large-scale anisotropy searches in data of the pierre auger observatory. a thorough search for large-scale anisotropies in the distribution of arrival directions of cosmic rays detected above 10^18 ev at the pierre auger observatory is reported. for the first time, these large-scale anisotropy searches are performed as a function of both the right ascension and the declination and expressed in terms of dipole and quadrupole moments. within the systematic uncertainties, no significant deviation from isotropy is revealed. upper limits on dipole and quadrupole amplitudes are derived under the hypothesis that any cosmic ray anisotropy is dominated by such moments in this energy range. these upper limits provide constraints on the production of cosmic rays above 1018 ev, since they allow us to challenge an origin from stationary galactic sources densely distributed in the galactic disk and emitting predominantly light particles in all directions.
anti-unification with type classes. the anti-unification problem is that of finding the most specific pattern of two terms. while dual to the unification problem, anti-unification has rarely been considered at the level of types. in this paper, we present an algorithm to compute the least general type of two types in haskell, using the logic programming power of type classes. that is, we define a type class for which the type class instances resolution performs anti-unification. we then use this type class to define a type-safe embedding of aspects in haskell.
a new approach to characterize the nanostructure of activated carbons from mathematical morphology applied to high resolution transmission electron microscopy images. a new characterization method of the nanoporous structure of activated carbons (acs) is proposed, based on mathematical morphology analysis of high resolution transmission electron microscopy (tem) images. it produces refined statistics describing the shape, size and orientation of the defective graphene sheets seen edge on as individual fringes on tem images. it also provides some quantitative information regarding their spatial arrangement. especially, assemblages composed of 2-4 nearly parallel fringe fragments could be detected, which were relevant of some partial stacking of the defective graphene sheets. such assemblages were possibly locally oriented along a common direction to form large continuous domains. to prove the ability of the image analysis tool to reveal distinctive features and degrees of disorder of the ac structures, a set of various commercial carbon adsorbents was characterized. the measured effective spaces separating the individual fringes, the stacks and the continuous domains were examined and compared with the porosity data derived from 77 k-n2 adsorption isotherms. consistency between the two sets of data was assessed and interpreted by considering the n2 diffusional limitations resulting from the micropore network connectivity.
adapting transformations to metamodel changes via external transformation composition. null
a typed monadic embedding of aspects. we describe a novel approach to embed pointcut/advice aspects in a typed functional programming language like haskell. aspects are first-class, can be deployed dynamically, and the pointcut language is extensible. type soundness is guaranteed by exploiting the un- derlying type system, in particular phantom types and a new anti- unification type class. the use of monads brings type-based rea- soning about effects for the first time in the pointcut/advice setting, thereby practically combining open modules and effectiveadvice, and enables modular extensions of the aspect language.
long-range angular correlations on the near and away side in p-pb collisions at sqrt(snn) = 5.02 tev. angular correlations between charged trigger and associated particles are measured by the alice detector in p-pb collisions at a nucleon-nucleon centre-of-mass energy of 5.02 tev for transverse momentum ranges within 0.5 &lt; pt,assoc &lt; pt,trig &lt; 4 gev/c. the correlations are measured over two units of pseudorapidity and full azimuthal angle in different intervals of event multiplicity, and expressed as associated yield per trigger particle. two long-range ridge-like structures, one on the near side and one on the away side, are observed when the per-trigger yield obtained in low-multiplicity events is subtracted from the one in high-multiplicity events. the excess on the near-side is qualitatively similar to that recently reported by the cms collaboration, while the excess on the away-side is reported for the first time. the two-ridge structure projected onto azimuthal angle is quantified with the second and third fourier coefficients as well as by near-side and away-side yields and widths. the yields on the near side and on the away side are equal within the uncertainties for all studied event multiplicity and pt bins, and the widths show no significant evolution with event multiplicity or pt. these findings suggest that the near-side ridge is accompanied by an essentially identical away-side ridge.
large-scale distribution of arrival directions of cosmic rays detected above 10^18 ev at the pierre auger observatory. a thorough search for large-scale anisotropies in the distribution of arrival directions of cosmic rays detected above 10^18 ev at the pierre auger observatory is presented. this search is performed as a function of both declination and right ascension in several energy ranges above 10^18 ev, and reported in terms of dipolar and quadrupolar coefficients. within the systematic uncertainties, no significant deviation from isotropy is revealed. assuming that any cosmic-ray anisotropy is dominated by dipole and quadrupole moments in this energy range, upper limits on their amplitudes are derived. these upper limits allow us to test the origin of cosmic rays above 10^18 ev from stationary galactic sources densely distributed in the galactic disk and predominantly emitting light particles in all directions.
action-perception trade-offs for anguilliform swimming robotic platforms with an electric sense. the work presented addresses the combination of anguilliform swimming-based propulsion with the use of an electric sensing modality for a class of unmanned underwater vehicles, and in particular investigates the relative influence of adjustments to the swimming gait on the platform's displacement speed and on sensing performance. this influence is quantified, for a relevant range of swimming gaits, using experimental data recordings of displacement speeds, and a boundary element method-based numerical simulation tool allowing to reconstruct electric measures. results show that swimming gaits providing greater movement speeds tend to degrade sensing performance. conversely, gaits yielding accurate sensing tend to prove slower. to reconcile opposing tendencies, a simple action-perception cost function is designed, with the purpose of adjusting an anguilliform swimmer's gait shape, in accordance with respective importance afforded to action (i.e. movement speed) and perception.
a hybrid dynamic model of an insect-like mav with soft wings. this paper presents a hybrid dynamic model of a 3-d aerial insect-like robot. the soft-bodied insect wings modeling is based on a continuous version of the newton-euler dynamics where the leading edge is treated as a continuous cosserat beam. these wings are connected to an insect's rigid thorax using a discrete recursive algorithm based on the newton-euler equations. here we detail the inverse dynamic model algorithm. this version of the dynamic model solves the following two problems involved in any locomotion task: 1◦) it enables the net motion of a reference body to be computed from the known data of internal motions (strain fields); 2◦) it gives the internal torques required to impose these internal (strain fields) motions. the essential fluid effects have been taken into account using a simplified analytical hovering flight aerodynamic model. to facilitate the analysis of numerical results, a visualization tool is developed.
estimation of relative position and coordination of mobile underwater robotic platforms through electric sensing. in the context of underwater robotics, positioning and coordination of mobile agents can prove a challenging problem. to address this issue, we propose the use of electric sensing, with a technique inspired by weakly electric fishes. in particular, the approach relies on one or several of the agents applying an electric field to their environment. using electric measures, others agents are able to reconstruct their relative position with respect to the emitter, over a range that is function of the geometry of the emitting agent and of the power applied to the environment. efficacy of the technique is illustrated using a number of numerical examples. the approach is shown to allow coordination of unmanned underwater vehicles, including that of bio-inspired swimming robotic platforms.
macro-continuous dynamics for hyper-redundant robots: application to locomotion bio-inspired by elongated animals. this article presents a unified dynamic modeling approach of continuum robots. the robot is modeled as a geometrically exact beam continuously actuated through an active strain law. once included into the geometric mechanics of locomotion, the approach applies to any hyper-redundant or continuous robot devoted to manipulation and/or locomotion. furthermore, exploiting the nature of the resulting models as being a continuous version of the newton-euler models of discrete robots, an algorithm is proposed which is capable of computing the internal control torques (and/or forces) as well as the rigid overall motions of the locomotor robot. the efficiency of the approach is finally illustrated through many examples directly related to the terrestrial locomotion of elongated animals as snakes, worms or caterpillars and their associated bio-mimetic artifacts.
evaluation of the anthropogenic influx of metal and metalloid contaminants into the moulay bousselham lagoon, morocco, using chemometric methods coupled to geographical information systems. superficial and cored sediment samples from the moulay bousselham lagoon and sub-watershed were analyzed for al, fe, cu, zn, pb, mn, ni, cr, as, hg and cd. the temporal and spatial distributions of the main contamination sources of heavy metals were identified and described using chemometric and gis methods. sediments from coastal lagoons near urban and agricultural areas are commonly contaminated with heavy metals and the concentrations found in surface sediments are significantly higher than those from 50-100 years ago. the concentrations of these elements decrease sharply with depth in the sediment column and the elements are preferentially enriched in the &lt;2 µm-size fraction of the sediment. the zones of enhanced risk of heavy metals were detected by means of gis-based geostatistical modeling. according to sediment pollution indices and statistical analysis, heavy metals (pb, cu, ni, zn, cr and hg) that pose a risk have become largely enriched in the lagoon sediments during the recent period of agricultural intensification.
the conjunction of interval among constraints. an among constraint holds if the number of variables that belong to a given value domain is between given bounds. this paper focuses on the case where the variable and value domains are intervals. we investigate the conjunction of among constraints of this type. we prove that checking for satisfiability -- and thus, enforcing bound consistency -- can be done in polynomial time. the proof is based on a specific decomposition that can be used as such to filter inconsistent bounds from the variable domains. we show that this decomposition is incomparable with the natural conjunction of \textsc{among} constraints, and that both decompositions do not ensure bound consistency. still, experiments on randomly generated instances reveal the benefits of this new decomposition in practice. this paper also introduces a generalization of this problem to several dimensions and shows that satisfiability is np-complete in the multi-dimensional case.
let effects on the hydrogen production induced by the radiolysis of pure water. radiation chemical primary yields g(h2) have been determined for irradiations performed with 60co γ-rays source of lcp (orsay, france) and with helium ion beams (eα=5.0 mev-64.7 mev) using protective agent bromide anions in solution. the α (4he2+) irradiation experiments were performed either at cemhti or at the new arronax cyclotron facility (2010). both sources (γ and cyclotrons) allow working with a large let value range between 0.23 and 151.5 kev/μm. on one hand, the obtained results have been compared with those available in the literature and plotted as a function of the let parameter in order to discuss the effects of track structure on the production of molecular hydrogen. on the other hand, the primary radiation chemistry yield g(h2) values are compared with global radiation chemical yields g(h2) obtained during irradiations of pure water irradiated under air or argon without scavenging. for each system, it appears that radiation chemical yields increase with the let value. our results suggest that using bromide anions, at low concentration, as a protective agent becomes ineffective when the let value used is higher than 120±20 kev/μm.
optimality criteria for measurement poses selection in calibration of robot stiffness parameters. the paper focuses on the accuracy improvement of industrial robots by means of elasto-static parameters calibration. it proposes a new optimality criterion for measurement poses selection in calibration of robot stiffness parameters. this criterion is based on the concept of the manipulator test pose that is defined by the user via the joint angles and the external force. the proposed approach essentially differs from the traditional ones and ensures the best compliance error compensation for the test configuration. the advantages of this approach and its suitability for practical applications are illustrated by numerical examples, which deal with calibration of elasto-static parameters of planar manipulator with rigid links and compliant actuated joints.
stability of manipulator configuration under external loading. the paper is devoted to the analysis of robotic manipulator behavior under internal and external loadings. the main contributions are in the area of stability analysis of manipulator configurations corresponding to the loaded static equilibrium. in contrast to other works, in addition to usually studied the end-platform behavior with respect to the disturbance forces, the problem of configuration stability for each kinematic chain is considered. the proposed approach extends the classical notion of the stability for the static equilibrium configuration that is completely defined the properties of the cartesian stiffness matrix only. the advantages and practical significance of the proposed approach are illustrated by several examples that deal with serial kinematic chains and parallel manipulators. it is shown that under the loading the manipulator workspace may include some specific points that are referred to as elastostatic singularities where the chain configurations become unstable.
design of experiments for calibration of planar anthropomorphic manipulators. the paper presents a novel technique for the design of optimal calibration experiments for a planar anthropomorphic manipulator with n degrees of freedom. proposed approach for selection of manipulator configurations allows essentially improving calibration accuracy and reducing parameter identification errors. the results are illustrated by application examples that deal with typical anthropomorphic manipulators.
about the nature of the bonding in ato+. null
characterization of at- and ato+ species in simple media by high performance ion exchange chromatography coupled to gamma detector. application for astatine speciation in human serum. null
speciation of rn in blood serum: a key issue in alpha therapy. null
introducing the elf topological analysis in the field of quasirelativistic quantum calculations. we present an original formulation of the electron localization function (elf) in the field of relativistic two-component dft calculations. using i2 and at2 species as a test set, we show that the elf analysis is suitable to evaluate the spin-orbit effects on the electronic structure. beyond these examples, this approach opens up new opportunities for the bonding analysis of large molecular systems involving heavy and super-heavy elements.
on matrices, automata, and double counting in constraint programming. matrix models are ubiquitous for constraint problems. many such problems have a matrix of variablesm, with the same constraint c defined by a finitestate automaton a on each row ofmand a global cardinality constraint gcc on each column of m. we give two methods for deriving, by double counting, necessary conditions on the cardinality variables of the gcc constraints from the automaton a. the first method yields linear necessary conditions and simple arithmetic constraints. the second method introduces the cardinality automaton, which abstracts the overall behaviour of all the row automata and can be encoded by a set of linear constraints. we also provide a domain consistency filtering algorithm for the conjunction of lexicographic ordering constraints between adjacent rows ofmand (possibly different) automaton constraints on the rows. we evaluate the impact of our methods in terms of runtime and search effort on a large set of nurse rostering problem instances.
review of chemical and radiotoxicological properties of polonium. the discovery of polonium (po) was first published in july, 1898 by p. curie and m. curie. it was the first element to be discovered by the radiochemical method. polonium can be considered as a famous but neglected element: only a few studies of polonium chemistry have been published, mostly between 1950 and 1990. the recent (2006) event in which 210po evidently was used as a poison to kill a. litvinenko has raised new interest in polonium. 2011 being the 100th anniversary of the marie curie nobel prize in chemistry, the aim of this review is to look at the several aspects of polonium linked to its chemical properties and its radiotoxicity, including (i) its radiochemistry and interaction with matter; (ii) its main sources and uses; (iii) its physicochemical properties; (iv) its main analytical methods; (v) its background exposure risk in water, food, and other environmental media; (vi) its biokinetics and distribution following inhalation, ingestion, and wound contamination; (vii) its dosimetry; and (viii) treatments available (decorporation) in case of internal contamination.
effects of experience and workplace culture in human-robot team interaction in robotic surgery: a case study. robots are being used in the operating room to aid in surgery, prompting changes to workflow and adaptive behavior by the users. this case study presents a methodology for examining human-robot team interaction in a complex environment, along with the results of its application in a study of the effects of experience and workplace culture, for human-robot team interaction in the operating room. the analysis of verbal and non-verbal events in robotic surgery in two different surgical teams (one in the us and one in france) revealed differences in workflow, timeline, roles, and communication patterns as a function of experience and workplace culture. longer preparation times and more verbal exchanges related to uncertainty in use of the robotic equipment were found for the french team, who also happened to be less experienced. this study offers an effective method for studying human-robot team interaction and has implications for the future design and training of teamwork with robotic systems in other complex work environments.
haptic communication to support biopsy procedures learning in virtual environments. in interventional radiology, physicians require high haptic sensitivity and fine motor skills development because of the limited real-time visual feedback of the surgical site. the transfer of this type of surgical skill to novices is a challenging issue. this paper presents a study on the design of a biopsy procedure learning system. our methodology, based on a task-centered design approach, aims to bring out new design rules for virtual learning environments. a new collaborative haptic training paradigm is introduced to support human-haptic interaction in a virtual environment. the interaction paradigm supports haptic communication between two distant users to teach a surgical skill. in order to evaluate this paradigm, a user experiment was conducted. sixty volunteer medical students participated in the study to assess the influence of the teaching method on their performance in a biopsy procedure task. the results show that to transfer the skills, the combination of haptic communication with verbal and visual communications improves the novices' performance compared to conventional teaching methods. furthermore, the results show that, depending on the teaching method, participants developed different needle insertion profiles. we conclude that our interaction paradigm facilitates expert-novice haptic communication and improves skills transfer; and new skills acquisition depends on the availability of different communication channels between experts and novices. our findings indicate that the traditional fellowship methods in surgery should evolve to an off-patient collaborative environment that will continue to support visual and verbal communication, but also haptic communication, in order to achieve a better and more complete skills training.
influence of contextual objects on spatial interactions and viewpoints sharing in virtual environments. collaborative virtual environments (cves) are 3d spaces in which users share virtual objects, communicate, and work together. to collaborate efficiently, users must develop a common representation of their shared virtual space. in this work, we investigated spatial communication in virtual environments. in order to perform an object co-manipulation task, the users must be able to communicate and exchange spatial information, such as object position, in a virtual environment. we conducted an experiment in which we manipulated the contents of the shared virtual space to understand how users verbally construct a common spatial representation of their environment. forty-four students participated in the experiment to assess the influence of contextual objects on spatial communication and sharing of viewpoints. the participants were asked to perform in dyads an object co-manipulation task. the results show that the presence of a contextual object such as fixed and lateralized visual landmarks in the virtual environment positively influences the way male operators collaborate to perform this task. these results allow us to provide some design recommendations for cves for object manipulation tasks.
stiffness modeling of non-perfect parallel manipulators. the paper focuses on the stiffness modeling of parallel manipulators composed of non-perfect serial chains, whose geometrical parameters differ from the nominal ones. in these manipulators, there usually exist essential internal forces/torques that considerably affect the stiffness properties and also change the end-effector location. these internal load-ings are caused by elastic deformations of the manipulator ele-ments during assembling, while the geometrical errors in the chains are compensated for by applying appropriate forces. for this type of manipulators, a non-linear stiffness modeling tech-nique is proposed that allows us to take into account inaccuracy in the chains and to aggregate their stiffness models for the case of both small and large deflections. advantages of the developed technique and its ability to compute and compensate for the compliance errors caused by different factors are illustrated by an example that deals with parallel manipulators of the or-thoglide family.
compliance error compensation technique for parallel robots composed of non-perfect serial chains. the paper presents the compliance errors compensation technique for over-constrained parallel manipulators under external and internal loadings. this technique is based on the non-linear stiffness modeling which is able to take into account the influence of non-perfect geometry of serial chains caused by manufacturing errors. within the developed technique, the deviation compensation reduces to an adjustment of a target trajectory that is modified in the off-line mode. the advantages and practical significance of the proposed technique are illustrated by an example that deals with groove milling by the orthoglide manipulator that considers different locations of the workpiece. it is also demonstrated that the impact of the compliance errors and the errors caused by inaccuracy in serial chains cannot be taken into account using the superposition principle.
optimization of measurement configurations for geometrical calibration of industrial robot. the paper is devoted to the geometrical calibration of industrial robots employed in precise manufacturing. to identify geometric parameters, an advanced calibration technique is proposed that is based on the non-linear experiment design theory, which is adopted for this particular application. in contrast to previous works, the calibration experiment quality is evaluated using a concept of the user-defined test-pose. in the frame of this concept, the related optimization problem is formulated and numerical routines are developed, which allow user to generate optimal set of manipulator configurations for a given number of calibration experiments. the efficiency of the developed technique is illustrated by several examples.
industry-oriented performance measures for design of robot calibration experiment. the paper focuses on the accuracy improvement of geometric and elasto-static calibration of industrial robots. it proposes industry-oriented performance measures for the calibration experiment design. they are based on the concept of manipulator test-pose and referred to the end-effector location accuracy after application of the error compensation algorithm, which implements the identified parameters. this approach allows the users to define optimal measurement configurations for robot calibration for given work piece location and machining forces/torques. these performance measures are suitable for comparing the calibration plans for both simple and complex trajectories to be performed. the advantages of the developed techniques are illustrated by an example that deals with machining using robotic manipulator.
optimal selection of measurement configurations for stiffness model calibration of anthropomorphic manipulators. the paper focuses on the calibration of elastostatic parameters of spatial anthropomorphic robots. it proposes a new strategy for optimal selection of the measurement configurations that essentially increases the efficiency of robot calibration. this strategy is based on the concept of the robot test-pose and ensures the best compliance error compensation for the test configuration. the advantages of the proposed approach and its suitability for practical applications are illustrated by numerical examples, which deal with calibration of elastostatic parameters of a 3 degrees of freedom anthropomorphic manipulator with rigid links and compliant actuated joints.
design of calibration experiments for identification of manipulator elastostatic parameters. the paper is devoted to the elastostatic calibration of industrial robots, which is used for precise machining of large-dimensional parts made of composite materials. in this technological process, the interaction between the robot and the workpiece causes essential elastic deflections of the manipulator components that should be compensated by the robot controller using relevant elastostatic model of this mechanism. to estimate parameters of this model, an advanced calibration technique is applied that is based on the non-linear experiment design theory, which is adopted for this particular application. in contrast to previous works, it is proposed a concept of the user-defined test-pose, which is used to evaluate the calibration experiments quality. in the frame of this concept, the related optimization problem is defined and numerical routines are developed, which allow generating optimal set of manipulator configurations and corresponding forces/torques for a given number of the calibration experiments. some specific kinematic constraints are also taken into account, which insure feasibility of calibration experiments for the obtained configurations and allow avoiding collision between the robotic manipulator and the measurement equipment. the efficiency of the developed technique is illustrated by an application example that deals with elastostatic calibration of the serial manipulator used for robot-based machining.
upsilon (1s+2s+3s) production in d+au and p+p collisions at sqrt(s_nn)=200 gev and cold-nuclear matter effects. the three upsilon states, upsilon(1s+2s+3s), are measured in d+au and p+p collisions at sqrt(s_nn)=200 gev and rapidities 1.2&lt;|y|&lt;2.2 by the phenix experiment at the relativistic heavy-ion collider. cross sections for the inclusive upsilon(1s+2s+3s) production are obtained. the inclusive yields per binary collision for d+au collisions relative to those in p+p collisions (r_dau) are found to be 0.62 +/- 0.26 (stat) +/- 0.13 (syst) in the gold-going direction and 0.91 +/- 0.33 (stat) +/- 0.16 (syst) in the deuteron-going direction. the measured results are compared to a nuclear-shadowing model, eps09 [jhep 04, 065 (2009)], combined with a final-state breakup cross section, sigma_br, and compared to lower energy p+a results. we also compare the results to the phenix j/psi results [phys. rev. lett. 107, 142301 (2011)]. the rapidity dependence of the observed upsilon suppression is consistent with lower energy p+a measurements.
modular and flexible causality control on the web. ajax has allowed javascript programmers to create interactive, collaborative, and user-centered web applications, known as web 2.0 applications. these web applications behave as distributed systems because processors are user machines that are used to send and receive messages between one another. unsurprisingly, these applications have to address the same causality issues present in distributed systems like the need a) to control the causality between messages sent and responses received and b) to react to distributed causal relations. javascript programmers overcome these issues using rudimentary and alternative techniques that largely ignore the distributed computing theory. in addition, these techniques are not very flexible and need to intrusively modify these web applications. in this paper, we study how causality issues affect these applications and present weca, a practical library that allows for modular and flexible control over these causality issues in web applications. in contrast to current proposals, weca is based on (stateful) aspects, message ordering strategies, and vector clocks. we illustrate weca in action with several practical examples from the realm of web applications. for instance, we analyze the flow of information in web applications like twitter using weca.
a search for point sources of eev neutrons. a thorough search of the sky exposed at the pierre auger cosmic ray observatory reveals no statistically significant excess of events in any small solid angle that would be indicative of a flux of neutral particles from a discrete source. the search covers from -90° to +15° in declination using four different energy ranges above 1 eev (1018 ev). the method used in this search is more sensitive to neutrons than to photons. the upper limit on a neutron flux is derived for a dense grid of directions for each of the four energy ranges. these results constrain scenarios for the production of ultrahigh energy cosmic rays in the galaxy.
how do software architects consider non-functional requirements: an exploratory study. dealing with non-functional requirements (nfrs) has posed a challenge onto software engineers for many years. over the years, many methods and techniques have been proposed to improve their elicitation, documentation, and validation. knowing more about the state of the practice on these topics may benefit both practitioners' and researchers' daily work. a few empirical studies have been conducted in the past, but none under the perspective of software architects, in spite of the great influence that nfrs have on daily architects' practices. this paper presents some of the findings of an empirical study based on 13 interviews with software architects. it addresses questions such as: who decides the nfrs, what types of nfrs matter to architects, how are nfrs documented, and how are nfrs validated. the results are contextualized with existing previous work.
a model driven reverse engineering framework for extracting business rules out of a java application. in order to react to the ever-changing market, every organization needs to periodically reevaluate and evolve its company policies. these policies must be enforced by its information system (is) by means of a set of business rules that drive the system behavior and data. clearly, policies and rules must be aligned at all times but unfortunately this is a challenging task. in most iss implementation of business rules is scattered among the code so appropriate techniques must be provided for the discovery and evolution of evolving business rules. in this paper we describe a model driven reverse engineering framework aiming at extracting business rules out of java source code. the use of modeling techniques facilitate the representation of the rules at a higher-abstraction level which enables stakeholders to understand and manipulate them.
model-driven software engineering in practice. this book is an agile and flexible tool that introduces you to the model-driven engineering world. it presents its basic principles and techniques, and puts them at work on freely available eclipse-based tools. this lets you choose the ideal set of mde instruments so to get benefit from mde right away.
a model seeker: extracting global constraint models from positive examples. we describe a system which generates finite domain constraint models from positive example solutions, for highly structured problems. the system is based on the global constraint catalog, providing the library of constraints that can be used in modeling, and the constraint seeker tool, which finds a ranked list of matching constraints given one or more sample call patterns. we have tested the modeler with 230 examples, ranging from 4 to 6,500 variables, using between 1 and 7,000 samples. these examples come from a variety of domains, including puzzles, sports-scheduling, packing &amp; placement, and design theory. when comparing against manually specified "canonical" models for the examples, we achieve a hit rate of 50\%, processing the complete benchmark set in less than one hour on a laptop. surprisingly, in many cases the system finds usable candidate lists even when working with a single, positive example.
some research challenges and remarks on cp. rather than providing general challenges, we first focus on a small set of concrete questions that we think are worth investigating. this means by no way that other issues, such as integrating continuous and discrete constraints, are not important, but we preferred to state focussed challenges. we then make some points regarding the development of sustainable cp solvers and the way cp interacts with other computer science areas.
building global constraint models from positive examples. we present a system which generates global constraint models from few positive examples of problem solutions. in contrast to previous constraint acquisition work, we present a novel approach based on the global constraint catalog and the constraint seeker tool which generates models for problems which can be expressed as regular conjunctions of similar constraints. our system first generates regular groupings of variables in the given samples. the constraint seeker is then used to find ranked, typical constraints which match all given positive examples. a dominance check, which removes implied constraints based on meta-data in the constraint catalog, leads to a final ranked set of candidate constraints for each problem. the system is implemented in sicstus prolog, and heavily relies on the constraint description and evaluators in the global constraint catalog. we show results for more than 200 example problems, ranging from puzzles to sports scheduling, placement and layout problems. the problems range from 4 to over 6000 variables, and use between one and 7000 samples, utilizing over 50 global constraints of the catalog. we achieve an overall hit-rate of about 50%.
on the reification of global constraints. we introduce a simple idea for deriving reified global constraints in a systematic way. it is based on the observation that most global constraints can be reformulated as a conjunction of pure functional dependency constraints together with a constraint that can be easily reified. we first show how the core constraints of the global constraint catalogue can be reified and we then identify several reification categories that apply to at least 82% of the constraints in the global constraint catalogue.
an o(n log n) bound consistency algorithm for the conjunction of an alldifferent and an inequality between a sum of variables and a constant, and its generalization. this paper gives an o(nlog n) bound-consistency filtering algorithm for the conjunction alldifferent(v0,v1,...,vn−1)∧ f(v0)⌖f(v1)⌖...⌖f(vn−1)≤ cst, (v0,v1,...,vn−1,cst∊ +), where (,⌖) is a commutative group, f is a unary function, and both ⌖ and f are monotone increasing. this complexity is equal to the complexity of the bound-consistency algorithm of the alldifferent constraint.
a scalable sweep algorithm for the cumulative constraint. this paper presents a sweep based algorithm for the cumulative constraint, which can operate in filtering mode as well as in greedy assignment mode. given n tasks, this algorithm has a worst-case time complexity of o(n^2). in practice, we use a variant with better average-case complexity but worst-case complexity of o(n 2 logn), which goes down to o(n log n) when all tasks have unit duration, i.e. in the bin-packing case. despite its worst-case time complexity, this algorithm scales well in practice, even when a significant number of tasks can be scheduled in parallel. it handles up to 1 million tasks in one single cumulative constraint in both choco and sicstus.
9th international conference on integration of ai and or techniques in constraint programming for combinatorial optimization problems (cpaior'12). the 9th international conference on integration of artificial intelligence and operations research techniques in constraint programming was held in nantes, france, may 28-june 1, 2012. the aim of the cpaior conference series is to bring together interested researchers from constraint programming (cp), artificial intelligence (ai), and operations research (or) to present new techniques or new applications in com- binatorial optimization and to provide an opportunity for researchers in one area to learn about techniques in the others. a main objective of this conference se- ries is also to give these researchers the occasion to show how the integration of techniques from different fields can lead to interesting results on large and complex problems. therefore papers that actively combine, integrate, or contrast approaches from more than one of the areas were especially solicited. high-quality papers from a single area were also welcome. finally, application papers showcasing cp/ai/or techniques on innovative and challenging applications or experience reports on such applications were strongly encouraged. submissions for this year were 64 papers. each paper received at least three independent peer reviews which formed the basis for the acceptance of 26 papers. these papers are published in full in these proceedings. the program committee made a good job of providing thorough reviews and discussions.
global constraint catalog, 2nd edition (revision a). this report presents a catalogue of global constraints where each constraint is explicitly described in terms of graph properties and/or automata and/or first order logical formulae with arithmetic. when available, it also presents some typical usage as well as some pointers to existing filtering algorithms.
a generalized arc-consistency algorithm for a class of counting constraints:revised edition that incorporates one correction. this paper introduces the seq bin meta-constraint with a polytime algorithm achieving generalized arc-consistency according to some properties. seq bin can be used for encoding counting constraints such as change, smooth or increasing nvalue. for some of these constraints and some of their variants gac can be enforced with a time and space complexity linear in the sum of domain sizes, which improves or equals the best known results of the literature.
learning structured constraint models: a first attempt. in this paper we give an overview of a novel tool which learns structured constraint models from flat, positive examples of solutions. it is based on previous work on a constraint seeker, which finds constraints in the global constraint catalog satisfying positive and negative examples. in the current tool we extend this system to find structured conjunctions of constraints on regular subsets of variables in the given solutions. two main elements of the approach are a bi-criteria optimization problem which finds conjunctions of constraints which are both regular and relevant, and a syntactic dominance check between conjunctions, which removes implied constraints without requiring a full theorem prover, using meta-data in the constraint catalog. some initial experiments on a proof-of-concept implementation show promising results.
using the global constraint seeker for learning structured constraint models: a first attempt. considering problems that have a strong internal structure, this paper shows how to generate constraint models from a set of positive, flat samples (i.e., solutions) without knowing a priori neither the constraint candidates, nor the way variables are shared within constraints. we describe two key contributions to building such a model generator: (1) first, learning is modeled as a bicriteria optimization problem over ranked constraint candidates returned by the constraint seeker, where we optimize both the compactness of the model, and the rank (or appropriateness) of the selected constraints. (2) second, filtering out irrelevant candidate models is achieved by using meta data of the global constraint catalog that describe links between constraints. some initial experiments on a proof-of-concept implementation show promising results.
a generalized arc-consistency algorithm for a class of counting constraints. this paper introduces the seq_bin meta-constraint with a polytime algorithm achieving generalized arc-consistency. seq_bin can be used for encoding counting constraints such as change, smooth, or increasing nvalue. for all of them the time and space complexity is linear in the sum of domain sizes, which improves or equals the best known results of the literature.
a constraint seeker: finding and ranking global constraints from examples. in this paper we describe a constraint seeker application which provides a web interface to search for global constraints in the global constraint catalog, given positive and negative, fully instantiated (ground) examples. based on the given instances the tool returns a ranked list of matching constraints, the rank indicating whether the constraint is likely to be the intended constraint of the user. we give some examples of use cases and generated output, describe the different elements of the search and ranking process, discuss the role of constraint programming in the different tools used, and provide evaluation results over the complete global constraint catalog. the constraint seeker is an example for the use of generic meta-data provided in the catalog to solve a specific problem.
focus: a constraint for concentrating high costs. many constraint programming models use integer cost variables aggregated in an objective criterion. in this context, some constraints involving exclusively cost variables are often imposed. such constraints are complementary to the objective function. they characterize the solutions which are acceptable in practice. this paper deals with the case where the set of costs is a sequence, in which high values should be concentrated in a few number of areas. representing such a property through a search heuristic may be complex and overall not precise enough. to solve this issue, we introduce a new constraint, focus(x,yc,len, k), where x is a sequence of n integer variables, yc an integer variable, and len and k are two integers. to satisfy focus, the minimum number of distinct sub-sequences of consecutive variables in x, of length at most len and that involve exclusively values strictly greater than k, should be less than or equal to yc . we present two examples of problems involving focus. we propose a complete filtering algorithm in o(n) time complexity.
a theta(n) bound-consistency algorithm for the increasing sum constraint. given a sequence of variables x = 〈x 0, x 1, ..., x n − 1 〉, we consider the increasingsum constraint, which imposes ∀ i ∈ [0, n − 2] x i ≤ x i + 1, and ∑xi∈xxi=s . we propose an θ(n) bound-consistency algorithm for increasingsum.
filtering algorithms for discrete cumulative problems with overloads of resource. many cumulative problems are such that the horizon is fixed and cannot be delayed. in this situation, it often occurs that all the activities cannot be scheduled without exceeding the capacity at some points in time. moreover, this capacity is not necessarily always the same during the scheduling period. this article introduces a new constraint for solving this class of problems. we adapt two filtering algorithms to our context: sweep and p. vilím's edge-finding algorithm. we emphasize that in some problems violations are imposed. we design a new filtering procedure specific to this kind of events. we introduce a search heuristic specific to our constraint. we successfully experiment our constraint.
a note on the paper "minimizing total tardiness on parallel machines with preemptions" by kravchenko and werner (2012). in this note, we point out two major errors in the paper "minimizing total tardiness on parallel machines with preemptions" by kravchenko and werner (2012). more precisely, they claimed to have proved that both problems p|pmtn|∑t j and p|r j ,p j =p,pmtn|∑t j are np -hard. we give a counter-example to their proofs, letting the complexity of these two problems open.
extracting models from source code in software modernization. model-driven software modernization is a discipline in which model-driven development (mdd) techniques are used in the modernization of legacy systems. when existing software artifacts are evolved, they must be transformed into models to apply mdd techniques such as model transformations. since most modernization scenarios (e.g., application migration) involve dealing with code in general-purpose programming languages (gpl), the extraction of models from gpl code is an essential task in a model-based modernization process. this activity could be performed by tools to bridge grammarware and mdd technical spaces, which is normally carried out by dedicated parsers. grammar-to-model transformation language (gra2mol) is a domain-specific language (dsl) tailored to the extraction of models from gpl code. this dsl is actually a text-to-model transformation language which can be applied to any code conforming to a grammar. gra2mol aims to reduce the effort needed to implement grammarware-mdd bridges, since building dedicated parsers is a complex and time-consuming task. like atl and rubytl languages, gra2mol incorporates the binding concept needed to write mappings between grammar elements and metamodel elements in a simple declarative style. the language also provides a powerful query language which eases the retrieval of scattered information in syntax trees. moreover, it incorporates extensibility and grammar reuse mechanisms. this paper describes gra2mol in detail and includes a case study based on the application of the language in the extraction of models from delphi code.
comparison between internal and external dsls via rubytl and gra2mol. domain specific languages (dsl) are becoming increasingly more important with the emergence of model-driven paradigms. most literature on dsls is focused on describing particular languages, and there is still a lack of works that compare different approaches or carry out empirical studies regarding the construction or usage of dsls. several design choices must be made when building a dsl, but one important question is whether the dsl will be external or internal, since this affects the other aspects of the language. this chapter aims to provide developers confronting the internal-external dichotomy with guidance, through a comparison of the rubytl and gra2mol model transformations languages, which have been built as an internal dsl and an external dsl, respectively. both languages will first be introduced, and certain implementation issues will be discussed. the two languages will then be compared, and the advantages and disadvantages of each approach will be shown. finally, some of the lessons learned will be presented.
static analysis of model transformations for effective test generation. model transformations are an integral part of several computing systems that manipulate interconnected graphs of objects called models in an input domain specified by a metamodel and a set of invariants. test models are used to look for faults in a transformation. a test model contains a specific set of objects, their interconnections and values for their attributes. can we automatically generate an effective set of test models using knowledge from the transformation? we present a white-box testing approach that uses static analysis to guide the automatic generation of test inputs for transformations. our static analysis uncovers knowledge about how the input model elements are accessed by transformation operations. this information is called the input metamodel footprint due to the transformation. we transform footprint, input metamodel, its invariants, and transformation pre-conditions to a constraint satisfaction problem in alloy. we solve the problem to generate sets of test models containing traces of the footprint. are these test models effective? with the help of a case study transformation we evaluate the effectiveness of these test inputs. we use mutation analysis to show that the test models generated from footprints are more effective (97.62% avg. mutation score) in detecting faults than previously developed approaches based on input domain coverage criteria (89.9% avg.) and unguided generation (70.1% avg.).
heavy quark quenching from rhic to lhc and the consequences of gluon damping. in this contribution to the quark matter 2012 conference, we study whether energy loss models established for rhic energies to describe the quenching of heavy quarks can be applied at lhc with the same success. we also benefit from the larger $p_t$-range accessible at this accelerator to test the impact of gluon damping on observables such as the nuclear modification factor.
cut generation for obtaining strong makespan lower bounds for the multi-skill project scheduling problem. null
branch and price approach for the multi-skill project scheduling problem. null
cut generation for the multi-skill project scheduling problem. null
recovering beam search approach for the multi-skill project scheduling problem. null
study of pionless two-nucleon k$-$ absorptions at rest with finuda. null
integrated column generation and lagrangian relaxation approach for the multi-skill project scheduling problem. null
search for the neutron-rich hypernucleus 9{\lambda}he. search for the neutron-rich hypernucleus 9lhe is reported by the finuda experiment at dafne, infn-lnf, studying (pi+, pi-) pairs in coincidence from the k-stop + 9be --&gt; 9lhe + pi+ production reaction followed by 9lhe --&gt; 9li + pi- weak decay. an upper limit of the production rate of 9lhe undergoing this two-body pi- decay is determined to be (2.3 +/- 1.9) 10-6/k-stop at 90% confidence level.
results of a self-triggered prototype system for radio-detection of extensive air showers at the pierre auger observatory. we describe the experimental setup and the results of rauger, a small radio-antenna array, consisting of three fully autonomous and self-triggered radio-detection stations, installed close to the center of the surface detector (sd) of the pierre auger observatory in argentina. the setup has been designed for the detection of the electric field strength of air showers initiated by ultra-high energy cosmic rays, without using an auxiliary trigger from another detection system. installed in december 2006, rauger was terminated in may 2010 after 65 registered coincidences with the sd. the sky map in local angular coordinates (i.e., zenith and azimuth angles) of these events reveals a strong azimuthal asymmetry which is in agreement with a mechanism dominated by a geomagnetic emission process. the correlation between the electric field and the energy of the primary cosmic ray is presented for the first time, in an energy range covering two orders of magnitude between 0.1 eev and 10 eev. it is demonstrated that this setup is relatively more sensitive to inclined showers, with respect to the sd. in addition to these results, which underline the potential of the radio-detection technique, important information about the general behavior of self-triggering radio-detection systems has been obtained. in particular, we will discuss radio self-triggering under varying local electric-field conditions.
alpha localized radiolysis and corrosion mechanisms at the iron/water interface: role of molecular species. this paper is devoted to the iron corrosion phenomena induced by the α (4he2+) water radiolysis species studied in conjunction with the production/consumption of h2 at the solid/solution interface. on one hand, the solid surface is characterized during the 4he2+ ions irradiation by in situ raman spectroscopy; on another hand, the h2 gas produced by the water radiolysis is monitored by ex situ gas measurements. the 4he2+ ions irradiation experiments are provided either by the cemhti (e = 5.0 mev) either by the arronax (e = 64.7 mev) cyclotron facilities. the iron corrosion occurs only under irradiation and can be slowed down by h2 reductive atmosphere. pure iron and carbon steel solids are studied in order to show two distinct behaviors of these surfaces vs. the 4he2+ ions water irradiation: the corrosion products identified are the magnetite phase (fe(ii)fe(iii)2o4) correlated to an h2 consumption for pure iron and the lepidocrocite phase (γ-fe(iii)ooh) correlated to an h2 production for carbon steel sample. this paper underlined the correlation between the iron corrosion products formation onto the solid surface and the h2 production/consumption mechanisms. h2o2 species is considered as the single water radiolytic species involved into the corrosion reaction at the solid surface with an essential role in the oxidation reaction of the iron surface. we propose to bring some light to these mechanisms, in particular the h2 and h2o2 roles, by the in situ raman spectroscopy during and after the 4he2+ ions beam irradiation. this in situ experiment avoids the evolution of the solid surface, in particular phases which are reactive to the oxidation processing.
maintaining arc consistency asynchronously in synchronous distributed search. we recently proposed nogood-based asynchronous forward checking (afc-ng), an efficient and robust algorithm for solving distributed constraint satisfaction problems (discsps). afc-ng performs an asynchronous forward checking phase during synchronous search. in this paper, we propose two new algorithms based on the same mechanism as afc-ng. however, instead of using forward checking as a filtering property, we pro- pose to maintain arc consistency asynchronously (maca). the first algorithm we propose, maca-del, enforces arc consistency thanks to an additional type of messages, deletion messages. the second algorithm, maca-not, achieves arc consistency without any new type of message. we provide a theoretical analysis and an experimental evaluation of the proposed approach. our experiments show the good performance of maca algorithms, particularly those of maca-not.
development of a readout electronic for the measurement of ionization in liquid xenon compton telescope containing micro-patterns. null
bacterial syntenies: an exact approach with gene quorum. background: the automatic identification of syntenies across multiple species is a key step in comparative genomics that helps biologists shed light both on evolutionary and functional problems. results: in this paper, we present a versatile tool to extract all syntenies from multiple bacterial species based on a clear-cut and very flexible definition of the synteny blocks that allows for gene quorum, partial gene correspondence, gaps, and a partial or total conservation of the gene order. conclusions: we apply this tool to two different kinds of studies. the first one is a search for functional gene associations. in this context, we compare our tool to a widely used heuristic--i-adhore--and show that at least up to ten genomes, the problem remains tractable with our exact definition and algorithm. the second application is linked to evolutionary studies: we verify in a multiple alignment setting that pairs of orthologs in synteny are more conserved than pairs outside, thus extending a previous pairwise study. we then show that this observation is in fact a function of the size of the synteny: the larger the block of synteny is, the more conserved the genes are.
community-driven dsl development with collaboro. developing a dsl, textual or graphical, is rarely a single-person task. involving several users and developers at the same time is often required to ensure that the finally produced solution actually fits the expected needs. thus, when using the emf and related tools to define and implement a dsl, having more collaboration features could really bring practical additional value to that process. collaboro, hosted in eclipse labs, recently proposes an approach to make language development processes more participative, meaning that both dsl developers and users can collaborate more together. in this lightning talk, we quickly present the current main features of the prototype as well as some envisioned evolutions using other modeling projects.
erratum: measurement of transverse single-spin asymmetries for j/psi production in polarized p+p collisions at sqrt(s)=200 gev [phys. rev. d 82, 112008 (2010)]. we previously reported [phys. rev. d 82, 112008 (2010)] measurements of transverse single-spin asymmetries, a_n, in j/psi production from transversely polarized p+p collisions at sqrt(s)=200 gev with data taken by the phenix experiment at the relativistic heavy ion collider in 2006 and 2008. subsequently, we have found errors in the analysis procedures for the 2008 data, which resulted in an erroneous value for the extracted a_n. the errors affected the sorting of events into the correct left/right and forward/backward bins. this produced an incorrect value for the 2008 result, but the 2006 result is unaffected. we have conducted two independent reanalyses with these errors corrected, and we present here the corrected values for the 2008 data and the combined results for 2006 and 2008. the new combined spin asymmetry in the forward region is a_n = -0.026+/-0.026(stat)+/-0.003(sys). since this asymmetry is consistent with zero, we no longer claim that our results suggest a possible non-zero trigluon correlation function in transversely polarized protons.
empirical research in software product line engineering. null
hybrid multi micropattern gaseous photomultiplier for detection of liquid-xenon scintillation. null
a liquid xenon tpc for a medical imaging compton telescope. null
measurement cross sections for radioisotopes production. new radioactive isotopes for nuclear medicine can be produced using particle accelerators. this is one goal of arronax, a high energy - 70 mev - high intensity - 2*350 µa - cyclotron set up in nantes. a priority list was established containing beta- - 47sc, 67cu - beta+ - 44sc, 64cu, 82sr/82rb, 68ge/68ga - and alpha emitters - 211at. among these radioisotopes, the scandium 47 and the copper 67 have a strong interest in targeted therapy. the optimization of their productions required a good knowledge of their cross-sections but also of all the contaminants created during irradiation. we launched on arronax a program to measure these production cross-sections using the stacked-foils' technique. it consists in irradiating several groups of foils - target, monitor and degrader foils - and in measuring the produced isotopes by gamma-spectrometry. the monitor - cu or ni - is used to correct beam loss whereas degrader foils are used to lower beam energy. we chose to study the natti(p,x)47sc and 68zn(p,2p)67cu reactions. targets are respectively natural titanium foil - bought from goodfellow - and enriched zinc 68 deposited on silver. in the latter case, zn targets were prepared in-house - electroplating of 68zn - and a chemical separation between copper and gallium isotopes has to be made before gamma counting. cross-section values for more than 40 different reactions cross-sections have been obtained from 18 mev to 68 mev. a comparison with the talys code is systematically done. several parameters of theoretical models have been studied and we found that is not possible to reproduce faithfully all the cross-sections with a given set of parameters.
fission barriers and half-lives of actinides in the quasimolecular shape valley. null
pseudorapidity density of charged particles in p-pb collisions at sqrt(snn) = 5.02 tev. the charged-particle pseudorapidity density measured over 4 units of pseudorapidity in non-single-diffractive (nsd) p-pb collisions at a centre-of-mass energy per nucleon pair sqrt(snn) = 5.02 tev is presented. the average value at midrapidity is measured to be 16.81 \pm 0.71 (syst.), which corresponds to 2.14 \pm 0.17 (syst.) per participating nucleon. this is 16% lower than in nsd pp collisions interpolated to the same collision energy, and 84% higher than in d-au collisions at sqrt(snn) = 0.2 tev. the measured pseudorapidity density in p-pb collisions is compared to model predictions, and provides new constraints on the description of particle production in high-energy nuclear collisions.
dynamic vehicle routing : solution methods and computational tools. within the wide scope of logistics management,transportation plays a central role and is a crucialactivity in both production and service industry.among others, it allows for the timely distributionof goods and services between suppliers, productionunits, warehouses, retailers, and final customers.more specifically, vehicle routing problems(vrps) deal with the design of a set of minimal costroutes that serve the demand for goods orservices of a set of geographically spread customers,satisfying a group of operational constraints.while it was traditionally a static problem, recenttechnological advances provide organizations withthe right tools to manage their vehicle fleet in realtime. nonetheless, these new technologies alsointroduce more complexity in fleet managementtasks, unveiling the need for decision support systemsdedicated to dynamic vehicle routing. in thiscontext, the contributions of this ph.d. thesis arethreefold : (i) it presents a comprehensive reviewof the literature on dynamic vehicle routing ; (ii)it introduces flexible optimization frameworks thatcan cope with a wide variety of dynamic vehiclerouting problems ; (iii) it defines a new vehicle routingproblem with numerous applications.
transverse momentum distribution and nuclear modification factor of charged particles in p-pb collisions at sqrt{s_nn} = 5.02 tev. the transverse momentum (p_t) distribution of primary charged particles is measured in non single-diffractive p-pb collisions at sqrt{s_nn} = 5.02 tev with the alice detector at the lhc. the p_t spectra measured near central rapidity in the range 0.5 &lt; p_t &lt; 20 gev/c exhibit a weak pseudorapidity dependence. the nuclear modification factor r_ppb is consistent with unity for p_t above 2 gev/c. this measurement indicates that the strong suppression of hadron production at high p_t observed in pb-pb collisions at the lhc is not due to an initial-state effect. the measurement is compared to theoretical calculations.
an exact algorithm for the close enough traveling salesman problem with arc covering constraints. null
a fast re-optimization approach for dynamic vehicle routing. the present work deals with dynamic vehicle routing problems in which new customers appear during the design or execution of the routing. we propose a parallel adaptive large neighborhood search (palns) that produces high quality routes in a limited computational time. then, we introduce the notion of driver inconvenience and define a bi-objective optimization problem that minimizes the cost of routing while maintaining its consistency throughout the day. we consider a problem setting in which vehicles have an initial routing plan at the beginning of the day, that is periodically updated by a decision maker. we introduce a measure of the driver inconvenience resulting from each update and propose a bi-objective approach based on palns that is able to produce a set of non-dominated solutions in reasonable computational time. these solutions offer different tradeoffs between cost efficiency and consistency, and can be used by the decision maker to update the routing of the vehicles introducing a controlled number of changes.
on the dynamic technician routing and scheduling problem. the technician routing and scheduling problem (trsp) deals with a limited crew of technicians that serves a set of requests. in the trsp, each technician has a set of skills, tools, and spare parts, while requests require a subset of each. the problem is then to design a set of tours of minimal total duration such that each request is visited exactly once, within its time window, by a technician with the required skills, tools, and spare parts. the trsp naturally arises in a wide range of applications, including telecoms, public utilities, and maintenance operations. to the best of our knowledge, no work considers neither tools, nor spare parts, nor the arrival of new requests, three important components of real-world applications. the present work addresses this aspect and proposes an optimization approach for the dynamic version of the problem, noted d-trsp, where new requests arrive during the execution of the routes.
a review of dynamic vehicle routing problems. a number of technological advances have led to a renewed interest on dynamic vehicle routing problems. this survey classifies routing problems from the perspective of information quality and evolution. after presenting a general description of dynamic routing, we introduce the notion of degree of dynamism, and present a comprehensive review of applications and solution methods for dynamic vehicle routing problems.
a parallel matheuristic for the technician routing and scheduling problem. the technician routing and scheduling problem (trsp) consists in routing staff to serve requests for service, taking into account time windows, skills, tools, and spare parts. typical applications include maintenance operations and staff routing in telecoms, public utilities, and in the health care industry. in this paper, we present a formal definition of the trsp, discuss its relation with the vehicle routing problem with time windows (vrptw), and review related research. from a methodological perspective, we describe a matheuristic composed of a constructive heuristic, a parallel adaptive large neighborhood search (palns), and a mathematical programming based post-optimization procedure that successfully tackles the trsp. we validate the matheuristic on the solomon vrptw instances, where we achieve an average gap of 0.23%, and matched 44 out of 55 optimal solutions. finally, we illustrate how the matheuristic successfully solves a set of trsp instances extended from the solomon benchmark.
control through operators for quantum chemistry. we consider the problem of operator identification in quantum control. the free hamiltonian and the dipole moment are searched such that a given target state is reached at a given time. a local existence result is obtained. as a by-product, our works reveals necessary conditions on the laser field to make the identification feasible. in the last part of this work, some newton algorithms are proposed together with a continuation method to compute effectively these operators.
half-lives of cluster radioactivity with a generalized liquid-drop model. null
measurement of heavy-flavour decay muon production at forward rapidity in pp and pb-pb collisions at sqrt(s_nn)=2.76 tev with the alice experiment. the alice experiment measured the heavy-flavour production in the semi-muonic decay channel at forward rapidities ($2.5.
greywater treatment for reuse by slow sand filtration : study of pathogenic microorganisms and phage survival. in recent decades, most countries of the world have experienced a shortage of water and increase its rate of consumption. today, every country in the world are interested in this problem by trying to find alternatives to address this shortage. one solution is reuse greywater (gw) for irrigation after treatment. gw is all water generated from household except toilet water. the risks associated with the reuse of these waters are the presence of pathogens that can infect humans, animals and plants. in this thesis focused on studying treatment by slow sand filtration and the survival of representatives of pathogens, such as e. coli, p. aeruginosa , e. faecalis and bacteriophage ms2 which could be found in the greywater. the study factors was a physico-chemicals factors such as; temperature (6±2,23±2,42±2°c), salinity (1.75 and 3.5% nacl), oxygen (aerobic and anaerobic condition), nutrient ( rich media , 50%: 50% salt and poor media ), light with photocatalysis ( uv and visible lights) and slow sand filter (egyptian desert sand and swimming pool sand). a combination of high temperature, sunlight and photocatlysis are mainly responsible for the rapid decline of bacteria and ms2 coliphage. slow sand filter have clearly less influence on the survival of bacteria in the greywater, but it effective to decline turbidity and cod for short times.
modeling of electrolocation for bio-robotics. the goal of the european project angels is to build an eel-like robot capable to navigate by the electric sense and to divide itself in several mono-agents for exploration purposes. in the context of this project my work consisted in creating a perception model inspired from the electric fish with the virtue of being fast and so, to be usable in-line. two models of perception were built and were based on a simple but realistic geometry of sensor. the two models named after the « poly-spherical model » and the « reflexions model » which come respectively from a physical intuition and an appropriate mathematical model are calibrated once for all with an electrical simulator in order to have analytical forms. coupled with models of the electrical response of objects, the two models of perception permit the robot to achieve some basic tasks like detection and obstacles’ avoidance, which is a novelty in the history. in addition we have built a generic formalism of theelectrical response that extents the application of the models of perception. finally we have begun to estimate the influence of a complex geometry of sensor, that exhibs large insulating surfaces, on the measurement in order to open the way for the rapid modelling of a sensor of arbitrary shape.
architecture quality revisited. there is a common belief in the software community that nonfunctional quality is fundamentally important for architecture sustainability and project success. a recent study, however, suggests that nonfunctional quality is of little relevance for users and customers, but instead mainly a concern for architects. nontechnical constraints, such as licenses and technology providers, appear to be driving design as prominently as quality requirements. quality requirements, such as performance, are mainly defined by architects on the basis of their experiences, and are often poorly documented and validated. this column explores whether the software community actually overestimates the relevance of nonfunctional qualities or whether the study's observations indicate a valid position on nonfunctional quality for certain types of application domains, development approaches, and organizational setups.
alf-veriﬁer: an eclipse plugin for verifying alf/uml executable models. in this demonstration we present an eclipse plugin that implements a lightweight method for verifying ﬁne-grained operations at design time. this tool suﬃces to check that the execution of the operations (speciﬁed in alf action language) is consistent with the integrity constraints deﬁned in the class diagram (speciﬁed in uml) and returns a meaningful feedback that helps correcting them otherwise.
sla-driven capacity planning for cloud applications. cloud computing paradigm has become the solution to provide good service quality and exploit economies of scale. however, the management of such elastic resources, with different quality-of-service (qos) combined with on-demand self-service, is a complex issue. this paper proposes an approach driven by service level agreement (sla) for optimizing the capacity planning for cloud applications. the main challenge for a service provider is to determine the best trade-off between profit and customer satisfaction. in order to address this issue, we follow a queueing network proposal and present an analytical performance model to predict cloud service performance. based on a utility function and a capacity planning method, our solution calculates the optimal configuration. we rely on autonomic computing to adjust continuously the configuration. simulation experiments indicate that our model i) faithfully captures the performance of cloud applications for a number of workloads and configurations and ii) successfully keeps the best trade-off.
plasma damping effects on the radiative energy loss of relativistic particles. the energy loss of a relativistic charge undergoing multiple scatterings while traversing an infinite, polarizable and absorptive plasma is investigated. polarization and absorption mechanisms in the medium are phenomenologically modeled by a complex index of refraction. apart from the known ter-mikaelian effect related to the dielectric polarization of matter, we find an additional, substantial reduction of the energy loss due to the damping of radiation. the observed effect is more prominent for larger damping and/or larger energy of the charge. a conceivable analog of this phenomenon in qcd could influence the study of jet quenching phenomena in ultrarelativistic heavy-ion collisions at rhic and lhc.
a model-driven approach for the extraction of network access-control policies. network security constitutes a critical concern when developing and maintaining nowadays corporate information systems. firewalls are a key element of network security by filtering the traffic of the network in compliance with a number of access control rules that enforce a given security policy. unfortunately, once implemented, and due to the complexity of firewall configuration languages and the underlying network topology, knowing which security policy is actually being enforced by the network system is a complex and time consuming task that requires low-level and, often, vendor- specific expertise. in an always-evolving context, where security policies are often updated to respond to new security requirements, this discovery phase becomes critical since it could hamper the proper evolution of the system and compromise its security. to tackle this problem, our approach generates an abstract model of the firewall configurations in a network that facilitates the understanding and evolution of network security policies.
a catalogue of refactorings for model-to-model transformations. in object-oriented programming, continuous refactorings are used as the main mechanism to increase the maintainability of the code base. unfortunately, in the field of model transformations, such refactoring support is so far missing. this paper tackles this limitation by adapting the notion of refactorings to model-to-model (m2m) transformations. in particular, we present a dedicated catalogue of refactorings for improving the quality of m2m transformations. the refactorings have been explored by analyzing existing transformation examples defined in atl. however, the refactorings are not specifically tailored to atl, but applicable also to other m2m transformation languages.
model-driven and software product line engineering. many approaches to creating software product lines have emerged that are based on model-driven engineering. this book introduces both software product lines and model-driven engineering, which have separate success stories in industry, and focuses on the practical combination of them. it describes the challenges and benefits of merging these two software development trends and provides the reader with a novel approach and practical mechanisms to improve software development productivity. the book is aimed at engineers and students who wish to understand and apply software product lines and model-driven engineering in their activities today. the concepts and methods are illustrated with two product line examples: the classic smart-home systems and a collection manager information system.
inclusive cross section and single-transverse-spin asymmetry for very forward neutron production in polarized p+p collisions at sqrt(s)=200 gev. the energy dependence of the single-transverse-spin asymmetry, a_n, and the cross section for neutron production at very forward angles were measured in the phenix experiment at rhic for polarized p+p collisions at sqrt(s)=200 gev. the neutrons were observed in forward detectors covering an angular range of up to 2.2 mrad. we report results for neutrons with momentum fraction of x_f=0.45 to 1.0. the energy dependence of the measured cross sections were consistent with x_f scaling, compared to measurements by an isr experiment which measured neutron production in unpolarized p+p collisions at sqrt(s)=30.6--62.7 gev. the cross sections for large x_f neutron production for p+p collisions, as well as those in e+p collisions measured at hera, are described by a pion exchange mechanism. the observed forward neutron asymmetries were large, reaching a_n=-0.08+/-0.02 for x_f=0.8; the measured backward asymmetries, for negative x_f, were consistent with zero. the observed asymmetry for forward neutron production is discussed within the pion exchange framework, with interference between the spin-flip amplitude due to the pion exchange and nonflip amplitudes from all reggeon exchanges. within the pion exchange description, the measured neutron asymmetry is sensitive to the contribution of other reggeon exchanges even for small amplitudes.
new antineutrino energy spectra predictions from the summation of beta decay branches of the fission products. in this paper, we study the impact of the inclusion of the recently measured beta decay properties of the $^{102;104;105;106;107}$tc, $^{105}$mo, and $^{101}$nb nuclei in an updated calculation of the antineutrino energy spectra of the four fissible isotopes $^{235, 238}$u, and $^{239,241}$pu. these actinides are the main contributors to the fission processes in pressurized water reactors. the beta feeding probabilities of the above-mentioned tc, mo and nb isotopes have been found to play a major role in the $\gamma$ component of the decay heat of $^{239}$pu, solving a large part of the $\gamma$ discrepancy in the 4 to 3000\,s range. they have been measured using the total absorption technique (tas), avoiding the pandemonium effect. the calculations are performed using the information available nowadays in the nuclear databases, summing all the contributions of the beta decay branches of the fission products. our results provide a new prediction of the antineutrino energy spectra of $^{235}$u, $^{239,241}$pu and in particular of $^{238}$u for which no measurement has been published yet. we conclude that new tas measurements are mandatory to improve the reliability of the predicted spectra.
antennas for the detection of radio emission pulses from cosmic-ray induced air showers at the pierre auger observatory. the pierre auger observatory is exploring the potential of the radio detection technique to study extensive air showers induced by ultra-high energy cosmic rays. the auger engineering radio array (aera) addresses both technological and scientific aspects of the radio technique. a first phase of aera has been operating since september 2010 with detector stations observing radio signals at frequencies between 30 and 80 mhz. in this paper we present comparative studies to identify and optimize the antenna design for the final configuration of aera consisting of 160 individual radio detector stations. the transient nature of the air shower signal requires a detailed description of the antenna sensor. as the ultra-wideband reception of pulses is not widely discussed in antenna literature, we review the relevant antenna characteristics and enhance theoretical considerations towards the impulse response of antennas including polarization effects and multiple signal reflections. on the basis of the vector effective length we study the transient response characteristics of three candidate antennas in the time domain. observing the variation of the continuous galactic background intensity we rank the antennas with respect to the noise level added to the galactic signal.
transport properties of hot gluonic matter. we discuss the temperature dependence of the scaled jet quenching parameter of hot gluonic matter within a quasiparticle approach. a pronounced maximum in the vicinity of the transition temperature is observed, where the ratio of the scaled jet quenching parameter and the inverse specific shear viscosity increases above typical values for weakly coupled systems.
an automatic reversible transformation from composite to visitor in java. we build reversible transformations between composite and visitor design patterns in java programs. such transformations represent an automatic reversible switching between different program architectures with a guarantee of semantic preservation. in this paper, we detail the algorithms of the transformations implemented by composing elementary refactoring operations. the transformations were automated with the refactoring tool of a popular ide: intellij idea.
coherent j/psi photoproduction in ultra-peripheral pb-pb collisions at \sqrt{s_nn} = 2.76 tev. the alice collaboration has made the first measurement at the lhc of j/psi photoproduction in ultra-peripheral pb-pb-collisions at \sqrt{s_nn} = 2.76 tev. the j/psi is identified via its dimuon decay in the forward rapidity region with the muon spectrometer for events where the hadronic activity is required to be minimal. the analysis is based on an event sample corresponding to an integrated luminosity of about 55 mub-1. the cross section for coherent j/psi production in the rapidity interval -3.6 &lt; y &lt; -2.6 is measured to be dsigma/dy = 1.00 +/- 0.18 (stat) +0.24/-0.26 (syst) mb. the result is compared to theoretical models for coherent j/psi production and found to be in good agreement with models which include nuclear gluon shadowing.
experimental study of aerosol behavior and their deposits in a bucket elevator : impact on carry-over of micro-ingredients in animal feed industry. carry-over of additives and/or medicated products is a common issue in feed industry and, by extension in most of powder handling industries. currently carry-over rate of a production line can be accurately defined but the causes are not identified yet. it can be broken into 2 phases : firstly, particle deposit during one batch processing and then, their collecting during the following batches. experimental studies, carried out on industrial sites or on test benches, charged the bucket elevator situated just after the mixer to be responsible for a significant increase of cross contamination rate of industrial feed production lines. therefore this work focuses on this handling device. it transfers mixing of several raw materials in powdery forms, which may contain micro-ingredients, especially additives or medicated products. the aim of this study is to understand how process operations affect cross contamination rates during bucket elevator handling. a test bench of this handling device, a reference product and laboratory methods have been setup. moreover, an experimental fractional factorial design highlights the effects of several process parameters : on one hand the discharge phase on elevator head (linked to belt velocity) and the discharge spout angle act on microingredients deposit mass. on the other hand, spacing between buckets and the leg's inner surface influences micro-ingredients collected mass. furthermore, ideal position of process parameters has been defined. by this way, cross contamination rate on the test bench has been decreased from 9 to 7 percent. finally, velocity fields observations during the discharge phase leads to better understanding of how these process parameters influence cross contamination rate.
constancy of energy partition in central heavy-ion reactions at intermediate energies. semiclassical transport simulation of nucleus-nucleus collisions for the range of incident energy from about the fermi energy up to a few hundred mev per nucleon evidences that the maximal excitation energy put into a nuclear system during the early compact stage of heavy-ion reaction is a constant fraction of the center-of-mass available energy of the system. analysis of experimental data without presuming reaction mechanism dominating the collision process on the best corroborate the found constancy of energy partition in central heavy-ion reactions.
j/psi elliptic flow measurement in pb-pb collisions at \sqrt{s_{nn}} = 2.76 tev at forward rapidity with the alice experiment. j/psi suppression induced by color screening of its constituent quarks was proposed 26 years ago as a signature of the formation of a quark gluon plasma in heavy-ion collisions. recent results from alice in pb-pb collisions exhibit a smaller suppression with respect to previous measurements at the sps and rhic. the study of azimuthal anisotropy in particle production gives information on the collective hydrodynamic expansion at the early stage of the fireball, where the matter created in high-energy nuclear collisions is expected to be in a deconfined state. in particular, j/psi elliptic flow v2 is important to test the degree of thermalization of heavy quarks. together with the production yields, the elliptic flow is a powerful observable to address the question of suppression and regeneration of j/psi in qgp. we present the first inclusive j/psi elliptic flow measurement performed with the muon spectrometer of alice, in pb-pb collisions, at forward rapidity. integrated and pt-differential v2 results are presented and a comparison with recent star results and with a parton transport model is also performed.
comment on "on the subtleties of searching for dark matter with liquid xenon detectors". in a recent manuscript (arxiv:1208.5046) peter sorensen claims that xenon100's upper limits on spin-independent wimp-nucleon cross sections for wimp masses below 10 gev "may be understated by one order of magnitude or more". having performed a similar, though more detailed analysis prior to the submission of our new result (arxiv:1207.5988), we do not confirm these findings. we point out the rationale for not considering the described effect in our final analysis and list several potential problems with his study.
radiative energy loss in the absorptive qgp: taming the long formation lengths in coherent emission. in an absorptive plasma, damping of radiation mechanisms can influence the bremsstrahlung formation in case of large radiation formation lengths. we study qualitatively the influence of this effect on the gluon bremsstrahlung spectrum off heavy quarks in the quark-gluon plasma. independent of the heavy-quark mass, the spectrum is found to be strongly suppressed in an intermediate gluon energy region which grows with increasing gluon damping rate and increasing energy of the heavy quark. thus, just as polarization effects in the plasma render the bremsstrahlung spectra independent of the quark mass in the soft gluon regime, damping effects tend to have a similar impact for larger gluon energies.
fusion excitation function revisited. we report on a comprehensive systematics of fusion-evaporation and/or fusion-fission cross sections for a very large variety of systems over an energy range 4-155 a.mev. scaled by the reaction cross sections, fusion cross sections do not show a universal behavior valid for all systems although a high degree of correlation is present within subsets of appropriately selected data: regularities show up when data are ordered by the system mass asymmetry. for the rather light and close to mass-symmetric systems the main characteristics of the complete and incomplete fusion excitation functions can be precisely determined. despite an evident lack of data above 15a.mev for all heavy systems the available data suggests that geometrical effects could explain the persistence of incomplete fusion at incident energies as high as 155a.mev.
a grasp for real life inventory routing problem: application to bulk gas distribution. null
a modelling framework for procurement of a retail distribution system with economic and environmental goals. null
adding high-level concurrency to escala. on the one hand, languages like eventjava combine event- based programming with concurrency. on the other hand, extending aspect-oriented programming with concurrency has been studied as well. seamlessly combining both styles with concurrency in a single language is possible with the right building blocks. we claim that the join is such a build- ing block.
information acquisition of new technology performance for maintenance/investment decisions. the possibility of new technology occurrence has an important impact on the maintenance/ replacement decision. therefore a challenge in maintenance decision marking is to determine maintenance policy and replacement investment plan under an uncertainty of technology improvement. in each period, we must decide whether to gather additional information on the potential improvement of a new technology, then chose the appropriate action for the asset (do nothing, maintenance or replacement). to formulate this scenario, we use a non stationary markov decision process (mdp) model and provide some properties of the optimal policy based on a given set of numerical examples.
impact of maintenance on replacement investment under technological improvement. an unexplored important area in the equipment investment problem under technological improvement is the impact of maintenance policy. in fact, maintenance not only helps to maximize the profitability of the asset, but also prolong its economic life while waiting the apparition of better technology in the near future. therefore, we propose a model that allows us to consider how replacement investment in a new or improved asset will be influenced by maintenance. the investment decisions are based on information about the profitability of the current asset and the technological environment. for the maintenance process, we also consider the dependency of its cost and efficiency on the deterioration state of asset that is represented by a profit parameter. we use a non-stationary markov decision process to solve for the optimal investment/maintenance policy and illustrate the potential benefits of integrating maintenance policies in the investment strategy through different numerical analysis.
production of k*(892)^0 and phi(1020) in pp collisions at sqrt(s)=7 tev. the production of k*(892)^0 and phi(1020) in pp collisions at sqrt(s)=7 tev was measured by the alice experiment at the lhc.the yields and the transverse momentum spectra d^2n/dydpt at midrapidity |y|&lt;0.5 in the range 0.
proceedings of the seventh workshop on domain-specific aspect languages (dsal 2012). it is our great pleasure to host the seventh edition of the domain-specific aspect languages workshop (dsal12), as part of aosd 2012: perspectives on modularity, the 11th international conference on aspect-oriented software development. the tendency to raise the abstraction level in programming languages towards a particular domain is also a major driving force in the research domain of aspect-oriented programming languages. as a matter of fact, pioneering work in this field was conducted by devising small domain-specific aspect languages (dsals) such as cool for concurrency management and ridl for serialization, rg, aml, and others. after a dominating focus on general-purpose languages, research in the aosd community is again taking this path in search of innovative approaches, insights and a deeper understanding of fundamentals behind aop. based on the successful dsal'06-'11 workshops, and the special issue of iet software journal on domain- specific aspect languages, this workshop series continues to support a growing trend in aosd research. the workshop aims to bring the research communities of domain-specific language engineering and domain-specific aspect design together. in the previous successful editions held at gpce06/oopsla06 and aosd07 we approached domain-specific aspect languages both from a design and a language implementation point of view. at aosd08-10 we also invited contributions of work on adding domain-specific extensions (dsxs) to general-purpose aspect languages (gpals). last year and this year our focus is on the use of multiple dsals, or multidomain aop, and how dsals may ease composition issues. if an application uses multiple dsals, one for each domain, how can interactions be treated and what advantages do dsals bring to this setting? this year we accepted 6 papers for presentation and publication, 4 technical papers and 2 short papers. in addition to this, we have two invited talks: "language-oriented modularity through awesome dsals", by david lorenz, and "disl: an extensible language for efficient and comprehensive dynamic program analysis", by lukas marek, danilo ansaloni, and walter binder. both talks serve as a complement to the presentations of the respective authors in the research track of the aosd12 conference.
proceedings of the sixth annual workshop on domain-specific aspect languages (dsal 2011). null
high-precision measurement of total fission cross sections in spallation reactions of 208pb and 238u. total cross sections for proton- and deuteron-induced-fission of 208pb and 238u have been determined in the energy range between 500 mev and 1 gev. the experiment has been performed in inverse kinematics at gsi darmstadt, facilitating the counting of the projectiles and the identification of the reaction products. high precision between 5 and 7 percent has been achieved by individually counting the beam particles and by registering both fission fragments in coincidence with high efficiency and full z resolution. fission was clearly distinguished from other reaction channels. the results were found to deviate by up to 30 percent from prokofiev's systematics on total fission cross sections. there is good agreement with an elaborate experiment performed in direct kinematics.
a common aspect languages interpreter. the value of using different (possibly domain-specific) aspect languages to deal with a variety of crosscutting concerns in the development of complex software systems is well recognized. one should be able to use several of these languages together in a single program. however, on the one hand, developing a new domain-specific aspect language (dsal) in order to capture all common programming patterns of the domain takes a lot of time, and on the other hand, the designer of a new language should manage the interactions with the other languages when they are used together. &lt;br/&gt; in this thesis, we introduce support for rapid prototyping and composing aspect languages based on interpreters. we start from a base interpreter of a subset of java and we analyze and present a solution for its modular extension to support aop based on a common semantics aspect base defined once and for all. the extension, called the aspect interpreter, implements a common aspect mechanism and leaves holes to be defined when developing concrete languages. the power of this approach is that the aspect languages are directly implemented from their operational semantics. this is illustrated by implementing a lightweight version of aspectj. to apply the same approach and the same architecture to full java without changing its interpreter (jvm), we reuse aspectj to perform a first step of static weaving, which we complement by a second step of dynamic weaving, implemented through a thin interpretation layer. this can be seen as an interesting example of reconciling interpreters and compilers. we validate our approach by describing prototypes for aspectj, eaop, cool and a couple of other dsals and demonstrating the openness of our aspectj implementation with two extensions, one dealing with dynamic scheduling of aspects and another with alternative pointcut semantics. different aspect languages implemented with our framework can be easily composed. moreover, we provide support for customizing this composition.
measurement of inelastic, single- and double-diffraction cross sections in proton--proton collisions at the lhc with alice. measurements of cross sections of inelastic and diffractive processes in proton--proton collisions at lhc energies were carried out with the alice detector. the fractions of diffractive processes in inelastic collisions were determined from a study of gaps in charged particle pseudorapidity distributions: for single diffraction (diffractive mass $m_x &lt; 200$ gev/$c^2$) $\sigma_{\rm sd}/\sigma_{\rm inel} = 0.21 \pm 0.03, 0.20^{+0.07}_{-0.08}$, and $0.20^{+0.04}_{-0.07}$, respectively at centre-of-mass energies $\sqrt{s} = 0.9, 2.76$, and 7 tev; for double diffraction (for a pseudorapidity gap $\delta\eta &gt; 3$) $\sigma_{\rm dd}/\sigma_{\rm inel} = 0.11 \pm 0.03, 0.12 \pm 0.05$, and $0.12^{+0.05}_{-0.04}$, respectively at $\sqrt{s} = 0.9, 2.76$, and 7 tev. to measure the inelastic cross section, beam properties were determined with van der meer scans, and, using a simulation of diffraction adjusted to data, the following values were obtained: $\sigma_{\rm inel} = 62.8^{+2.4}_{-4.0} (model) \pm 1.2 (lumi)$ mb at $\sqrt{s} =$ 2.76 tev and $73.2^{+2.0}_{-4.6} (model) \pm 2.6 (lumi)$ mb at $\sqrt{s}$ = 7 tev. the single- and double-diffractive cross sections were calculated combining relative rates of diffraction with inelastic cross sections. the results are compared to previous measurements at proton--antiproton and proton--proton colliders at lower energies, to measurements by other experiments at the lhc, and to theoretical models.
simple temporal problems in route scheduling for the dial-a-ride problem with transfers. the dial-a-ride problem (darp) consists in defining a set of routes that satisfy transportation requests between a set of pickup points and a set of delivery points. this paper addresses a variant of the darp where requests can change of vehicle during their trip. this transshipment is made on specific locations called "transfer points". the corresponding problem is called the dial-a-ride problem with transfers (darpt). solving the darpt yields modeling and algorithmic di culties. in this paper, we focus on e ciently checking the feasibility of routes with regards to the problem temporal constraints in a large neighborhood search. this feasibility problem is a simple temporal problem, well studied in particular in artificial intelligence. we propose necessary and su cient conditions to fasten the detection of unfeasible or feasible routes.
an alns and route scheduling algorithms for the dial-a-ride problem with transfers. in the dial-a-ride problem with transfers (darpt), a passenger may be transfered from the vehicle that picked him up to another vehicle at some predetermined location, called transfer point. transfers may generate non negligible savings in terms of routing cost. we propose an adaptive large neighborhood search (alns) to solve the pickup and delivery problem with transfers (pdpt). we present heuristics introduced to integrate transfers in routes and how the alns has been adapted to the darpt. checking whether a solution of the darpt remains feasible after inserting a request in one route, or in two routes using a transfer point, becomes a non-trivial problem. indeed, routes should be synchronized at transfer points and therefore, cannot be scheduled independently anymore. for a set of routes, the feasibility problem given by the darpt precedence and time constraints can be stated as a simple temporal problem (stp). since the stp has to be solved after all request insertions, solving it e ciently is a crucial issue. we propose some necessary and su cient feasibility conditions that reduce the time needed to establish feasibility or unfeasibility.
testing inexecutable conditions on input pointers in c programs with sante. combinations of static and dynamic analysis techniques make it possible to detect the risk of out-of-bounds memory access in c programs and to confirm it on concrete test data. however, this is not directly possible for input arrays/pointers in c functions. this paper presents a specific technique allowing the interpretation and execution of assertions involving the size of an input array (pointer) of a c function. we show how this technique was successfully exploited in the sante tool where it allowed potential out-of-bounds access errors to be detected and classified in several real-life programs. keywords: run-time errors, c pointers, static analysis, test generation.
deviation from quark number scaling of the anisotropy parameter v2 of pions, kaons, and protons in au+au collisions at √snn=200 gev. measurements of the anisotropy parameter v2 of identified hadrons (pions, kaons, and protons) as a function of centrality, transverse momentum pt, and transverse kinetic energy ket at midrapidity (|η|&lt;0.35) in au + au collisions at √snn=200 gev are presented. pions and protons are identified up to pt= 6 gev/c, and kaons up to pt= 4 gev/c, by combining information from time-of-flight and aerogel čerenkov detectors in the phenix experiment. the scaling of v2 with the number of valence quarks (nq) has been studied in different centrality bins as a function of transverse momentum and transverse kinetic energy. a deviation from previously observed quark-number scaling is observed at large values of ket/nq in noncentral au + au collisions (20-60%), but this scaling remains valid in central collisions (0-10%).
j/ψ suppression at forward rapidity in pb-pb collisions at √snn=2.76 tev. null
j/$\psi$ production at high transverse momenta in p+p and au+au collisions at sqrt(s_{nn}) = 200 gev. we report $j/\psi$ spectra for transverse momenta $p_t$&gt; 5 gev/$c$ at mid-rapidity in p+p and au+au collisions at sqrt(s_{nn}) = 200 gev.the inclusive $j/\psi$ spectrum and the extracted $b$-hadron feed-down are compared to models incorporating different production mechanisms. we observe significant suppression of the $j/\psi$ yields for $p_t$&gt; 5 gev/$c$ in 0-30% au+au collisions relative to the p+p yield scaled by the number of binary nucleon-nucleon collisions in au+au collisions. in 30-60% collisions, no such suppression is observed.the level of suppression is consistently less than that of high-$p_t$ $\pi^{\pm}$ and low-$p_t$ $j/\psi$.
verification of atl transformations using transformation models and model finders. in model-driven engineering, models constitute pivotal elements of the software to be built. if models are specified well, transformations can be employed for different purposes, e.g., to produce final code. however, it is important that models produced by a transformation from valid input models are valid, too, where validity refers to the metamodel constraints, often written in ocl. transformation models are a way to describe this hoare-style notion of partial correctness of model transformations using only metamodels and constraints. in this paper, we provide an automatic translation of declarative, rule-based atl transformations into such transformation models, providing an intuitive and versatile encoding of atl into ocl that can be used for the analysis of various properties of transformations. we furthermore show how existing model verifiers (satisfiability checkers) for ocl-annotated metamodels can be applied for the verification of the translated atl transformations, providing evidence for the effectiveness of our approach in practice.
centrality dependence of charged particle production at large transverse momentum in pb--pb collisions at $\sqrt{s_{\rm{nn}}} = 2.76$ tev. the inclusive transverse momentum ($p_{\rm t}$) distributions of primary charged particles are measured in the pseudo-rapidity range $|\eta|&lt;0.8$ as a function of event centrality in pb--pb collisions at $\sqrt{s_{\rm{nn}}}=2.76$ tev with alice at the lhc. the data are presented in the $p_{\rm t}$ range $0.1530$ gev/$c$. in peripheral collisions (70--80%), the suppression is weaker with $r_{\rm{aa}} \approx 0.7$ almost independently of $p_{\rm t}$. the measured nuclear modification factors are compared to other measurements and model calculations.
dtpa complexation of bismuth in human blood serum. the in vivo212pb/212bi generator is promising for application in targeted alpha therapy (tat) of cancer. one main limitation of its therapeutic application is due to potential release of 212bi from the radioconjugate upon radioactive decay of the mother nuclide 212pb, potentially leading to irradiation of healthy tissue. the objective of the present work is to assess whether the chelate chx-a′′-dtpa (n-(2-aminoethyl)-trans-1,2-diaminocyclohexane-n,n′,n′′-pentaacetic acid) bound to a biological carrier molecule may be able to re-complex released 212bi under in vivo conditions to limit its translocation from the target site. chx-a′′-dtpa was bound to bovine gamma globulin (bgg) to mimic a model conjugate and the stability of the bi-chx-a′′-dtpa-bgg conjugate was studied in blood serum by ultrafiltration. trlfs experiments using cm(iii) as a fluorescent probe demonstrated that linking chx-a′′-dtpa to bgg does not affect the coordination properties of the ligand. furthermore, comparable stability constants were observed between bi(iii) and free chx-a′′-dtpa, bgg-bound chx-a′′-dtpa and dtpa. the complexation constants determined between bi(iii) and the chelate molecules are sufficiently high to allow ultra trace amounts of the ligand to efficiently compete with serum transferrin controlling bi(iii) speciation in blood plasma conditions. nevertheless, chx-a′′-dtpa is not able to complex bi(iii) generated in blood serum because of the strong competition between bi(iii) and fe(ii) for the ligand. in other words, chx-a′′-dtpa is not "selective" enough to limit bi(iii) release in the body when applying the 212pb/212bi in vivo generator.
the strong interaction at the collider and cosmic-rays frontiers. first data on inclusive particle production measured in proton-proton collisions at the large hadron collider (lhc) are compared to predictions of various hadron-interaction monte carlos (qgsjet, epos and sibyll) used commonly in high-energy cosmic-ray physics. while reasonable overall agreement is found for some of the models, none of them reproduces consistently the sqrt(s) evolution of all the measured observables. we discuss the implications of the new lhc data for the modeling of the non-perturbative and semihard parton dynamics in hadron-hadron and cosmic-rays interactions at the highest energies studied today.
d_s meson production at central rapidity in proton--proton collisions at sqrt(s) = 7 tev. the pt-differential inclusive production cross section of the prompt charm-strange meson d_s in the rapidity range |y|&lt;0.5 was measured in proton--proton collisions at sqrt(s)=7 tev at the lhc using the alice detector. the analysis was performed on a data sample of 2.98 10^8 events collected with a minimum-bias trigger. the corresponding integrated luminosity is l_int=4.8 nb^-1. reconstructing the decay d_s -.&gt; phi pi, with phi -&gt; kk, and its charge conjugate, about 480 d_s mesons were counted, after selection cuts, in the transverse momentum range 2.
pion, kaon, and proton production in central pb--pb collisions at $\sqrt{s_{nn}} = 2.76$ tev. in this letter we report the first results on $\pi^\pm$, k$^\pm$, p and pbar production at mid-rapidity (|y|&lt;0.5) in central pb-pb collisions at sqrt{s_{nn}} = 2.76 tev, measured by the alice experiment at the lhc. the pt distributions and yields are compared to previous results at sqrt{s_{nn}} = 200 gev and expectations from hydrodynamic and thermal models. the spectral shapes indicate a strong increase of the radial flow velocity with sqrt{s_{nn}}, which in hydrodynamic models is expected as a consequence of the increasing particle density. while the k/pi ratio is in line with predictions from the thermal model, the p/pi ratio is found to be lower by a factor of about 1.5. this deviation from thermal model expectations is still to be understood.
measurement of electrons from beauty hadron decays in pp collisions at sqrt{s} = 7 tev. the production cross section of electrons from semileptonic decays of beauty hadrons was measured at mid-rapidity (|y| &lt; 0.8) in the transverse momentum range 1 &lt; pt &lt; 8 gev/c with the alice experiment at the cern lhc in pp collisions at a center of mass energy sqrt{s} = 7 tev using an integrated luminosity of 2.2 nb^{-1}. electrons from beauty hadron decays were selected based on the displacement of the decay vertex from the collision vertex. a perturbative qcd calculation agrees with the measurement within uncertainties. the data were extrapolated to the full phase space to determine the total cross section for the production of beauty quark-antiquark pairs.
dilepton production in proton-proton and pb+pb collisions at sqrt(s_nn)=2.76 tev. we study e^+e^- pair production in proton-proton and central pb+pb collisions at sqrt(s_nn)=2.76 tev within two models: an extended statistical hadronization model (shm) and the parton-hadron-string dynamics (phsd) transport approach. we find that the phsd calculations roughly agree with the dilepton spectrum from hadronic sources with the 'cocktail' estimates from the statistical hadronization model matched to available data at lhc energies. the dynamical simulations within the phsd show a moderate increase of the low mass dilepton yield essentially due to the in-medium modification of the rho-meson. furthermore, pronounced traces of the partonic degrees of freedom are found in the phsd results in the intermediate mass regime. the dilepton production from the strongly interacting quark-gluon plasma (sqgp) exceeds that from the semi-leptonic decays of open charm and bottom mesons. additionally, we observe that a transverse momentum cut of 1 gev/c further suppresses the relative contribution of the heavy meson decays to the dilepton yield, such that the sqgp radiation strongly dominates the spectrum for masses from 1 to 3 gev, allowing a closer look at the electromagnetic emissivity of the partonic plasma in the early phase of pb+pb collisions.
the rapid atmospheric monitoring system of the pierre auger observatory. the pierre auger observatory is a facility built to detect air showers produced by cosmic rays above 10^17 ev. during clear nights with a low illuminated moon fraction, the uv fluorescence light produced by air showers is recorded by optical telescopes at the observatory. to correct the observations for variations in atmospheric conditions, atmospheric monitoring is performed at regular intervals ranging from several minutes (for cloud identification) to several hours (for aerosol conditions) to several days (for vertical profiles of temperature, pressure, and humidity). in 2009, the monitoring program was upgraded to allow for additional targeted measurements of atmospheric conditions shortly after the detection of air showers of special interest, e.g., showers produced by very high-energy cosmic rays or showers with atypical longitudinal profiles. the former events are of particular importance for the determination of the energy scale of the observatory, and the latter are characteristic of unusual air shower physics or exotic primary particle types. the purpose of targeted (or "rapid") monitoring is to improve the resolution of the atmospheric measurements for such events. in this paper, we report on the implementation of the rapid monitoring program and its current status. the rapid monitoring data have been analyzed and applied to the reconstruction of air showers of high interest, and indicate that the air fluorescence measurements affected by clouds and aerosols are effectively corrected using measurements from the regular atmospheric monitoring program. we find that the rapid monitoring program has potential for supporting dedicated physics analyses beyond the standard event reconstruction.
measurement of the proton-air cross-section at $\sqrt{s}=57$ tev with the pierre auger observatory. we report a measurement of the proton-air cross-section for particle production at the center-of-mass energy per nucleon of 57 tev. this is derived from the distribution of the depths of shower maxima observed with the pierre auger observatory: systematic uncertainties are studied in detail. analysing the tail of the distribution of the shower maxima, a proton-air cross-section of $[505\pm22(stat)^{+28}_{-36}(sys)]$ mb is found.
reactor electron antineutrino disappearance in the double chooz experiment. the double chooz experiment has observed 8,249 candidate electron antineutrino events in 227.93 live days with 33.71 gw-ton-years (reactor power x detector mass x livetime) exposure using a 10.3 cubic meter fiducial volume detector located at 1050 m from the reactor cores of the chooz nuclear power plant in france. the expectation in case of theta13 = 0 is 8,937 events. the deficit is interpreted as evidence of electron antineutrino disappearance. from a rate plus spectral shape analysis we find sin^2 2{\theta}13 = 0.109 \pm 0.030(stat) \pm 0.025(syst). the data exclude the no-oscillation hypothesis at 99.9% cl (3.1{\sigma}).
interactions between nuclear fuel and water at the fukushima daiichi reactors. used nuclear fuel is a redox-sensitive semiconductor consisting of uranium dioxide containing a few percent of fission products and up to about one percent transuranium elements, mainly plutonium. the rapid increase in temperature in the cores of the fukushima reactors was caused by the loss of coolant in the aftermath of the damage from the tsunami. temperatures probably well above 2000 °c caused melting of not only the uo2 in the fuel but also the zircaloy cladding and steel, forming a quenched melt, termed corium. substantial amounts of volatile fission products, such as cs and i, were released during melting, but the less volatile fission products and the actinides (probably &gt;99.9%) were incorporated into the corium as the melt cooled and was quenched. the corium still contains these radionuclides, which leads to a very large long-term radiotoxicity of the molten reactor core. the challenge for environmental scientists is to assess the long-term interactions between water and the mixture of corium and potentially still-existing unmelted fuel, particularly if the molten reactor core is left in place and covered with a sarcophagus for hundreds of years. part of the answer to this question can be found in the knowledge that has been gained from research into the disposal of spent nuclear fuel in a geologic repository.
dark matter results from 225 live days of xenon100 data. we report on a search for particle dark matter with the xenon100 experiment, operated at the laboratori nazionali del gran sasso (lngs) for 13 months during 2011 and 2012. xenon100 features an ultra-low electromagnetic background of (5.3\pm0.6)\times10^-3 events (kg day kevee)^-1 in the energy region of interest. a blind analysis of 224.6 live days \times 34 kg exposure has yielded no evidence for dark matter interactions. the two candidate events observed in the pre-defined nuclear recoil energy range of 6.6-30.5 kevnr are consistent with the background expectation of (1.0 \pm 0.2) events. a profile likelihood analysis using a 6.6-43.3 kevnr energy range sets the most stringent limit on the spin-independent elastic wimp-nucleon scattering cross section for wimp masses above 8 gev/c^2, with a minimum of 2 \times 10^-45 cm^2 at 55 gev/c^2 and 90% confidence level.
net-charge fluctuations in pb-pb collisions at \sqrt(s)_nn = 2.76 tev. we report the first measurement of the net-charge fluctuations in pb-pb collisions at \surd s_nn 2.76 tev, measured with the alice detector at the cern large hadron collider. the dynamical fluctuations per unit entropy are observed to decrease when going from peripheral to central collisions. an additional reduction in the amount of fluctuations is seen in comparison to the results from lower energies. we examine the dependence of fluctuations on the pseudo-rapidity interval, which may account for the dilution of fluctuations during the evolution of the system. we find that the alice data points are between the theoretically predicted values for a hadron gas and a quark-gluon plasma.
from implicit to explicit pavings. a combinatorial search can either be performed by using an implicit search tree, where an initial state is recursively transformed until some goal state is reached, or by using an explicit search tree, where an initial tree structure containing the root state is iteratively expanded until the leaves match the set of goal states. this paper proposes an exploratory study aimed at showing that explicit search trees can play a distinguished role in the field of numerical constraints. the first advantage of an explicit search is expressiveness: we can write new algorithms or reformulate existing ones in a simple and unified way. the second advantage is efficiency, since an implicit search may also lead to a blowup of redundant computations. this is illustrated through various examples.
search for point-like sources of ultra-high energy neutrinos at the pierre auger observatory and improved limit on the diffuse flux of tau neutrinos. the surface detector array of the pierre auger observatory can detect neutrinos with energy e ν between 1017 ev and 1020 ev from point-like sources across the sky south of +55° and north of -65° declinations. a search has been performed for highly inclined extensive air showers produced by the interaction of neutrinos of all flavors in the atmosphere (downward-going neutrinos), and by the decay of tau leptons originating from tau neutrino interactions in earth's crust (earth-skimming neutrinos). no candidate neutrinos have been found in data up to 2010 may 31. this corresponds to an equivalent exposure of ~3.5 years of a full surface detector array for the earth-skimming channel and ~2 years for the downward-going channel. an improved upper limit on the diffuse flux of tau neutrinos has been derived. upper limits on the neutrino flux from point-like sources have been derived as a function of the source declination. assuming a differential neutrino flux k ps * e -2 ν from a point-like source, 90% confidence level upper limits for k ps at the level of ≈5 × 10-7 and 2.5 × 10-6 gev cm-2 s-1 have been obtained over a broad range of declinations from the searches for earth-skimming and downward-going neutrinos, respectively.
recent results on heavy quark quenching in ultrarelativistic heavy ion collisions. in this contribution, we present some predictions for the production of d and b mesons in ultrarelativistic heavy ion collisions at rhic and lhc energies and confront them with experimental results obtained so far by the star, phenix, alice and cms collaborations. we next discuss some preliminary results obtained with an improved description of the medium based on epos initial conditions, and its possible implications on the nuclear modification factor and on the elliptic flow of heavy quarks.
durability of an as2s3 chalcogenide glass: optical properties and dissolution kinetics. the durability of a as2s3 chalcogenide glass composition was studied in de-ionized water at different temperatures (60-90 °c) for different periods of time, up to 120 days. the evolutions of the chemical composition and the ph of the solutions as well as the optical transmission of bulk samples, in the 2-10 μm region, were measured as a function of corrosion time. atomic force microscopy and optical microscopy were used to investigate the roughness of corroded surfaces and the evolution of surface defects. the water corrosion of as2s3 glass was found to follow a congruent dissolution mechanism, a possible glass-water reaction mechanism was proposed. the optical transmission of the glass was found to be affected by the corrosion. the optical loss increased from 4 to 21% with corrosion time, this variation was attributed to the texturation of the surface by the reaction of corrosion. moreover, the experimental results show that high temperature value enhances the corrosion reaction: an activation energy of 103 ± 2 kj/mol was computed from experimental measurements.
energy and system-size dependence of two- and four-particle $v_2$ measurements in heavy-ion collisions at rhic and their implications on flow fluctuations and nonflow. we present star measurements of azimuthal anisotropy by means of the two- and four-particle cumulants $v_2$ ($v_2\{2\}$ and $v_2\{4\}$) for au+au and cu+cu collisions at center of mass energies $\sqrt{s_{_{\mathrm{nn}}}} = 62.4$ and 200 gev. the difference between $v_2\{2\}^2$ and $v_2\{4\}^2$ is related to $v_{2}$ fluctuations ($\sigma_{v_2}$) and nonflow $(\delta_{2})$. we present an upper limit to $\sigma_{v_2}/v_{2}$. following the assumption that eccentricity fluctuations $\sigma_{\epsilon}$ dominate $v_2$ fluctuations $\frac{\sigma_{v_2}}{v_2} \approx \frac{\sigma_{\epsilon}}{\epsilon}$ we deduce the nonflow implied for several models of eccentricity fluctuations that would be required for consistency with $v_2\{2\}$ and $v_2\{4\}$. we also present results on the ratio of $v_2$ to eccentricity.
an event-driven optimization framework for dynamic vehicle routing. the real-time operation of a fleet of vehicles introduces challenging optimization problems researches in a wide range of applications, thus, it is appealing to both academia and practitioners in industry. in this work we focus on dynamic vehicle routing problems and present an event-driven framework that can anticipate unknown changes in the problem information. the proposed framework is intrinsically parallelized to take advantage of modern multi-core and multi-threaded computing architectures. it is also designed to be easily embeddable in decision support systems that cope with a wide range of contexts and side constraints. we illustrate the flexibility of the framework by showing how it can be adapted to tackle the dynamic vehicle routing problem with stochastic demands. computational results show that while our approach is competitive against state-of-the art algorithms, it still ensures greater reactivity and requires less assumptions (e.g., demand distributions).
lightweight string reasoning for ocl. models play a key role in assuring software quality in the modeldriven approach. precise models usually require the definition of ocl expressions to specify model constraints that cannot be expressed graphically. techniques that check the satisfiability of such models and find corresponding instances of them are important in various activities, such as model-based testing and validation. several tools to check model satisfiability have been developed but to our knowledge, none of them yet supports the analysis of ocl expressions including operations on strings in general terms. as, in contrast, many industrial models do contain such operations, there is evidently a gap. there has been much research on formal reasoning on strings in general, but so far the results could not be included into model finding approaches. for model finding, string reasoning only contributes a sub-problem, therefore, a string reasoning approach for model finding should not add up front too much computational complexity to the global model finding problem. we present such a lightweight approach based on constraint satisfaction problems and constraint rewriting. our approach efficiently solves several common kinds of string constraints and it is integrated into the emftocsp model finder.
on verifying atl transformations using 'off-the-shelf' smt solvers. mde is a software development process where models constitute pivotal elements of the software to be built. if models are well-specified, transformations can be employed for various purposes, e.g., to produce final code. however, transformations are only meaningful when they are 'correct': they must produce valid models from valid input models. a valid model has conformance to its meta-model and fulfils its constraints, usually written in ocl. in this paper, we propose a novel methodology to perform automatic, unbounded verification of atl transformations. its main component is a novel first-order semantics for atl transformations, based on the interpretation of the corresponding rules and their execution semantics as first-order predicates. although, our semantics is not complete, it does cover a significant subset of the atl language. using this semantics, transformation correctness can be automatically verified with respect to non-trivial ocl pre- and postconditions by using smt solvers, e.g. z3 and yices.
charge separation relative to the reaction plane in pb-pb collisions at $\sqrt{s_{nn}}= 2.76$ tev. measurements of charge dependent azimuthal correlations with the alice detector at the lhc are reported for pb-pb collisions at $\sqrt{s_{nn}} = 2.76$ tev. two- and three-particle charge-dependent azimuthal correlations in the pseudo-rapidity range $|\eta | &lt; 0.8$ are presented as a function of the collision centrality, particle separation in pseudo-rapidity, and transverse momentum. a clear signal compatible with the expectation of a charge-dependent separation relative to the reaction plane is observed, which shows little or no collision energy dependence when compared to measurements at rhic energies. models incorporating effects of local parity violation in strong interactions fail to describe the observed collision energy dependence.
well-typed services cannot go wrong. service-oriented applications are frequently used in highly dynamic contexts: ser- vice compositions may change dynamically, in particular, because new services are discovered at runtime. moreover, subtyping has recently been identified as a strong requirement for service dis- covery. correctness guarantees over service compositions, provided in particular by type systems, are highly desirable in this context. however, while service oriented applications can be built using various technologies and protocols, none of them provides decent support ensuring that well-typed services cannot go wrong. an emitted message, for instance, may be dangling and remain as a ghost message in the network if there is no agent to receive it. in this article, we introduce a formal model for service compositions and define a type system with subtyping that ensures type soundness by combining static and dynamic checks. we also demonstrate how to preserve type soundness in presence of malicious agents and insecure communication channels.
enhanced stiffness modeling of serial and parallel manipulators for robotic-based processing of high performance materials. the thesis focuses on the enhancement of stiffness modeling of serial and parallel manipulators with passive joints under an essential loading. the developed technique takes into account different types of loadings: external force/torque applied to the end-point, internal preloading in the joints and auxiliary forces/torques applied to intermediate points. in contrast to previous works, the proposed technique includes computing an equilibrium configuration, which exactly corresponds to the loading. this allows to obtain the full-scale force-deflection relation for any given workspace point and to linearise it taking into account variation of the manipulator jacobian because of the external force/torque. the proposed approach also enables designer to evaluate critical forces that may provoke non-linear behaviours of the manipulators, such as sudden failure due to elastic instability (buckling), which has not been previously studied in robotics. in the frame of this work, it is assumed that the manipulator elasticity is described by a multidimensional lumped-parameter model, which corresponds to a set of rigid bodies connected by 6-dof virtual springs. each of these springs characterize flexibility of the corresponding link or actuated joint and takes into account both their compliance and joint particularities. to increase the model accuracy, the stiffness parameters of the spring are evaluated using fea-based virtual experiments and dedicated identification technique developed in the thesis. this gives almost the same accuracy as fea but essentially reduces the computational effort, eliminating repetitive re-meshing through the workspace.
inclusive charged hadron elliptic flow in au + au collisions at $\sqrt{s_{nn}}$ = 7.7 - 39 gev. a systematic study is presented for centrality, transverse momentum ($p_t$) and pseudorapidity ($\eta$) dependence of the inclusive charged hadron elliptic flow ($v_2$) at midrapidity ($|\eta| &lt; 1.0$) in au+au collisions at $\sqrt{s_{nn}}$ = 7.7, 11.5, 19.6, 27 and 39 gev. the results obtained with different methods, including correlations with the event plane reconstructed in a region separated by a large pseudorapidity gap, and 4-particle cumulants ($v_2\{4\}$), are presented in order to investigate non-flow correlations and $v_2$ fluctuations. we observe that the difference between $v_2\{2\}$ and $v_2\{4\}$ is smaller at the lower collision energies. values of $v_2$, scaled by the initial coordinate space eccentricity, $v_{2}/\varepsilon$, as a function of $p_t$ are larger in more central collisions, suggesting stronger collective flow develops in more central collisions, similar to the results at higher collision energies. these results are compared to measurements at higher energies at rhic ($\sqrt{s_{nn}}$ = 62.4 and 200 gev) and at lhc (pb + pb collisions at $\sqrt{s_{nn}}$ = 2.76 tev). the $v_2(p_t)$ values for fixed $p_t$ rise with increasing collision energy within the $p_t$ range studied ($&lt; 2 {\rm gev}/c$). we compare the $v_2$ results to urqmd and ampt transport model calculations, and physics implications on the dominance of partonic versus hadronic phases in the system created at bes energies are discussed.
atltest: a white-box test generation approach for atl transformations. mde is being applied to the development of increasingly complex systems that require larger model transformations. given that the specification of such transformations is an error-prone task, techniques to guarantee their quality must be provided. testing is a well-known technique for finding errors in programs. in this sense, adoption of testing techniques in the model transformation domain would be helpful to improve their quality. so far, testing of model transformations has focused on black-box testing techniques. instead, in this paper we provide a white-box test model generation approach for atl model transformations.
report on the detection and acquisition systems at nfs. null
transforming very large models in the cloud: a research roadmap. model transformations are widely used by model-driven engineering (mde) platforms to apply different kinds of operations over models, such as model translation, evolution or composition. however, existing solutions are not designed to handle very large models (vlms), thus facing scalability issues. coupling mde with cloud-based platforms may help solving these issues. since cloud-based platforms are relatively new, researchers still need to investigate if/how/when mde solutions can benefit from them. in this paper, we investigate the problem of transforming vlms in the cloud by addressing the two phases of 1) model storage and 2) model transformation execution in the cloud. for both aspects we identify a set of research questions, possible solutions and probable challenges researchers may face.
monte carlo simulation of ion-exchange response of copper sulfide ion selective electrode for metal pollutant detection. the present work aims at studying the potentiometric response of an ion-selective electrode. the target material is a copper sulfide (cus) thin film designed for the detection of cu+2 ions in solutions. the electrode is prepared by mean of an electrochemical deposition of copper sulfide on a silicon substrate. the cu+2 response is then studied and a near nernstian behavior of this electrode is observed in the range of pcu 6-1. in order to quantitatively explain the exchange process behind the cu+2 response, a stochastic computational method is established to explain the potentiometric response of the copper sulfide sensors that can be used for water pollutant detection. the numerical scheme is based on monte carlo simulation of ion exchange between the solution and the cus membrane surface. the probability of this cu+2 -ion exchange is implemented as the main factor, which governs the variation of the electrode potential vs. the cu+2 concentration in the solution. three characteristics of the detection response are studied, namely the detection threshold, the slope and the saturation concentration. the model validation is achieved by predicting the nernstian behavior for the studied cus sensor and by comparing the obtained results with data from the literature dealing with cu+2 detection.
self-management of cloud applications and infrastructure for energy optimization. null
hsv-1 cgal+ infection promotes quaking rna binding protein production and induces nuclear-cytoplasmic shuttling of quaking i-5 isoform in human hepatoma cells. herpesvirus type 1 (hsv-1) based oncolytic vectors arise as a promising therapeutic alternative for neoplastic diseases including hepatocellular carcinoma. however, the mechanisms mediating the host cell response to such treatments are not completely known. it is well established that hsv-1 infection induces functional and structural alterations in the nucleus of the host cell. in the present work, we have used gel-based and shotgun proteomic strategies to elucidate the signaling pathways impaired in the nucleus of human hepatoma cells (huh7) upon hsv-1 cgal(+) infection. both approaches allowed the identification of differential proteins suggesting impairment of cell functions involved in many aspects of host-virus interaction such as transcription regulation, mrna processing, and mrna splicing. based on our proteomic data and additional functional studies, cellular protein quaking content (qki) increases 4 hours postinfection (hpi), when viral immediate-early genes such as icp4 and icp27 could be also detected. depletion of qki expression by small interfering rna results in reduction of viral immediate-early protein levels, subsequent decrease in early and late viral protein content, and a reduction in the viral yield indicating that qki directly interferes with viral replication. in particular, hsv-1 cgal(+) induces a transient increase in quaking i-5 isoform (qki-5) levels, in parallel with an enhancement of p27(kip1) protein content. moreover, immunofluorescence microscopy showed an early nuclear redistribution of qki-5, shuttling from the nucleus to the cytosol and colocalizing with nectin-1 in cell to cell contact regions at 16-24 hpi. this evidence sheds new light on mechanisms mediating hepatoma cell response to hsv-1 vectors highlighting qki as a central molecular mediator.
photocatalytic oxidation of volatile organic compounds and monitor of their reaction intermediates : investigation of static and dynamic reactors at typical concentrations of indoor air. heterogeneous photocatalysis is a technique of oxidation used for the removal of volatile organic compounds (vocs). aim is to study the degradation of initial vocs and the production of reaction intermediates during this process in conditions close to the indoor air (voc concentration in mixture). three model vocs (toluene, decane, trichloroethylene) are studied separately and then in mixture in a static reactor and in a dynamic multi-pass reactor. the obtained results show that (i) the degradation efficiency depends on the nature and the number of vocs, on the photocatalyst characteristics and on process conditions, (ii) the major and the most persistent intermediates are light aldehydes, (iii) the elimination of aldehydes is inhibited when the initial vocs are in mixture, (iv) increasing the residence time on the photocatalyst provides a higher removal rate of initial vocs and of byproducts.
local search for a personnel scheduling problem with fixed jobs and an equity objective. null
a tour scheduling problem with fixed jobs: use of constraint programming. null
improving the asymmetric tsp by considering graph structure. recent works on cost based relaxations have improved constraint programming (cp) models for the traveling salesman problem (tsp). we provide a short survey over solving asymmetric tsp with cp. then, we suggest new implied propagators based on general graph properties. we experimentally show that such implied propagators bring robustness to pathological instances and highlight the fact that graph structure can significantly improve search heuristics behavior. finally, we show that our approach outperforms current state of the art results.
k0s-k0s correlations in pp collisions at sqrt{s}=7 tev from the lhc alice experiment. identical neutral kaon pair correlations are measured in sqrt{s}=7 tev pp collisions in the alice experiment. one-dimensional k0s-k0s correlation functions in terms of the invariant momentum difference of kaon pairs are formed in two multiplicity and two transverse momentum ranges. the femtoscopic parameters for the radius and correlation strength of the kaon source are extracted. the fit includes quantum statistics and final-state interactions of the a0/f0 resonance. k0s-k0s correlations show an increase in radius for increasing multiplicity and a slight decrease in radius for increasing transverse mass, mt, as seen in pion-pion correlations in the pp system and in heavy-ion collisions. transverse mass scaling is observed between the k0s-k0s and pion-pion radii. also, the first observation is made of the decay of the f2'(1525) meson into the k0s-k0s channel in pp collisions.
single spin asymmetry $a_n$ in polarized proton-proton elastic scattering at $\sqrt{s}=200$ gev. we report a high precision measurement of the transverse single spin asymmetry $a_n$ at $\sqrt{s}=200$ gev in elastic proton-proton scattering by the star experiment at rhic. the $a_n$ was measured in the four-momentum transfer $t$ range $0.003 \leqslant |t| \leqslant 0.035$ $\gevcsq$, the region of a significant interference between the electromagnetic and hadronic scattering amplitudes. the measured values of $a_n$ and its $t$-dependence are consistent with the absence of a hadronic spin-flip amplitude, thus providing strong constraints on the ratio of the single spin-flip to the non-flip amplitudes. since the hadronic amplitude is dominated by the pomeron amplitude at this $\sqrt{s}$, we conclude that this measurement addresses the question about the presence of a hadronic spin flip due to the pomeron exchange in polarized proton-proton elastic scattering.
interval-based robustness of linear parametrized filters. this article deals with the resilient implementation of parametrized linear filters (or controllers), i.e. realizations that are robust with respect to their implementation with fixed-point arithmetic. the implementation of a linear filter/controller in an embedded device is a difficult task because the numerical version of such algorithms suff ers from a deterioration in performances and characteristics. this degradation has two separate origins, corresponding to the quantization of the embedded coefficients and the round-off occurring during the computations. the optimal realization problem is to find, for a given filter, the most resilient realization. we here consider linear filters that depends on a set of parameters that are not exactly known during the design. they are used for example in automotive control, where a very late re-tuning is required. the paper presents results on fwl resiliency analyzis using interval optimization methods [2], and compare them to those obtained with the sensitivity approach.
structural changes upon lithium insertion in ni0.5 tiopo4. null
measurement of charm production at central rapidity in proton-proton collisions at sqrt(s) = 2.76 tev. the pt-differential production cross sections of the prompt (b feed-down subtracted) charmed mesons d0, d+, and d*+ in the rapidity range |y|&lt;0.5, and for transverse momentum 1&lt; pt &lt;12 gev/c, were measured in proton-proton collisions at sqrt(s) = 2.76 tev with the alice detector at the large hadron collider. the analysis exploited the hadronic decays d0 -&gt; k pi, d+ -&gt; k pi pi, d*+ -&gt; d0 pi, and their charge conjugates, and was performed on a l_{int} = 1.35 nb^{-1} event sample collected in 2011 with a minimum-bias trigger. the total charm production cross section at sqrt(s) = 2.76 tev and at 7 tev was evaluated by extrapolating to the full phase space the pt-differential production cross sections at sqrt(s) = 2.76 tev and our previous measurements at sqrt(s) = 7 tev. the results were compared to existing measurements and to perturbative-qcd calculations. the fraction of cubar d mesons produced in a vector state was also determined.
towards an electric-sense-based bioinspired embodied robotic perception system: the modelling aspect. in the context of the new paradigm of embodied intelligence invoked by both the roboticians and the cognitive scientists, a sensor bio-inspired from the electric fish was built. a certain geometry was pointed out for an accurate analytical prediction of the electrical measurements on the body in the presence of exterior objects. such perception model can establish potentially a direct relation between the location of the object and the body positions measurements as well as the shape of the object and the body direction. in addition to a potential novel tomography technique and a new potential electrolocation system it o ers new insights in understanding the role of the body in perceiving the world.
direct-photon production in p+p collisions at sqrt(s)=200 gev at midrapidity. the differential cross section for the production of direct photons in p+p collisions at sqrt(s)=200 gev at midrapidity was measured in the phenix detector at the relativistic heavy ion collider. inclusive-direct photons were measured in the transverse-momentum range from 5.5--25 gev/c, extending the range beyond previous measurements. event structure was studied with an isolation criterion. next-to-leading-order perturbative-quantum-chromodynamics calculations give a good description of the spectrum. when the cross section is expressed versus x_t, the phenix data are seen to be in agreement with measurements from other experiments at different center-of-mass energies.
anisotropic flow of charged hadrons, pions and (anti-)protons measured at high transverse momentum in pb-pb collisions at $\snn=2.76$ tev. the elliptic, $v_2$, triangular, $v_3$, and quadrangular, $v_4$, flow coefficients are measured for unidentified charged particles, pions and (anti-)protons in pb-pb collisions at $\snn = 2.76$ tev with the alice detector at the large hadron collider. results obtained with the event plane and four-particle correlation methods are reported for the pseudo-rapidity range $|\eta|&lt;0.8$ at different collision centralities and as a function of transverse momentum, $\pt$, out to $\pt=20$ gev/$c$. the observed non-zero elliptic and triangular flow depends only weakly on transverse momentum for $\pt&gt;8$ gev/$c$. the small $\pt$ dependence of the difference between elliptic flow results obtained from two- and four-particle cumulant methods suggests a common origin of flow fluctuations up to $\pt=8$ gev/$c$. the magnitude of the (anti-)proton elliptic and triangular flow is larger than that of pions out to at least $\pt=8$ gev/$c$ indicating that the particle type dependence persists out to high $\pt$.
study of the production yield of j/psi and single muons in collisions protn-proton with the muon spectrometer of the alice experiment at lhc. the quark-gluon plasma is a state of nuclear matter appearing at very high temperature. in the laboratory, it is possible to reach such conditions using heavy-ion collisions at ultra-relativistic energies. the alice experiment at lhc is dedicated to the study of the quark-gluon plasma with pb-pb collisions at 2.76 tev. the first part of this study, presented as an annex, will consist in the very first results of the alice muon spectrometer, obtained using cosmic rays. the second part will present the evolution of the reconstruction efficiency of the muon spectrometer during its first two years of running. this study will show that the total reconstruction efficiency of the tracking chamber is more than 90% in proton-proton collisions, and 85% in lead-lead collisions. a track selection method based on the value of the product momentum - distance of closest approach will also be presented. this selection will allow to remove the tracks coming from muons produced in collisions between the beam and the residual gas in the beam line, and fake tracks in the most central pb-pb collisions. finally, this thesis will present a first analysis of the production yield of j/psi and single muons as a function of the collision's charged-particle multiplicity in proton-proton collisions.
transverse single-spin asymmetry and cross-section for pi0 and eta mesons at large feynman-x in polarized p+p collisions at sqrt(s)=200 gev. measurements of the differential cross-section and the transverse single-spin asymmetry, a_n, vs. x_f for pi0 and eta mesons are reported for 0.4 &lt; x_f &lt; 0.75 at an average pseudorapidity of 3.68. a data sample of approximately 6.3 pb^{-1} was analyzed, which was recorded during p+p collisions at sqrt{s} = 200 gev by the star experiment at rhic. the average transverse beam polarization was 56%. the cross-section for pi0 is consistent with a perturbative qcd prediction, and the eta/pi0 cross-section ratio agrees with previous mid-rapidity measurements. for 0.55 &lt; x_f &lt; 0.75, a_n for eta (0.210 +- 0.056) is 2.2 standard deviations larger than a_n for pi0 (0.081 +- 0.016).
noninteger flux - why it does not work. we consider the dirac operator on a 2-sphere without one point in the case of non-integer magnetic flux. we show that the spectral problem for the hamiltonian (the square of dirac operator) can always be well defined, if including in the hilbert space only nonsingular on 2-sphere wave functions. however, this hilbert space is not invariant under the action of the dirac operator; the action of the latter on some nonsingular states produces singular functions. this breaks explicitly the supersymmetry of the spectrum. in the integer flux case, the supersymmetry can be restored if extending the hilbert space to include locally regular sections of the corresponding fiber bundle. for non-integer fluxes, such an extention is not possible.
once more on the witten index of 3d supersymmetric ym-cs theory. the problem of counting the vacuum states in the supersymmetric 3d yang-mills-chern-simons theory is reconsidered. we resolve the controversy between its original calculation by witten at large volumes and the calculation based on the evaluation of the effective lagrangian in the small volume limit. we show that the latter calculation suffers from uncertainties associated with the singularities in the moduli space of classical vacua where the born-oppenheimer approximation breaks down. we also show that these singularities can be accurately treated in the hamiltonian born-oppenheimer method, where one has to match carefully the effective wave functions on the abelian valley and the wave functions of reduced non-abelian qm theory near the singularities. this gives the same result as original witten's calculation.
real and complex supersymmetric d = 1 sigma models with torsions. we derive and discuss, at both the classical and the quantum levels, generalized n = 2 supersymmetric quantum mechanical sigma models describing the motion over an arbitrary real or an arbitrary complex manifold with extra torsions. we analyze the relevant vacuum states to make explicit the fact that their number is not affected by adding the torsion terms.
v2 scaling in pbpb collisions at 2.76 tev. we investigate scaling properties of the elliptical flow parameter v_{2} in pbpb collisions at 2.76 tev within a recently introduced new theoretical scheme which accounts for hydrodynamically expanding bulk matter, jets, and the interaction between the two.
production of muons from heavy flavour decays at forward rapidity in pp and pb-pb collisions at $\sqrt {s_{nn}}$ = 2.76 tev. the alice collaboration has measured the inclusive production of muons from heavy flavour decays at forward rapidity, 2.5 &lt; y &lt; 4, in pp and pb-pb collisions at $\sqrt {s_{nn}}$ = 2.76 tev. the pt-differential inclusive cross section of muons from heavy flavour decays in pp collisions is compared to perturbative qcd calculations. the nuclear modification factor is studied as a function of pt and collision centrality. a weak suppression is measured in peripheral collisions. in the most central collisions, a suppression of a factor of about 3-4 is observed in 6 &lt; pt &lt; 10 gev/c. the suppression shows no significant pt dependence.
measurement of prompt and non-prompt j/psi production cross sections at mid-rapidity in pp collisions at sqrt(s) = 7 tev. the alice experiment at the lhc has studied j/psi production at mid-rapidity in pp collisions at {\surd}s = 7 tev through its electron pair decay on a data sample corresponding to an integrated luminosity l_int = 5.6nb-1. the fraction of j/psi from the decay of long-lived beauty hadrons was determined for j/psi candidates with transverse momentum pt &gt; 1.3 gev/c and rapidity |y| &lt; 0.9. the cross section for prompt j/psi mesons, i.e. directly produced j/psi and prompt decays of heavier charmonium states such as the psi(2s) and csi_c resonances, is sigma_prompt-j/psi(pt &gt; 1.3 gev/c, |y| &lt; 0.9) = 7.2 +- 0.7(stat.) +- 1.0(syst.)+1.3-1.2 (syst.pol.) mb. the pt-differential cross section for prompt j/psi has also been measured. the cross section for the production of b-hadrons decaying to j/psi with transverse momentum greater than 1.3 gev/c in the rapidity range |y| &lt; 0.9 is sigma_{j/psi&lt;-h_b} = 1.26 +- 0.33 (stat.) +0.23-0.28 (syst.) mb. the results are compared to qcd model predictions. the shape of the pt and y distributions of b-quarks predicted by perturbative qcd model calculations are used to extrapolate the measured cross section to derive the bb pair total cross section and dsigma/dy at midrapidity.
heavy flavour measurements in pb-pb collisions at $\sqrt{s_{nn}}$ = 2.76 tev whith the alice experiment. null
measurement of electrons from semileptonic heavy-flavour hadron decays in pp collisions at \sqrt{s} = 7 tev. the differential production cross section of electrons from semileptonic heavy-flavour hadron decays has been measured at mid-rapidity ($|y| &lt; 0.5$) in proton-proton collisions at $\sqrt{s} = 7$ tev with alice at the lhc. data were collected in the transverse momentum range 0.5 $&lt;$ p_t $&lt;$ 8 gev/$c$. predictions from a fixed order perturbative qcd calculation with next-to-leading-log resummation agree with the data within the theoretical and experimental uncertainties.
the synthesis problem for trusted service-based collaborations. the growth of internet has extended the scope of software applications, leading to network-based ar- chitectures. the main characteristic of these architectures is that they restrict the communication between remote components to message passing. service-oriented computing is a solution to organise the exchange of messages in a network-based architecture, by using services as primitive components. since a service- oriented application typically spans a number of different organisations, its execution is subject to stringent requirements at two levels : business and security. at each level, the requirements could be implemen- ted by a centralised control maintaining the correct interactions. but this solution is not realistic. indeed, whereas each partner develops its own services, the whole application results from the collaboration of all the services in a truly concurrent way, without any centralised control. thus, the partners involved gene- rally define a contract at the global level in order to enforce interaction policies, dealing with business and security functionalities. by the contract, they increase their mutual trust. from the contract, each partner deduces by projection a specification of the functionalities that it must locally implement. of course, all these projections must ensure that the local functionalities, once gathered, effectively collaborate to realise the global contract. for each partner, it remains to realise the projection, by a local implementation.
towards a unified formal model for service orchestration and choreography. the growth of internet has extended the scope of software applications, leading to network-based architectures. the main characteristic of these architectures is that they restrict the communication between remote components to message passing. service-oriented computing is a solution to organise the exchange of messages in a network-based architecture, by using services as primitive components. thus, each component can be a client, a server or both. since a service-oriented application typically spans a number of different organizations, its executions is subject to stringent security requirements. that is the reason why the partners involved generally define a contract at the global level in order to enforce some security policy. from the contract, each partner deduces by projection a specification of the security functionalities that it must locally implement. of course, in order to be useful, all these projections must ensure that the local functionalities effectively collaborate to realize the global contract.
compensation of tool deflection in robotic-based milling. null
di-electron spectrum at mid-rapidity in $p+p$ collisions at $\sqrt{s} = 200$ gev. we report on mid-rapidity mass spectrum of di-electrons and cross sections of pseudoscalar and vector mesons via $e^{+}e^{-}$ decays, from $\sqrt{s} = 200$ gev $p+p$ collisions, measured by the large acceptance experiment star at rhic. the ratio of the di-electron continuum to the combinatorial background is larger than 10% over the entire mass range. simulations of di-electrons from light-meson decays and heavy-flavor decays (charmonium and open charm correlation) are found to describe the data. the extracted $\omega\rightarrow e^{+}e^{-}$ invariant yields are consistent with previous measurements. the mid-rapidity yields ($dn/dy$) of $\phi$ and $j/\psi$ are extracted through their di-electron decay channels and are consistent with the previous measurements of $\phi\rightarrow k^{+}k^{-}$ and $j/\psi\rightarrow e^{+}e^{-}$. our results suggest a new upper limit of the branching ratio of the $\eta \rightarrow e^{+}e^{-}$ of $1.7\times10^{-5}$ at 90% confidence level.
transverse sphericity of primary charged particles in minimum bias proton-proton collisions at sqrt(s)=0.9, 2.76 and 7 tev. measurements of the sphericity of primary charged particles in minimum bias proton--proton collisions at sqrt(s)=0.9, 2.76 and 7 tev with the alice detector at the lhc are presented. the observable is linearized to be collinear safe and is measured in the plane perpendicular to the beam direction using primary charged tracks with $p_{\rm t}\geq0.5$ gev/c in $|\eta|\leq0.8$. the mean sphericity as a function of the charged particle multiplicity at mid-rapidity ($n_{\rm ch}$) is reported for events with different $p_{\rm t}$ scales ("soft" and "hard") defined by the transverse momentum of the leading particle. in addition, the mean charged particle transverse momentum versus multiplicity is presented for the different event classes, and the sphericity distributions in bins of multiplicity are presented. the data are compared with calculations of standard monte carlo event generators. the transverse sphericity is found to grow with multiplicity at all collision energies, with a steeper rise at low $n_{\rm ch}$, whereas the event generators show the opposite tendency. the combined study of the sphericity and the mean $p_{\rm t}$ with multiplicity indicates that most of the tested event generators produce events with higher multiplicity by generating more back-to-back jets resulting in decreased sphericity (and isotropy). the pythia6 generator with tune perugia-2011 exhibits a noticeable improvement in describing the data, compared to the other tested generators.
longitudinal and transverse spin asymmetries for inclusive jet production at mid-rapidity in polarized p+p collisions at sqrt{s}=200 gev. we report star measurements of the longitudinal double-spin asymmetry a_ll, the transverse single-spin asymmetry a_n, and the transverse double-spin asymmetries a_sigma and a_tt for inclusive jet production at mid-rapidity in polarized p+p collisions at a center-of-mass energy of sqrt{s} = 200 gev. the data represent integrated luminosities of 7.6 /pb with longitudinal polarization and 1.8 /pb with transverse polarization, with 50-55% beam polarization, and were recorded in 2005 and 2006. no evidence is found for the existence of statistically significant jet a_n, a_sigma, or a_tt at mid-rapidity. recent model calculations indicate the a_n results may provide new limits on the gluon sivers distribution in the proton. the asymmetry a_ll significantly improves the knowledge of gluon polarization in the nucleon.
environmental information adaptive condition-based maintenance policies. this paper deals with the construction and optimisation of accurate condition based maintenance policies for cumulative deteriorating systems. in this context, the system condition behaviour can be influenced by different environmental factors which contribute to an increase or decrease the degradation rate. the observed condition can deviate from the expected condition if the degradation model does not embrace these environmental factors. moreover, if more information is available on the environment variations, the maintenance decision framework should take advantage of this new information and update the decision. the question is how shall we model the decision framework for this? we propose to model the effect of the random operating environment on the system behaviour with a randomization of the gamma process-degradation parameters. a new decision rule is introduced to update the maintenance decision in case of a significant deviation from the expected condition. the performance of the introduction of this new decision rule is discussed according different contexts: the level of knowledge on the real stress data and the restriction of a potential updating in the policy. a mathematical framework and optimization procedures are presented and numerical experiments are conducted to highlight the benefits of the different models.
antimatter production in proton-proton and heavy-ion collisions at ultrarelativistic energies. one of the striking features of particle production at high beam energies is the near equal abundance of matter and antimatter in the central rapidity region. in this paper we study how this symmetry is reached as the beam energy is increased. in particular, we quantify explicitly the energy dependence of the approach to matter/antimatter symmetry in proton-proton and in heavy-ion collisions. expectations are presented also for the production of more complex forms of antimatter like antihypernuclei.
evolution of the differential transverse momentum correlation function with centrality in au+au collisions at $\sqrt{s_{nn}} = 200$ gev. we present first measurements of the evolution of the differential transverse momentum correlation function, {\it c}, with collision centrality in au+au interactions at $\sqrt{s_{nn}} = 200$ gev. {\it c} exhibits a strong dependence on collision centrality that is qualitatively similar to that of number correlations previously reported. we use the observed longitudinal broadening of the near-side peak of {\it c} with increasing centrality to estimate the ratio of the shear viscosity to entropy density, $\eta/s$, of the matter formed in central au+au interactions. we obtain an upper limit estimate of $\eta/s$ that suggests that the produced medium has a small viscosity per unit entropy.
material screening and selection for xenon100. results of the extensive radioactivity screening campaign to identify materials for the construction of xenon100 are reported. this dark matter search experiment is operated underground at laboratori nazionali del gran sasso (lngs), italy. several ultra sensitive high purity germanium detectors (hpge) have been used for gamma ray spectrometry. mass spectrometry has been applied for a few low mass plastic samples. detailed tables with the radioactive contaminations of all screened samples are presented, together with the implications for xenon100.
soft hadron production at the lhc. the phenomenon of color coherence in quantum chromodynamics (qcd) is presented, and the basic observable to test it experimentally is introduced. first results for the cms experiment at the lhc are discussed.
first results on a sensor bio-inspired by electric fish. this article presents the first results of a work which aims at designing an active sensor inspired by the electric fish. its interest is its potential for robotics underwater navigation and exploration tasks in conditions where vision and sonar would meet difficulty. it could also be used as a complementary omnidirectional, short range sense to vision and sonar. combined with a well defined engine geometry, this sensor can be modeled analytically. in this article, we focus on a particular measurement mode where one electrode of the sensor acts as a current emitter and the others as current receivers. in spite of the high sensitivity required by electric sense, the first results show that we can obtain a detection range of the order of the sensor length, which suggests that this sensor principle could be used in future for robotics obstacle avoidance.
antenna design and distribution of the lofar super station. the nançay radio astronomy observatory and associated laboratories are developing the concept of a "super station" for extending the lofar station now installed and operational in nançay. the lofar super station (lss) will increase the number of high sensitivity long baselines, provide short baselines, act as an alternate core, and be a large standalone instrument. it will operate in the low frequency band of lofar (15-80 mhz) and extend this range to lower frequencies. three key developments for the lss are described here: (i) the design of a specific antenna, and the distribution of such antennas; (ii) at small-scale (analog-phased mini-array); and (iii) at large-scale (the whole lss).
a search for anisotropy in the arrival directions of ultra high energy cosmic rays recorded at the pierre auger observatory. observations of cosmic ray arrival directions made with the pierre auger observatory have previously provided evidence of anisotropy at the 99% cl using the correlation of ultra high energy cosmic rays (uhecrs) with objects drawn from the véron-cetty véron catalog. in this paper we report on the use of three catalog independent methods to search for anisotropy. the 2pt-l, 2pt+ and 3pt methods, each giving a different measure of self-clustering in arrival directions, were tested on mock cosmic ray data sets to study the impacts of sample size and magnetic smearing on their results, accounting for both angular and energy resolutions. if the sources of uhecrs follow the same large scale structure as ordinary galaxies in the local universe and if uhecrs are deflected no more than a few degrees, a study of mock maps suggests that these three methods can efficiently respond to the resulting anisotropy with a p-value = 1.0% or smaller with data sets as few as 100 events. using data taken from january 1, 2004 to july 31, 2010 we examined the 20,30,...,110 highest energy events with a corresponding minimum energy threshold of about 49.3 eev. the minimum p-values found were 13.5% using the 2pt-l method, 1.0% using the 2pt+ method and 1.1% using the 3pt method for the highest 100 energy events. in view of the multiple (correlated) scans performed on the data set, these catalog-independent methods do not yield strong evidence of anisotropy in the highest energy cosmic rays.
stiffness modeling of robotic-manipulators under auxiliary loadings. the paper focuses on the extension of the virtual-joint-based stiffness modeling technique for the case of different types of loadings applied both to the robot end-effector and to manipulator intermediate points (auxiliary loading). it is assumed that the manipulator can be presented as a set of compliant links separated by passive or active joints. it proposes a computationally efficient procedure that is able to obtain a non-linear force-deflection relation taking into account the internal and external loadings. it also produces the cartesian stiffness matrix. this allows to extend the classical stiffness mapping equation for the case of manipulators with auxiliary loading. the results are illustrated by numerical examples.
using models of partial knowledge to test model transformations. testers often use partial knowledge to build test models. this knowledge comes from sources such as requirements, known faults, existing inputs, and execution traces. in model-driven engineering, test inputs are models executed by model transformations. modelers build them using partial knowledge while meticulously satisfying several well-formedness rules imposed by the modelling language. this manual process is tedious and language constraints can force users to create complex models even for representing simple knowledge. in this paper, we want to simplify the development of test models by presenting an integrated methodology and semi-automated tool that allow users to build only small partial test models directly representing their testing intent. we argue that partial models are more readable and maintainable and can be automatically completed to full input models while considering language constraints. we validate this approach by evaluating the size and fault-detecting effectiveness of partial models compared to traditionally-built test models. we show that they can detect the same bugs/faults with a greatly reduced development effort.
emf profiles: a lightweight extension approach for emf models. null
lower bounds for a fixed job scheduling problem with an equity objective function. null
ridges in heavy ion collisions and pp scatterings at the lhc. null
light sterile neutrinos: a white paper. this white paper addresses the hypothesis of light sterile neutrinos based on recent anomalies observed in neutrino experiments and the latest astrophysical data.
uniform description of bulk observables in hydrokinetic model of a+a collisions at rhic and lhc. a successful simultaneous description of the hadronic yields, pion, kaon and proton spectra, elliptic flows and femtoscopy scales in hydrokinetic model of a+a collisions is presented at different centralities for the top rhic and lhc energies. the only changed parameter at different collision energies and centralities, except for peripheric events, is normalization to number of all charged particles. the hydrokinetic model is used in its hybrid version that allows one to switch correctly to the urqmd cascade at the isochronic hypersurface which separates the cascade stage and decaying hydrodynamic one. the results are compared with the standard hybrid model where hydrodynamics and hadronic cascade are matching just at the non-space-like hypersurface of chemical freeze-out. the initial conditions are based on both glauber- and kln- monte-carlo simulations, the results are compared. it seems that the observables, especially the femtoscopy data, prefer the glauber initial conditions. the modification of the particle number ratios caused, in particular, by the particle annihilations at the afterburn stage is analyzed.
on validation of atl transformation rules by transformation models. model-to-model transformations constitute an important ingredient in model-driven engineering. as real world transformations are complex, systematic approaches are required to ensure their correctness. the atlas transformation language (atl) is a mature transformation language which has been successfully applied in several areas. however, the executable nature of atl is a barrier for the validation of transformations. in contrast, transformation models provide an integrated structural description of the source and target metamodels and the transformation between them. while not being executable, transformation models are well-suited for analysis and verification of transformation properties. in this paper, we discuss (a) how atl transformations can be translated into equivalent transformation models and (b) illustrate how these surrogates can be employed to validate properties of the original transformation.
monte carlo simulation to reveal the copper dissolution kinetics of an ion selective electrode based on copper sulfide. the present work aims at studying copper dissolution of a cu2+ ion-selective electrode based on a cus thin film. the electrode is prepared using electrochemical deposition of cus on a silicon substrate. the obtained film exhibits an apparent cohesive granular structure with an average grain size of about 33 μm, a small porosity content (&lt;4%) and a thickness of about 7.48 μm. the cu2+ electrochemical response shows a nearly nernstian behavior in the range of pcu 6-1. the copper dissolution is experimentally studied in a wide ph range. in order to quantitatively predict copper mass dissolution, an original numerical model is developed based on monte carlo simulation. our main hypothesis is based on dissolution probability that triggers the whole dissolution process through solution/electrode surface exchanges. several probability forms are suggested accounting for the real observed electrochemical kinetics. the experimental results show that, under a low ph, the dissolution process severely leads to the consumption of large material. moreover, our predictions suggest a dissolution profile as a two-stage process irrespective of ph. our numerical model is able to fit correctly the observed kinetics considering an exponential probability form under all ph conditions.
a practical monadic aspect weaver. we present monascheme, an extensible aspect-oriented programming language based on monadic aspect weaving. extensions to the aspect language are deﬁned as monads, enabling easy, simple and modular prototyping. the language is implemented as an embedded language in racket. we illustrate the approach with an execution level monad and a level-aware exception transformer. semantic variations can be obtained through monad combinations. this work is also a ﬁrst step towards a framework for controlling aspects with monads in the pointcut and advice model of aop.
taming aspects with membranes. in most aspect-oriented languages, aspects have an unrestricted global view of computation. several approaches for aspect scoping and more strongly encapsulated modules have been formulated to restrict this controversial power of aspects. this paper leverages the concept of programmable membranes of boudol, schmitt and stefani, as a means to tame aspects by customizing the semantics of aspect weaving locally. membranes have the potential to subsume previous proposals in a uniform framework. because membranes give structure to computation, they enable ﬂexible scoping of aspects; because they are programmable, they enable visibility and safety constraints, both for the advised program and for the aspects. the power and simplicity of membranes open interesting perspectives to unify multiple approaches that tackle the unrestricted power of aspects.
j/psi suppression in p-a collisions from parton energy loss in cold qcd matter. the effects of energy loss in cold nuclear matter on j/psi suppression in p-a collisions are studied. a simple model based on first principles and depending on a single free parameter is able to reproduce j/psi suppression data at large xf and at various center-of-mass energies. these results strongly support energy loss as a dominant effect in quarkonium suppression. they also give some hint on its hadroproduction mechanism suggesting color neutralization to happen on long time-scales. predictions for j/psi and upsilon suppression in p-pb collisions at the lhc are made.
measurements of $d^{0}$ and $d^{*}$ production in $p$ + $p$ collisions at $\sqrt{s}$ = 200 gev. we report measurements of charmed-hadron ($d^{0}$, $d^{*}$) production cross sections at mid-rapidity in $p$ + $p$ collisions at a center-of-mass energy of 200 gev by the star experiment. charmed hadrons were reconstructed via the hadronic decays $d^{0}\rightarrow k^{-}\pi^{+}$, $d^{*+}\rightarrow d^{0}\pi^{+}\rightarrow k^{-}\pi^{+}\pi^{+}$ and their charge conjugates, covering the $p_t$ range of 0.6$-$2.0 gev/$c$ and 2.0$-$6.0 gev/$c$ for $d^{0}$ and $d^{*+}$, respectively. from this analysis, the charm-pair production cross section at mid-rapidity is $d\sigma/dy|_{y=0}^{c\bar{c}}$ = 170 $\pm$ 45 (stat.) $^{+38}_{-59}$ (sys.) $\mu$b. the extracted charm-pair cross section is compared to perturbative qcd calculations. the transverse momentum differential cross section is found to be consistent with the upper bound of a fixed-order next-to-leading logarithm calculation.
femtoscopy within a hydrodynamic approach based on flux tube initial conditions. null
radiative energy loss reduction in an absorptive plasma. null
you need to extend your models? emf facet vs. emf profiles. when using the eclipse modeling framework (emf), directly or as part of eclipse-based solutions, one often faces the problem of having to extend emf models in an efficient and structured way. however, either due to technical or business reasons, the respective original emf models/metamodels cannot be modified or "polluted" with the intended additional information. to this end, an advanced and lightweight model extension mechanism is worth a mint! emf facet and emf profiles, respectively hosted in eclipse-emft and eclipse labs, are two projects implementing such an extension mechanism. the former offers a way to dynamically extend models at runtime, while the latter provides a uml-like profile mechanism adapted to be used for any emf model. in this lightning talk, we introduce both tools very briefly and directly demonstrate on a simple example how they can be used in a complementary way.
strangeness production close to the threshold in proton-nucleus and heavy-ion collisions. we discuss strangeness production close to the threshold in p+a and a+a collision. comparing the body of available k+, k0, k-, and λ data with the iqmd transport code and for some key observables as well with the hsd transport code, we find good agreement for the large majority of the observables. the investigation of the reaction with the help of these codes reveals the complicated interaction of the strange particles with hadronic matter which makes strangeness production in heavy-ion collisions very different from that in elementary interactions. we show how different strange particle observables can be used to study the different facets of this interaction (production, rescattering and potential interaction) which finally merge into a comprehensive understanding of these interactions. we identify those observables which allow for studying (almost) exclusively one of these processes to show how the future high precision experiments can improve our quantitative understanding. finally, we discuss how the k+ multiplicity can be used to study the hadronic equation of state.
lambda over kaon enhancement in heavy ion collisions at several tev. we introduced recently a new theoretical scheme which accounts for hydrodynamically expanding bulk matter, jets, and the interaction between the two. important for the particle production at intermediate values of transverse momentum (p_t) are jet-hadrons produced inside the fluid. they pick up quarks and antiquarks (or diquarks) from the thermal matter rather than creating them via the schwinger mechanism -- the usual mechanism of hadron production from string fragmentation. these hadrons carry plasma properties (flavor, flow), but also the large momentum of the transversely moving string segment connecting quark and antiquark (or diquark). they therefore show up at quite large values of p_t, not polluted by soft particle production. we will show that this mechanism leads to a pronounced peak in the lambda / kaon ratio at intermediate p_t. the effect increases substantially with centrality, which reflects the increasing transverse size with centrality.
on the formation of bremsstrahlung in an absorptive qed/qcd medium. the radiative energy loss of a relativistic charge in a dense, absorptive medium can be affected significantly by damping phenomena. the effect is more pronounced for large energies of the charge and/or large damping of the radiation. this can be understood in terms of a competition between the formation time of bremsstrahlung and a damping time scale. discussing this competition in detail for the absorptive qed and qcd medium, we identify the regions in energy and parameter space, in which either coherence or damping effects are of major importance for the radiation spectrum. we show that damping mechanisms lead to a stronger suppression of the spectrum than coherence effects. this might be visible experimentally in correlations between hadrons at large momenta.
emftocsp: a tool for the lightweight verification of emf models. the increasing popularity of mde results in the creation of larger models and model transformations, hence converting the specification of mde artefacts in an error-prone task. therefore, mechanisms to ensure quality and absence of errors in models are needed to assure the reliability of the mde-based development process. formal methods have proven their worth in the verification of software and hardware systems. however, the adoption of formal methods as a valid alternative to ensure model correctness is compromised for the inner complexity of the problem. to circumvent this complexity, it is common to impose limitations such as reducing the type of constructs that can appear in the model, or turning the verification process from automatic into user assisted. since we consider these limitations to be counterproductive for the adoption of formal methods, in this paper we present emftocsp, a new tool for the fully automatic, decidable and expressive verification of emf models that uses constraint logic programming as the underlying formalism.
compensation of compliance errors in parallel manipulators composed of non-perfect kinematic chains. the paper is devoted to the compliance errors compensation for parallel manipulators under external loading. proposed approach is based on the non-linear stiffness modeling and reduces to a proper adjusting of a target trajectory. in contrast to previous works, in addition to compliance errors caused by machining forces, the problem of assembling errors caused by inaccuracy in the kinematic chains is considered. the advantages and practical significance of the proposed approach are illustrated by examples that deal with groove milling with orthoglide manipulator.
considerations concerning the fluctuations of the ratios of two observables. we discuss several possible caveats which arise with the interpretation of measurements of fluctuations in heavy-ion collisions. we especially focus on the ratios of particle yields, which have been advocated as a possible signature of a critical point in the qcd phase diagram. we conclude that current experimental observables are not well defined and are without a proper quantitative meaning.
elastostatic modeling and shape optimization of a 6-dof haptic interface device. this paper deals with the shape optimization of a six degree-of-freedom haptic interface device. this six-dof epicyclic-parallel manipulator has all actuators located on the ground. a regular dexterous workspace is introduced to represent the mobility of user's hand. throughout this workspace, the deviation of the mobile platform is bounded to provide a better feeling to the user and the masses in motion are minimized to increase the transparency of the haptic device. the stiffness model is written using a virtual joint method and compared with the results obtained with the finite element analysis to be validated. finally, the shape of the links are optimized in order to minimize the masses in motion while guaranteeing a given stiffness throughout the regular workspace of the mechanism.
nuclear-modification factor for open-heavy-flavor production at forward rapidity in cu+cu collisions at sqrt(s_nn)=200 gev. background: heavy-flavor production in p+p collisions tests perturbative-quantum-chromodynamics (pqcd) calculations. modification of heavy-flavor production in heavy-ion collisions relative to binary-collision scaling from p+p results, quantified with the nuclear-modification factor (r_aa), provides information on both cold- and hot-nuclear-matter effects. purpose: determine transverse-momentum, pt, spectra and the corresponding r_aa for muons from heavy-flavor mesons decay in p+p and cu+cu collisions at sqrt(s_nn)=200 gev and y=1.65. method: results are obtained using the semi-leptonic decay of heavy-flavor mesons into negative muons. the phenix muon-arm spectrometers measure the p_t spectra of inclusive muon candidates. backgrounds, primarily due to light hadrons, are determined with a monte-carlo calculation using a set of input hadron distributions tuned to match measured-hadron distributions in the same detector and statistically subtracted. results: the charm-production cross section in p+p collisions at sqrt{s}=200 gev, integrated over pt and in the rapidity range 1.4.
a lesson on structural testing with pathcrawler-online.com. abstract. pathcrawler is a test generation tool developed at cea list for structural testing of c programs. the new version of pathcrawler is developed in an entirely novel form: that of a test-case generation web service which is freely accessible at pathcrawler-online.com. this service allows many test-case generation sessions to be run in parallel in a completely robust and secure way. this tool demo and teaching experience paper presents pathcrawler-online.com in the form of a lesson on structural software testing, showing its beneﬁts, limitations and illustrating the usage of the tool on simple examples.
an optimal constraint programming approach to the open-shop problem. this paper presents an optimal constraint programming approach for the open-shop scheduling problem, which integrates recent constraint propagation and branching techniques with new upper bound heuristics. randomized restart policies combined with nogood recording allow us to search diversification and learning from restarts. this approach is compared with the best-known metaheuristics and exact algorithms, and it shows better results on a wide range of benchmark instances.
extending type theory with forcing. this paper presents an intuitionistic forcing translation for the calculus of constructions (coc), a translation that corresponds to an internalization of the presheaf construction in coc. depending on the chosen set of forcing conditions, the resulting type system can be extended with extra logical principles. the translation is proven correct-in the sense that it preserves type checking-and has been implemented in coq. as a case study, we show how the forcing translation on integers (which corresponds to the internalization of the topos of trees) allows us to define general inductive types in coq, without the strict positivity condition. using such general inductive types, we can construct a shallow embedding of the pure \lambda-calculus in coq, without defining an axiom on the existence of an universal domain. we also build another forcing layer where we prove the negation of the continuum hypothesis.
bin repacking scheduling in virtualized datacenters. a datacenter can be viewed as a dynamic bin packing system where servers host applications with varying resource requirements and varying relative placement constraints. when those needs are no longer satisfied, the system has to be reconfigured. virtualization allows to distribute applications into virtual machines (vms) to ease their manipulation. in particular, a vm can be freely migrated without disrupting its service, temporarily consuming resources both on its origin and destination. we introduce the bin repacking scheduling problem in this context. this problem is to find a final packing and to schedule the transitions from a given initial packing, accordingly to new resource and placement requirements, while minimizing the average transition completion time. our cp-based approach is implemented into entropy, an autonomous vm manager which detects reconfiguration needs, generates and solves the cp model, then applies the computed decision. cp provides the awaited flexibility to handle heterogeneous placement constraints and the ability to manage large datacenters with up to 2,000 servers and 10,000 vms.
multi-strange baryon production in pp collisions at (s)^1/2 = 7 tev with alice. a measurement of the multi-strange xi- and omega- baryons and their antiparticles by the alice experiment at the cern large hadron collider (lhc) is presented for proton-proton collisions at centre of mass energy of 7 tev. the transverse momentum (pt) distributions were studied at mid-rapidity (|y| &lt; 0.5) in the range of 0.6 &lt; pt &lt; 8.5 gev/c for xi- and xi+ baryons, and in the range of 0.8 &lt; pt &lt; 5 gev/c for omega- and omega+. baryons and anti-baryons were measured as separate particles and we find that the baryon to antibaryon ratio of both particle species is consistent with unity over the entire range of the measurement. the statistical precision of the current lhc data has allowed us to measure a difference between the mean pt of xi- (xi+) and omega- (omega+). particle yields, mean pt, and the spectra in the intermediate pt range are not well described by the pythia perugia 2011 tune monte carlo event generator, which has been tuned to reproduce the early lhc data. the discrepancy is largest for omega- (omega+). this pythia tune approaches the pt spectra of xi- and xi+ baryons below pt &lt; 0.85 gev/c and describes the xi- and xi+ spectra above pt &gt; 6.0 gev/c. we also illustrate the difference between the experimental data and model by comparing the corresponding ratios of (omega-+omega+)/(xi-+xi+) as a function of transverse mass.
physical mechanisms involved in grooved flat heat pipes: experimental and numerical analyses. an experimental database, obtained with flat plate heat pipes (fphp) with longitudinal grooves is presented. the capillary pressure measured by confocal microscopy and the temperature field in the wall are presented in various experimental conditions (vapour space thickness, filing ratio, heat transfer rate, tilt angle, fluid). coupled hydrodynamic and thermal models are developed. experimental results are compared to results of numerical models. physical mechanisms involved in grooved heat pipes are discussed, including the boiling limit and the effect of the interfacial shear stress. finally, recommendations for future experimental and theoretical research to increase the knowledge on fphp are discussed.
phenomenological interpolation of the inclusive j/psi cross section to proton-proton collisions at 2.76 tev and 5.5 tev. we present a study of the inclusive j/psi cross section at 2.76 tev and 5.5 tev. the energy dependence of the cross section, rapidity and transverse momentum distributions are evaluated phenomenologically. their knowledge is crucial as a reference for the interpretation of a-a and p-a j/psi results at the lhc. our approach is the following: first, we estimate the energy evolution of the pt-integrated j/psi cross section at mid-rapidity; then, we evaluate the rapidity dependence; finally, we study the transverse momentum distribution trend. whenever possible, both theory driven (based on pqcd predictions) and functional form (data driven fits) calculations are discussed. our predictions are compared with the recently obtained results by the alice collaboration in pp collisions at 2.76 tev.
jets, bulk matter, and their interaction in heavy ion collisions at several tev. we discuss a theoretical scheme which accounts for bulk matter, jets, and the interaction between the two. the aim is a complete description of particle production at all transverse momentum ($p_{t}$) scales. in this picture, the hard initial scatterings result in mainly longitudinal flux tubes, with transversely moving pieces carrying the $p_{t}$ of the partons from hard scatterings. these flux tubes constitute eventually both bulk matter (which thermalizes and flows) and jets. we introduce a criterion based on parton energy loss to decide whether a given string segment contributes to the bulk or leaves the matter to end up as a jet of hadrons. essentially low $p_{t}$ segments from inside the volume will constitute the bulk, high $p_{t}$ segments (or segments very close to the surface) contribute to the jets. the latter ones appear after the usual flux tube breaking via q-qbar production (schwinger mechanism). interesting is the transition region: intermediate $p_{t}$ segments produced inside the matter close to the surface but having enough energy to escape, are supposed to pick up q-qbar pairs from the thermal matter rather than creating them via the schwinger mechanism. this represents a communication between jets and the flowing bulk matter (fluid-jet interaction). also very important is the interaction between jet hadrons and the soft hadrons from the fluid freeze out. we employ the new picture to investigate pbpb collisions at 2.76 tev. we discuss the centrality and $p_{t}$ dependence of particle production and long range dihadron correlations at small and large $p_{t}$.
synchronization of multiple autonomic control loops: application to cloud computing. over the past years, autonomic computing has become very popular, especially in scenarios of cloud computing, where there might be several autonomic loops aiming at turning each layer of the cloud stack more autonomous, adaptable and aware of the runtime environment. nevertheless, due to conflicting objectives, non-synchronized autonomic loops may lead to global inconsistent states. for instance, in order to maintain its quality of service, an application provider might request more and more resources while the the infrastructure provider, due power shortage may be forced to reduce the resource provisioning. in this paper, we propose a generic model to deal with the synchronization and coordination of autonomic loops and how it can be applied in the context of cloud computing. we present some simulation results to show the scalability and feasibility of our proposal.
non-thermal $p/\pi$ ratio at lhc as a consequence of hadronic final state interactions. recent lhc data on pb+pb reactions at sqrt s_{nn}=2.7 tev suggests that the p/pi is incompatible with thermal models. we explore several hadron ratios (k/pi, p/pi, lambda/pi, xi/pi) within a hydrodynamic model with hadronic after burner, namely urqmd 3.3, and show that the deviations can be understood as a final state effect. the measured values of the hadron ratios do then allow to gauge the transition energy density from hydrodynamics to the boltzmann description. we find that the data can be explained with transition energy densities of 840 +- 150 mev/fm^3.
characterization of at- species in simple and biological media by high performance anion exchange chromatography coupled to gamma detector. astatine is a rare radioelement belonging to the halogen group. considering the trace amounts of astatine produced in cyclotrons, its chemistry cannot be evaluated by spectroscopic tools. analytical tools, provided that they are coupled with a radioactive detection system, may be an alternative way to study its chemistry. in this research work, high performance anion exchange chromatography (hpaec) coupled to a gamma detector (γ) was used to evaluate astatine species under reducing conditions. also, to strengthen the reliability of the experiments, a quantitative analysis using a reactive transport model has been done. the results confirm the existence of one species bearing one negative charge in the ph range 27.5. with respect to the other halogens, its behavior indicates the existence of negative ion, astatide at-. the methodology was successfully applied to the speciation of the astatine in human serum. under fixed experimental conditions (ph 7.47.5 and redox potential of 250 mv) astatine exists mainly as astatide at- and does not interact with the major serum components. also, the method might be useful for the in vitro stability assessment of 211at-labelled molecules potentially applicable in nuclear medicine.
inclusive j/psi production in pp collisions at sqrt(s) = 2.76 tev. the alice collaboration has measured inclusive j/psi production in pp collisions at a center of mass energy sqrt(s)=2.76 tev at the lhc. the results presented in this letter refer to the rapidity ranges |y|&lt;0.9 and 2.5.
use of fluorescence spectroscopy and voltammetry for the analysis of metal-organic matter interactions in the new caledonia lagoon. fluorescence, polarographic and potentiometric analysis of sea water from the new caledonia lagoon (located south of noumea) allowed the determination of the specific properties of the dissolved and particulate phases of organic matter (om)-metal complexes according to various regions of the lagoon. in particular, om complexes with ni, zn, pb, cu, cd were chosen in this study due to the sensitivity of these complexes to affect biocenosis of the nearby enclosed coral reef as well as their availability to enter the coast from erosion (terrigenous om) or human activities from nickel extraction or pollution from waste sites (anthopogenic om) that exist throughout new caledonia. combined with geochemical modelling, the om-metal complexes analysis allowed the determination of their conditional stability constants which in turn helped in predicting the fate of the metal pollution in the lagoon. for the first time, fluorescence, polarographic and potentiometric techniques combined with geochemical models that employed discrete pka distribution on om enabled the determination of the origin of the om, as either natural or anthopogenic.
lymphocyte subset reconstitution after unrelated cord blood or bone marrow transplantation in children. we report the post-transplant lymphocyte subset recovery of 226 children treated with unrelated cord blood transplant (ucbt) (n = 112) or unrelated bone marrow transplant (ubmt) (n = 114) for malignant or non-malignant diseases. absolute numbers of natural killer (nk), b and t cells were monitored by flow cytometry up to 5 years post-transplant. immunological endpoints were: time to achieve a cd3(+) cell count &gt; 0*5 and 1*5 × 10⁹/l, cd4(+) &gt; 0*2 and 0*5 × 10⁹/l, cd8(+) &gt; 0*25 ×10⁹/l, cd19(+) &gt; 0*2 × 10⁹/l, nk &gt; 0*1 × 10⁹/l. these endpoints were analysed through the use of cumulative incidence curves in the context of competing risks. cd8(+) t cell recovery was delayed after ucbt with a median time to reach cd8(+) t cells &gt; 0*25 × 10⁹/l of 7*7 months whereas it was 2*8 months in ubmt (p &lt; 0*001). b cell recovery was better in ucbt, with a median time to reach cd19(+) cells &gt; 0*2 × 10⁹/l of 3*2 months in ucbt and 6*4 months in ubmt (p = 0*03). median time for cd4(+) t cell and nk cell recovery was similar in ucbt and ubmt. cd4(+) t cells recovery was negatively correlated to age (better reconstitution in younger patients, p = 0*002). cd8(+) t cells recovery was shorter in recipients with a positive cytomegalovirus serology (p =0*001).
experimental development of a liquid xenon compton telescope for functional medical imaging. 3γ imaging is a new nuclear medical imaging technique which has been suggested by subatech laboratory. this technique involves locating three-dimensional position of the decay of an innovative radioisotope (β+, γ) emitter the 44sc. the principle consist in the detection of two photons of 511 kev gamma rays from the decay of the positron, provided by a pet ring detector, associated to the detection of the third photon by a liquid xenon compton telescope. the energy deposited in the interaction between the photon and xenon and its position are identified by measuring the ionization signal with a micromegas chamber (micromesh gaseous structure), while the trigger and time measurement of the interaction are provided by the detection of thescintillation signal. the principle of the tpc is thus usedto compton imaging.in order to demonstrate experimentally the feasibility of imaging 3γ, a small prototype, xemis (xenon medicalimaging system) was developed. this thesis is an important step towards the proof of feasibility. in this work are exposed the characterization of the detector response for a beam of 511 kev gamma rays and the analysis of data derived from it. the measurement of energy and time resolutions will be presented, as well as the purity of the liquid xenon.
extended h2 - h∞ controller synthesis for linear time invariant descriptor systems. the descriptor systems have been attracting the attention of many researchers over recent decades due to their capacity to preserve the structure of physical systems and to describe static constraints and impulsive behaviors. within the descriptor framework, the contributions of this dissertation are threefold: i) review of existing results for state-space systems, ii) generalization of classical results to descriptor systems, iii) exact and analytical solutions to non standard control problems. a realization independent kalman-yakubovich-popov (kyp) lemma and dilated lmi characterizations are deduced for descriptor systems. the solvability and corresponding numerical algorithms of generalized sylvester equations and generalized algebraic riccati equations (gare) associated with descriptor systems are provided. in addition, the simultaneous h∞ control problem is considered through extending recently reported results. a sufficient condition is proposed through a combination of a generalized algebraic riccati equation and a set of lmis. moreover, the nonstandard h2 and h∞ control problems with unstable and/or nonproper weighting functions or subject to regulation constraints are addressed. these contributions allow, without approximation or transformation, dealing with many practical problems defined within h2 or h∞ control methodologies, where the control signals are penalized at high frequency or unstable internal models specified by external signals is involved.
measurement of the cross section for electromagnetic dissociation with neutron emission in pb-pb collisions at {\surd}snn = 2.76 tev. the first measurement of neutron emission in electromagnetic dissociation of 208pb nuclei at the lhc is presented. the measurement is performed using the neutron zero degree calorimeters of the alice experiment, which detect neutral particles close to beam rapidity. the measured cross sections of single and mutual electromagnetic dissociation of pb nuclei at {\surd}snn = 2.76 tev with neutron emission are {\sigma}_single emd = 187.2{\pm}0.2 (stat.) +13.8-12.0 (syst.) b and {\sigma}_mutual emd = 6.2 {\pm} 0.1 (stat.) {\pm}0.4 (syst.) b respectively. the experimental results are compared to the predictions from a relativistic electromagnetic dissociation model.
suppression of high transverse momentum d mesons in central pb--pb collisions at $\sqrt{s_{nn}}=2.76$ tev. the production of the prompt charm mesons $d^0$, $d^+$, $d^{*+}$, and their antiparticles, was measured with the alice detector in pb-pb collisions at the lhc, at a centre-of-mass energy $\sqrt{s_{nn}}=2.76$ tev per nucleon--nucleon collision. the $\pt$-differential production yields in the range $2.
computational molecular modeling of the multi-scale dynamics of water and ions at cement interfaces. structural and dynamic behavior of h2o molecules and aqueous at in-terfaces and in nanopores of model c-s-h binding phase (tobermorite) is quanti-fied on the basis of molecular dynamics computer simulations. at the (001) sur-face of tobermorite in contact with 0.25 m kcl aqueous solution, we can effectively distinguish water molecules that spend most of their time within chan-nels between the drierketten chains of silica on the tobermorite surface from the adsorbed molecules residing slightly above the interface. within the channels, h2o molecules donate h-bonds to both the bridging and non-bridging oxygens of the si-tetrahedra as well as to other h2o. some of these molecules form very strong h-bonds persisting over 100 ps and longer, but many others undergo fre-quent librations and occasional diffusional jumps from one surface site to another. the average diffusion coefficients of the surface-associated h2o molecules that spend most of their time in the channels and those that lie above the nominal inter-face differ by about an order of magnitude (dh2o[internal]=5.0×10-11 m2/s and dh2o[external]=6.0×10-10 m2/s, respectively). the average diffusion coefficient for all surface-associated h2o molecules is about 1.0×10-10 m2/s. all of these values are significantly less than the value of 2.3×10-9 m2/s, characteristic of h2o self-diffusion in bulk liquid water, but they are in very good quantitative agreement with experimental data on the dynamics surface-associated water in similar ce-ment materials obtained be 1h nmr [1,2].
effects of organics on the adsorption and mobility of metal cations in clay systems: computational molecular modeling approach. understanding and prediction of many natural and anthropogenic environmental processes ultimately depend on a fundamental understanding of the chemistry occurring at the mineral-fluid interfaces. clay-related minerals and natural organic matter (nom) are ubiquitous in the environment, and metal-nom complexation induces strong correlations between the nom concentration in water and the capacity of clay particles to bind metals, thus affecting their speciation, solubility and toxicity in the environment. despite significant geochemical, environmental and technological interest, the molecular-level mechanisms and dynamics of the physical and chemical processes involving nom are not yet well understood. in this presentation we compare three different molecular dynamics (md) computer simulations of metal-nom complexation in aqueous solutions. the simulation results indicate that despite some obvious quantitative variations in the computed values depending on the size of the simulated system and on the parameters of the force field models used, all three simulations are quite robust and consistent. in particular, approximately 35-50% of ca2+ ions in all simulations are associated with the carboxylic groups of nom at near-neutral ph. the stability of bidentate-coordinated contact ion pair complexes is also always strongly preferred. easy association of metal cations with negatively charged nom functional groups and negatively charged clay surfaces allows us to predict that cationic bridging could be the most probable mechanism of nom association with clays in natural environments. new md simulations are currently in progress to quantitatively assess these predictions on a molecular scale for nuclear waste disposal applications. new larger-scale clay models incorporate a more realistic representation of the structural and compositional disorder of natural illites and smectites and employ clayff - a fully flexible general force field suitable for the molecular simulations of hydrated mineral systems in the presence of organics.
is dtpa a good competing chelating agent for th(iv) in human serum and suitable in targeted alpha therapy?. the interaction between thorium and human serum components was studied using difference ultraviolet spectroscopy (dus), ultrafiltration and high-pressure-anion exchange chromatography (hpaec) with external inductively conducted plasma mass spectrometry (icp-ms) analysis. experimental data are compared with modelling results based on the law of mass action. human serum transferrin (hstf) interacts strongly with th(iv), forming a ternary complex including two synergistic carbonate anions. this complex governs th(iv) speciation under blood serum conditions. considering the generally used langmuir-type model, values of 1033.5 and 1032.5 were obtained for strong and weak sites, respectively. we showed that trace amounts of diethylene triamine pentaacetic acid (dtpa) cannot complex th(iv) in the blood serum at equilibrium. unexpectedly this effect is not related to the competition with hstf but is due to the strong competition with major divalent metal ions for dtpa. however, th-dtpa complex was shown to be stable for a few hours when it is formed before addition in the biological medium; this is related to the high kinetic stability of the complex. this makes dtpa a potential chelating agent for synthesis of 226th-labeled biomolecules for application in targeted alpha therapy.
highlights from the star experiment at rhic. experiments using heavy ion collisions at ultrarelativistic energies aim to explore the qcd phase transition and map out the qcd phase diagram. a wealth of remarkable results in this field have been reported recently, for example the upsilon suppression discovered recently. we discuss recent results from the star experiment focusing on strangeness, charm and beauty production.
stability of the fragments and thermalization at peak center-of-mass energy. we simulate the central reactions of nearly symmetric, and asymmetric systems, for the energies at which the maximum production of imfs occurs (e$_{c.m.}^{peak}$).this study is carried out by using hard eos along with cugnon cross section and employing mstb method for clusterization. we study the various properties of fragments. the stability of fragments is checked through persistence coefficient and gain term. the information about the thermalization and stopping in heavy-ion collisions is obtained via relative momentum, anisotropy ratio, and rapidity distribution. we find that for a complete stopping of incoming nuclei very heavy systems are required. the mass dependence of various quantities (such as average and maximum central density, collision dynamics as well as the time zone for hot and dense nuclear matter) is also presented. in all cases (i.e., average and maximum central density, collision dynamics as well as the time zone for hot and dense nuclear matter) a power law dependence is obtained.
erratum to "the lateral trigger probability function for the ultra-high energy cosmic ray showers detected by the pierre auger observatory" [astroparticle physics 35 (2011) 266-276]. null
criojo: a pivot language for service-oriented computing - the introspective chemical abstract machine. interoperability remains a significant challenge in service-oriented computing. after proposing a pivot architecture to solve three interoperability problems, namely adaptation, integration and coordination problems between clients and servers, we explore the theoretical foundations for this architecture. a pivot architecture requires a universal language for orchestrating services and a universal language for interfacing resources. since there is no evidence today that web services technologies can provide this basis, we propose a new language called criojo and essentially show that it can be considered as a pivot language. we formalize the language criojo and its operational semantics, by resorting to a chemical abstract machine, and give an account of formal translations into criojo: in a distributed context, we deal with idiomatic languages for four major programming paradigms: imperative programming, logic programming, functional programming and concurrent programming.
constraint singularity-free design of the irsbot-2. null
cooperative and reactive scheduling in large-scale virtualized platforms with dvms. null
tabu search and lower bound for an industrial complex shop scheduling problem. this paper deals with an industrial shop scheduling problem that arises in a metal goods production group. the scheduling problem can be seen as a multi-mode job shop with assembly. jobs have additional constraints such as release date, due date and sequence-dependent setup times. the aim of the decision-makers is to minimize the maximum lateness. this article introduces a tabu search procedure to solve the whole problem and a valid lower bound used to evaluate the tabu search procedure.
csla : a language for improving cloud sla management. cloud computing is a paradigm for enabling remote, on-demand access to a set of configurable computing resources as a service. the pay-per-use model enables service providers to offer their services to customers in different quality-of-service (qos) levels. service level agreement (sla) is a negotiated agreement between a service provider and a customer where qos parameters specify the quality level of service that the service provider have to guarantee. however, due to the dynamic nature of the cloud and its instability, some sla violations can occurred and the service providers can be charged for penalties. in this paper, we aim at addressing the cloud instability to better control sla management (in particular sla violations) and indirectly the cloud elasticity. we propose csla, a new sla language directly integrating some features dealing with qos uncertainty and cloud fluctuation. in our evaluation, we present a novel profit model for service provider and new algorithms (for admission control and scheduling) to meet sla requirements (e.g. prevent sla violations) while tackling scalability and dynamic issues.
a multiple plan approach for the dynamic technician routing and scheduling problem. the dynamic technician routing and scheduling problem (dtrsp) deals with a crew of technicians that serves dynamically appearing requests. in the dtrsp, each technician has a set of skills, tools, and spare parts, while requests require a subset of each. the problem is then to design a set of tours of minimal total duration such that each request is visited exactly once, within its time window, by a compatible technician, and to dynamically insert new requests into existing tours. we propose a multiple plan approach to solve the dtrsp and illustrate its performance on benchmark instances.
route consistency vehicle routing: a bi-objective approach. dynamic vehicle routing problems (d-vrps) are an extension of classical vrps in which the information available to the decision maker changes or is updated dynamically. most studies on d-vrps consider that routes can be designed online, which means that vehicle drivers do not know their next destination until they finish serving their current customer. although this assumption is theoretically appealing, it may not be desirable in a practical context in which drivers are used to know their routes from the beginning of the day. in this work we propose an optimization algorithm able to optimize the minimization of a cost function (the total traveled distance), and the minimization of the changes made in the vehicles routes in a dynamic routing context, and we study the tradeoff between both objectives.
hinf control with unstable and nonproper weights for descriptor systems. this paper is concerned with a nonstandard hinf output feedback control problem for continuous-time descriptor systems, where unstable and nonproper weighting functions are used. based on two generalized sylvester equations and two generalized algebraic riccati equations (gares) together with a spectral radius condition, an explicit parametrization of all desirable controllers is deduced. a numerical example is included to illustrate the validity of the proposed result.
probing the qgp phase boundary with thermal properties of $\phi$ mesons. a novel attempt has been made to probe the qcd phase boundary by using the experimental data for transverse momenta of {\phi} mesons produced in nuclear collisions at ags, sps and rhic energies. the data are confronted with simple thermodynamic expectations and lattice qcd results. the experimental data indicate a first-order phase transition, with a mixed phase stretching the energy density between \sim1 and 3.2 gev/fm3 corresponding to sps energies.
pion femtoscopy in p+p collisions at sqrt(s)=200 gev. the star collaboration at rhic has measured two-pion correlation functions from p+p collisions at sqrt(s)=200 gev. spatial scales are extracted via a femtoscopic analysis of the correlations, though this analysis is complicated by the presence of strong non-femtoscopic effects. our results are put into the context of the world dataset of femtoscopy in hadron-hadron collisions. we present the first direct comparison of femtoscopy in p+p and heavy ion collisions, under identical analysis and detector conditions.
reactor simulation for antineutrino experiments using dragon and mure. rising interest in nuclear reactors as a source of antineutrinos for experiments motivates validated, fast, and accessible simulations to predict reactor fission rates. here we present results from the dragon and mure simulation codes and compare them to other industry standards for reactor core modeling. we use published data from the takahama-3 reactor to evaluate the quality of these simulations against the independently measured fuel isotopic composition. the propagation of the uncertainty in the reactor operating parameters to the resulting antineutrino flux predictions is also discussed.
cross sections and double-helicity asymmetries of midrapidity inclusive charged hadrons in p+p collisions at sqrt(s)=62.4 gev. unpolarized cross sections and double-helicity asymmetries of single-inclusive positive and negative charged hadrons at midrapidity from p+p collisions at sqrt(s)=62.4 gev are presented. the phenix measurements for 1.0 &lt; p_t &lt; 4.5 gev/c are consistent with perturbative qcd calculations at next-to-leading order in the strong coupling constant, alpha_s. resummed pqcd calculations including terms with next-to-leading-log accuracy, yielding reduced theoretical uncertainties, also agree with the data. the double-helicity asymmetry, sensitive at leading order to the gluon polarization in a momentum-fraction range of 0.05 ~&lt; x_gluon ~&lt; 0.2, is consistent with recent global parameterizations disfavoring large gluon polarization.
optimization of a shared passengers and goods urban transportation network. optimizing passengers and goods flows reduces the costs and ecological footprint of transportation systems, as well as congestion in cities. since passengers and goods do not have the same nature (a person is active and a good is passive), modeling each flow separately seems natural. but local authorities need to consider passenger and freight transport together as a single logistic system. in this paper, we assess the efficiency of an existing transportation system where the spare capacity of public transport is used to distribute goods toward the city core. we propose two optimization approaches. the first one considers the problem as a multidepot vrptw followed by an assignment problem. the second one solves the whole model as a pickup and delivery problem with transfers (pdpt). these approaches are evaluated on data sets generated following a field study in the medium-sized city of la rochelle in france.
a state of the art of supply chain design models and methods integrating the principles of sustainable development. the aim of this study is to survey the optimization models and methods for supply chain design problems considering the concepts of sustainable development. supply chain design is one of the most significant issues in supply chain management. the mathematical models in the field of facility location, logistics, supply chain design and strategic planning have been incorporating more and more features of the real-life applications, such as such as collaborative supply chain, supplier selection, dynamic aspect, uncertainty and risk management. although sustainable development is one of the most challenging features to be taken into account, it is yet poorly considered in the or models and almost not mentioned in the existing reviews. we will develop the following issues: (i) the constituent parts (economic, environmental and social factors) and ingredients of sustainable development that are included in the mathematical models, (ii) the consequences of considering sustainable development factors on mathematical modeling and optimization methods, (iii) applications in various economic sectors.
program transformation based views for modular maintenance. modular programming is a practical solution for separation of concerns but the support for modularity provided by programming languages does not resolve the classic expression problem and more generally the tyranny of the dominant decomposition: evolutions are modular only on the principal axis of decomposition. to solve this problem, a practical solution would be to be able to choose the architecture of an application each time one has to make it evolve. we provide a prototype tool for the haskell language to support that. our tool allows to build transformations to switch haskell programs from one structure to another. we do this by driving a refactoring tool for haskell (hare): transformations are built by chaining elementary operations of refactoring. since each elementary refactoring operation preserve the semantics, the whole transformations also do.
stiffness matrix of manipulators with passive joints: computational aspects. the paper focuses on stiffness matrix computation for manipulators with passive joints, compliant actuators and flexible links. it proposes both explicit analytical expressions and an efficient recursive procedure that are applicable in the general case and allow obtaining the desired matrix either in analytical or numerical form. advantages of the developed technique and its ability to produce both singular and non-singular stiffness matrices are illustrated by application examples that deal with stiffness modeling of two stewart-gough platforms.
self-management of applications and systems to optimize energy in data centers. as a direct consequence of the increasing popularity of cloud computing solutions, data centers are amazingly growing and hence have to urgently face with the energy consumption issue. available solutions are focused basically on the system layer, by leveraging virtualization technologies to improve energy efficiency. another body of works relies on cloud computing models and virtualization techniques to scale up/down application based on their performance metrics. although those proposals can reduce the energy footprint of applications and by transitivity of cloud infrastructures, they do not consider the internal characteristics of applications to finely define a trade-­‐off between applications quality of service and energy footprint. in this paper, we propose a self-­‐adaptation approach that considers both application internals and system to reduce the energy footprint in cloud infrastructure. each application and the infrastructure are equipped with control loops, which allows them to autonomously optimize their executions. we implemented the control loops and simulated them in order to show their feasibility. in addition, we show how our solution fits in federated clouds through a motivating scenario. finally, we provide some discussion about open issues on models and implementation of our proposal.
j/psi production as a function of charged particle multiplicity in pp collisions at sqrt{s} = 7 tev. the alice collaboration reports the measurement of the inclusive j/psi yield as a function of charged particle pseudorapidity density dn_{ch}/deta in pp collisions at sqrt{s} = 7 tev at the lhc. j/psi particles are detected for p_t &gt; 0, in the rapidity interval |y| &lt; 0.9 via decay into e+e-, and in the interval 2.5 &lt; y &lt; 4.0 via decay into mu+mu- pairs. an approximately linear increase of the j/psi yields normalized to their event average (dn_{j/psi}/dy)/ with (dn_{ch}/deta)/ is observed in both rapidity ranges, where dn_{ch}/deta is measured within |eta| &lt; 1 and p_t &gt; 0. in the highest multiplicity interval with = 24.1, corresponding to four times the minimum bias multiplicity density, an enhancement relative to the minimum bias j/psi yield by a factor of about 5 at 2.5 &lt; y &lt; 4 (8 at |y| &lt; 0.9) is observed.
newton-euler approach for bio-robotics locomotion dynamics : from discrete to continuous systems. this thesis proposes a general and unified methodological framework suitable for studying the locomotion of a wide range of robots, especially bio-inspired. the objective of this thesis is twofold. first, it contributes to the classification of locomotion robots by adopting the mathematical tools developed by the american school of geometric mechanics.secondly, by taking advantage of the recursive nature of the newton-euler formulation, it proposes numerous efficient tools in the form of computational algorithms capable of solving the external direct dynamics and the internal inverse dynamics of any locomotion robot considered as a mobile multi-body system. these generic tools can help the engineers or researchers in the design, control and motion planning of manipulators as well as locomotion robots with a large number of internal degrees of freedom. the efficient algorithms are proposed for discrete and continuous robots. these methodological tools are applied to numerous illustrative examples taken from the bio-inspired robotics such as snake-like robots, caterpillars, and others like snake-board, etc.
preferred solutions computed with a label setting algorithm based on choquet integral for multi-objective shortest paths. the problem investigated in this paper concerns the integration of a decision maker preference model within a labeling algorithm for the multi-objective shortest path problem. the aim is to use a preference model built a priori for computing efficiently exact preferred solutions. the approach is based on the choquet integral, which can model not only relative importances but also interactions between criteria. the paper introduces choquet dominance rules, which replaces the pareto dominance. the rules are integrated within the label setting algorithm originally proposed in 1984 by martins. numerical experiments report significant performance improvements, and conclude on the efficiency of the rules for reducing the search space.
a lower bound of the choquet integral integrated within martins' algorithm. the problem investigated in this work concerns the integration of a decision-maker preference model within an exact algorithm in multiobjective combinatorial optimization. rather than computing the complete set of efficient solutions and choosing a solution afterwards, our aim is to efficiently compute one solution satisfying the decision maker preferences elicited a priori. the preference model is based on the choquet integral. the reference optimization problem is the multiobjective shortest path problem, where martins' algorithm is used. a lower bound of the choquet integral is proposed that aims to prune useless partial paths at the labeling stage of the algorithm. various procedures exploiting the proposed bound are presented and evaluated on a collection of benchmarks. numerical experiments show significant improvements compared to the exhaustive enumeration of solutions.
a unified formal model for service oriented architecture to enforce security contracts. in this paper we introduce a model as a foundation for heterogeneous services, therefore unifying web services technologies in soa (service oriented architecture), specifically, soap/ws* and restful models. this model abstracts away from service implementations, in order to verify and to enforce some important security properties.
a message-passing model for service oriented computing. service-based applications can be built according to multiple technologies. although there is a clear need for a model integrating them in multiple real-world contexts, no integrated model does (yet) exist. in this paper we introduce a model as a foundation for heterogeneous services, with particularly studying soap/ws* and restful models. the model completely abstracts away from service implementations, composes them in a truly concurrent manner, and supports asynchronous message passing as well as mobility of typed channels. we consider the application of this model to the problem of type checking messages and communications in presence of channel mobility and malicious agents.
design of a two dof gain scheduled frequency shaped lq controller for narrow tilting vehicles. narrow tilting vehicles (ntvs) are the convergence of a car and a motorcycle. they are expected to be the new generation of city cars considering their practical dimensions and lower energy consumption. but considering their height to breadth ratio, in order to maintain lateral stability, ntvs should tilt when cornering. unlike the motorcycle's case, where the driver tilts the vehicle himself, the tilting of an ntv should be automatic. the control objective in this paper is to reduce the perceived lateral acceleration through tilting, what would increase lateral stability and also the driver comfort with regard to centrifugal acceleration. a two degree of freedom tilting controller is proposed in the paper, using all the measurement available, controlling directly the perceived lateral acceleration (measured), while considering the steering angle as the main disturbance source. the control problem is recast as an optimal h2 problem, considering a model of the typical steering angle in addition to the vehicle model. the steering angle model allows predicting the near future of the vehicle trajectory, leading to an improved result in terms of energy requirements. although initially designed for a given constant speed, the lpv controller is finally proposed with guaranties of good performance for a large range of speed and acceleration.
trajectory generation for high speed pick-and-place robots. null
elasto-dynamic model of robotic milling process considering interaction between tool and workpiece. null
radiative and collisional energy loss of heavy quarks in deconfined matter. we extend our recently advanced model on collisional energy loss of heavy quarks in a quark gluon plasma (qgp) by including radiative energy loss. we discuss the approach and present calculations for pbpb collisions at $\sqrt{s}=2.76 tev$. the transverse momentum spectra, raa, and the elliptic flow $v_2$ of heavy quarks have been obtained using the model of kolb and heinz for the hydrodynamical expansion of the plasma.
supersymmetric proof of the hirzebruch-riemann-roch theorem for non-kähler manifolds. we present the proof of the hrr theorem for a generic complex compact manifold by evaluating the functional integral for the witten index of the appropriate supersymmetric quantum mechanical system.
redox-active phases and radionuclide equilibrium valence state in subsurface environments - new insights from 6th ec fp ip funmig. within the 6th ec fp integrated project "fundamental processes of radionuclide migration" (funmig), progress has been made to improve knowledge about the phases and reaction mechanisms involved in complex reduction processes of radionuclide contaminants in natural subsurface environments. this review paper gives an overview of the achievements made by the research groups involved in this project, and puts the scope and results of the studies in a more global context. firstly, both thermodynamic and experimental evidence show that green rust is present and reactive in subsurface groundwater with a composition that spans the fe(ii)/fe(iii) redox boundary. green rust has been shown to reduce np(v), se(vi) and se(iv), but the pathways for the redox processes and the reaction products that result are complicated, and change as a function of the reaction parameters. secondly, considerable evidence has emerged that se(iv) is reduced on fe(ii)-bearing minerals which are ubiquitous in subsurface environments. the stable se valence state in the presence of fes2 has been shown to be se(0). also, natural dissolved humic substances that contain sufficient electron donating capacity are capable of interacting with, and possibly reducing, se(iv) to lower valence states. thirdly, the influence of hco-3 and organic ligands on the uptake and reduction of u(vi) on fe(ii)-bearing minerals was investigated. while it appeared that hco-3 decreased the extent of u(vi) uptake by the reducing surface, the fraction of reduced u(iv) in the solid phase increased with increasing hco-3 concentration. in contrast with the observations for hco-3, organic ligands decreased both the extent of u uptake, as well as the fraction of u(iv) found in the solid phase. the studies performed within funmig show that investigating reduction-oxidation mechanisms require (1) a detailed control over reaction conditions (anoxic atmosphere, purification of solid phases, initial radionuclide speciation), (2) a rigorous follow-up of reaction products (both solution chemistry and spectroscopic methods), and (3) the consideration of slow kinetics in the setting up of an experiment. these requirements make the study and assessment of redox processes one of the most demanding scientific challenges for geochemists who are asked to make predictions for radionuclide transport behaviour in the environment.
directed and elliptic flow of charged particles in cu+cu collisions at $\sqrt{\bm {s_{nn}}} =$ 22.4 gev. this paper reports results for directed flow $v_{1}$ and elliptic flow $v_{2}$ of charged particles in cu+cu collisions at $\sqrt{s_{nn}}=$ 22.4 gev at the relativistic heavy ion collider. the measurements are for the 0-60% most central collisions, using charged particles observed in the star detector. our measurements extend to 22.4 gev cu+cu collisions the prior observation that $v_1$ is independent of the system size at 62.4 and 200 gev, and also extend the scaling of $v_1$ with $\eta/y_{\rm beam}$ to this system. the measured $v_2(p_t)$ in cu+cu collisions is similar for $\sqrt{s_{nn}} = 22.4-200$ gev. we also report a comparison with results from transport model (urqmd and ampt) calculations. the model results do not agree quantitatively with the measured $v_1(\eta), v_2(p_t)$ and $v_2(\eta)$.
system size and energy dependence of near-side di-hadron correlations. two-particle azimuthal ($\delta\phi$) and pseudorapidity ($\delta\eta$) correlations using a trigger particle with large transverse momentum ($p_t$) in $d$+au, cu+cu and au+au collisions at $\sqrt{s_{{nn}}}$ =\xspace 62.4 gev and 200~gev from the star experiment at rhic are presented. the \ns correlation is separated into a jet-like component, narrow in both $\delta\phi$ and $\delta\eta$, and the ridge, narrow in $\delta\phi$ but broad in $\delta\eta$. both components are studied as a function of collision centrality, and the jet-like correlation is studied as a function of the trigger and associated $p_t$. the behavior of the jet-like component is remarkably consistent for different collision systems, suggesting it is produced by fragmentation. the width of the jet-like correlation is found to increase with the system size. the ridge, previously observed in au+au collisions at $\sqrt{s_{{nn}}}$ = 200 gev, is also found in cu+cu collisions and in collisions at $\sqrt{s_{{nn}}}$ =\xspace 62.4 gev, but is found to be substantially smaller at $\sqrt{s_{{nn}}}$ =\xspace 62.4 gev than at $\sqrt{s_{{nn}}}$ = 200 gev for the same average number of participants ($ \langle n_{\mathrm{part}}\rangle$). measurements of the ridge are compared to models.
theory of heavy quark energy loss. we briefly review some of the models and theoretical schemes established to describe heavy quark quenching in ultrarelativistic heavy ions collisions. some lessons are derived from rhic and early lhc data, especially as for the contraints they impose on those models.
training effects of a visual aid on haptic sensitivity in a needle insertion task. this paper describes an experiment conducted to measure human's haptic sensitivity and the effects of haptic training with and without visual aid on a needle insertion task. the haptic training protocol consisted of a needle insertion task using dual-layer silicon samples. a visual aid was provided as a multimodal cue for haptic perception. results show that for a novices' group, training with a visual aid inhibited haptic perception. hence, haptic skills must be trained differently from visuo-motor skills.
robotics modelling and tilting control of an innovative urban vehicle. modeling and simulating are fundamental tools to develop new vehicles. the aim of this thesis is to model and simulate a urban narrow tilting car whose structure contains closed mechanical chains. hence the goal is to build a physical model more precise and realistic than the bicycle model or quarter vehicle model used usually for some control purposes. the modeling approach is based on the modified denavit&amp;hartenberg description, commonly used in robotics, by considering the vehicle as a multi-body poly-articulated system whose the terminal links are the wheels. this description allows calculating automatically the symbolic expression of the geometric, kinematic and dynamic models, by using robotics techniques and a symbolic software package named symoro+. the dynamic model is calculated recursively thanks to the newton-euler algorithm. simulations of different dynamical model of vehicles have been performed, analyzed and compared. they validate in some sense the modeling methodology presented as an efficient way to get realistic model of non-standard vehicles.
effects of ca2+ on supramolecular aggregation of natural organic matter in aqueous solutions: a comparison of molecular modeling approaches. natural organic matter (nom) represents a complex molecular system that cannot be fully characterized compositionally or structurally in full atomistic detail. this makes the application of molecular modeling approaches very difficult and significantly hinders quantitative investigation of nom properties and behavior by these otherwise very efficient computational techniques. here we report and analyze three molecular dynamics (md) simulations of ca2+ complexation with nom in aqueous solutions in an attempt to quantitatively assess possible effects of model- and system size-dependence in such simulations. despite some obvious variations in the computed results that depend on the size of the simulated system and on the parameters of the force field models used, all three simulations are quite robust and consistent. they show ca2+ ions associated with 35-50% of the nom carboxylic groups at near-neutral ph and point to a strong preference for the stability of bidentate-coordinated contact ion pairs. the degree and potential mechanisms of nom supramolecular aggregation in the presence of ca2+ ions in solution are also assessed on a semi-quantitative level from two larger-scale md simulations.
new insights in elf analysis using relativistic two-component approach: an application to astatine compounds. null
sorption of model radionuclides on callovian-oxfordian clayey formation. null
a monadic interpretation of execution levels and exceptions for aop. aspect-oriented programming (aop) started ten years ago with the remark that modularization of so-called crosscutting functionalities is a fundamental problem for the engineering of large-scale applications. originating at xerox parc, this observation has sparked the development of a new style of programming featured that is gradually gaining traction. however, aop lacks theoretical foundations to clarify new ideas showing up in its wake. this paper proposes to put a bridge between aop and the notion of 2-category to enhance the conceptual understanding of aop. starting from the connection between the λ-calculus and the theory of categories, we provide an internal language for 2-categories and show how it can be used to deﬁne the ﬁrst categorical semantics for a realistic functional aop language, called minaml. we then take advantage of this new categorical framework to introduce the notion of computational 2-monads for aop. we illustrate their conceptual power by deﬁning a 2-monad for éric tanter's execution levels--which constitutes the ﬁrst algebraic semantics for execution levels--and then introducing the ﬁrst exception monad transformer speciﬁc to aop that gives rise to a non-ﬂat semantics for exceptions by taking levels into account.
invertible program restructurings for continuing modular maintenance. when one chooses a main axis of structural decompostion for a software, such as function- or data-oriented decompositions, the other axes become secondary, which can be harmful when one of these secondary axes becomes of main importance. this is called the tyranny of the dominant decomposition. in the context of modular extension, this problem is known as the expression problem and has found many solutions, but few solutions have been proposed in a larger context of modular maintenance. we solve the tyranny of the dominant decomposition in maintenance with invertible program transformations. we illustrate this on the typical expression problem example. we also report our experiments with java and haskell programs and discuss the open problems with our approach.
toward systems biology in brown algae to explore acclimation and adaptation to the shore environment. brown algae belong to a phylogenetic lineage distantly related to land plants and animals. they are almost exclusively found in the intertidal zone, a harsh and frequently changing environment where organisms are submitted to marine and terrestrial constraints. in relation with their unique evolutionary history and their habitat, they feature several peculiarities, including at the level of their primary and secondary metabolism. the establishment of ectocarpus siliculosus as a model organism for brown algae has represented a framework in which several omics techniques have been developed, in particular, to study the response of these organisms to abiotic stresses. with the recent publication of medium to high throughput profiling data, it is now possible to envision integrating observations at the cellular scale to apply systems biology approaches. as a first step, we propose a protocol focusing on integrating heterogeneous knowledge gained on brown algal metabolism. the resulting abstraction of the system will then help understanding how brown algae cope with changes in abiotic parameters within their unique habitat, and to decipher some of the mechanisms underlying their (1) acclimation and (2) adaptation, respectively consequences of (1) the behavior or (2) the topology of the system resulting from the integrative approach.
pollutants transfer in urban stormwater runoff basin - additional biological treatment. the objective of this work is to evaluate the heavy metal transfer into an urban stormwater treatment device and to develop a treatment process which would allow improving the treatment of these waters. the scientific approach consisted first to characterize the metal loads forwarded into a retention pond receiving stormwater runoff coming from a highway and to evaluate the transfer of this metal pollution to the surrounding vegetation. the results of this preliminary study demonstrated: (1) a heavy metal transfer (cd, ni and zn) from the water and soil compartments to the aquatic macrophytes present on the studied site, (2) the capacity of these macrophytes to accumulate metal pollutants into their tissues and (3) the bioindicator value of these macrophytes for biomonitoring of stormwater metal pollution. regarding the capacity of plants to accumulate metals, especially in their roots, a phytoremediation process called floating treatment wetlands was proposed to improve urban stormwater quality. the treatment performances of these systems were evaluated through a microcosm experiment and the technical feasibility for implanting such floating systems directly on the surface of existing ponds was also evaluated. the results showed that floating treatment wetlands can be operated on the surface of retention pond provided that the material chosen for the construction of these systems are well adapted to environmental conditions. the results also brought to light the efficiency of floating treatment wetlands for metal uptake from water as well as the importance of the root system on pollutant retention and the filtration of fine suspended particles. finally, this study showed that floating treatment wetlands can be considered as sustainable treatment systems that need low maintenance.
heavy flavour decay muon production at forward rapidity in proton--proton collisions at \sqrt(s) = 7 tev. the production of muons from heavy flavour decays is measured at forward rapidity in proton--proton collisions at \sqrt(s) = 7 tev collected with the alice experiment at the lhc. the analysis is carried out on a data sample corresponding to an integrated luminosity l_{int} = 16.5 nb^{-1}. the transverse momentum and rapidity differential production cross sections of muons from heavy flavour decays are measured in the rapidity range 2.5 &lt; y &lt; 4, over the transverse momentum range 2 &lt; p_{t} &lt; 12 gev/c. the results are compared to predictions based on perturbative qcd calculations.
sorption speciation of nickel(ii) onto ca-montmorillonite: batch, exafs techniques and modeling. a.
determination of ni(ii) uptake mechanisms on mordenite surfaces: a combined macroscopic and microscopic approach. a.
chemical dosimetry during alpha irradiation: a specific system for uv-vis in situ measurement. null
measurement of event background fluctuations for charged particle jet reconstruction in pb-pb collisions at $\sqrt{s_{nn}} = 2.76$ tev. the effect of event background fluctuations on charged particle jet reconstruction in pb-pb collisions at sqrt(s_nn) = 2.76 tev has been measured with the alice experiment. the main sources of non-statistical fluctuations are characterized based purely on experimental data with an unbiased method, as well as by using single high p_t particles and simulated jets embedded into real pb-pb events and reconstructed with the anti-kt jet finder. the influence of a low transverse momentum cut-off on particles used in the jet reconstruction is quantified by varying the minimum track p_t between 0.15 gev/c and 2 gev/c. for embedded jets reconstructed from charged particles with p_t &gt; 0.15 gev/c, the uncertainty in the reconstructed jet transverse momentum due to the heavy-ion background is measured to be 11.3 gev/c (standard deviation) for the 10% most central pb-pb collisions, slightly larger than the value of 11.0 gev/c measured using the unbiased method. for a higher particle transverse momentum threshold of 2 gev/c, which will generate a stronger bias towards hard fragmentation in the jet finding process, the standard deviation of the fluctuations in the reconstructed jet transverse momentum is reduced to 4.8-5.0 gev/c for the 10% most central events. a non-gaussian tail of the momentum uncertainty is observed and its impact on the reconstructed jet spectrum is evaluated for varying particle momentum thresholds, by folding the measured fluctuations with steeply falling spectra.
description of atmospheric conditions at the pierre auger observatory using the global data assimilation system (gdas). atmospheric conditions at the site of a cosmic ray observatory must be known for reconstructing observed extensive air showers. the global data assimilation system (gdas) is a global atmospheric model predicated on meteorological measurements and numerical weather predictions. gdas provides altitude-dependent profiles of the main state variables of the atmosphere like temperature, pressure, and humidity. the original data and their application to the air shower reconstruction of the pierre auger observatory are described. by comparisons with radiosonde and weather station measurements obtained on-site in malargüe and averaged monthly models, the utility of the gdas data is shown.
the neutrons for science facility at spiral‐2. the "neutrons for science" (nfs) facility will be a component of spiral‐2, the future accelerator dedicated to the production of very intense radioactive ion beams, under construction at ganil in caen (france). nfs will be composed of a pulsed neutron beam for in‐flight measurements and irradiation stations for cross‐section measurements and material studies. continuous and quasi‐monokinetic energy spectra will be available at nfs respectively produced by the interaction of deuteron beam on thick a be converter and by the 7li(p,n) reaction on a thin converter. the flux at nfs will be up to 2 orders of magnitude higher than those of other existing time‐of‐flight facilities in the 1 mev to 40 mev range. nfs will be a very powerful tool for physics and fundamental research as well as applications like the transmutation of nuclear waste, design of future fission and fusion reactors, nuclear medicine or test and development of new detectors.
indication of reactor electron antineutrinos disappearance in the double chooz experiment. the double chooz experiment presents an indication of reactor electron antineutrino disappearance consistent with neutrino oscillations. an observed-to-predicted ratio of events of 0.944±0.016(stat)±0.040(syst) was obtained in 101 days of running at the chooz nuclear power plant in france, with two 4.25gwth reactors. the results were obtained from a single 10m3 fiducial volume detector located 1050 m from the two reactor cores. the reactor antineutrino flux prediction used the bugey4 flux measurement after correction for differences in core composition. the deficit can be interpreted as an indication of a nonzero value of the still unmeasured neutrino mixing parameter sin⁡22θ13. analyzing both the rate of the prompt positrons and their energy spectrum, we find sin⁡22θ13=0.086±0.041(stat)±0.030(syst), or, at 90% c.l., 0.017.
aspectualizing component models : implementation and interferences analysis. using aop to model non-modular concerns in cbse ensures better modularity and reusability of components. in this thesis, we provide a model independent approach for modeling aspects in component models. in the approach we model aspects as wrappers on views of component systems. a view describes an adequate component system configuration where all the components of interest of an aspect are encapsulated in the same composite.for declarative definition of views, we provide a declarative language vil. we illustrate how views are implemented in component models(e.g., fractal). we provide a formal framework for aspect interferences analysis. in the framework component systems and aspects are modeled as automata and uppaal model checker is used for the detection of aspect interferences. for interferences resolution, we provide a set of composition operators as templates to be instantiated for any two arbitrary aspects. our approach is illustrated with an airport wireless access example.
haptic sensitivity in needle insertion: the effects of training and visual aid. this paper describes an experiment conducted to measure haptic sensitivity and the effects of haptic training with and without visual aid. the protocol for haptic training consisted of a needle insertion task using dual-layer silicon samples. a visual aid was provided as a multimodal cue for the haptic perception task. results showed that for a group of novices (subjects with no previous experience in needle insertion), training with a visual aid resulted in a longer time to task completion, and a greater applied force, during post-training tests. this suggests that haptic perception is easily overshadowed, and may be completely replaced, by visual feedback. therefore, haptic skills must be trained differently from visuomotor skills.
haptic communication for a 2-d pointing task in a virtual environment. this paper examines the properties of haptic communication between two human operators using kinesthetic haptic devices in a collaborative task in a virtual environment. twenty subjects, divided into 10 dyads, participated in a 2d pointing task. each dyad consisted of a supervisor and an acting agent. the supervisor's role was to guide the acting agent towards a target in the virtual environment through either verbal or haptic communication only. verbal communication was found to be the most efficient means of communication, but collaboration was also effective using haptic communication. several different haptic communication strategies were observed, all with equal effectiveness as measured by task completion time. these strategies followed the same pattern as the verbal strategies. these results suggest that haptic communication in a virtual environment is possible, allowing for future designs of haptically enhanced collaborative work in virtual environments.
a programming model integrating classes, events and aspects. object-oriented programming (oop) has become the de facto programming paradigm. event-based programming (ebp) and aspect-oriented programming (aop) complement oop, covering some of its deficiencies when building complex software. today's applications combine the three paradigms. however, oop, ebp and aop have not yet been properly integrated. their underlying concepts are in general provided as distinct language constructs, whereas they are not completely orthogonal. this lack of integration and orthogonality complicates the development of software as it reduces its understandability, its composability and increases the required glue code. this thesis proposes an integration of oop, ebp and aop leading to a simple and regular programming model. this model integrates the notions of class and aspect, the notions of event and join point, and the notions of piece of advice, method and event handler. it re- duces the number of language constructs while keeping expressiveness and offering additional programming options. we have designed and implemented two programming languages based on this model: ejava and ecaesarj. ejava is an extension of java implementing the model. we have validated the expressiveness of this language by implementing a well-known graphical editor, jhotdraw, reducing its glue code and improving its design. ecaesarj is an extension of caesarj that combines our model with mixins and language support for state machines. this combination was shown to greatly facilitate the implementation of a smart home application, an industrial- strength case study that aims to coordinate different devices in a house and automatize their behaviors.
search for ultrahigh energy neutrinos in highly inclined events at the pierre auger observatory. the surface detector of the pierre auger observatory is sensitive to neutrinos of all flavors above 0.1 eev. these interact through charged and neutral currents in the atmosphere giving rise to extensive air showers. when interacting deeply in the atmosphere at nearly horizontal incidence, neutrinos can be distinguished from regular hadronic cosmic rays by the broad time structure of their shower signals in the water-cherenkov detectors. in this paper we present for the first time an analysis based on down-going neutrinos. we describe the search procedure, the possible sources of background, the method to compute the exposure and the associated systematic uncertainties. no candidate neutrinos have been found in data collected from 1 january 2004 to 31 may 2010. assuming an e-2 differential energy spectrum the limit on the single-flavor neutrino is e2dn/de&lt;1.74×10-7gevcm-2s-1sr-1 at 90% c.l. in the energy range 1×1017ev.
how to deal with your it legacy? what is coming up in modisco. null
btrscript : a safe management system for virtualized data center. virtual machine management in data centers is more and more complex and this is due to the increasing total number of virtual machines. virtual machine resources and scheduled policies (e.g., consolidation) define the virtual machine placement. this placement is difficult to compute for large infrastructures. administrators maintain a correct placement by performing actions (e.g., migrate virtual machines, power off servers...) and some time using autonomic schedulers. we propose btrscript: a safe autonomic system for virtual machine management that includes actions and placement rules. actions are imperative operations to reconfigure the data center and declarative rules specify the virtual machine placement. administrators schedule both actions and rules, to manage their data center(s). they can also interact with the btrscript system in order to monitor the data center and compute the correct virtual machine placement.
contribution to the emergy theory : application to recycling. the continuous development of tools to measure sustainability led to the emergy theory. the emergy of a resource or product is defined by converting all resource (raw materials) and energy inputs in the form of solar energy equivalents (solar energy unit, sej), cf odum (1996, 2000). the main objective of this thesis is to adapt the method of emergy evaluation to industrial recycling practices. the principal scientific contribution from the study can be summarized as: contribution to th emergy theory in discrete time applied to recycling. under certain assumptions, the emergy of a recycled product can be expressed in the form of a geometric series. if the emergy of a product deteriorates, there is a cost to the emergy of recycling with similarities to the carnot principle. as a result, a 'factor' is introduced which could be included on emergy evaluation tables to reflect increases in transformity due to multiple recycling. fi nally, the developed approach is successfully applied to the use of recycle materials in a low energy building.
directed flow of identified particles in au + au collisions at $\sqrtsnn = 200$ gev at rhic. star's measurements of directed flow ($v_1$) around midrapidity for $\pi^{\pm}$, k$^{\pm}$, k$_s^0$, $p$ and $\bar{p}$ in au + au collisions at $\sqrtsnn = 200$ gev are presented. a negative $v_1(y)$ slope is observed for most of produced particles ($\pi^{\pm}$, k$^{\pm}$, k$_{s}^{0}$ and $\bar{p}$). the proton $v_1(y)$ slope is found to be much closer to zero compared to antiprotons. a sizable difference is seen between $v_1$ of protons and antiprotons in 5-30% central collisions. the $v_1$ excitation function is presented. comparisons to model calculations (rqmd, urqmd, ampt, qgsm with parton recombination, and a hydrodynamics model with a tilted source) are made. anti-flow alone cannot explain the centrality dependence of the difference between the $v_1(y)$ slopes of protons and antiprotons.
mie and flame velocity of partially oxidised aluminium dust. this work presents experimental tools for the determination of minimum ignition energy (mie) and results concerning the influence of an initial oxidation state on ignition threshold energies and flame velocity. these studies are carried out with micrometric aluminium particles which are oxidised using an anodising process. the first part of this work concerns the description of the experimental devices (hartmann tube, for mie measurements, and constant volume combustion chamber for flame velocity measurement with using high speed recording shadowgraphy). in the second part, a review of some results obtained for the sensitivity (mie) of aluminium particle evolution versus particle diameter, air-fuel equivalence ratio and oxide content is presented. the effect of the oxide content is demonstrated: the mie increases with the initial oxide content. the sensitivity of oxidised dust remains relatively high for high oxide contents (17.1wt%). the flame velocity is also modified and decreases as the oxide content increases. the most important result seems to be the role of the water content contained in the oxide shell which increases the reactivity of the oxidised aluminium dust.
measurement of the $w \to e \nu$ and $z/\gamma^* \to e^+e^-$ production cross sections at mid-rapidity in proton-proton collisions at $\sqrt{s}$ = 500 gev. we report measurements of the charge-separated $w^{+(-)} \to e^{+(-)} + \nu_e(\bar{\nu}_e)$ and $z/\gamma^* \to e^+e^-$ production cross sections at mid-rapidity in proton-proton collisions at $\sqrt{s}$ = 500 gev. these results are based on 13.2 pb$^{-1}$ of data recorded in 2009 by the star detector at rhic. production cross sections for w bosons that decay via the $e \nu$ channel were measured to be $\sigma(pp \to w^+ x) \cdot br(w^+ \to e^+ \nu_e)$ = 117.3 \pm 5.9(stat) \pm 6.2(syst) \pm 15.2(lumi) pb, and $\sigma(pp \to w^- x) \cdot br(w^- \to e^- \bar{\nu}_e)$ = 43.3 \pm 4.6(stat) \pm 3.4(syst) \pm 5.6(lumi) pb. for $z/\gamma^*$ production, $\sigma(pp \to z/\gamma^* x) \cdot br(z/\gamma^* \to e^+ e^-)$ = 7.7 \pm 2.1(stat) $^{+0.5}_{-0.9}$(syst) \pm 1.0(lumi) pb for di-lepton invariant masses $m_{e^+e^-}$ between 70 and 110 gev/$c^2$. first measurements of the w cross section ratio, $\sigma(pp \to w^+ x) / \sigma(pp \to w^- x)$, at $\sqrt{s}$ = 500 gev are also reported. theoretical predictions, calculated using recent parton distribution functions, are found to agree with the measured cross sections.
light vector meson production in pp collisions at sqrt(s) = 7 tev. the alice experiment has measured low-mass dimuon production in pp collisions at \sqrt{s} = 7 tev in the dimuon rapidity region 2.5.
probing the microscopic nuclear matter self-organization processes in the neutron star crust. we investigate microscopic self-organization processes of nuclear matter in the outermost layers of neutron star crusts with the dywan model. in this framework a pure mean-field description of nuclear dynamics has been performed. starting from initial crystalline lattices, which are expected in the most external regions, the system organizes itself in exotic structures. the present work focuses on the effects of both the initial lattice symmetries and the nuclear species on the morphology and on the evolution of those structures. the response of the system is analyzed when it is subjected to random fluctuations of the initial lattice.
solution controls for dissolved silica at 25, 50 and 90 °c for quartz, callovo-oxfordian claystone, illite and mx80 bentonite. clay host rock and engineered barrier systems are the key elements in the concept adopted by several countries to isolate the high level nuclear waste from the biosphere. mainly composed by illite, mixed-layer illite-smectite (i/s) and montmorillonite, these clays are characterized by their properties of high retention and low permeability for released radionuclides. after closure of the repository in deep geological formation, groundwater in equilibrium with the host rock, forming the pore water, will saturate the engineered barrier system and come in contact with the nuclear glass waste package. the leaching of the different silicate minerals as well as the uptake of dissolved silicic acid by these phases is an important factor in the dissolution of the nuclear glass waste. to understand how the si interacts with clay materials, this study aims at evaluating the solubility as well as the dynamics of solid/solution exchange reactions, which control the dissolved silicic acid concentration in solution in contact with callovo-oxfordian claystone. the results were compared with the dissolution of illite, bentonite and quartz at 25, 50 and 90 °c in pore water. the experiments were conducted in batch system and in controlled atmosphere conditions and were followed in continue until the equilibrium between the solid and solution is reached. the results present a stabilization of the ph-values at 8.2 after 211 days for all the samples. the nature of solids and the temperature were not the determining factors on the ph-value but the chemical composition of the pore water and the working atmosphere (here, nitrogen). dissolution and precipitation rates were calculated from the concentration of si released from solids and the activity of 32si-radiotracer added in solution, respectively. dissolution rates were in the range of (8.7 ± 0.4) × 10−12-(6.8 ± 0.3) × 10−11 mol si/m2/s for quartz, (1.6 ± 0.1) × 10−13-(6.4 ± 0.3) × 10−13 mol si/m2/s for callovo-oxfordian claystone, (2.4 ± 0.1) × 10−13-(9.4 ± 0.5) × 10−13 mol si/m2/s for illite du puy and (1.2 ± 0.6) × 10−12-(9.1 ± 0.5) × 10−12 mol si/m2/s for bentonite. precipitation rates were in the range of (8.4 ± 0.4) × 10−12-(2.0 ± 0.1) × 10−11 mol si/m2/s for quartz, (2.0 ± 0.1) × 10−13-(2.5 ± 0.1) × 10−12 for callovo-oxfordian claystone, (2.4 ± 0.1) × 10−12-(5.2 ± 0.3) × 10−12 mol si/m2/s for illite and (1.9 ± 0.1) × 10−13-(6.9 ± 0.3) × 10−13 mol si/m2/s for mx80 bentonite. the plot of dissolution rates versus precipitation rates gave a slope near one indicating a dynamic process of dissolution/precipitation. through the experiment with the addition of a spike of 32si-radiotracer in solution, we have showed that the activity of 32si decreases in contact with clay containing at least 6% of fe and al, as for example illite, which seems to be a necessary condition. finally, in the condition of nuclear glass waste disposal, the callovo-oxfordian claystone would be the phase controlling the solubility of dissolved si from the nuclear waste and clay with a solubility value of 4.8 × 10−4 mol/l at 90 °c in pore water of bure site.
h2 production by γ and he ions water radiolysis, effect of presence tio2 nanoparticles. the effect of tio2 particles on the yield of h2 formation under water radiolysis is measured. irradiations were performed using a 60co γ−ray source as well as with he ions particles (4he2+) generated by a cyclotron with an external beam energy of 6 mev. the resulting hydrogen as a stable product of radiolysis was measured by mass spectrometry. g(h2) obtained for water radiolysis by he ions−irradiation in aerated and argon water are found to be 1.91 × 10−7 and 1.35 × 10−7 mol j−1, respectively. in the presence of titanium oxide anatase−type dispersed in water, under he ions−irradiation, g(h2) is found to increase slightly from 1.04 × 10−7 to 1.35 × 10−7 mol j−1 by increasing the specific surface from 8 to 253 m2/g, respectively. under γ-irradiation, g(h2) is found to be 0.41 × 10−7 mol j−1 close to primary yield of hydrogen in presence of oh. radical scavenger. in addition, radiolysis of water adsorbed in the titanium oxide with low water content, which corresponds to a few layers of water sorbed onto the solid surface gives a huge values of the g(h2). for the same amount of water, with using the dose absorbed by tio2 particles, for he ions-irradiation, g(h2) increases from 14.5 × 10−7 to 35 × 10−7 mol j-1 by increasing the surface area of tio2 nanoparticles from 4 to 52 m2/g, respectively. for γ−irradiation g(h2) is found to be 5.25 × 10−7 mol j-1 for the sample with 8 m2/g specific surface area.
studies of (cs,ba)-hollandite dissolution under gamma irradiation at 95 °c and at ph 2.5, 4.4 and 8.6. in the frame of the former french 1991-law on waste management, which was extended in 2006-law, hollandite ceramic was studied as a potential specific conditioning matrix for caesium isotopes (long-life radionuclide 135cs and the strong heat generating radionuclide 137cs). in this general study of cs-containment in a ceramic matrix, the chemical durability was pointed out as a key property. leaching experiments in static mode were conducted during 240 days at various ph-values from acidic to alkaline range. the initial leaching rates between 0 and 45 days are faster for cs than for ba and the average for the caesium are (1.4 ± 0.1) × 10−4 g/m2/d (ph 2.5), (6.4 ± 0.9) × 10−5 g/m2/d (ph 4.4) and (3.1 ± 0.6) × 10−5 g/m2/d (ph 8.6), and for the barium (6 ± 1) × 10−5 g/m2/d (ph 2.5), (2.8 ± 0.3) × 10−5 g/m2/d (ph 4.4), and (2 ± 2) × 10−6 g/m2/d (ph 8.6). at the equilibrium between 45 and 240 days, the normalised mass losses average for caesium are (8.2 ± 0.3) × 10−3 g/m2 (ph 2.5), (5.2 ± 0.4) × 10−3 g/m2 (ph 4.4) and (4.1 ± 0.2) × 10−3 g/m2 (ph 8.6), and for barium (3.7 ± 0.4) × 10−3 g/m2 (ph 2.5), (2.0 ± 0.1) × 10−3 g/m2 (ph 4.4) and (4 ± 2) × 10−4 g/m2 (ph 8.6). caesium and barium are incongruently released in solution with a correlation slope close to 0.5 at ph 2.5 and ph 4.4 and very low (near 0.02) in alkaline solution. sorption experiments with radioactive isotopes (137cs and 133ba) were conducted on hollandite pre-leached in aqueous solutions. caesium and barium release is controlled by the surface reactions. leaching experiments and isotopic addition experiments (137cs- and 133ba-radiotracer) indicate that caesium behaviour is independent on ph-values, whereas barium behaviour is strongly dependent. additional experiments in the presence of gamma irradiation (60co source) did not show any significant effect on hollandite leaching process.
underlying event measurements in pp collisions at sqrt(s) = 0.9 and 7 tev with the alice experiment at the lhc. we present measurements of underlying event observables in pp collisions at sqrt(s) = 0.9 and 7 tev. the analysis is performed as a function of the highest charged-particle transverse momentum pt,lt in the event. different regions are defined with respect to the azimuthal direction of the leading (highest transverse momentum) track: toward, transverse and away. the toward and away regions collect the fragmentation products of the hardest partonic interaction. the transverse region is expected to be most sensitive to the underlying event activity. the study is performed with charged particles above three different pt thresholds: 0.15, 0.5 and 1.0 gev/c. in the transverse region we observe an increase in the multiplicity of a factor 2-3 between the lower and higher collision energies, depending on the track pt threshold considered. data are compared to pythia 6.4, pythia 8.1 and phojet. on average, all models considered underestimate the multiplicity and summed pt in the transverse region by about 10-30%.
the objective sum constraint. constraint toolkits generally propose a sum constraint where a global objective variable should be equal to a sum of local objective variables, on which bound-consistency is achieved. to solve optimization problems this propagation is poor. therefore, ad-hoc techniques are designed for pruning the global objective variable by taking account of the interactions between constraints defined on local objective variables. each technique is specific to each (class of) practical problem. in this paper, we propose a new global constraint which deals with this issue in a generic way. we propose a sum constraint which exploits the propagation of a set of constraints defined on local objective variables.
inner regions and interval linearizations for global optimization. researchers from interval analysis and constraint (logic) programming communities have studied intervals for their ability to manage infinite solution sets of numerical constraint systems. in particular, inner regions represent subsets of the search space in which all points are solutions. our main contribution is the use of recent and new inner region extraction algorithms in the upper bounding phase of constrained global optimization. convexification is a major key for efficiently lower bounding the objective function. we have adapted the convex interval taylorization proposed by lin &amp; stadtherr for producing a reliable outer and inner polyhedral approximation of the solution set and a linearization of the objective function. other original ingredients are part of our optimizer, including an efficient interval constraint propagation algorithm exploiting monotonicity of functions. we end up with a new framework for reliable continuous constrained global optimization. our interval b&amp;b is implemented in the interval-based explorer ibex and extends this free c++ library. our strategy significantly outperforms the best reliable global optimizers.
the effect of the geomagnetic field on cosmic ray energy estimates and large scale anisotropy searches on data from the pierre auger observatory. we present a comprehensive study of the influence of the geomagnetic field on the energy estimation of extensive air showers with a zenith angle smaller than $60^\circ$, detected at the pierre auger observatory. the geomagnetic field induces an azimuthal modulation of the estimated energy of cosmic rays up to the ~2% level at large zenith angles. we present a method to account for this modulation of the reconstructed energy. we analyse the effect of the modulation on large scale anisotropy searches in the arrival direction distributions of cosmic rays. at a given energy, the geomagnetic effect is shown to induce a pseudo-dipolar pattern at the percent level in the declination distribution that needs to be accounted for.
nonlinear motion control of cpg-based movement with applications to a class of swimming robots. in bio-inspired robotics, use of a central pattern generator (cpg) to coordinate actuation is fairly common. the gait achieved depends on a number of cpg parameters, which can be adjusted to control the robot's motion. this paper presents an output feedback motion control framework, addressing issues encountered when dealing with this type of control problem, including partial state measurements and system uncertainty. efficacy of the presented approach is illustrated by results of numerical simulations in the case of a swimming robot.
preface (ocl 2011 proceedings). null
the mde diploma: first international postgraduate specialization in model-driven engineering. model-driven engineering (mde) is changing the way we build, operate, and maintain our software-intensive systems. several projects using mde practices are reporting signi cant improvements in quality and perfor- mance but, to be able to handle these projects, software engineers need a set of technical and interpersonal skills that are currently not widely available. the mde postgraduate diploma intends to ll this gap by o ering a full-time one year formation on mde. the course syllabus is designed to teach students how to work at a higher abstraction level by the rigorous use of (software) models as the main artifacts in all software engineering activities. contents include the conceptual framework of mde plus all techniques and tools (e.g., for de ning modeling languages, models, model transformations and so on) required to successfully complete software engineering projects following a mde approach. the organizational impact (and challenges) of adopting mde are exempli ed using real industrial experiences. this paper describes the mde diploma and the lessons that we learned after completing its first edition.
discovery, beyond the clouds - distributed and cooperative framework to manage virtual environments autonomically: a prospective study. although the use of virtual environments provided by cloud computing infrastructures is gaining consensus from the scienti c community, running applications in these environments is still far from reaching the maturity of more usual computing facilities such as clusters or grids. indeed, current solutions for managing virtual environments are mostly based on centralized approaches that barter large-scale concerns such as scalability, reliability and reactivity for simplicity. however, considering current trends about cloud infrastructures in terms of size (larger and larger) and in terms of usage (cross-federation), every large-scale concerns must be addressed as soon as possible to e ciently manage next generation of cloud computing platforms. in this work, we propose to investigate an alternative approach leveraging distributed and cooperative mechanisms to manage virtual environments autonomically (discovery). this initiative aims at overcoming the main limitations of the traditional server-centric solutions while integrating all mandatory mechanisms into a uni ed distributed framework. the system we propose to implement, relies on a peer-to-peer model where each agent can e ciently deploy, dynamically schedule and periodically checkpoint the virtual environments they manage. the article introduces the global design of the discovery proposal and gives a preliminary description of its internals.
revisiting the tree constraint. this paper revisits the tree constraint introduced in [2] which partitions the nodes of a n-nodes, m-arcs directed graph into a set of node-disjoint anti-arborescences for which only certain nodes can be tree roots. we introduce a new filtering algorithm that enforces generalized arc-consistency in o(n + m) time while the original filtering algorithm reaches o(nm) time. this result allows to tackle larger scale problems involving graph partitioning.
reusing legacy software in a self-adaptive middleware framework. software that adapts its behavior to an operational context and/or feedback from within is self-adaptive. for instance, a computer vision system to detect people may change its behavior due to change in context such as nightfall. this may entail automatic change in architecture, software com- ponents and their parameters at runtime. legacy software components do not possess this ability. therefore we ask, can legacy software be successfully cast into a self-adaptive middleware framework ? we present tekio, a self-adaptive middleware platform to dynamically compose legacy soft- ware behavior. tekio is based on dynamic component load- ing available in a java implementation of open service gate- way interface (osgi). tekio contains generic components to capture context/feedback, plan an adaptation strategy, and reconfigure domain-specific components. the domain- specific components encapsulate legacy behavior implemented possibly in native languages such as c/c++. we implement a self-adaptive vision system in tekio as a case study. we perform experiments to validate that the self-adaptive layer based on osgi has negligible effects on the performance of the legacy library namely opencv. we also demonstrate that the self-adaptive middleware can handle about 30 adap- tations in a span of 2 seconds while producing meaningful output.
a vpa-based aspect language. this thesis focuses on the development of an advanced history-based aspect language and approaches to certain related issues ranging from applications to analysis methods. the aspect language, namely vpa-based aspect language, is defined upon visibly pushdown au- tomata (vpas) [21]. this language is essentially an extension from an existing framework [47] of regular aspect languages. it features vpa-based pointcuts and provides, in particu- lar, constructors for the declarative definition of pointcuts based on regular and non-regular structures. we have also extended and developed the technique for detecting automatically potential interactions among vpa-based aspects. despite several advantages of the class of visibly pushdown automata, there has been no practical support for them available. therefore, we have realized a library called vpalib that provides the implementation of essential data structures and operations for the vpa. this library is essential to enable the construction and analysis of vpa-based aspects. for instance, we have successfully performed certain analysis for detecting interactions among aspects using this library. in order to motivate the use of vpa-based aspects, we have studied two basic kinds of distributed applications, one representing typical systems with nested login sessions, and the other representing a grid computing system over peer-to-peer network. we have shown how vpa-based aspects can be useful for the realization of certain functionalities of these typical distributed applications. thanks to their highly expressive pointcuts, another important application of vpa-based aspects is to define evolution on component-based systems, especially those with explicit component protocols. the use of aspects over component protocols, however, may break the coherence between the components of the system. we have further developed proof methods to establish the preservation of fundamental correctness properties, such as compatibility and substitutability relations between software components after the application of vpa-based aspects. finally, we have considered the use of model checking techniques to verify systems that are modified by aspects. the goal of the verification is to check whether an aspect violates the global properties of a base system or the properties of other aspects. we have chosen the approach in which we create an abstract model from the vpa model and then run a model checker that is capable of checking the abstract model against the properties. we formally define the abstraction process and demonstrate our model checking approach via examples.
cross-layer sla selection for cloud services. cloud computing is a paradigm for enabling remote, on-demand access to a set of configurable computing resources as a service. the pay-per-use model enables service providers to offer their services to customers in different quality-of-service (qos) levels. these qos parameters are used to compose some bipartisan service level agreement (sla) between a service provider and a service consumer. a main challenge for a service provider is to manage slas for its service consumers, i.e. automatically determine the appropri- ate resources required from the lower layer in order to respect the qos requirements of his consumers. this paper proposes an optimization framework driven by consumer preferences to address the sla dependencies problem across the different cloud layers as well as the need of flexibility and dynamicity required by the domain of cloud computing. our approach aims to select the optimal vertical business process designed by cross-layer cloud services, enforcing sla dependen- cies between layers. based on constraint programming (cp), our approach can take into account dynamic qos parameters in a flexible manner to compose the best vertical business process. experimental results demonstrate the flexibility and effectiveness of our approach.
api2mol: automating the building of bridges between apis and model-driven engineering. context: a software artefact typically makes its functionality available through a specialized application programming interface (api) describing the set of services offered to client applications. in fact, building any software system usually involves managing a plethora of apis, which complicates the development process. in model-driven engineering (mde), where models are the key elements of any software engineering activity, this api management should take place at the model level. therefore, tools that facilitate the integration of apis and mde are clearly needed. objective: our goal is to automate the implementation of api-mde bridges for supporting both the creation of models from api objects and the generation of such api objects from models. in this sense, this paper presents the api2mol approach, which provides a declarative rule-based language to easily write mapping definitions to link api specifications and the metamodel that represents them. these definitions are then executed to convert api objects into model elements or vice versa. the approach also allows both the metamodel and the mapping to be automatically obtained from the api specification (bootstrap process). method: after implementing the api2mol engine, its correctness was validated using several apis. since apis are normally large, we then developed a tool to implement the bootstrap process, which was also validated. results: we provide a toolkit (language and bootstrap tool) for the creation of bridges between apis and mde. the current implementation focuses on java apis, although its adaptation to other statically typed object-oriented languages is straightforward. the correctness, expressiveness and completeness of the approach have been validated with the swing, swt and jtwitter apis. conclusion: api2mol frees developers from having to manually implement the tasks of obtaining models from api objects and generating such objects from models. this helps to manage api models in mde-based solutions.
measurements of inelastic neutron scattering at 96 mev from carbon, iron, yttrium and lead. inelastic neutron scattering for (12)c, (58)fe, (89)y and (208)pb have been measured at 96 mev at the the svedberg laboratory in uppsala and double-differential cross sections are reported. data cover an excitation energy range of 0-45 mev and the angular intervals are 28 - 58 degrees for (12)c, 26 - 65 degrees for (58)fe and 26 - 52 degrees for (89)y and (208)pb. in this experiment, neutron detection is based on conversion to protons in an active scintillator converter. an analysis technique in which the neutron spectra have been obtained through a folding procedure using the response of the detector system has been used. the results are compared to and are in reasonable agreement with several model predictions and with inelastic neutron scattering data at 65 mev from university of california, davis, usa.
towards a general composition semantics for rule-based model transformation. as model transformations have become an integral part of the automated software engineering lifecycle, reuse, modularisation, and composition of model transformations becomes important. one way to compose model transformations is to compose modules of transformation rules, and execute the composition as one transformation (internal composition). this kind of composition can provide richer semantics, as it is part of the transformation language. this paper aims to generalise two internal composition mechanisms for rule-based transformation languages, module import and rule inheritance, by providing executable semantics for the composition mechanisms within a virtual machine. the generality of the virtual machine is demonstrated for different rule-based transformation languages by compiling those languages to, and executing them on this virtual machine. we will discuss how atl and graph transformations can be mapped to modules and rules inside the virtual machine.
self-management of applications qos for energy optimization in datacenters. as a direct consequence of the increasing popularity of cloud computing solutions, data centers are amazingly growing and hence have to urgently face with the energy consumption issue. available solutions rely on cloud computing models and virtualization techniques to scale up/down application based on their performance metrics. although those proposals can reduce the energy footprint of applications and by transitivity of cloud infrastructures, they do not consider the internal characteristics of applications to finely de fine a trade-off between applications quality of service and energy footprint. in this paper, we propose a self-adaptation approach that considers both application internals and system to reduce the energy footprint in cloud infrastructure. each application and the infrastructure are equipped with their own control loop, which allows them to autonomously optimize their executions. simulations show that the approach may lead to appreciable energy savings without interfering on application provider revenues.
an open source-based approach for industrializing research tools. from a business perspective, software engineering is an enormous market which is always evolving and progressing from both economical and technical sides. such a market requires constant adaptation, which is often made possible via the regular adoption of significant innovations. thus, research labs, as priority innovation providers, are key actors of this market. unfortunately industrialization of research works is a very challenging process, thus (too) few research prototypes end up as successful and popular largely used products. based on our concrete experience and different feedback collected in this area, we have proposed a pragmatic business model for transforming the results of scientific experimentation into practical industrial solutions. the two key elements in this collaborative model are: 1) the introduction of a third entity, a technology provider, as an interface between the two classic partners in such a process (i.e. the research lab and the big company or community playing the role of the actual end user); 2) the fact that we rely on the use of open source and its ecosystem (i.e. the eclipse foundation) to largely facilitate the communication and exchanges between all the involved partners. we will illustrate our business model by describing our successful collaboration experiences within the context of the atl and modisco eclipse "modeling" projects.
virtual emf - transparent composition, weaving and linking of models. when using the eclipse modeling framework (emf), one frequently faces the problem of having to deal with several large heterogeneous and interrelated models. the information relevant for a specific user at a given time is often scattered across those models. therefore, we often have the need for composing, weaving or simply linking (parts of) these models in order to provide a more unified and usable view of the modeled system(s). with the currently available technologies, this is not a trivial task. ideally, we would like to have a kind of virtual emf resource offering a centralized and transparent access point to a global view on these different interconnected models. it should be implemented in a way such that: 1) the virtual emf resource behaves as a normal model, so interoperability (compatibility with existing emf-based solutions/tools) is guaranteed; 2) there is a perfect synchronization between the composed view (virtual resource) and the original models; 3) performance is not an issue, because neither creating nor accessing the global view results in additional costs (loading time, memory usage, etc.). as a solution, this talk introduces the brand new virtual emf tool which enables users to efficiently access, handle and combine a set of interrelated emf models in a completely transparent way.
j/psi polarization in pp collisions at sqrt(s)=7 tev. the alice collaboration has studied j/psi production in pp collisions at sqrt(s)=7 tev at the lhc through its muon pair decay. the polar and azimuthal angle distributions of the decay muons were measured, and results on the j/psi polarization parameters lambda_theta and lambda_phi were obtained. the study was performed in the kinematic region 2.5.
dark matter results from 100 live days of xenon100 data. we present results from the direct search for dark matter with the xenon100 detector, installed underground at the laboratori nazionali del gran sasso of infn, italy. xenon100 is a two-phase time-projection chamber with a 62 kg liquid xenon target. interaction vertex reconstruction in three dimensions with millimeter precision allows the selection of only the innermost 48 kg as the ultralow background fiducial target. in 100.9 live days of data, acquired between january and june 2010, no evidence for dark matter is found. three candidate events were observed in the signal region with an expected background of (1.8±0.6) events. this leads to the most stringent limit on dark matter interactions today, excluding spin-independent elastic weakly interacting massive particle (wimp) nucleon scattering cross sections above 7.0×10-45  cm2 for a wimp mass of 50  gev/c2 at 90% confidence level.
implications on inelastic dark matter from 100 live days of xenon100 data. the xenon100 experiment has completed a dark matter search with 100.9 live days of data, taken from january to june 2010. events with energies between 8.4 and 44.6  kevnr in a fiducial volume containing 48 kg of liquid xenon have been analyzed. a total of three events have been found in the predefined signal region, compatible with the background prediction of (1.8±0.6) events. based on this analysis we present limits on the wimp-nucleon cross section for inelastic dark matter. with the present data we are able to rule out the explanation for the observed dama/libra modulation as being due to inelastic dark matter scattering off iodine, at a 90% confidence level.
shear and bulk viscosities of the gluon plasma in a quasiparticle description. shear and bulk viscosities of deconfined gluonic matter are investigated within an effective kinetic theory by describing the strongly interacting medium phenomenologically in terms of quasiparticle excitations with medium-dependent self-energies. we show that the resulting transport coefficients reproduce the parametric dependencies on temperature and coupling obtained in perturbative qcd at large temperatures and small running coupling. the extrapolation into the nonperturbative regime results in a decreasing specific shear viscosity with decreasing temperature, exhibiting a minimum in the vicinity of the deconfinement transition, while the specific bulk viscosity is sizable in this region, falling off rapidly with increasing temperature. the temperature dependence of specific shear and bulk viscosities found within this quasiparticle description of the pure gluon plasma is in agreement with available lattice qcd results.shear and bulk viscosities of deconfined gluonic matter are investigated within an effective kinetic theory by describing the strongly interacting medium phenomenologically in terms of quasiparticle excitations with medium-dependent self-energies. we show that the resulting transport coefficients reproduce the parametric dependencies on temperature and coupling obtained in perturbative qcd at large temperatures and small running coupling. the extrapolation into the nonperturbative regime results in a decreasing specific shear viscosity with decreasing temperature, exhibiting a minimum in the vicinity of the deconfinement transition, while the specific bulk viscosity is sizable in this region, falling off rapidly with increasing temperature. the temperature dependence of specific shear and bulk viscosities found within this quasiparticle description of the pure gluon plasma is in agreement with available lattice qcd results.
measurement of charm production at central rapidity in proton-proton collisions at sqrt(s) = 7 tev. the pt-differential inclusive production cross sections of the prompt charmed mesons d0, d+, and d*+ in the rapidity range |y|&lt;0.5 were measured in proton-proton collisions at sqrt(s) = 7 tev at the lhc using the alice detector. reconstructing the decays d0-&gt;k-pi+, d+-&gt;k-pi+pi+, d*+-&gt;d0pi+, and their charge conjugates, about 8,400 d0, 2,900 d+, and 2,600 d*+ mesons with 1.
extended h2 controller synthesis for continuous descriptor systems. this paper presents a complete solution to the nonstandard h2 output feedback control problem for continuous descriptor systems where unstable and nonproper weighting functions are used. in such a problem, the desired controller has to satisfy two conditions simultaneously: (i) the closed-loop is admissible and has a minimum h2 norm, (ii) only the internal stability of a part of the closed-loop is sought. the condition of the existence of such a controller is deduced. an explicit characterization of the optimal solution is also formulated, based on two generalized algebraic riccati equations (gares) and two generalized sylvester equations. a numerical example is included to illustrate the validity of the proposed results.
bilayer graphene inclusions in rotational-stacked multilayer epitaxial graphene. additional component in multi-layer epitaxial graphene grown on the c-terminated surface of sic, which exhibits the characteristic electronic properties of a ab-stacked graphene bilayer, is identified in magneto-optical response of this material. we show that these inclusions represent a well-defined platform for accurate magneto-spectroscopy of unperturbed graphene bilayers.
clusterization in the shape isomers of the 56ni nucleus. the interrelation of the quadrupole deformation and clusterization is investigated in the example of the 56ni nucleus. the shape isomers, including superdeformed and hyperdeformed states, are obtained as stability regions of the quasidynamical u(3) symmetry based on a nilsson calculation. their possible binary clusterizations are investigated by considering both the consequences of the pauli exclusion principle and the energetic preference.
assault frequency and preformation probability of the alpha emission process. a study of the assault frequency and preformation factor of the α-decay description is performed from the experimental α-decay constant and the penetration probabilities calculated from the generalized liquid-drop model (gldm) potential barriers. to determine the assault frequency a quantum-mechanical method using a harmonic oscillator is introduced and leads to values of around 1021 s−1, similar to the ones calculated within the classical method. the preformation probability is around 10−1-10−2. the results for even-even po isotopes are discussed for illustration. while the assault frequency presents only a shallow minimum in the vicinity of the magic neutron number 126, the preformation factor and mainly the penetrability probability diminish strongly around n=126.
neutron production in neutron-induced reactions at 96 mev on 56fe and 208pb. null
search for signatures of magnetically-induced alignment in the arrival directions measured by the pierre auger observatory. we present the results of an analysis of data recorded at the pierre auger observatory in which we search for groups of directionally-aligned events (or 'multiplets') which exhibit a correlation between arrival direction and the inverse of the energy. these signatures are expected from sets of events coming from the same source after having been deflected by intervening coherent magnetic fields. the observation of several events from the same source would open the possibility to accurately reconstruct the position of the source and also measure the integral of the component of the magnetic field orthogonal to the trajectory of the cosmic rays. we describe the largest multiplets found and compute the probability that they appeared by chance from an isotropic distribution. we find no statistically significant evidence for the presence of multiplets arising from magnetic deflections in the present data.
robust optimization of inventory routing for bulk gas distribution. we address the 'rich' (i.e., with real-world features and constraints) inventory routing problem for bulk gas distribution under uncertainty. we consider that the uncertainty occurs on the supply side and consists of outages at the production plant. we propose a general methodology for generating, classifying and selecting 'robust' solutions: solutions that are less impacted when uncertain events occur such asplant outages. this methodology is applied to real data provided by the air liquide company in the context of bulk gas distribution, and we show that for a relatively small increase in cost, the robustness of routes and schedules for the bulk gas distribution with regard to possible plant outages is improved. results show that we can reduce the extra cost induced by plant outage, while only slightlyincreasing the cost in the cases where no outages occur.
design of structured control laws (hierarchical, decentralized) applied to powertrains. a conventional engine control unit (ecu) consists of two levels, called powertrain and engine levels. historically, the design of this device is based on an organic approach. thus, developments concerning gasoline and diesel control are today decoupled. based on a functional approach, the aim of this thesis is to propose a generic architecture and a cross level consistency that could be used with diesel, gasoline or hybrid structures, this reduces costs and design time. achieving this aim, we propose a new ecu hierarchical architecture consistent with control schemes and based on optimal and predictive/preview control. moreover, we consider a simplified torque and cruise control problems to illustrate the feasibility and the relevance of the proposition.
optimal resurfacing decisions for road maintenance : a pomdp perspective. we develop an optimal maintenance policy for a road section to minimize the total maintenance cost over the infinite horizon when some deterioration and decision parameters are not observable. both perfect and imperfect maintenance actions are possible through the application of various thicknesses of resurfacing layers. we use a two-phase deterioration process based on two parameters: the longitudinal cracking percentage and the deterioration growth rate. our deterioration model is a state-based model based on the state-dependent gamma process for the longitudinal cracking percentage and the bilateral gamma process for the deterioration growth rate. moreover the maintenance decision is constrained by a maximum road thickness that makes the maintenance decisions more complex as it becomes how much surface layer to add as well as to remove. because only one of the two deterioration parameters is observable, we formulate the problem as a partially observed markov decision process and solve it using a grid-based value iteration algorithm. numerical examples have shown that our model provides a preventive maintenance policy that slows down the initiation as well as the propagation of longitudinal cracks and that may ameliorate the road state to a better than as-good-as-new one by altering its composition through additive resurfacing layers.
optimizing road milling and resurfacing actions. a condition-based maintenance optimization approach is developed for the road-cracking problem in order to derive optimal maintenance policies that minimize a total discounted maintenance cost. the approach is based on a markov decision process that takes into ac- count multiple actions with varying effects on future road performance. maintaining the road consists of adding a new asphalt layer; however, as resurfacing actions are constrained by a maximum total road thickness, the maintenance decision is not only how thick a layer to apply, but also how much old road to remove. each combination of these actions leads to different maintenance costs and different future degradation behaviours. the road state is modelled by a dependent bivariate deterioration variable (the longitudinal cracking percentage and the deterioration growth rate), for taking these diﬀerent changes in the cracking patterns into account. moreover, the sensitivity to cracking for existing roads can be reduced with the addition of new layers, and thus actions that can lead to states better than good-as-new have to be considered. a numerical analysis is provided to illustrate the benefits of the introduction of the deterioration speed in the decision framework, as well as the belief that initially building a road to its maximum thickness is not optimal. the trade-oﬀs in the design decisions and the exploitation/maintenance costs are also explored.
j/psi production at forward rapidity in pb-pb collisions at sqrt(s_nn) = 2.76 tev, measured with the alice detector. in the alice experiment, at forward rapidity (2.5 &lt; y &lt; 4), the production of heavy quarkonium states is measured via their mu+ mu- decay channels. we present the first measurement of inclusive j/psi production, down to pt = 0, from pb-pb data collected at the lhc at sqrt(s_nn) = 2.76 tev. preliminary results on the nuclear modification factor (r_aa) and the central to peripheral nuclear modification factor (r_cp) show j/psi suppression with no significant centrality dependence and an integrated r_aa(0-80%) = 0.49 \pm 0.03(stat.) \pm 0.11(syst.).
multi-physics model of an electric fish-like robot : numerical aspects and application to obstacle avoidance. the paper deals with the modeling of a fish- like robot equipped with the electric sense, suited to study sensorimotor loops. the proposed multi-physics model merges a swimming dynamic model of a fish-like robot with an electric model of an embedded electrolocation sensor. based on a tcp- ip and threaded framework, the resulting simulator works in real time. after presenting the modeling aspects of this work, this article focuses on two numerical studies. in the first, the in- teractions between body deformations and perception variables are studied and a current correction process is proposed. in the second study, an electric exteroceptive feedback loop based on a direct current measurement method is designed and tested for obstacle avoidance.
recursive inverse dynamics of multibody systems with joints and wheels. null
three-dimensional extension of lighthill's large-amplitude elongated-body theory of fish locomotion . null
geometreically exact kirchhoff beam theory : application to cable dynamics. null
the pierre auger observatory iv: operation and monitoring. technical reports on operations and monitoring of the pierre auger observatory.
the pierre auger observatory iii: other astrophysical observations. astrophysical observations of ultra-high-energy cosmic rays with the pierre auger observatory.
the pierre auger observatory ii: studies of cosmic ray composition and hadronic interaction models. studies of the composition of the highest energy cosmic rays with the pierre auger observatory, including examination of hadronic physics effects on the structure of extensive air showers.
spectroscopy of $^{18}$na: bridging the two-proton radioactivity of $^{19}$mg. the unbound nucleus $^{18}$na, the intermediate nucleus in the two-proton radioactivity of $^{19}$mg, was studied by the measurement of the resonant elastic scattering reaction $^{17}$ne(p,$^{17}$ne)p performed at 4 a.mev. spectroscopic properties of the low-lying states were obtained in a r-matrix analysis of the excitation function. using these new results, we show that the lifetime of the $^{19}$mg radioactivity can be understood assuming a sequential emission of two protons via low energy tails of $^{18}$na resonances.
modeling the visual and motor control of steering with an eye to shared-control automation. null
shop and batch scheduling with constraints. solving a scheduling problem consists of organizing a set of tasks, that is assigning their starting and ending times and allocating resources such that all constraints are satisfied. in this thesis, we propose new constraint programming approaches for two categories of np-hard scheduling problems which are validated experimentally by the implementation of a set of new features within the constraint solver choco. in shop scheduling, a set of n jobs, consisting each of m tasks, must be processed on m distinct machines. a machine can process only one task at a time. the processing orders of tasks which belong to a job can vary (global order, order per job, no order). we consider the construction of non-preemptive schedules of minimal makespan. we first propose a study and a classification of different constraint models and search algorithms. then, we introduce a new flexible approach for these classical problems. a batch processing machine can process several jobs simultaneously as a batch. the starting and ending times of a task are the ones of the batch to which they belong. the studied problem consists of minimizing the maximal lateness for a batch processing machine on which a finite number of tasks of non-identical sizes must be scheduled. the sum of the sizes of the jobs that are in a batch should not exceed the capacity b of the machine. we propose, within this context, a constraint model based on a decomposition of the problem. then, we define a new optimization constraint based on the resolution of a relaxed problem enhanced by cost-based domain filtering techniques which improves the resolution.
spectral ratio: an observable to determine $k^{+}$ nucleus potential and $k^{+}$ n scattering cross section. here we aim to show that the ratio of the momentum spectra of $k^{+}$ at small transverse momentum measured for symmetric systems of different sizes can be such an observable.
$k^{+}$ and $k^{-}$ potentials in hadronic matter can be observed. we aim to show that k+ and k- spectra at low transverse momentum measured in light symmetric systems at around 2agev depend strongly on the k potential. the ratio of the spectra can allow therefore for a direct determination of the strength of the k+ as well as that of the k- potential in a hadronic environment.
in-medium effects on $k^{+}$ and $k^{-}$ spectra in lighter systems. we aim to explore the in-medium effects on the transverse momentum ($p_{t}$) spectra of $k^{+}$ and $k^{-}$ in lighter mass system $^{12}c+^{12}c$.
identified hadron compositions in p+p and au+au collisions at high transverse momenta at $\sqrt{s_{_{nn}}} = 200$ gev. we report transverse momentum ($p_{t} \leq15$ gev/$c$) spectra of $\pi^{\pm}$, $k^{\pm}$, $p$, $\bar{p}$, $k_{s}^{0}$, and $\rho^{0}$ at mid-rapidity in p+p and au+au collisions at $\sqrt{s_{_{nn}}}$ = 200 gev. perturbative qcd calculations are consistent with $\pi^{\pm}$ spectra in p+p collisions but do not reproduce $k$ and $p(\bar{p})$ spectra. the observed decreasing antiparticle-to-particle ratios with increasing $p_t$ provide experimental evidence for varying quark and gluon jet contributions to high-$p_t$ hadron yields. the relative hadron abundances in au+au at $p_{t}{}^{&gt;}_{\sim}8$ gev/$c$ are measured to be similar to the p+p results, despite the expected casimir effect for parton energy loss.
rotating hyperdeformed states in light nuclear systems. the existence of rotating quasimolecular hyperdeformed states formed in the entrance channel of capture reactions of light nuclei is predicted within a rotational liquid-drop model, including the nuclear proximity energy. the l-dependent capture barrier heights and positions, as well as the angular momentum, the energy, and the moment of inertia ranges of these very deformed high-spin states, are given for the reactions 13c + 13c, 16o + 16o, 28si + 12c, 28si + 16o, 24mg + 24mg, 28si + 24mg, 28si + 28si, 28si + 40ca, 40ca + 40ca, 40ca + 48ca, 48ca + 48ca, and 58ni + 58ni. analytical formulas are provided for any reaction between light nuclei.
particle-yield modification in jet-like azimuthal di-hadron correlations in pb-pb collisions at $\sqrt(s_nn)$ = 2.76 tev. the yield of charged particles associated with high-pt trigger particles (8 &lt; pt &lt; 15 gev/c) is measured with the alice detector in pb-pb collisions at sqrt(s_nn) = 2.76 tev relative to proton-proton collisions at the same energy. the conditional per-trigger yields are extracted from the narrow jet-like correlation peaks in azimuthal di-hadron correlations. in the 5% most central collisions, we observe that the yield of associated charged particles with transverse momenta pt &gt; 3 gev/c on the away-side drops to about 60% of that observed in pp collisions, while on the near-side a moderate enhancement of 20-30% is found.
transport coefficients in gluodynamics: from weak coupling towards the deconfinement transition. we study the ratio of bulk to shear viscosity in gluodynamics within a phenomenological quasiparticle model. we show that at large temperatures this ratio exhibits a quadratic dependence on the conformality measure as known from weak coupling perturbative qcd. in the region of the deconfinement transition, however, this dependence becomes linear as known from specific strongly coupled theories. the onset of the strong coupling behavior is located near the maximum of the scaled interaction measure. this qualitative behavior of the viscosity ratio is rather insensitive to details of the equation of state.
"disrupted in renal carcinoma 2" (dirc2) - a novel transporter of the lysosomal membrane - is proteolytically processed by cathepsin l. "disrupted in renal carcinoma 2" (dirc2) has been initially identified as a breakpoint spanning gene in a chromosomal translocation putatively associated with the development of renal cancer. the dirc2 protein belongs to the major facilitator superfamily (mfs) and has been previously detected by organellar proteomics as a tentative constituent of lysosomal membranes. in the present study, lysosomal residence of overexpressed as well as endogenous dirc2 was shown by several approaches. dirc2 is proteolytically processed into a n-glycosylated n-terminal and a non-glycosylated c-terminal fragment, respectively. proteolytic cleavage occurs in lysosomal compartments and critically depends on the activity of cathepsin l which was found to be indispensable for this process in murine embryonic fibroblasts. the cleavage site within dirc2 was mapped between amino acid residues 214 and 261 using internal epitope tags and is presumably located within the tentative fifth intralysosomal loop assuming the typical mfs topology. lysosomal targeting of dirc2 was demonstrated to be mediated by a n-terminal dileucine motif. by disrupting this motif, dirc2 can be redirected to the plasma membrane. finally, in a whole-cell electrophysiological assay based on heterologous expression of the targeting mutant at the plasma membrane of xenopus oocytes, the application of a complex metabolic mixture evokes an outward current associated with the surface expression of full-length dirc2. taken together, these data strongly support the idea that dirc2 is an electrogenic lysosomal metabolite transporter which is subjected to and presumably modulated by limited proteolytic processing.
estimation of absolute orientation for a bipedal robot: experimental results. this paper deals with a planar biped. the aim is the estimation, during the imbalance phases of a walking cyclic gait, of its absolute orientation by only using the measurement of the actuated joint variables. the main contribution is the experimental evaluation of an original finite-time convergent posture observer.
exploring membranes for controlling aspects. in most aspect-oriented languages, aspects have an unrestricted global view of computation. several approaches for aspect scoping and more strongly encapsulated modules have been formulated to restrict this controversial power of aspects. this paper proposes to leverage the concept of programmable membranes developed by boudol, schmitt and stefani, as a means to tame aspects by customizing the semantics of aspect weaving locally. membranes subsume previous proposals in a uniform framework. because membranes give structure to computation, they enable ﬂexible scoping of aspects; because they are programmable, they make it possible to deﬁne visibility and safety constraints, both for the advised program and for the aspects. we ﬁrst de- scribe membranes for aop without committing to any speciﬁc language design. in addition, we then illustrate an extension of aspectscheme with membranes, and explore the instantiation of programmable membranes in the kell calculus. the power and simplicity of membranes open interesting perspectives to unify multiple approaches that tackle the unrestricted power of aspect-oriented programming.
measurement of the neutrino velocity with the opera detector in the cngs beam. the opera neutrino experiment at the underground gran sasso laboratory has measured the velocity of neutrinos from the cern cngs beam over a baseline of about 730 km with much higher accuracy than previous studies conducted with accelerator neutrinos. the measurement is based on high-statistics data taken by opera in the years 2009, 2010 and 2011. dedicated upgrades of the cngs timing system and of the opera detector, as well as a high precision geodesy campaign for the measurement of the neutrino baseline, allowed reaching comparable systematic and statistical accuracies. an early arrival time of cngs muon neutrinos with respect to the one computed assuming the speed of light in vacuum of (60.7 \pm 6.9 (stat.) \pm 7.4 (sys.)) ns was measured. this anomaly corresponds to a relative difference of the muon neutrino velocity with respect to the speed of light (v-c)/c = (2.48 \pm 0.28 (stat.) \pm 0.30 (sys.)) \times 10-5.
parametrization of extended stabilizing controllers for continuous-time descriptor systems. this paper investigates the extended stabilization control problem for continuous-time descriptor systems (also refereed to as singular systems, implicit systems or generalized state-space systems) where unstable and nonproper weighting filters are used. in such nonstandard problem, the desired controller, called the extended stabilizing controller, has to satisfy two conditions simultaneously: (i) the closed-loop is admissible; (ii) only the internal stability of a part of the closed-loop is required. in terms of two generalized sylvester-type equations, necessary and sufficient conditions for the existence of an observer-based extended stabilizing controller are given. moreover, a parametrization of the class of extended stabilizing controllers is formulated.
on the thermal phase structure of qcd at vanishing chemical potentials. the hypothesis is investigated, that the thermal structure of qcd phases at and near zero chemical potentials is determined by long range coherence, inducing the gauge boson pair condensate, and its thermal extension, representing a fundamental order parameter. a consistent model for thermal behavior including interactions is derived in which the condensate does not produce any latent heat as it vanishes at the critical temperature inducing a second-order phase transition with respect to energy density neglecting eventual numerically small critical exponents. localization and delocalization of color fields are thus separated by a unique critical temperature.
stability of the fragments and thermalization at the peak centre-of-mass energy. we simulated the central reactions of nearly symmetric and asymmetric systems, for energies at which maximum production of intermediate mass fragments (imfs) occurred (e(c.m.)(peak)). this study was carried out using hard eos along with cugnon cross-section employing mstb method for clusterization. we studied the various properties of fragments. the stability of fragments was checked through persistence coefficient and gain term. the information about the thermalization and stopping in heavy-ion collisions was obtained via relative momentum, anisotropy ratio and rapidity distribution. we found that for a complete stopping of incoming nuclei very heavy systems are required. the mass dependence of various quantities (such as average and maximum central density, collision dynamics as well as the time zone for hot and dense nuclear matter) was also presented. in all cases (i.e., average and maximum central density, collision dynamics as well as the time zone for hot and dense nuclear matter) a power-law dependence was obtained.
$\rho^{0}$ photoproduction in auau collisions at $\sqrt{s_{nn}}$=62.4 gev with star. vector mesons may be photoproduced in relativistic heavy-ion collisions when a virtual photon emitted by one nucleus scatters from the other nucleus, emerging as a vector meson. the star collaboration has previously presented measurements of coherent $\rho^0$ photoproduction at center of mass energies of 130 gev and 200 gev in auau collisions. here, we present a measurement of the cross section at 62.4 gev; we find that the cross section for coherent $\rho^0$ photoproduction with nuclear breakup is $10.5\pm1.5\pm 1.6$ mb at 62.4 gev. the cross-section ratio between 200 gev and 62.4 gev is $2.8\pm0.6$, less than is predicted by most theoretical models. it is, however, proportionally much larger than the previously observed $15\pm 55$% increase between 130 gev and 200 gev.
scheduling of a speed-dating event. null
on the technician routing and scheduling problem. the technician routing and scheduling problem consists in routing and scheduling a crew of technicians in order to attend a set of service requests, subject to skill, tool, and spare part constraints. in this study we propose a formal definition of the problem and present a constructive heuristic and a large neighborhood search optimization algorithm.
a dynamic approach for the vehicle routing problem with stochastic demands. the vehicle routing problem with stochastic demands (vrpsd) is a variation of the classical capacitated vehicle routing problem (cvrp). in contrast to the deterministic cvrp, in the vrpsd the demand of each customer is modeled as a random variable and its realization is only known upon vehicle arrival to the customer site. under this uncertain scenario, a possible outcome is that the demand of a customer ends up exceeding the remaining capacity of the vehicle, leading to a route failure. in this study we will focus on the single vehicle vrpsd in which the fleet is limited to one vehicle with finite capacity, that can execute various routes sequentially. the present work is based on an adaptation of an optimization framework developed initially for the vehicle routing problem with dynamic customers (i.e., customers appear while the vehicles are executing their routes).
dynamic vehicle routing problems: state of the art and prospects. this scientific report summarizes the results of a literature review on dynamic vehicle routing problems. after a brief description of vehicle routing problems in general, a classification is introduced to distinguish between static and dynamic problems. then a more precise definition of dynamism is presented, supported by example of real-world applications of such problems. finally, a detailed study of the current state of the art in dynamic vehicle routing optimization is drawn.
a constraint programming approach for a batch processing problem with non-identical job sizes. null
constructive heuristics for the multicompartment vehicle routing problem with stochastic demands. the vehicle routing problem with stochastic demands (vrpsd) consists of designing transportation routes of minimal expected cost to satisfy a set of customers with random demands of known probability distribution. this paper tackles a generalization of the vrpsd known as the multicompartment vrpsd (mc-vrpsd), a problem in which each customer demands several products that, because of incompatibility constraints, must be loaded in independent vehicle compartments. to solve the problem, we propose three simple and effective constructive heuristics based on a stochastic programming with recourse formulation. one of the heuristics is an extension to the multicompartment scenario of a savings-based algorithm for the vrpsd; the other two are different versions of a novel look-ahead heuristic that follows a route-first, cluster-second approach. in addition, to enhance the performance of the heuristics these are coupled with a post-optimization procedure based on the classical 2-opt heuristic. the three algorithms were tested on instances of up to 200 customers from the mc-vrpsd and vrpsd literature. the proposed heuristics unveiled 26 and 12 new best known solutions for a set of 180 mc-vrpsd problems and a 40-instance testbed for the vrpsd, respectively.
clone evolution: a systematic review. detection of code clones - similar or identical source code fragments - is of concern both to researchers and to practitioners. an analysis of the clone detection results for a single source code version provides a developer with information about a discrete state in the evolution of the software system. however, tracing clones across multiple source code versions permits a clone analysis to consider a temporal dimension. such an analysis of clone evolution can be used to uncover the patterns and characteristics exhibited by clones as they evolve within a system. developers can use the results of this analysis to understand the clones more completely, which may help them to manage the clones more effectively. thus, studies of clone evolution serve a key role in understanding and addressing issues of cloning in software. in this paper we present a systematic review of the literature on clone evolution. in particular, we present a detailed analysis of 30 relevant papers that we identified in accordance with our review protocol. the review results are organized to address three research questions. through our answers to these questions, we present the methods that researchers have used to study clone evolution, the patterns that researchers have found evolving clones to exhibit, and the evidence that researchers have established regarding the extent of inconsistent change undergone by clones during software evolution. overall, the review results indicate that while researchers have conducted several empirical studies of clone evolution, there are contradictions among the reported findings, particularly regarding the lifetimes of clone lineages and the consistency with which clones are changed during software evolution. we identify human-based empirical studies and classification of clone evolution patterns as two areas in particular need of further work.
harmonic decomposition of two-particle angular correlations in pb--pb collisions at $\mathbf{\sqrt{s_{\rm nn}} = 2.76}$ tev. angular correlations between unidentified charged trigger ($t$) and associated ($a$) particles are measured by the alice experiment in \pbpb\ collisions at $\snn=2.76$ tev for transverse momenta $0.25 &lt; p_{t}^{t,\, a} &lt; 15$ gev/$c$, where $p_{t}^t &gt; p_{t}^a$. the shapes of the pair correlation distributions are studied in a variety of collision centrality classes between 0 and 50% of the total hadronic cross section for particles in the pseudorapidity interval $|\eta| &lt; 1.0$. distributions in relative azimuth $\delta\phi \equiv \phi^t - \phi^a$ are analyzed for $|\delta\eta| \equiv |\eta^t - \eta^a| &gt; 0.8$, and are referred to as "long-range correlations". fourier components $v_{n\delta} \equiv &lt;\cos(n\delta\phi)&gt;$ are extracted from the long-range azimuthal correlation functions. if the particle pair correlation arises dominantly from production mechanisms that distribute according to a common plane of symmetry, then the pair $\vnd$ coefficients are expected to factorize as the product of single-particle anisotropies $v_n (\pt)$, i.e. $v_{n\delta}(p_{t}^t, p_{t}^a) = v_n(p_{t}^t) \, v_n(p_{t}^a)$. this expectation is tested for $1 \leq n \leq 5$ by applying a global fit of all $\vnd (p_{t}^t, p_{t}^a)$ to obtain the best values $\vngf (\pt)$. it is found that for $2 \leq n \leq 5$, the factorization holds for associated particle momenta up to $\pta \sim 3$-4 gev/$c$, with a trend of increasing deviation between the data and the factorization hypothesis as $p_{t}^t$ and $p_{t}^a$ are increased or as collisions become more peripheral. $v_{1\delta}$ does not factorize precisely at any $\pt$ or centrality, as indicated by the lack of a good global fit over the full $\ptt, \pta$ range. the $\vngf$ values for $2 \leq n \leq 5$ from the global fit are in close agreement with previous measurements.
cooperative dynamic scheduling of virtual machines in distributed systems. cloud computing aims at outsourcing data and applications hosting and at charging clients on a per-usage basis. these data and ap- plications may be packaged in virtual machines (vm), which are them- selves hosted by nodes, i.e. physical machines. consequently, several frameworks have been designed to manage vms on pools of nodes. unfortunately, most of them do not efficiently address a common objective of cloud providers: maximizing system utilization while ensuring the quality of service (qos). the main reason is that these frameworks schedule vms in a static way and/or have a centralized design. in this article, we introduce a framework that enables to schedule vms cooperatively and dynamically in distributed systems. we evaluated our prototype through simulations, to compare our approach with the cen- tralized one. preliminary results showed that our scheduler was more reactive. as future work, we plan to investigate further the scalability of our framework, and to improve reactivity and fault-tolerance aspects.
component types qualification in java legacy code driven by communication integrity rules. to fight software architectural erosion, new languages and development methods are proposed that make explicit the architectural decisions in the source code for the benefit of the programmers. component based software engineering is a way to improve software modularization and to embed architectural concerns. to restructure legacy code with components in mind we need tools to asses the compliance with component programming principles. the communication integrity property is one of the major principles to implement software architectures. however, there is a lack of tooling for assessing the quality of components codes. to cope with this issue, we define a component model in java and a tool for identifying component types. the tool relies on a set of rules to statically check potential violations of the communication integrity property in java source code. we illustrate its application on a case study and report the results of our experiments with the tool.
managing information flow in spl development processes. null
introduction. null
aspect-oriented, model-driven software product lines the ample way. software product lines provide a systematic means of managing variability in a suite of products. they have many benefits but there are three major barriers that can prevent them from reaching their full potential. first, there is the challenge of scale: a large number of variants may exist in a product line context and the number of interrelationships and dependencies can rise exponentially. second, variations tend to be systemic by nature in that they affect the whole architecture of the software product line. third, software product lines often serve different business contexts, each with its own intricacies and complexities. the ample (http://www.ample-project.net/) approach tackles these three challenges by combining advances in aspect-oriented software development and model-driven engineering. the full suite of methods and tools that constitute this approach are discussed in detail in this edited volume and illustrated using three real-world industrial case studies.
nuclear reaction measurements of 95mev/u 12c interactions on pmma for hadrontherapy. the ion dose deposition in tissues is characterized by a favorable depth dose profile (i.e. bragg peak) and a small lateral spread. in order to keep these benefits of ions in cancer treatments, a very high accuracy is required on the dose deposition (±3%). for given target stoechiometry and geometry, the largest uncertainty on the physical dose deposition is due to the ion nuclear fragmentation. we have performed an experiment at ganil with a 95mev/u 12c beam on thick tissue equivalent pmma targets (thicknesses: 5, 10, 15, 20 and 25mm). the main goals of this experiment are to provide experimental fragmentation data for benchmarking the physical models used for treatment planning. production rates, energy and angular distributions of charged fragments have been measured. the purpose of this paper is to present the results of this experiment.
cloning in dsls: experiments with ocl. code cloning (i.e., similar code fragments) in general purpose languages has been a major focus of the research community. for domain specific languages (dsls), cloning related to domain-specific graphical languages has also been considered. this paper focuses on domain-specific textual languages in an effort to evaluate cloning in these dsls where instances of such dsls allow for less code to express domain-specific features, but potentially more frequently used code constructs. we suggest potential application scenarios of using clone detection for the maintenance of dsl code. we introduce a clone detection mechanism using a model driven engineering (mde) based approach to evaluate the extent of cloning in an initial dsl (i.e., the object constraint language (ocl)). the evaluation reveals the existence of cloning in ocl, which suggests the relevance and potential applications of clone detection and analysis in dsls.
model for a sensor inspired by electric fish. this article reports the first results from a programme of work aimed at developing a swimming robot equipped with electric sense. after having presented the principles of a bio- inspired electric sensor, now working, we will build the models for electrolocation of objects that are suited to this kind of sensor. the produced models are in a compact analytical form in order to be tractable on the onboard computers of the future robot. these models are tested by comparing them with numerical simulations based on the boundary elements method. the results demonstrate the feasibility of the approach and its compatibility with online objects electrolocation, another parallel programme of ours.
assessment of an effective quasirelativistic methodology designed to study astatine chemistry in aqueous solution. a cost-effective computational methodology designed to study astatine (at) chemistry in aqueous solution has been established. it is based on two-component spin-orbit density functional theory calculations and solvation calculations using the conductor-like polarizable continuum model in conjunction with specific astatine cavities. theoretical calculations are confronted with experimental data measured for complexation reactions between metallic forms of astatine (at+ and ato+) and inorganic ligands (cl-, br- and scn-). for each reaction, both 1:1 and 1:2 complexes are evidenced. the experimental trends regarding the thermodynamic constants (k) can be reproduced qualitatively and quantitatively. the mean signed error on computed log k values is -0.4, which corresponds to a mean signed error smaller than 1 kcal mol-1 on free energies of reaction. theoretical investigations show that the reactivity of cationic species of astatine is highly sensitive to spin-orbit coupling and solvent effects. at the moment, the presented computational methodology appears to be the only tool to gain an insight into astatine chemistry at a molecular level.
the pierre auger observatory v: enhancements. ongoing and planned enhancements of the pierre auger observatory.
the pierre auger observatory i: the cosmic ray energy spectrum and related measurements. studies of the cosmic ray energy spectrum at the highest energies with the pierre auger observatory.
state feedback h2 optimal controllers under regulation constraints for descriptor systems. this paper is concerned with a non-standard multi-objective state feedback control problem for continuous descriptor systems. in this problem an output is to be regulated asymptotically with presence of an infinite-energy exo-system, while a desired h2 performance from a finite external disturbance to a tracking error has also to be satisfied. thanks to the descriptor framework, not only unstable but also nonproper behaviors can be treated. a parametrization of all optimal dynamic and static controllers solving the proposed multi-objective control problem is given. moreover, an application to the non-standard lqr problem is investigated. a numerical example shows the efficiency of the proposed results.
how to deal with your it legacy? reverse engineering using models - modisco in a nutshell!. free download of the full journal issue from http://jaxenter.com/java-tech-journal/jtj-2011-06.
the lateral trigger probability function for the ultra-high energy cosmic ray showers detected by the pierre auger observatory. in this paper we introduce the concept of lateral trigger probability (ltp) function, i.e., the probability for an extensive air shower (eas) to trigger an individual detector of a ground based array as a function of distance to the shower axis, taking into account energy, mass and direction of the primary cosmic ray. we apply this concept to the surface array of the pierre auger observatory consisting of a 1.5 km spaced grid of about 1600 water cherenkov stations. using monte carlo simulations of ultra-high energy showers the ltp functions are derived for energies in the range between 1017 and 1019 ev and zenith angles up to 65°. a parametrization combining a step function with an exponential is found to reproduce them very well in the considered range of energies and zenith angles. the ltp functions can also be obtained from data using events simultaneously observed by the fluorescence and the surface detector of the pierre auger observatory (hybrid events). we validate the monte-carlo results showing how ltp functions from data are in good agreement with simulations.
evolving security requirements in multi-layered service-oriented-architectures. due to today's rapidly changing corporate environments, business processes are increasingly subject to dynamic configuration and evolution. the evolution of new deployment architectures, as illustrated by the move towards mobile platforms and the internet of services, and the introduction of new security regulations (imposed by national and international regulatory bodies, such as sox4 or basel5) are an im- portant constraint in the design and development of business processes. in such context, it is not sufficient to apply the corresponding adapta- tions only at the service orchestration or at the choreography level; there is also the need for controlling the impact of new security requirements to several architectural layers, specially in cloud computing, where the notion of platforms as services and infrastructure as services are fun- damental. in this paper we survey several research questions related to security cross-domain and cross-layer security functionality in service oriented architectures, from an original point of view. we provide the first insights on how a general service model empowered with aspect oriented programming capabilities can provide clean modularization to such cross-cutting security concerns.
on qgp formation in pp collisions at 7 tev. the possibility of qgp formation in central pp collisions at ultra-high collision energy is discussed. centrality-dependent $\pt$-spectra and (pseudo)rapidity spectra of thermal photons (charged hadrons) from pp collisions at 7 tev are presented (addressed). minimal-bias $\pt$-spectrum of direct photons and charged hadrons is compared under the framework with and without hydrodynamical evolution process.
quarkonium production measurements with the alice detector at the lhc. in this new energy regime, quarkonium provides a unique probe to study the properties of the high-density, strongly interacting system formed in the early stages of high-energy heavy-ion collisions. in alice, quarkonium states are reconstructed down to p_t=0 via their mu+mu- decay channel in the muon spectrometer (2.5.
a general approach for optimizing regular criteria in the job-shop scheduling problem. even though a very large number of solution methods has been developed for the job-shop scheduling problem, a majority has been designed for the makespan criterion. in this paper, we propose a general approach for optimizing any regular criterion in the job-shop scheduling problem. the approach is a local search method that uses a disjunctive graph model and neighborhoods generated by swapping critical arcs. the connectivity property of the neighborhood structure is proved and a novel efficient method for evaluating moves is presented. besides its generality, another prominent advantage of the proposed approach is its simple implementation that only requires to tune the range of one parameter. extensive computational experiments carried out on various criteria (makespan, total weighted flow time, total weighted tardiness, weighted sum of tardy jobs, maximum tardiness) show the efficiency of the proposed approach. best results were obtained for some problem instances taken from the literature.
revisiting scaling properties of medium-induced gluon radiation. discussing the general case of a hard partonic production process, we show that the notion of parton energy loss is not always sufficient to fully address medium-induced gluon radiation. the broader notion of gluon radiation associated to a hard process has to be used, in particular when initial and final state radiation amplitudes interfere, making the medium-induced radiated energy different from the energy loss of any well-identified parton. our arguments are first presented in an abelian qed model, and then applied to large-xf quarkonium hadroproduction. in this case, we show that the medium-induced radiated energy is qualitatively similar (but not identical) to the radiative energy loss of an "asymptotic massive parton" undergoing transverse momentum broadening when travelling through the nucleus. in particular, it scales as the incoming parton energy, which suggests to reconsider gluon radiation as a possible explanation of large-xf quarkonium suppression in p-a collisions. we expect a similar effect in open heavy-flavour and possibly light-hadron hadroproduction at large xf, depending on the precise definition of the nuclear suppression factor in the latter case.
considering clay rock heterogeneity in radionuclide retention. null
from uml profiles to emf profiles and beyond. null
theory and practice of model transformations - 4th international conference, icmt 2011, zurich, switzerland, june 27-28, 2011. proceedings. null
spy on your models. emf is now widely used by various kinds of systems based on eclipse modeling project components. as emf models become the heart of these systems, it becomes critical to be able to inspect them very precisely. it is the main objective of the modisco model browser to provide a good insight of the content of any emf model, especially when these models are large and complex. this talk will present the main features of this tool: 1) directly access to instances of a given eclass 2) navigate through the relations between model elements 3) dynamically customize the rendering of model elements 4) edit the model elements with a tabular view 5) integrate the browser components with the common navigation framework.
cartesian stiffness matrix of manipulators with passive joints: analytical approach. the paper focuses on stiffness matrix computation for manipulators with passive joints. it proposes both explicit analytical expressions and an efficient recursive procedure that are applicable in general case and allow obtaining the desired matrix either in analytical or numerical form. advantages of the developed technique and its ability to produce both singular and non-singular stiffness matrices are illustrated by application examples that deal with stiffness modeling of two stewart-gough platforms.
lightweight verification of executable models. executable models play a key role in many development methods by facilitating the immediate simulation/implementation of the software system under development. this is possible because executable models include a fine-grained specification of the system behaviour. unfortunately, a quick and easy way to check the correctness of behavioural specifications is still missing, which compromises their quality (and in turn the quality of the system generated from them). in this paper, a lightweight verification method to assess the strong executability of fine-grained behavioural specifications (i.e. operations) at design-time is provided. this method suffices to check that the execution of the operations is consistent with the integrity constraints defined in the structural model and returns a meaningful feedback that helps correcting them otherwise.
two basic correctness properties for atl transformations: executability and coverage. model transformations play a cornerstone role with the emergence of model driven engineering (mde), where models are transformed from higher to lower levels of abstraction. unfortunately, a quick and easy way to check the correctness of model transformations is still missing, which compromises their quality (and in turn, the quality of the target models generated from them). in this paper we propose a lightweight and efficient method that performs a static analysis of the atl rules with respect to two correctness properties we define: (1) weak executability, which determines if there is some scenario in which an atl rule can be safely applied without breaking the target metamodel integrity constraints; and (2) coverage, which ensures a set of atl rules allow addressing all elements of the source and target metamodels. in both cases, our method returns meaningful feedback that helps repairing the possible detected inconsistencies.
moscript: a dsl for querying and manipulating model repositories. abstract. growing adoption of model-driven engineering has hugely increased the number of modelling artefacts (models, metamodels, trans- formations, ...) to be managed. therefore, development teams require ap- propriate tools to search and manipulate models stored in model repos- itories, e.g. to find and reuse models or model fragments from previous projects. unfortunately, current approaches for model management are either ad-hoc (i.e., tied to specific types of repositories and/or models), do not support complex queries (e.g., based on the model structure and its relationship with other modelling artefacts) or do not allow the manipu- lation of the resulting models (e.g., inspect, transform). this hinders the probability of efficiently reusing existing models or fragments thereof. in this paper we introduce moscript, a textual domain-specific language for model management. with moscript, users can write scripts containing queries (based on model content, structure, relationships, and behaviour derived through on-the-fly simulation) to retrieve models from model repositories, manipulate them (e.g., by running transformations on sets of models), and store them back in the repository. moscript relies on the megamodeling concept to provide a homogeneous model-based interface to heterogeneous repositories.
lazy execution of model-to-model transformations. the increasing adoption of model-driven engineering in in- dustrial contexts highlights scalability as a critical limitation of several mde tools. most of the current model-to-model transformation engines have been designed for one-shot translation of input models to output models, and present efficiency issues when applied to very large models. in this paper, we study the application of a lazy-evaluation approach to model transformations. we present a lazy execution algorithm for atl, and we empirically evaluate a prototype implementation. with it, the elements of the target model are generated only when (and if) they are accessed, enabling also transformations that generate infinite target models. we achieve our goal on a significant subset of atl by extending the atl compiler.
extending atl for native uml profile support: an experience report 49-62. with the rise of model-driven engineering (mde) the ap- plication field of model transformations broadens drastically. current model transformation languages provide appropriate support for stan- dard mde scenarios such as model-to-model transformations specified between metamodels. however, for other transformation scenarios often the escape to predefined apis for handling specific model manipulations is required such as is the case for supporting uml profiles in transforma- tions. thus, the need arises to extend current transformation languages for natively supporting such additional model manipulations. in this paper we report on extending atl for natively supporting uml profiles in transformations. the extension is realized by providing an extended atl syntax comprising keywords for handling uml profiles which is reduced by a preprocessor based on a higher-order transfor- mation (hot) again to the standard atl syntax. in particular, we elab- orate on our methodology of extending atl by presenting the extension process step-by-step as well as reporting on lessons learned. with this experience report we aim at providing design guidelines for extending atl as well as stimulating the research of providing further extensions for atl.
investigation of alendronate-doped apatitic cements as a potential technology for the prevention of osteoporotic hip fractures: critical influence of the drug introduction mode on the in vitro cement properties. null
product line implementation with ecaesarj. this chapter takes a closer look at the difficulties of feature-oriented modularisation of product lines and demonstrate how a better modularisation can be achieved with the ecaesarj programming language, through a type-safe and stable decomposition of a broad spectrum of software abstractions: classes, methods, events, and state machines, based on late binding and mixin composition.
representing clones in a localized manner. code clones (i.e., duplicate sections of code) can be scattered throughout the source files of a program. manually evaluating a group of such clones requires observing each clone in its original location (i.e., opening each file and finding the source location of each clone), which can be a time-consuming process. as an alternative, this paper introduces a technique that localizes the representation of code clones to provide a summary of the properties of two or more clones in one location. in our approach, the results of a clone detection tool are analyzed in an automated manner to determine the properties (i.e., similarities and differences) of the clones. these properties are visualized directly within the source editor. the localized representation is realized as part of the features of an eclipse plug-in called cedar.
virtual composition of emf models. model composition is a very important modeling task as it allows to combine various perspectives of a system (represented by various models) into a single specialized view (a composed model). several approaches have been proposed to tackle this problem, but they present some important limitations concerning efficiency, interoperability, and/or synchronization issues (mainly due to the element cloning mechanism used to create the composed model). in this paper we propose a new model composition method based on the virtualization of the composition mechanism. in our approach, the composed model is in fact created as a virtual model that redirects all its model access and manipulation requests directly to the set of base models from which it was generated. this is done transparently for the designer. our mechanism improves the composition process with relation to the limitations mentioned above. the solution has been implemented and validated in a prototype tool on top of emf.
static analysis of aspect interaction and composition in component models. component based software engineering and aspect orientation are claimed to be two complementary approaches. while the former ensures the modularity and the reusability of software entities, the latter enables the modularity of crosscutting concerns that cannot be modularized by regular components. nowadays, several approaches and frameworks are dedicated to integrate aspects into component models. however, when several aspects are woven, interferences may appear which results on undesirable behaviors. the contribution of this paper is twofold. first, we show how aspectualized component models can be formally modeled in uppaal model checker in order to detect potential interferences among aspects. second, we provide an extendible catalog of composition operators used for aspect composition. we illustrate our general approach with an airport internet service example.
composable controllers in fractal: implementation and interference analysis. fractal component model provides controllers for adding extra-functional capabilities to component behaviors. however, controllers may interfere one with another and their composition is still a challenge. in this article, we extend fractal with a support for composing controllers with reusable operators. then, we discuss how to formally model and analyze, in uppaal, fractal systems with several controllers. this enables us to detect when controllers interfere and to check whether their composition is interference-free.
architecture for the next generation system management tools. to get more results or greater accuracy, computational scientists execute their applications on distributed computing platforms such as clusters, grids and clouds. these platforms are different in terms of hardware and software resources as well as locality: some span across multiple sites and multiple administrative domains whereas others are limited to a single site/domain. as a consequence, in order to scale their applications up the scientists have to manage technical details for each target platform. from our point of view, this complexity should be hidden from the scientists who, in most cases, would prefer to focus on their research rather than spending time dealing with platform configuration concerns. in this article, we advocate for a system management framework that aims to automatically setup the whole run-time environment according to the applications' needs. the main difference with regards to usual approaches is that they generally only focus on the software layer whereas we address both the hardware and the software expectations through a unique system. for each application, scientists describe their requirements through the definition of a virtual platform (vp) and a virtual system environment (vse). relying on the vp/vse definitions, the framework is in charge of: (i) the configuration of the physical infrastructure to satisfy the vp requirements, (ii) the setup of the vp, and (iii) the customization of the execution environment (vse) upon the former vp. we propose a new formalism that the system can rely upon to successfully perform each of these three steps without burdening the user with the specifics of the configuration for the physical resources, and system management tools. this formalism leverages goldberg's theory for recursive virtual machines by introducing new concepts based on system virtualization (identity, partitioning, aggregation) and emulation (simple, abstraction). this enables the definition of complex vp/vse configurations without making assumptions about the hardware and the software resources. for each requirement, the system executes the corresponding operation with the appropriate management tool. as a proof of concept, we implemented a first prototype that currently interacts with several system management tools (e.g. oscar, the grid'5000 toolkit, and xtreemos) and that can be easily extended to integrate new resource brokers or cloud systems such as nimbus, opennebula or eucalyptus for instance.
using traceability links and higher order transformations for easing regression testing of web applications. null
anisotropy and chemical composition of ultra-high energy cosmic rays using arrival directions measured by the pierre auger observatory. the pierre auger collaboration has reported evidence for anisotropy in the distribution of arrival directions of the cosmic rays with energies e &gt; eth = 5.5 × 1019 ev. these show a correlation with the distribution of nearby extragalactic objects, including an apparent excess around the direction of centaurus a. if the particles responsible for these excesses at e &gt; eth are heavy nuclei with charge z, the proton component of the sources should lead to excesses in the same regions at energies e/z. we here report the lack of anisotropies in these directions at energies above eth/z (for illustrative values of z = 6,13,26). if the anisotropies above eth are due to nuclei with charge z, and under reasonable assumptions about the acceleration process, these observations imply stringent constraints on the allowed proton fraction at the lower energies.
thomas-fermi approximation to pairing in finite fermi systems. the weak coupling regime. we present a new semiclassical theory for describing pairing in finite fermi systems. it is based on taking the $\hbar \to 0$, i.e. thomas-fermi, limit of the gap equation written in the basis of the mean field (weak coupling). in addition to the position dependence of the fermi momentum, the size dependence of the pairing force is also taken into account in this theory. along isotopic chains the thomas-fermi gaps average the well known arch structure shown by the quantal gaps. this structure can be almost recovered in our formalism if some shell fluctuations are included in the level density. we point out that at the drip line nuclear pairing is strongly reduced. this fact is illustrated with the behavior of the gap in the inner crust of neutron stars.
multi-agent electro-location and the among constraint. in this paper, we give a new approach for the localization of several autonomous fish robots equipped with the electric sense. the approach is based on interval arithmetic and constraint programming. it introduces a generalization of the among constraint which was used so far in the different context of resource allocation in logistics. we prove that the bound consistency for a conjunction of among constraints with interval domains is np-complete in the vector case and polynomial in the one-dimensional case. although it was designed for electric fish robots, this work is not restricted to this type of systems. the approach can be easily extended to other multi-agent systems with low-range sensing.
higher harmonic anisotropic flow measurements of charged particles in pb-pb collisions at 2.76 tev. we report on the first measurement of the triangular v3, quadrangular v4, and pentagonal v5 charged particle flow in pb-pb collisions at 2.76 tev measured with the alice detector at the cern large hadron collider. we show that the triangular flow can be described in terms of the initial spatial anisotropy and its fluctuations, which provides strong constraints on its origin. in the most central events, where the elliptic flow v2 and v3 have similar magnitude, a double peaked structure in the two-particle azimuthal correlations is observed, which is often interpreted as a mach cone response to fast partons. we show that this structure can be naturally explained from the measured anisotropic flow fourier coefficients.
influence of haptic communication on a shared manual task in a collaborative virtual environment. with the advent of new haptic feedback devices, researchers are giving serious consideration to the incorporation of haptic communication in collaborative virtual environments. for instance, haptic interactions based tools can be used for medical and related education whereby students can train in minimal invasive surgery using virtual reality before approaching human subjects. to design virtual environments that support haptic communication, a deeper understanding of humans' haptic interactions is required. in this paper, human's haptic collaboration is investigated. a collaborative virtual environment was designed to support performing a shared manual task. to evaluate this system, 60 medical students participated to an experimental study. participants were asked to perform in dyads a needle insertion task after a training period. results show that compared to conventional training methods, a visual-haptic training improves user's collaborative performance. in addition, we found that haptic interaction influences the partners' verbal communication when sharing haptic information. this indicates that the haptic communication training changes the nature of the users' mental representations. finally, we found that haptic interactions increased the sense of copresence in the virtual environment: haptic communication facilitates users' collaboration in a shared manual task within a shared virtual environment. design implications for including haptic communication in virtual environments are outlined.
the journey of biorthogonal logical relations to the realm of assembly code. logical relations appeared to be very fruitful for the development of modular proofs of compiler correctness. in this field, logical relations are parametrized by a high-level type system, and are even sometimes directly relating low level pieces of code to high-level programs. all those works rely crucially on biorthogonality to get extensionality and compositionality properties. but the use of biorthogonality in the definitions also complicates matters when it comes to operational correctness. most of the time, such correctness results amount to show an unfolding lemma that makes reduction more explicit than in a biorthogonal definition. unfortunately, unfolding lemmas are not easy to derive for rich languages and in particular for assembly code. in this paper, we focus on three different situations that enable to reach step-by-step the assembly code universe: the use of curry-style polymorphism, the presence of syntactical equality in the language and finally an ideal assembly code with a notion of code pointer.
k+ and k- potentials in hadronic matter are observable quantities. the comparison of $k^+$ and $k^-$ spectra at low transverse momentum in light symmetric heavy ion reactions at energies around 2 agev allows for a direct experimental determination of the strength of the $k^+$ as well as of t he $k^-$ nucleus potential. other little known or unknown input quantities like the production or rescattering cross sections of $k^+$ and $k^-$ mesons do not spoil this signal. this result, obtained by simulations of these reactio ns with the isospin quantum molecular dynamics (iqmd) model, may solve the longstanding question of the behaviour of the $k^-$ in hadronic matter and especially whether a $k^-$ condensate can be formed in heavy ion collisions.
ground and excited charmonium state production in p+p collisions at sqrt(s)=200 gev. we report on charmonium measurements [j/psi(1s), psi'(2s), and chi_c(1p)] in p+p collisions at sqrt(s)=200 gev. we find that the fraction of j/psi coming from the feed-down decay of psi' and chi_c in the midrapidity region ($|\eta|&lt;0.35$) is 9.6+/-2.4% and 32+/-9%, respectively. we also report new, higher statistics p_t and rapidity dependencies of the j/psi yield via dielectron decay in the same midrapidity range and at forward rapidity (1.2&lt;|eta|&lt;2.4) via dimuon decay. these results are compared with measurements from other experiments and discussed in the context of current charmonium production models.
modelling and solving a practical flexible job-shop scheduling problem with blocking constraints. this paper presents a study of a practical job-shop scheduling problem modelled and solved when helping a company to design a new production workshop. the main characteristics of the problem are that some resources are flexible, and blocking constraints have to be taken into account. the problem and the motivation for solving it are detailed. the modelling of the problem and the proposed resolution approach, a genetic algorithm, are described. numerical experiments using real data are presented and analysed. we also show how these results were used to support choices in the design of the workshop.
rapidity and transverse momentum dependence of inclusive j/psi production in pp collisions at sqrt(s) = 7 tev. the alice experiment at the lhc has studied inclusive j/psi production at central and forward rapidities in pp collisions at sqrt(s) = 7 tev. in this letter, we report on the first results obtained detecting the j/psi through its dilepton decay into e+e- and mu+mu- pairs in the rapidity range |y|&lt;0.9 and 2.5.
typing artifacts in megamodeling. null
a model-driven framework for aspect weaver construction. null
a probabilistic study of bound consistency for the alldifferent constraint. this paper introduces a mathematical model for bound consistency of the con- straint alldifferent. it allows us to compute the probability that the filtering algorithm effectively removes at least one value in the variable domains. a complete study of the bound consistency properties is then proposed. it identifies several behaviors depending on some macroscopic quantities related to the variables and the domains. finally, it is shown that the probability for an alldifferent constraint to be bound consistent can be asymptotically estimated in constant time. the experiments illustrate that the precision is good enough for a practical use in constraint programming.
sensitivity of the transverse flow towards symmetry energy. we study the sensitivity of transverse flow towards symmetry energy in the fermi energy region as well as at high energies. we find that transverse flow is sensitive to symmetry energy as well as its density dependence in the fermi energy region. we also show that the transverse flow can address the symmetry energy at densities about twice the saturation density, however it shows the insensitivity towards the symmetry energy at densities $\rho/\rho_{0}$ $&gt;$ 2. the mechanism for the sensitivity of transverse flow towards symmetry energy as well as its density dependence is also discussed.
semiclassical description of average pairing properties in nuclei. we present a new semiclassical theory for describing pairing in finite fermi systems. it is based on taking the ħ → 0, i.e. thomas-fermi, limit of the gap equation written in the basis of the mean field (weak coupling). in addition to the position dependence of the fermi momentum, the size dependence of the matrix elements of the pairing force is also taken into account in this theory. an example typical for the nuclear situation shows the improvment of this new approach over the standard local density approximation. we also show that if in this approach some shell fluctuations are introduced in the level density, the arch structure displayed by the quantal gaps along isotopic chains is almost recovered. we also point out that in heavy drip line nuclei pairing is strongly reduced.
ridges and soft jet components in untriggered di-hadron correlations in pb+pb collisions at 2.76 tev. we study untriggered di-hadron correlations in pb+pb at 2.76 tev, based on an event-by-event simulation of a hydrodynamic expansion starting from flux tube initial conditions. the correlation function shows interesting structures as a function of the pseudorapidity difference $\delta\eta$ and the azimuthal angle difference $\delta\phi$, in particular comparing different centralities. we can clearly identify a peak-like nearside structure associated with very low momentum components of jets for peripheral collisions, which disappears towards central collisions. on the other hand, a very broad ridge structure from asymmetric flow seen at central collisions, gets smaller and finally disappears towards peripheral collisions.
density dependence of symmetry energy and collective transverse in-plane flow. we study the sensitivity of the collective transverse in-plane flow to the symmetry energy and its density dependence at fermi energies and higher incident energies. we find that collective transverse in-plane flow is sensitive to the symmetry energy and its density dependence at fermi energies whereas it shows insensitivity at higher incident energies.
bose-einstein correlations in a fluid dynamical scenario for prototon-proton scattering at 7 tev. using a fluid dynamical scenario for pp scattering at 7 tev, we compute correlation functions for pi+pi+ paires. femtoscopic radii are extracted based on threedimensional parameterizations of the correlation functions. we study the radii as a function of the transverse momenta of the pairs, for different multiplicity classes, corresponding to recent experimental results from alice. we find the same decrease of the radii with k_t, more an more pronounced with increasing multiplicity, but absent for the lowest multiplicities. in the model we understand this as transition from string expansion (low multiplicity) towards a threedimensional hydrodynamical expansion (high multiplicity).
decomposing logical relations with forcing. logical relations have now the maturity to deal with program equivalence for realistic programming languages with features likes recursive types, higher-order references and first-class continuations. however, such advanced logical relations---which are defined with technical developments like step-indexing or heap abstractions using recursively defined worlds---can make a proof tedious. a lot of work has been done to hide step-indexing in proofs, using gödel-löb logic. but to date, step-indexes have still to appear explicitely in particular constructions, for instance when building recursive worlds in a stratified way. in this paper, we go one step further, proposing an extension of abadi-plotkin logic with forcing construction which enables to encapsulate reasoning about step-indexing or heap in different layers. moreover, it gives a uniform and abstract management of step-indexing for recursive terms or types and for higher-order references.
refining models with rule-based model transformations. several model-to-model transformation languages have been primarily designed to easily address the syntactic and semantic translation of read-only input models towards write-only output models. while this approach has been proven successful in many practical cases, it is not directly applicable to transformations that need to modify their source models, like refactorings. in this paper we investigate the application of a model-to-model transformation language to in-place transformations, by providing a systematic view of the problem, comparing alternative solutions and proposing a transformation semantics to address this problem in atl.
first detection of extensive air showers by the trend self-triggering radio experiment. an antenna array devoted to the autonomous radio-detection of high energy cosmic rays is being deployed on the site of the 21 cm array radio telescope in xinjiang, china. thanks in particular to the very good electromagnetic environment of this remote experimental site, self-triggering on extensive air showers induced by cosmic rays has been achieved with a small scale prototype of the foreseen antenna array. we give here a detailed description of the detector and present the first detection of extensive air showers with this prototype.
structured and flexible gray-box composition using invasive distributed patterns. the evolution of complex distributed software systems often requires intricate composition operations in order to adapt or add functionalities, to react to unanticipated changes, or to apply performance improvements that cannot be modularized in terms of existing services and components. these evolutions often need controlled access to selected parts of the implementation, e.g., to manage exceptional situations and crosscutting within services and their compositions. however, existing composition techniques typically support only interface-level (black-box) composition or arbitrary access to the implementation (gray-box or white-box composition). in this paper, we present a structured approach to the composition of complex software systems that require invasive modifications. concretely, we provide three contributions: (i) we present a small kernel composition language for structured gray-box composition using invasive distributed patterns; (ii) we motivate that gray-box composition approaches should be defined and evaluated in terms of the flexibility and control they provide, a notion of degrees of invasiveness is introduced to help assess this trade-off; (iii) we apply our approach to a new case study of evolution and evaluate it in the context of two previous studies involving two real-world software systems: benchmarking of grid algorithms with nasgrid and transactional replication with jboss cache. as a main result, we show that gray-box composition using invasive distributed patterns allows the declarative and modular definition of evolutions of real-world applications that need moderate to high degrees of invasive modifications.
aspect oriented programming: a language for 2-categories. aspect oriented programming (aop) started ten years ago with the remark that modularization of so-called crosscutting functionalities is a fundamental problem for the engineering of large-scale applications. originating at xerox parc, this observation has sparked the development of a new style of programming featured that is gradually gaining traction, as it is the case for the related concept of code injection, in the guise of frameworks such as swing and google guice. however, aop lacks theoretical foundations to clarify this new idea. this paper proposes to put a bridge between aop and the notion of 2-category to enhance the conceptual understanding of aop. starting from the connection between the $\lambda$-calculus and the theory of categories, we propose to see an aspect as a morphism between morphisms---that is as a program that transforms the execution of a program. to make this connection precise, we develop an advised lambda-calculus that provides an internal language for 2-categories and show how it can be used as a base for the definition of the weaving mechanism of a realistic functional aop language, called minaml. finally, we advocate for a formalization of more complex aop languages (eg. with references or exceptions) using the notion of enriched lawvere theories.
high $p_{t}$ non-photonic electron production in $p$+$p$ collisions at $\sqrt{s}$ = 200 gev. we present the measurement of non-photonic electron production at high transverse momentum ($p_t &gt; $ 2.5 gev/$c$) in $p$+$p$ collisions at $\sqrt{s}$ = 200 gev using data recorded during 2005 and 2008 by the star experiment at the relativistic heavy ion collider (rhic). the measured cross-sections from the two runs are consistent with each other despite a large difference in photonic background levels due to different detector configurations. we compare the measured non-photonic electron cross-sections with previously published rhic data and pqcd calculations. using the relative contributions of b and d mesons to non-photonic electrons, we determine the integrated cross sections of electrons ($\frac{e^++e^-}{2}$) at 3 gev/$c &lt; p_t &lt;~$10 gev/$c$ from bottom and charm meson decays to be ${d\sigma_{(b\to e)+(b\to d \to e)} \over dy_e}|_{y_e=0}$ = 4.0$\pm0.5$({\rm stat.})$\pm1.1$({\rm syst.}) nb and ${d\sigma_{d\to e} \over dy_e}|_{y_e=0}$ = 6.2$\pm0.7$({\rm stat.})$\pm1.5$({\rm syst.}) nb, respectively.
studies of di-jet survival and surface emission bias in au+au collisions via angular correlations with respect to back-to-back leading hadrons. we report first results from an analysis based on a new multi-hadron correlation technique, exploring jet-medium interactions and di-jet surface emission bias at rhic. pairs of back-to-back high transverse momentum hadrons are used for triggers to study associated hadron distributions. in contrast with two- and three-particle correlations with a single trigger with similar kinematic selections, the associated hadron distribution of both trigger sides reveals no modification in either relative pseudo-rapidity or relative azimuthal angle from d+au to central au+au collisions. we determine associated hadron yields and spectra as well as production rates for such correlated back-to-back triggers to gain additional insights on medium properties.
enhanced stiffness modeling of manipulators with passive joints. the paper presents a methodology to enhance the stiffness analysis of serial and parallel manipulators with passive joints. it directly takes into account the loading influence on the manipulator configuration and, consequently, on its jacobians and hessians. the main contributions of this paper are the introduction of a non-linear stiffness model for the manipulators with passive joints, a relevant numerical technique for its linearization and computing of the cartesian stiffness matrix which allows rank-deficiency. within the developed technique, the manipulator elements are presented as pseudo-rigid bodies separated by multidimensional virtual springs and perfect passive joints. simulation examples are presented that deal with parallel manipulators of the ortholide family and demonstrate the ability of the developed methodology to describe non-linear behavior of the manipulator structure such as a sudden change of the elastic instability properties (buckling).
modelling and simulation of a two wheeled vehicle with suspensions by using robotic formalism. models, simulators and control strategies are required tools for the conception of secure and comfortable vehicles. the aim of this paper is to present an efficient way to develop models for dynamic vehicle, focusing on a two wheeled vehicles whose body involves six degrees of freedom. the resulting model is sufficiently generic to perform simulation of realistic cornering and accelerating behavior in various situations. it may be used in the context of motorcycle modeling, but also in various situations (e.g. for control application) as simplified model for 3 or 4 wheeled (tilting) cars. the approach is based on considering the vehicle as a multi-body poly-articulated system and the modeling is carried out using the robotics formalism based on the modified denavit-hartenberg geometric description. in that way, the dynamic model is easy to implement and the system can be used for control applications.
melo 2011 - 1st workshop on model-driven engineering, logic and optimization. the main goal of this workshop is to bring together two different communities: the model-driven engineering (mde) community and the logic programming community, to explore how each community can benefit from the techniques of the other. are both communities friends or foes?.
observation of the antimatter helium-4 nucleus. high-energy nuclear collisions create an energy density similar to that of the universe microseconds after the big bang, and in both cases, matter and antimatter are formed with comparable abundance. however, the relatively short-lived expansion in nuclear collisions allows antimatter to decouple quickly from matter, and avoid annihilation. thus, a high energy accelerator of heavy nuclei is an efficient means of producing and studying antimatter. the antimatter helium-4 nucleus ($^4\bar{he}$), also known as the anti-{\alpha} ($\bar{\alpha}$), consists of two antiprotons and two antineutrons (baryon number b=-4). it has not been observed previously, although the {\alpha} particle was identified a century ago by rutherford and is present in cosmic radiation at the 10% level. antimatter nuclei with b &lt; -1 have been observed only as rare products of interactions at particle accelerators, where the rate of antinucleus production in high-energy collisions decreases by about 1000 with each additional antinucleon. we present the observation of the antimatter helium-4 nucleus, the heaviest observed antinucleus. in total 18 $^4\bar{he}$ counts were detected at the star experiment at rhic in 10$^9$ recorded au+au collisions at center-of-mass energies of 200 gev and 62 gev per nucleon-nucleon pair. the yield is consistent with expectations from thermodynamic and coalescent nucleosynthesis models, which has implications beyond nuclear physics.
an mde-based approach for solving configuration problems: an application to the eclipse platform. most of us have experienced configuration issues when installing new software applications. finding the right configuration is often a challenging task since we need to deal with many dependencies between plug-ins, components, libraries, packages, etc; sometimes even regarding specific versions of the involved artefacts. right now, most configuration engines are adhoc tools designed for specific configuration scenarios. this makes their reuse in different contexts very difficult. in this paper we report on our experience in following a mde-based approach to solve configuration problems. in our approach, the configuration problem is represented as a model that abstracts all irrelevant technological details and facilitates the use of generic (constraint) solvers to find optimal solutions. this approach has been applied by an industrial partner to the management of plug-ins in the eclipse framework, a big issue for all the technology providers that distribute eclipse-based tools.
on a vector moment problem arising in the analysis of neutral type systems. the solvability of some new vector moment problem related with the exact controllability of neutral type systems is investigated. condition of equivallence of this problem with the controllability of some system is analyzed. the time of solvability is precised.
on non-exponential stability of delay systems of neutral type with non-singular neutral term. condition of non exponential stability are given in the general case of neutral type with non singular matrix in the neutral term.
a multi-criteria large neighborhood search for the transportation of disabled people. this article addresses the problem of optimizing the transportation of disabled persons from home to specialized centers or schools. it is modeled as a dial a ride problem (darp), where several people share the same destination. particular emphasis is placed on the objective function in order to consider several potentially conflicting interests. we propose a multi-criteria model from multi-attribute utility theory based on the choquet integral. the darp is then solved with an adaptive large neighborhood search (alns) algorithm. this method includes classical destroy and repair heuristics as well as new operators exploiting the common delivery nodes aspect, as well as criterion-specic operators. the algorithm is evaluated on a set of 14 real life instances with up to 200 requests and 51 destination points.
isospin effects in the disappearance of flow as a function of colliding geometry. we study the effect of isospin degree of freedom on the balance energy (e$_{bal}$) as well as its mass dependence throughout the mass range 48-270 for two sets of isobaric systems with n/z = 1 and 1.4 at different colliding geometries ranging from central to peripheral ones. our findings reveal the dominance of coulomb repulsion in isospin effects on e$_{bal}$ as well as its mass dependence throughout the range of the colliding geometry. our results also indicate that the effect of symmetry energy and nucleon-nucleon cross section on e$_{bal}$ is uniform throughout the mass range and throughout the colliding geometry. we also present the counter balancing of nucleon-nucleon collisions and mean field by reducing the coulomb and the counter balancing of coulomb and mean filed by removing the nucleon-nucleon collisions.
the g0 experiment: apparatus for parity-violating electron scattering measurements at forward and backward angles. in the g0 experiment, performed at jefferson lab, the parity-violating elastic scattering of electrons from protons and quasi-elastic scattering from deuterons is measured in order to determine the neutral weak currents of the nucleon. asymmetries as small as 1 part per million in the scattering of a polarized electron beam are determined using a dedicated apparatus. it consists of specialized beam-monitoring and control systems, a cryogenic hydrogen (or deuterium) target, and a superconducting, toroidal magnetic spectrometer equipped with plastic scintillation and aerogel cerenkov detectors, as well as fast readout electronics for the measurement of individual events. the overall design and performance of this experimental system is discussed.
generating operation specifications from uml class diagrams: a model transformation approach. one of the more tedious and complex tasks during the specification of conceptual schemas (css) is modeling the operations that define the system behavior. this paper aims to simplify this task by providing a method that automatically generates a set of basic operations that complement the static aspects of the cs and suffice to perform all typical life-cycle create/update/delete changes on the population of the elements of the cs. our method guarantees that the generated operations are executable, i.e. their executions produce a consistent state wrt the most typical structural constraints that can be defined in css (e.g. multiplicity constraints). in particular, our method takes as input a cs expressed as a unified modeling language (uml) class diagram (optionally defined using a profile to enrich the specification of associations) and generates an extended version of the cs that includes all necessary operations to start operating the system. if desired, these basic operations can be later used as building blocks for creating more complex ones. we show the formalization and implementation of our method by means of model-to-model transformations. our approach is particularly relevant in the context of model driven development approaches.
likelihood approach to the first dark matter results from xenon100. many experiments that aim at the direct detection of dark matter are able to distinguish a dominant background from the expected feeble signals, based on some measured discrimination parameter. we develop a statistical model for such experiments using the profile likelihood ratio as a test statistic in a frequentist approach. we take data from calibrations as control measurements for signal and background, and the method allows the inclusion of data from monte carlo simulations. systematic detector uncertainties, such as uncertainties in the energy scale, as well as astrophysical uncertainties, are included in the model. the statistical model can be used to either set an exclusion limit or to make a discovery claim, and the results are derived with a proper treatment of statistical and systematic uncertainties. we apply the model to the first data release of the xenon100 experiment, which allows to extract additional information from the data, and place stronger limits on the spin-independent elastic wimp-nucleon scattering cross-section. in particular, we derive a single limit, including all relevant systematic uncertainties, with a minimum of 2.4x10^-44 cm^2 for wimps with a mass of 50 gev/c^2.
a tabu search algorithm for the dial-a-ride problem with transfers. the dial-a-ride problem (darp) consists in determining and scheduling routes serviced by a set of vehicles in order to satisfy transportation requests from pickup points to delivery points. this paper introduces a variant of the darp where the client can be transferred from one vehicle to another on specic points called \transfer points". solving this variant of the darp yields new algorithmic diculties. in this paper, we propose a tabu search algorithm for the dial-a-ride problem with transfers (darpt) with a special emphasis on checking whether a modication of the current solution is feasible or not. the method is evaluated over generated and real instances.
a dc programming heuristic applied to the logistics network design problem. this paper proposes a new heuristic method for the logistics network design and planning problem based on linear relaxation and dc (difference of convex functions) programming. we consider a multi-period, multi-echelon, multi-commodity and multi-product problem defined as a large scale mixed integer linear programming (milp) model. the method is experimented on data sets of various size. the numerical results validate the efficiency of the heuristic for instances with up to several dozens facilities, 18 products and 270 retailers.
portolan: a model-driven cartography framework. processing large amounts of data to extract useful information is an essential task within companies. to help in this task, visualization techniques have been commonly used due to their capacity to present data in synthesized views, easier to understand and manage. however, achieving the right visualization display for a data set is a complex cartography process that involves several transformation steps to adapt the (domain) data to the (visualization) data format expected by visualization tools. to maximize the benefits of visualization we propose portolan, a generic model-driven cartography framework that facilitates the discovery of the data to visualize, the specification of view definitions for that data and the transformations to bridge the gap with the visualization tools. our approach has been implemented on top of the eclipse emf modeling framework and validated on three different use cases.
particle swarm optimization for the design of h∞ static output feedbacks. the design of h∞ reduced order controllers is known to be a non convex optimization problem for which no generic solution exists. in this paper, the use of particle swarm optimization (pso) for the computation of h∞ static output feedbacks is investigated. two approaches are tested. in a first part, a probabilistic-type pso algorithm is defined for the computation of discrete sets of stabilizing static output feedback controllers. this method relies on a technique for random sample generation in a given domain. it is therefore used for computing a suboptimal h∞ static output feedback solution. in a second part, the initial optimization problem is solved by pso, the decision variables being the feedback gains. results are compared with standard reduced order problem solvers using the compleib benchmark examples and appear to be much than satisfactory, proving the great potential of pso techniques.
a lateral control strategy for narrow tilting commuter vehicle based on the perceived lateral acceleration. narrow tilting commuter vehicles are expected to be the new generation of city cars, considering their practical dimensions and lower energy consumption. but their dimensions increase their tendency to overturn during cornering, facing lateral acceleration. this problem can be solved by tilting the vehicle in a way that reduces the perceived lateral acceleration at the cabin during cornering, as it is seen on two wheeled vehicles. in literature so far, the corresponding tilting angle is computed, and the control strategy aims at reaching this desired angle. this paper, presents another control approach, achieving a direct control of the perceived acceleration by appropriately tilting the vehicle. the strategy proposed is interesting in that it is both simple to implement, able to take advantage over accelerometer and gyroscope measures, and valid even on roads with non zero banking angle. as the lateral speed is not measured in practice, the paper proposes, for comparison purpose, different (original) ways to get it without deteriorating robustness. finally, the solution proposed reduces to a robust state feedback controller exploiting all the available measures.
a two level strategy for combustion engine starter robust control in the context of a hybrid power train. the main difficulty in controlling combustion engine start-up comes from its changing behaviour from load to motor. in the context of hybrid power trains considered here, the electric motor used for start-up is more powerful than a traditional starter. in order to get a robust control of the crankshaft speed, we propose to take inspiration from redundant control as formulated in the aeronautic domain (härkegard, 2003), (härkegard and glad, 2005), (buffington et al., 1996), (maciejowski, 1997), by considering that the engine and the electric motor are redundant torque providers. despite that the engine is the primary torque provider, it will be systematically seen as the failing actuator during the start-up stage. therefore, the electric motor has to compensate its deficiency.
towards a robust model for distributed aspects. in this paper, we present some of the problems we have found in distributed aspect models and introduce a set of criteria that we consider necessary for a robust distributed aspect system. we outline of a first version of model based on aspects and actors capable of meeting these criteria.
invasive composition for the evolution of a health information system. in this paper we show that some of the evolution tasks in openmrs, a health information system, may require the invasive modification of interfaces and implementations in order to offer an appropriate modularization. we introduce a new composition framework in java that supports the definition of expressive pattern-based invasive compositions. fur thermore, we show that the composition framework allows us to concisely define an evolution scenario of openmrs that supports the consolidation of patient data from differ- ent remote instances.
aspectizing java access control. it is inevitable that some concerns crosscut a sizeable application, resulting in code scattering and tangling. this issue is particularly severe for security-related concerns: it is difficult to be confident about the security of an application when the implementation of its security-related concerns is scattered all over the code and tangled with other concerns, making global reasoning about security precarious. in this study, we consider the case of access control in java, which turns out to be a crosscutting concern with a non-modular implementation based on runtime stack inspection. we describe the process of modularizing access control in java by means of aspect-oriented programming (aop). we first show a solution based on aspectj, the most popular aspect-oriented extension to java, that must rely on a separate automata infrastructure. we then put forward a novel solution via dynamic deployment of aspects and scoping strategies. both solutions, apart from providing a modular specification of access control, make it possible to easily express other useful policies such as the chinese wall policy. however, relying on expressive scope control results in a compact implementation, which, at the same time, permits the straightforward expression of even more interesting policies.
dynamic consolidation of highly available web applications. datacenters provide an economical and practical solution for hosting large scale n-tier web applications. when scalability and high availability are required, each tier can be implemented as multiple replicas, which can absorb extra load and avoid a single point of failure. realizing these benefits in practice, however, requires that replicas be assigned to datacenter nodes according to certain placement constraints. to provide the required quality of service to all of the hosted applications, the datacenter must consider of all of their specific constraints. when the constraints are not satisfied, the datacenter must quickly adjust the mappings of applications to nodes, taking all of the applications' constraints into account. this paper presents plasma, an approach for hosting highly available web applications, based on dynamic consolidation of virtual machines and placement constraint descriptions. the placement constraint descriptions allow the data- center administrator to describe the datacenter infrastructure and each appli- cation administrator to describe his requirements on the vm placement. based on the descriptions, plasma continuously optimizes the placement of the vms in order to provide the required quality of service. experiments on simulated configurations show that the plasma reconfiguration algorithm is able to man- age a datacenter with up to 2000 nodes running 4000 vms with 800 placement constraints. real experiments on a small cluster of 8 working nodes running 3 instances of the rubis benchmarks with a total of 21 vms show that con- tinuous consolidation is able to reach 85% of the load of a 21 working nodes cluster.
simultaneous hinf control for continuous-time descriptor systems. this study addresses the simultaneous hinf control problem for continuous-time descriptor systems, namely, a single controller is sought to stabilise a collection of descriptor systems with a prescribed hinf norm. first, this problem is transformed equivalently to the strong hinf stabilisation problem of an augmented system. then, a sufficient condition of the existence of strongly admissible hinf controllers is proposed in terms of strict linear matrix inequalities. finally, numerical examples are presented to show the effectiveness of the proposed method.
identified charged hadron production in p+p collisions at sqrt(s)=200 and 62.4 gev. transverse momentum distributions and yields for $\pi^{\pm}$, $k^{\pm}$, $p$ and $\bar{p}$ in $p+p$ collisions at $\sqrt{s}$=200 and 62.4 gev at midrapidity are measured by the phenix experiment at the relativistic heavy ion collider (rhic). these data provide important baseline spectra for comparisons with identified particle spectra in heavy ion collisions at rhic. we present the inverse slope parameter $t_{\rm inv}$, mean transverse momentum $$ and yield per unit rapidity $dn/dy$ at each energy, and compare them to other measurements at different $\sqrt{s}$ in $p+p$ and $p+\bar{p}$ collisions. we also present the scaling properties such as $m_t$ scaling, $x_t$ scaling on the $p_t$ spectra between different energies. to discuss the mechanism of the particle production in $p+p$ collisions, the measured spectra are compared to next-to-leading-order or next-to-leading-logarithmic perturbative quantum chromodynamics calculations.
direct photons at low transverse momentum -- a qgp signal in pp collisions at lhc. we predict that direct photon production in pp collisions at 7~tev will get at least 10 times enhanced compared to the next to leading order pqcd predictions, at low transverse momentum ($\pt$ $\lesssim$ 10~gev/c), due to the thermal photon emissions from a quark gluon plasma (qgp) formed in high multiplicity events. thus the enhancement of direct photon production at low $\pt$ can be a qgp signal in pp collisions.
the influence of bulk evolution models on heavy-quark phenomenology. we study the impact of different quark-gluon plasma expansion scenarios in heavy-ion collisions on spectra and elliptic flow of heavy quarks. for identical heavy-quark transport coefficients relativistic langevin simulations with different expansion scenarios can lead to appreciable variations in the calculated suppression and elliptic flow of the heavy-quark spectra, by up to a factor of two. a cross comparison with two sets of transport coefficients supports these findings, illustrating the importance of realistic expansion models for quantitative evaluations of heavy-quark observables in heavy-ion collisions. it also turns out that differences in freeze-out prescriptions and langevin realizations play a significant role in these variations. light-quark observables are essential in reducing the uncertainties associated with the bulk-matter evolution, even though uncertainties due to the freeze-out prescription persist.
generalised modal realisation as a practical and efficient tool for fwl implementation. finite word length (fwl) effects have been a critical issue in digital filter implementation for almost four decades. although some optimisations may be attempted to get an optimal realisation with regards to a particular effect, for instance the parametric sensitivity or the round-off noise gain, the purpose of this article is to propose an effective one, i.e. taking into account all the aspects. based on the specialised implicit form, a new effective and sparse structure, named rho-modal realisation, is proposed. this realisation meets simultaneously accuracy (low sensitivity, round-off noise gain and overflow risk), few and flexible computational efforts with a good readability (thanks to sparsity) and simplicity (no tricky optimisation is required to obtain it) as well. two numerical examples are included to illustrate the rho-modal realisation's interest.
the pierre auger observatory scaler mode for the study of solar activity modulation of galactic cosmic rays. since data-taking began in january 2004, the pierre auger observatory has been recording the count rates of low energy secondary cosmic ray particles for the self-calibration of the ground detectors of its surface detector array. after correcting for atmospheric effects, modulations of galactic cosmic rays due to solar activity and transient events are observed. temporal variations related with the activity of the heliosphere can be determined with high accuracy due to the high total count rates. in this study, the available data are presented together with an analysis focused on the observation of forbush decreases, where a strong correlation with neutron monitor data is found.
constraints from the first lhc data on hadronic event generators for ultra-high energy cosmic-ray physics. the determination of the primary energy and mass of ultra-high-energy cosmic-rays (uhecr) generating extensive air-showers in the earth's atmosphere, relies on the detailed modeling of hadronic multiparticle production at center-of-mass (c.m.) collision energies up to two orders of magnitude higher than those studied at particle colliders. the first large hadron collider (lhc) data have extended by more than a factor of three the c.m. energies in which we have direct proton-proton measurements available to compare to hadronic models. in this work we compare lhc results on inclusive particle production at energies sqrt(s) = 0.9, 2.36, and 7 tev to predictions of various hadronic monte carlo (mc) models used commonly in cosmic-ray (cr) physics (qgsjet, epos and sibyll). as a benchmark with a standard collider physics model we also show pythia (and phojet) predictions with various parameter settings. while reasonable overall agreement is found for some of the mc, none of them reproduces consistently the sqrt(s) evolution of all the observables. we discuss implications of the new lhc data for the description of cosmic-ray interactions at the highest energies.
views, program transformations, and the evolutivity problem in a functional language. we report on an experience to support multiple views of programs to solve the tyranny of the dominant decomposition in a functional setting. we consider two possible architectures in haskell for the classical example of the expression problem. we show how the haskell refactorer can be used to transform one view into the other, and the other way back. that transformation is automated and we discuss how the haskell refactorer has been adapted to be able to support this automated transformation. finally, we compare our implementation of views with some of the literature.
advanced functionality for radio analysis in the offline software framework of the pierre auger observatory. the advent of the auger engineering radio array (aera) necessitates the development of a powerful framework for the analysis of radio measurements of cosmic ray air showers. as aera performs "radio-hybrid" measurements of air shower radio emission in coincidence with the surface particle detectors and fluorescence telescopes of the pierre auger observatory, the radio analysis functionality had to be incorporated in the existing hybrid analysis solutions for fluoresence and surface detector data. this goal has been achieved in a natural way by extending the existing auger offline software framework with radio functionality. in this article, we lay out the design, highlights and features of the radio extension implemented in the auger offline framework. its functionality has achieved a high degree of sophistication and offers advanced features such as vectorial reconstruction of the electric field, advanced signal processing algorithms, a transparent and efficient handling of ffts, a very detailed simulation of detector effects, and the read-in of multiple data formats including data from various radio simulation codes. the source code of this radio functionality can be made available to interested parties on request.
ratio of bulk to shear viscosity in a quasigluon plasma: from weak to strong coupling. the ratio of bulk to shear viscosity is expected to exhibit a different behaviour in weakly and in strongly coupled systems. this can be expressed by the dependence of the ratio on the squared sound velocity. in the high temperature qcd plasma at small running coupling, the viscosity ratio is uniquely determined by a quadratic dependence on the conformality measure, whereas in certain strongly coupled and nearly conformal theories this dependence is linear. employing an effective kinetic theory of quasiparticle excitations with medium-modified dispersion relation, we analyze the ratio of bulk to shear viscosity of the gluon plasma. we show that in this approach the viscosity ratio comprises both dependencies found by means of weak coupling perturbative and strong coupling holographic techniques.
improved predictions of reactor antineutrino spectra. we report new calculations of reactor antineutrino spectra including the latest information from nuclear databases and a detailed error budget. the first part of this work is the so-called ab initio approach where the total antineutrino spectrum is built from the sum of all beta-branches of all fission products predicted by an evolution code. systematic effects and missing information in nuclear databases lead to final relative uncertainties in the 10 to 20% range. a prediction of the antineutrino spectrum associated with the fission of 238u is given based on this ab initio method. for the dominant isotopes 235u and 239pu, we developed a more accurate approach combining information from nuclear databases and reference electron spectra associated with the fission of 235u, 239pu and 241pu, measured at ill in the 80's. we show how the anchor point of the measured total beta-spectra can be used to suppress the uncertainty in nuclear databases while taking advantage of all the information they contain. we provide new reference antineutrino spectra for 235u, 239pu and 241pu isotopes in the 2-8 mev range. while the shapes of the spectra and their uncertainties are comparable to that of the previous analysis of the ill data, the normalization is shifted by about +3% on average. in the perspective of the re-analysis of past experiments and direct use of these results by upcoming oscillation experiments, we discuss the various sources of errors and their correlations as well as the corrections induced by off equilibrium effects.
production of pions, kaons and protons in pp collisions at sqrt(s)= 900 gev with alice at the lhc. the production of pi+, pi-, k+, k-, p, and pbar at mid-rapidity has been measured in proton-proton collisions at sqrt(s) = 900 gev with the alice detector. particle identification is performed using the specific energy loss in the inner tracking silicon detector and the time projection chamber. in addition, time-of-flight information is used to identify hadrons at higher momenta. finally, the distinctive kink topology of the weak decay of charged kaons is used for an alternative measurement of the kaon transverse momentum (pt) spectra. since these various particle identification tools give the best separation capabilities over different momentum ranges, the results are combined to extract spectra from pt = 100 mev/c to 2.5 gev/c. the measured spectra are further compared with qcd-inspired models which yield a poor description. the total yields and the mean pt are compared with previous measurements, and the trends as a function of collision energy are discussed.
centrality dependence of the charged-particle multiplicity density at midrapidity in pb-pb collisions at √snn=2.76  tev. null
femtoscopy of pp collisions at sqrt{s}=0.9 and 7 tev at the lhc with two-pion bose-einstein correlations. we report on the high statistics two-pion correlation functions from pp collisions at sqrt{s}=0.9 tev and sqrt{s}=7 tev, measured by the alice experiment at the large hadron collider. the correlation functions as well as the extracted source radii scale with event multiplicity and pair momentum. when analyzed in the same multiplicity and pair transverse momentum range, the correlation is similar at the two collision energies. a three-dimensional femtoscopic analysis shows an increase of the emission zone with increasing event multiplicity as well as decreasing homogeneity lengths with increasing transverse momentum. the latter trend gets more pronounced as multiplicity increases. this suggests the development of space-momentum correlations, at least for collisions producing a high multiplicity of particles. we consider these trends in the context of previous femtoscopic studies in high-energy hadron and heavy-ion collisions, and discuss possible underlying physics mechanisms. detailed analysis of the correlation reveals an exponential shape in the outward and longitudinal directions, while the sideward remains a gaussian. this is interpreted as a result of a significant contribution of strongly decaying resonances to the emission region shape. significant non-femtoscopic correlations are observed, and are argued to be the consequence of "mini-jet"-like structures extending to low p_t. they are well reproduced by the monte-carlo generators and seen also in pi^+ pi^- correlations.
two-pion bose-einstein correlations in central pbpb collisions at sqrt(s_nn) = 2.76 tev. the first measurement of two-pion bose--einstein correlations in central pbpb collisions at sqrt(s_nn) = 2.76 tev at the large hadron collider is presented. we observe a growing trend with energy now not only for the longitudinal and the outward but also for the sideward pion source radius. the pion homogeneity volume and the decoupling time are significantly larger than those measured at rhic.
reduced elastodynamic modelling of parallel robots for the computation of their natural frequencies. null
search for first harmonic modulation in the right ascension distribution of cosmic rays detected at the pierre auger observatory. we present the results of searches for dipolar-type anisotropies in different energy ranges above 2.5 × 10^17 ev with the surface detector array of the pierre auger observatory, reporting on both the phase and the amplitude measurements of the first harmonic modulation in the right-ascension distribution. upper limits on the amplitudes are obtained, which provide the most stringent bounds at present, being below 2% at 99% c.l. for eev energies. we also compare our results to those of previous experiments as well as with some theoretical expectations.
strange particle production in proton-proton collisions at $\sqrt{s}$ = 0.9 tev with alice at the lhc. the production of mesons containing strange quarks ($\kzs$, $\phi$) and both singly and doubly strange baryons ($\rmlambda$, $\rmalambda$, and $\xis$) are measured at central rapidity in pp collisions at $\sqrt{s}$ = 0.9 $\tev$ with the alice experiment at the lhc. the results are obtained from the analysis of about 250 k minimum bias events recorded in 2009. measurements of yields (dn/dy) and transverse momentum spectra at central rapidities for inelastic pp collisions are presented. for mesons, we report yields ($&lt; \dndy &gt;$) of $0.184 \pm 0.002 \stat \pm 0.006 \syst$ for $\kzs$ and $0.021 \pm 0.004 \stat \pm 0.003 \syst$ for $\phi$. for baryons, we find $&lt; \dndy &gt; = 0.048 \pm 0.001 \stat \pm 0.004 \syst$ for $\rmlambda$, $0.047 \pm 0.002 \stat \pm 0.005 \syst$ for $\rmalambda$ and $0.0101 \pm 0.0020 \stat \pm 0.0009 \syst$ for $\xis$. the results are also compared with predictions for identified particle spectra from qcd-inspired models and provide a baseline for comparisons with both future pp measurements at higher energies and heavy-ion collisions.
suppression of charged particle production at large transverse momentum in central pb--pb collisions at $\sqrt{s_{_{nn}}} = 2.76$ tev. inclusive transverse momentum spectra of primary charged particles in pb-pb collisions at $\sqrt{s_{_{nn}}}$ = 2.76 tev have been measured by the alice collaboration at the lhc. the data are presented for central and peripheral collisions, corresponding to 0-5% and 70-80% of the hadronic pb-pb cross section. the measured charged particle spectra in $|\eta|&lt;0.8$ and $0.3 &lt; p_t &lt; 20$ gev/$c$ are compared to the expectation in pp collisions at the same $\sqrt{s_{_{nn}}}$, scaled by the number of underlying nucleon-nucleon collisions. the comparison is expressed in terms of the nuclear modification factor $r_{aa}$. the result indicates only weak medium effects ($r_{aa} \approx $ 0.7) in peripheral collisions. in central collisions, $r_{aa}$ reaches a minimum of about 0.14 at $p_t=6$-7gev/$c$ and increases significantly at larger $p_t$. the measured suppression of high-$p_t$ particles is stronger than that observed at lower collision energies, indicating that a very dense medium is formed in central pb-pb collisions at the lhc.
operating systems and virtualization frameworks: from local to distributed similarities. virtualization technologies radically changed the way in which distributed architectures are exploited. with the contribution of vm capabilities and with the emergence of iaas platforms, more and more frameworks tend to manage vms across distributed architectures like operating systems handle processes on a single node. taking into account that most of these frameworks follow a centralized model – where roughly one node is in charge of the management of vms – and considering the growing size of infrastructures in terms of nodes and vms, new proposals relying on more autonomic and decentralized approaches should be submitted. designing and implementing such models is a tedious and complex task. however, as well as research studies on oses and hypervisors are complementary at the node level, we advocate that virtualization frameworks can benefit from lessons learnt from distributed operating system proposals. in this article, we motivate such a position by analyzing similarities between oses and virtualization frameworks. more precisely, we focus on the management of processes and vms, first at the node level and then on a cluster scale. from our point of view, such investigations can guide the community to design and implement new proposals in a more autonomic and distributed way.
prompt and non-prompt j/psi production in pp collisions at sqrt(s) = 7 tev. the production of j/psi mesons is studied in pp collisions at sqrt(s)=7 tev with the cms experiment at the lhc. the measurement is based on a dimuon sample corresponding to an integrated luminosity of 314 inverse nanobarns. the j/psi differential cross section is determined, as a function of the j/psi transverse momentum, in three rapidity ranges. a fit to the decay length distribution is used to separate the prompt from the non-prompt (b hadron to j/psi) component. integrated over j/psi transverse momentum from 6.5 to 30 gev/c and over rapidity in the range |y| &lt; 2.4, the measured cross sections, times the dimuon decay branching fraction, are 70.9 \pm 2.1 (stat.) \pm 3.0 (syst.) \pm 7.8(luminosity) nb for prompt j/psi mesons assuming unpolarized production and 26.0 \pm 1.4 (stat.) \pm 1.6 (syst.) \pm 2.9 (luminosity) nb for j/psi mesons from b-hadron decays.
cold nuclear matter effects on j/psi yields as a function of rapidity and nuclear geometry in deuteron-gold collisions at sqrt(s_nn) = 200 gev. we present measurements of j/psi yields in d+au collisions at sqrt(s_nn) = 200 gev recorded by the phenix experiment and compare with yields in p+p collisions at the same energy per nucleon-nucleon collision. the measurements cover a large kinematic range in j/psi rapidity (-2.2 &lt; y &lt; 2.4) with high statistical precision and are compared with two theoretical models: one with nuclear shadowing combined with final state breakup and one with coherent gluon saturation effects. to remove model dependent systematic uncertainties we also compare the data to a simple geometric model. we find that calculations where the nuclear modification is linear or exponential in the density weighted longitudinal thickness are difficult to reconcile with the forward rapidity data.
the exposure of the hybrid detector of the pierre auger observatory. the pierre auger observatory is a detector for ultra-high energy cosmic rays. it consists of a surface array to measure secondary particles at ground level and a fluorescence detector to measure the development of air showers in the atmosphere above the array. the "hybrid" detection mode combines the information from the two subsystems. we describe the determination of the hybrid exposure for events observed by the fluorescence telescopes in coincidence with at least one water-cherenkov detector of the surface array. a detailed knowledge of the time dependence of the detection operations is crucial for an accurate evaluation of the exposure. we discuss the relevance of monitoring data collected during operations, such as the status of the fluorescence detector, background light and atmospheric conditions, that are used in both simulation and reconstruction.
suppression of away-side jet fragments with respect to the reaction plane in au+au collisions at sqrt(s_nn) = 200 gev. pair correlations between large transverse momentum neutral pion triggers (p_t=4--7 gev/c) and charged hadron partners (p_t=3--7 gev/c) in central (0--20%) and midcentral (20--60%) au+au collisions are presented as a function of trigger orientation with respect to the reaction plane. the particles are at larger momentum than where jet shape modifications have been observed, and the correlations are sensitive to the energy loss of partons traveling through hot dense matter. an out-of-plane trigger particle produces only 26+/-20% of the away-side pairs that are observed opposite of an in-plane trigger particle. in contrast, near-side jet fragments are consistent with no suppression or dependence on trigger orientation with respect to the reaction plane. these observations are qualitatively consistent with a picture of little near-side parton energy loss either due to surface bias or fluctuations and increased away-side parton energy loss due to a long path through the medium. the away-side suppression as a function of reaction-plane angle is shown to be sensitive to both the energy loss mechanism in and the space-time evolution of heavy-ion collisions.
event structure and double helicity asymmetry in jet production from polarized p+p collisions at sqrt(s) = 200 gev. we report on event structure and double helicity asymmetry ($a_ll$) of jet production in longitudinally polarized p+p collisions at $\sqrt{s}$=200 gev. photons and charged particles were measured at midrapidity $|\eta| &lt; 0.35$ with the requirement of a high-momentum ($&gt;2$ gev/$c$) photon in each event. measured event structure is compared with {\sc pythia} and {\sc geant} simulations. the shape of jets and the underlying event were well reproduced at this collision energy. for the measurement of jet $a_{ll}$, photons and charged particles were clustered with a seed-cone algorithm to obtain the cluster $p_t$ sum ($p_t^{\rm reco}$). the effect of detector response and the underlying events on $p_t^{\rm reco}$ was evaluated with the simulation. the production rate of reconstructed jets is satisfactorily reproduced with the nlo pqcd jet production cross section. for $4 &lt; p_t^{\rm reco} &lt; 12$ gev/$c$ with an average beam polarization of $&lt; p &gt; = 49%$ we measured $a_{ll} = -0.0014 \pm 0.0037^{\rm stat}$ at the lowest $p_t^{\rm reco}$ bin (4-5 gev/$c$) and $-0.0181 \pm 0.0282^{\rm stat}$ at the highest $p_t^{\rm reco}$ bin (10-12 gev/$c$) with a beam polarization scale error of 9.4% and a $\pt$ scale error of 10%. jets in the measured $p_t^{\rm reco}$ range arise primarily from hard-scattered gluons with momentum fraction $0.02 &lt; x &lt; 0.3$ according to {\sc pythia}. the measured $a_{ll}$ is compared with predictions that assume various $\delta g(x)$ distributions based on the grsv parameterization. the present result imposes the limit $-1.1 &lt; \int_{0.02}^{0.3}dx \delta g(x, \mu^2 = 1 {\rm gev}^2) &lt; 0.4$ at 95% confidence level or $\int_{0.02}^{0.3}dx \delta g(x, \mu^2 = 1 {\rm gev}^2) &lt; 0.5$ at 99% confidence level.
measurement of the parity-violating longitudinal single-spin asymmetry for $w^{\pm}$ boson production in polarized proton-proton collisions at $\sqrt{s} = 500 $gev. we report the first measurement of the parity violating single-spin asymmetries for midrapidity decay positrons and electrons from $w^{+}$ and $w^{-}$ boson production in longitudinally polarized proton-proton collisions at $\sqrt{s}=500 $gev by the star experiment at rhic. the measured asymmetries, $a^{w^+}_{l}=-0.27\pm 0.10\;({\rm stat.})\pm 0.02\;({\rm syst.}) \pm 0.03\;({\rm norm.})$ and $a^{w^-}_{l}=0.14\pm 0.19\;({\rm stat.})\pm 0.02 \;({\rm syst.})\pm 0.01\;({\rm norm.})$, are consistent with theory predictions, which are large and of opposite sign. these predictions are based on polarized quark and antiquark distribution functions constrained by polarized dis measurements.
cross section and parity violating spin asymmetries of w^+/- boson production in polarized p+p collisions at sqrt(s)=500 gev. large parity violating longitudinal single spin asymmetries a^{e^-}_l= -0.86^{+0.14}_{-0.30} and a^{e^+}_l= 0.88^{+0.12}_{-0.71} are observed for inclusive high transverse momentum electrons and positrons in polarized pp collisions at a center of mass energy of \sqrt{s}=500\ gev with the phenix detector at rhic. these e^{+/-} come mainly from the decay of w^{+/-} and z^0 bosons, and the asymmetries directly demonstrate parity violation in the couplings of the w^{\pm} to the light quarks. the observed electron and positron yields were used to estimate w^\pm boson production cross sections equal to \sigma(pp --&gt; w^+ x) \times br(w^ --&gt; \nu_e)= 144.1+/-21.2(stat)^{+3.4}_{-10.3}(syst) +/- 15%(norm) pb, and \sigma(pp --&gt; w^{-}x) \times br(w^--&gt;e^-\bar{\nu_e}) = 31.7+/-12.1(stat)^{+10.1}_{-8.2}(syst)+/-15%(norm) pb.
k*0 production in cu+cu and au+au collisions at \sqrt{s_nn} = 62.4 gev and 200 gev. we report on k*0 production at mid-rapidity in au+au and cu+cu collisions at \sqrt{s_{nn}} = 62.4 and 200 gev collected by the solenoid tracker at rhic (star) detector. the k*0 is reconstructed via the hadronic decays k*0 \to k+ pi- and \bar{k*0} \to k-pi+. transverse momentum, pt, spectra are measured over a range of pt extending from 0.2 gev/c to 5 gev/c. the center of mass energy and system size dependence of the rapidity density, dn/dy, and the average transverse momentum, , are presented. the measured n(k*0)/n(k) and n(\phi)/n(k*0) ratios favor the dominance of re-scattering of decay daughters of k*0 over the hadronic regeneration for the k*0 production. in the intermediate pt region (2.0 &lt; pt &lt; 4.0 gev/c), the elliptic flow parameter, v2, and the nuclear modification factor, rcp, agree with the expectations from the quark coalescence model of particle production.
measurement of neutral mesons in p+p collisions at sqrt(s) = 200 gev and scaling properties of hadron production. the phenix experiment at the relativistic heavy ion collider has measured the invariant differential cross section for production of k^0_s , \omega, \eta prime, and \phi mesons in p + p collisions at = 200 gev. measurements \omega and \phi production in different decay channels give consistent results. new results for the \phi are in agreement with previously published data and extend the measured pt coverage. the spectral shapes of all hadron transverse momentum distributions measured by phenix are well described by a tsallis distribution functional form with only two parameters, n and t, determining the high-pt and characterizing the low-pt regions of the spectra, respectively. the values of these parameters are very similar for all analyzed meson spectra, but with a lower parameter t extracted for protons. the integrated invariant cross sections calculated from the fitted distributions are found to be consistent with existing measurements and with statistical model predictions.
new filtering for the \it cumulative constraint in the context of non-overlapping rectangles. this article describes new filtering methods for the cumulative constraint. the first method introduces the so called longest closed hole and longest open hole problems. for these two problems it first provides bounds and exact methods and then shows how to use them in the context of the non-overlapping constraint. the second method introduces balancing knapsack constraints which relate the total height of the tasks that end at a specific time-point with the total height of the tasks that start at the same time-point. experiments on tight rectangle packing problems show that these methods drastically reduce both the time and the number of backtracks for finding all solutions as well as for finding the first solution. for example, we found without backtracking all solutions to 65 perfect square instances of order 22-25 and sizes ranging from 192×192 to 661×661.
heavy quark production in p+p and energy loss and flow of heavy quarks in au+au collisions at sqrt(s_nn)=200 gev. transverse momentum (p^e_t) spectra of electrons from semileptonic weak decays of heavy flavor mesons in the range of 0.3 &lt; p^e_t &lt; 9.0 gev/c have been measured at mid-rapidity (|eta| &lt; 0.35) by the phenix experiment at the relativistic heavy ion collider in p+p and au+au collisions at sqrt(s_nn)=200 gev. the nuclear modification factor r_aa with respect to p+p collisions indicates substantial energy loss of heavy quarks in the produced medium. in addition, the azimuthal anisotropy parameter v_2 has been measured for 0.3 &lt; p^e_t &lt; 5.0 gev/c in au+au collisions. comparisons of r_aa and v_2 are made to various model calculations.
global propagation of side constraints for solving over-constrained problems. this article deals with the resolution of over-constrained problems using constraint programming, which often imposes to add to the constraint network new side constraints. these side constraints control how the initial constraints of the model should be satisfied or violated, to obtain solutions that have a practical interest. they are specific to each application. in our experiments, we show the superiority of a framework where side constraints are encoded by global constraints on new domain variables, which are directly included into the model. the case-study is a cumulative scheduling problem with over-loads. the objective is to minimize the total amount of over-loads. we augment the cumulative global constraint of the constraint programming solver choco with sweep and task interval violation-based algorithms. we provide a theoretical and experimental comparison of the two main approaches for encoding over-constrained problems with side constraints.
geomagnetic effects observed by the codalema experiment. the codalema experiment is measuring transient radio emissions associated to extended air showers produced by high energy cosmic rays. the experimental setup installed at the nancay radio observatory in france has recently undergone hardware upgrades and an extension of the surfaces covered by both the antenna and the scintillator detector arrays. the experimental data allow to investigate the main features of these radio signals and the underlying electric field production mechanisms. some of the latest experimental results of codalema are presented. they have been analyzed assuming a linear dependence of the electric field with respect to v ^ b. within the codalema observation conditions at nancay, the detection eficiency, the arrival direction distribution and the polarity of the radio signals can be interpreted in terms of a geomagnetic effect. a r&amp;d effort is currently underway to develop the hardware elements for the deployment of a large detector array based on active antennas. the main features of the first prototype of the codalema autonomous station are briefly described.
